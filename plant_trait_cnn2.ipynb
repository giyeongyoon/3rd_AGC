{
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "accelerator": "GPU",
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30627,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/giyeongyoon/3rd_AGC/blob/master/plant_trait_cnn2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install albumentations==1.1.0\n",
        "!pip install agml"
      ],
      "metadata": {
        "id": "P7YSvf_oKlPr",
        "execution": {
          "iopub.status.busy": "2023-12-18T06:31:44.926292Z",
          "iopub.execute_input": "2023-12-18T06:31:44.926662Z",
          "iopub.status.idle": "2023-12-18T06:32:10.533333Z",
          "shell.execute_reply.started": "2023-12-18T06:31:44.926635Z",
          "shell.execute_reply": "2023-12-18T06:32:10.532104Z"
        },
        "trusted": true
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import libraries"
      ],
      "metadata": {
        "id": "XeVhzZvgLCt7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torchvision import models, transforms\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "import albumentations as A\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import time"
      ],
      "metadata": {
        "id": "6BZ4j2lzK6wQ",
        "execution": {
          "iopub.status.busy": "2023-12-18T06:33:54.782802Z",
          "iopub.execute_input": "2023-12-18T06:33:54.783659Z",
          "iopub.status.idle": "2023-12-18T06:34:09.131801Z",
          "shell.execute_reply.started": "2023-12-18T06:33:54.783618Z",
          "shell.execute_reply": "2023-12-18T06:34:09.130906Z"
        },
        "trusted": true
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download 2021 Autonomous Greenhouse Challenge dataset"
      ],
      "metadata": {
        "id": "JhSMln48K_iJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import agml\n",
        "loader = agml.data.AgMLDataLoader('autonomous_greenhouse_regression', dataset_path = './')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "T3-oDY-ZLBdJ",
        "outputId": "0bedae3c-14e3-4299-95d0-ebdc1bb40b2b",
        "execution": {
          "iopub.status.busy": "2023-12-18T06:34:09.133680Z",
          "iopub.execute_input": "2023-12-18T06:34:09.134252Z",
          "iopub.status.idle": "2023-12-18T06:34:58.928141Z",
          "shell.execute_reply.started": "2023-12-18T06:34:09.134223Z",
          "shell.execute_reply": "2023-12-18T06:34:58.926537Z"
        },
        "trusted": true
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading autonomous_greenhouse_regression (size = 887.2 MB): 887226368it [00:09, 88905524.30it/s]                               \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[AgML Download]: Extracting files for autonomous_greenhouse_regression... Done!\n",
            "\n",
            "====================================================================================================\n",
            "You have just downloaded \u001b[1mautonomous_greenhouse_regression\u001b[0m.\n",
            "\n",
            "This dataset is licensed under the \u001b[1mCC BY-SA 4.0\u001b[0m license.\n",
            "To learn more, visit: https://creativecommons.org/licenses/by-sa/4.0/\n",
            "\n",
            "When using this dataset, please cite the following:\n",
            "\n",
            "@misc{https://doi.org/10.4121/15023088.v1,\n",
            "  doi = {10.4121/15023088.V1},\n",
            "  url = {https://data.4tu.nl/articles/_/15023088/1},\n",
            "  author = {Hemming,  S. (Silke) and de Zwart,  H.F. (Feije) and Elings,  A. (Anne) and bijlaard,  monique and Marrewijk,  van,  Bart and Petropoulou,  Anna},\n",
            "  keywords = {Horticultural Crops,  Mechanical Engineering,  FOS: Mechanical engineering,  Artificial Intelligence and Image Processing,  FOS: Computer and information sciences,  Horticultural Production,  FOS: Agriculture,  forestry and fisheries,  Autonomous Greenhouse Challenge,  autonomous greenhouse,  Artificial Intelligence,  image processing,  computer vision,  Horticulture,  Lettuce,  sensors,  non-destructive sensing},\n",
            "  title = {3rd Autonomous Greenhouse Challenge: Online Challenge Lettuce Images},\n",
            "  publisher = {4TU.ResearchData},\n",
            "  year = {2021},\n",
            "  copyright = {Creative Commons Attribution 4.0 International}\n",
            "}\n",
            "\n",
            "You can find additional information about this dataset at:\n",
            "https://data.4tu.nl/articles/dataset/3rd_Autonomous_Greenhouse_Challenge_Online_Challenge_Lettuce_Images/15023088/1\n",
            "\n",
            "This message will \u001b[1mnot\u001b[0m be automatically shown\n",
            "again. To view this message again, in an AgMLDataLoader\n",
            "run `loader.info.citation_summary()`. Otherwise, you\n",
            "can use `agml.data.source(<name>).citation_summary().`\n",
            "\n",
            "You can find your dataset at /content/autonomous_greenhouse_regression.\n",
            "====================================================================================================\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/agml/data/metadata.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0;31m# Some weird behavior with lookups can happen.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/agml/data/metadata.py\u001b[0m in \u001b[0;36mnum_to_class\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    213\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclass_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m                     \u001b[0mnums\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclass_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m                     \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclass_type\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnums\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclass_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'keys'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-2fbc67e4142a>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0magml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAgMLDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'autonomous_greenhouse_regression'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/agml/data/loader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0;34m'classes'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0;34m'num_classes'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m             \u001b[0;34m'num_to_class'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_to_class\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m             \u001b[0;34m'class_to_num'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_to_num\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m             'data_distributions': {self.name: self._info.num_images}}\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/agml/data/metadata.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_metadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m             raise AttributeError(\n\u001b[0m\u001b[1;32m    158\u001b[0m                 maybe_you_meant(\n\u001b[1;32m    159\u001b[0m                     \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"Received invalid info parameter: '{key}'.\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: Received invalid info parameter: 'num_to_class'."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define data and output directories"
      ],
      "metadata": {
        "id": "D5kHbdg3LMBF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sav_dir='model_weights/'\n",
        "if not os.path.exists(sav_dir):\n",
        "    os.mkdir(sav_dir)\n",
        "# Comment these two lines and uncomment the next two if you've already croppped the images to another directory\n",
        "RGB_Data_Dir   = './autonomous_greenhouse_regression/images/'\n",
        "Depth_Data_Dir = './autonomous_greenhouse_regression/depth_images/'\n",
        "\n",
        "\n",
        "# RGB_Data_Dir='./autonomous_greenhouse_regression/cropped_images/'\n",
        "# Depth_Data_Dir='./autonomous_greenhouse_regression/cropped_depth_images/'\n",
        "\n",
        "\n",
        "JSON_Files_Dir = './autonomous_greenhouse_regression/annotations.json'"
      ],
      "metadata": {
        "id": "8BTdADxwLLH0",
        "execution": {
          "iopub.status.busy": "2023-12-18T06:57:28.688438Z",
          "iopub.execute_input": "2023-12-18T06:57:28.689264Z",
          "iopub.status.idle": "2023-12-18T06:57:28.694559Z",
          "shell.execute_reply.started": "2023-12-18T06:57:28.689222Z",
          "shell.execute_reply": "2023-12-18T06:57:28.693637Z"
        },
        "trusted": true
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Crop"
      ],
      "metadata": {
        "id": "Xwp2Qb1KLPNm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "min_x=650\n",
        "max_x=1450\n",
        "min_y=200\n",
        "max_y=900\n",
        "cropped_img_dir='./autonomous_greenhouse_regression/cropped_images/'\n",
        "\n",
        "cropped_depth_img_dir='./autonomous_greenhouse_regression/cropped_depth_images/'\n",
        "\n",
        "if not os.path.exists(cropped_img_dir):\n",
        "    os.mkdir(cropped_img_dir)\n",
        "\n",
        "if not os.path.exists(cropped_depth_img_dir):\n",
        "    os.mkdir(cropped_depth_img_dir)\n",
        "\n",
        "for im in os.listdir(RGB_Data_Dir):\n",
        "    img = cv2.imread(RGB_Data_Dir+im)\n",
        "    crop_img = img[min_y:max_y,min_x:max_x]\n",
        "    cv2.imwrite(cropped_img_dir+im, crop_img)\n",
        "\n",
        "for depth_im in os.listdir(Depth_Data_Dir):\n",
        "    depth_img = cv2.imread(Depth_Data_Dir+depth_im, 0)\n",
        "    crop_depth_img = depth_img[min_y:max_y,min_x:max_x]\n",
        "    cv2.imwrite(cropped_depth_img_dir+depth_im, crop_depth_img)\n",
        "\n",
        "RGB_Data_Dir   = cropped_img_dir\n",
        "Depth_Data_Dir = cropped_depth_img_dir"
      ],
      "metadata": {
        "id": "5P0jSdmTLPyW",
        "execution": {
          "iopub.status.busy": "2023-12-18T06:57:30.265688Z",
          "iopub.execute_input": "2023-12-18T06:57:30.266551Z",
          "iopub.status.idle": "2023-12-18T06:58:07.053571Z",
          "shell.execute_reply.started": "2023-12-18T06:57:30.266515Z",
          "shell.execute_reply": "2023-12-18T06:58:07.052528Z"
        },
        "trusted": true
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create PyTorch dataset, create PyTorch dataloader, and split train/val/test"
      ],
      "metadata": {
        "id": "ZIGY6TuxLSs-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "split_seed = 12\n",
        "num_epochs = 400"
      ],
      "metadata": {
        "id": "Lfl_4s8ILTMC",
        "execution": {
          "iopub.status.busy": "2023-12-18T06:58:07.055214Z",
          "iopub.execute_input": "2023-12-18T06:58:07.055521Z",
          "iopub.status.idle": "2023-12-18T06:58:07.059515Z",
          "shell.execute_reply.started": "2023-12-18T06:58:07.055495Z",
          "shell.execute_reply": "2023-12-18T06:58:07.058583Z"
        },
        "trusted": true
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GreenhouseDataset(Dataset):\n",
        "    def __init__(self, rgb_dir, d_dir, jsonfile_dir, rgb_transforms=None, d_transforms=None):\n",
        "\n",
        "        self.df= pd.read_json(jsonfile_dir)\n",
        "        # flatten_json is a custom function to flat the nested json files!\n",
        "\n",
        "        self.rgb_transforms = rgb_transforms\n",
        "        self.d_transforms = d_transforms\n",
        "        self.rgb_dir = rgb_dir\n",
        "        self.d_dir = d_dir\n",
        "        self.num_outputs = len(self.df.iloc[0]['outputs']['regression'])\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # load images\n",
        "        row=self.df.iloc[idx]\n",
        "\n",
        "        rgb = plt.imread(self.rgb_dir+row['image'])\n",
        "        depth = plt.imread(self.d_dir+row['depth_image'])\n",
        "        depth = np.expand_dims(depth, 2)\n",
        "\n",
        "        target = list(row['outputs']['regression'].values())\n",
        "\n",
        "        #make sure your img and mask array are in this format before passing into albumentations transforms, img.shape=[H, W, C]\n",
        "        if self.rgb_transforms is not None:\n",
        "            aug_rgb = self.rgb_transforms(image=rgb)\n",
        "            rgb = aug_rgb['image']\n",
        "        elif self.d_transforms is not None:\n",
        "            aug_depth = self.d_transforms(image=depth)\n",
        "            depth = aug_depth['image']\n",
        "\n",
        "        rgb = np.transpose(rgb, (2,0,1))\n",
        "        depth = np.transpose(depth, (2,0,1))\n",
        "\n",
        "        #pytorch wants a different format for the image ([C, H, W])\n",
        "        rgb = torch.as_tensor(rgb, dtype=torch.float32)\n",
        "        depth = torch.as_tensor(depth, dtype=torch.float32)\n",
        "        target=torch.as_tensor(target, dtype=torch.float32)\n",
        "\n",
        "        return rgb, depth, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)"
      ],
      "metadata": {
        "id": "ZXdeZfsiLZIJ",
        "execution": {
          "iopub.status.busy": "2023-12-18T06:58:07.060951Z",
          "iopub.execute_input": "2023-12-18T06:58:07.061374Z",
          "iopub.status.idle": "2023-12-18T06:58:07.073398Z",
          "shell.execute_reply.started": "2023-12-18T06:58:07.061341Z",
          "shell.execute_reply": "2023-12-18T06:58:07.072556Z"
        },
        "trusted": true
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## FIGURE OUT HOW TO CROP ALL THE IMAGES TO GET RID OF EXTRANIOUS PIXELS\n",
        "def get_transforms(train, means, stds):\n",
        "    if train:\n",
        "        transforms = A.Compose([\n",
        "        # A.Crop(x_min=650, y_min=200, x_max=1450, y_max=900, always_apply=False, p=1.0),\n",
        "        A.Flip(p=0.5),\n",
        "        A.ShiftScaleRotate(always_apply=False, p=0.5, shift_limit=(-0.06, 0.06), scale_limit=(-0.1, 0.1), rotate_limit=(-5, 5), interpolation=0, border_mode=0, value=means, mask_value=None),\n",
        "        A.Normalize(mean=means, std=stds, max_pixel_value=1.0, always_apply=False, p=1.0)\n",
        "        ])\n",
        "    else:\n",
        "        transforms =  A.Compose([\n",
        "        # A.Crop(x_min=650, y_min=200, x_max=1450, y_max=900, always_apply=False, p=1.0),\n",
        "        A.Normalize(mean=means, std=stds, max_pixel_value=1.0, always_apply=False, p=1.0)\n",
        "        ])\n",
        "    return transforms"
      ],
      "metadata": {
        "id": "__m1vXj4LcDy",
        "execution": {
          "iopub.status.busy": "2023-12-18T06:58:07.076054Z",
          "iopub.execute_input": "2023-12-18T06:58:07.076376Z",
          "iopub.status.idle": "2023-12-18T06:58:07.086696Z",
          "shell.execute_reply.started": "2023-12-18T06:58:07.076342Z",
          "shell.execute_reply": "2023-12-18T06:58:07.085721Z"
        },
        "trusted": true
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the PyTorch datalaoder the autonomous greenhouse dataset.\n",
        "dataset = GreenhouseDataset(rgb_dir = RGB_Data_Dir,\n",
        "                            d_dir = Depth_Data_Dir,\n",
        "                            jsonfile_dir = JSON_Files_Dir,\n",
        "                            rgb_transforms = get_transforms(train=False, means=[0,0,0],stds=[1,1,1]),\n",
        "                            d_transforms = get_transforms(train=False, means=[0,0,0],stds=[1,1,1]))\n",
        "\n",
        "# Remove last 50 images from training/validation set. These are the test set.\n",
        "dataset.df= dataset.df.iloc[:-50]\n",
        "\n",
        "# Split train and validation set. Stratify based on variety.\n",
        "train_split, val_split = train_test_split(dataset.df,\n",
        "                                          test_size = 0.2,\n",
        "                                          random_state = split_seed,\n",
        "                                          stratify = dataset.df['outputs'].str['classification']) #change to None if you don't have class info\n",
        "train = torch.utils.data.Subset(dataset, train_split.index.tolist())\n",
        "val   = torch.utils.data.Subset(dataset, val_split.index.tolist())\n",
        "\n",
        "# Create train and validation dataloaders\n",
        "train_loader = torch.utils.data.DataLoader(train, batch_size=6, num_workers=6, shuffle=True)\n",
        "val_loader   = torch.utils.data.DataLoader(val,   batch_size=6, shuffle=False, num_workers=6)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zjw72weiLdB-",
        "outputId": "72833947-9bd6-410b-9e2c-31ad0003b3da",
        "execution": {
          "iopub.status.busy": "2023-12-18T06:58:07.087749Z",
          "iopub.execute_input": "2023-12-18T06:58:07.088100Z",
          "iopub.status.idle": "2023-12-18T06:58:07.124658Z",
          "shell.execute_reply.started": "2023-12-18T06:58:07.088075Z",
          "shell.execute_reply": "2023-12-18T06:58:07.123853Z"
        },
        "trusted": true
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Determine the mean and standard deviation of images for normalization (Only need to do once for a new dataset)"
      ],
      "metadata": {
        "id": "vGXtUu6LLiHV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# this part is just to check the MEAN and STD of the dataset (dont run unless you need mu and sigma)\n",
        "\n",
        "n_rgb = 0\n",
        "n_depth = 0\n",
        "mean_rgb = 0.\n",
        "std_rgb = 0.\n",
        "mean_depth = 0.\n",
        "std_depth = 0.\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=5, shuffle=False, num_workers=12)\n",
        "for rgb, depth, _ in dataloader:\n",
        "\n",
        "    # Rearrange batch to be the shape of [B, C, W * H]\n",
        "    rgb = rgb.view(rgb.size(0), rgb.size(1), -1)\n",
        "    depth = depth.view(depth.size(0), depth.size(1), -1)\n",
        "    # Update total number of images\n",
        "    n_rgb += rgb.size(0)\n",
        "    n_depth += depth.size(0)\n",
        "    # Compute mean and std here\n",
        "    mean_rgb += rgb.mean(2).sum(0)\n",
        "    std_rgb += rgb.std(2).sum(0)\n",
        "    mean_depth += depth.mean(2).sum(0)\n",
        "    std_depth += depth.std(2).sum(0)\n",
        "\n",
        "# Final step\n",
        "mean_rgb /= n_rgb\n",
        "std_rgb /= n_rgb\n",
        "mean_depth /= n_depth\n",
        "std_depth /= n_depth\n",
        "\n",
        "print('Mean of RGB: '+ str(mean_rgb))\n",
        "print('Standard Deviation of RGB', str(std_rgb))\n",
        "print('Mean of Depth: '+ str(mean_depth))\n",
        "print('Standard Deviation of Depth', str(std_depth))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OHD3FI6ULioT",
        "outputId": "1c226d19-36e7-4669-dcfa-b0f253d0fafa",
        "execution": {
          "iopub.status.busy": "2023-12-18T06:58:07.125749Z",
          "iopub.execute_input": "2023-12-18T06:58:07.126087Z",
          "iopub.status.idle": "2023-12-18T06:58:16.946956Z",
          "shell.execute_reply.started": "2023-12-18T06:58:07.126058Z",
          "shell.execute_reply": "2023-12-18T06:58:16.945869Z"
        },
        "trusted": true
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 12 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean of RGB: tensor([0.5482, 0.4620, 0.3602])\n",
            "Standard Deviation of RGB tensor([0.1639, 0.1761, 0.2659])\n",
            "Mean of Depth: tensor([0.0127])\n",
            "Standard Deviation of Depth tensor([0.0035])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Copy the output of the previous cells into here to avoid needing to redetermine mean and std every time"
      ],
      "metadata": {
        "id": "6eveLzxiLlUm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.means = [0.5482, 0.4620, 0.3602, 0.0127]  #these values were copied from the previous cell\n",
        "dataset.stds = [0.1639, 0.1761, 0.2659, 0.0035]   #copy and paste the values to avoid having\n",
        "                                                  # to rerun the previous cell for every iteration"
      ],
      "metadata": {
        "id": "qAp_KbHvLl0P",
        "execution": {
          "iopub.status.busy": "2023-12-18T06:58:16.948423Z",
          "iopub.execute_input": "2023-12-18T06:58:16.948785Z",
          "iopub.status.idle": "2023-12-18T06:58:16.953763Z",
          "shell.execute_reply.started": "2023-12-18T06:58:16.948750Z",
          "shell.execute_reply": "2023-12-18T06:58:16.952906Z"
        },
        "trusted": true
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set device"
      ],
      "metadata": {
        "id": "B9is_me-LndB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available else 'cpu')"
      ],
      "metadata": {
        "id": "AhORwK3MLo4y",
        "execution": {
          "iopub.status.busy": "2023-12-18T06:58:16.954946Z",
          "iopub.execute_input": "2023-12-18T06:58:16.955300Z",
          "iopub.status.idle": "2023-12-18T06:58:16.962715Z",
          "shell.execute_reply.started": "2023-12-18T06:58:16.955270Z",
          "shell.execute_reply": "2023-12-18T06:58:16.962025Z"
        },
        "trusted": true
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model"
      ],
      "metadata": {
        "id": "7dMdA76JLuEK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MidFusionSubnet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(MidFusionSubnet, self).__init__()\n",
        "    self.resNet1rgb = models.resnet18(pretrained = True)\n",
        "    self.resNet2rgb = models.resnet18(pretrained = True)\n",
        "    self.resNet3rgb = models.resnet18(pretrained = True)\n",
        "    self.resNet4rgb = models.resnet18(pretrained = True)\n",
        "    self.resNet5rgb = models.resnet18(pretrained = True)\n",
        "    self.depthEncoder1 = nn.Sequential(nn.Conv2d(1, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), nn.ReLU(), models.resnet18(pretrained = True))\n",
        "    self.depthEncoder2 = nn.Sequential(nn.Conv2d(1, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), nn.ReLU(), models.resnet18(pretrained = True))\n",
        "    self.depthEncoder3 = nn.Sequential(nn.Conv2d(1, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), nn.ReLU(), models.resnet18(pretrained = True))\n",
        "    self.depthEncoder4 = nn.Sequential(nn.Conv2d(1, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), nn.ReLU(), models.resnet18(pretrained = True))\n",
        "    self.depthEncoder5 = nn.Sequential(nn.Conv2d(1, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), nn.ReLU(), models.resnet18(pretrained = True))\n",
        "\n",
        "    dropout_p = 0.05\n",
        "    self.fcn1 = nn.Sequential(nn.Dropout(dropout_p), nn.ReLU(), nn.Linear(2000, 1000), nn.ReLU(), nn.Dropout(dropout_p), nn.Linear(1000, 500), nn.Dropout(dropout_p), nn.ReLU(), nn.Linear(500, 1), nn.ReLU())\n",
        "    self.fcn2 = nn.Sequential(nn.Dropout(dropout_p), nn.ReLU(), nn.Linear(2000, 1000), nn.ReLU(), nn.Dropout(dropout_p), nn.Linear(1000, 500), nn.Dropout(dropout_p), nn.ReLU(), nn.Linear(500, 1), nn.ReLU())\n",
        "    self.fcn3 = nn.Sequential(nn.Dropout(dropout_p), nn.ReLU(), nn.Linear(2000, 1000), nn.ReLU(), nn.Dropout(dropout_p), nn.Linear(1000, 500), nn.Dropout(dropout_p), nn.ReLU(), nn.Linear(500, 1), nn.ReLU())\n",
        "    self.fcn4 = nn.Sequential(nn.Dropout(dropout_p), nn.ReLU(), nn.Linear(2000, 1000), nn.ReLU(), nn.Dropout(dropout_p), nn.Linear(1000, 500), nn.Dropout(dropout_p), nn.ReLU(), nn.Linear(500, 1), nn.ReLU())\n",
        "    self.fcn5 = nn.Sequential(nn.Dropout(dropout_p), nn.ReLU(), nn.Linear(2000, 1000), nn.ReLU(), nn.Dropout(dropout_p), nn.Linear(1000, 500), nn.Dropout(dropout_p), nn.ReLU(), nn.Linear(500, 1), nn.ReLU())\n",
        "\n",
        "  def forward(self, rgb, depth):\n",
        "    # rgbIn = x[:, :3, :, :]\n",
        "    # depthIn = x[:, 3: , :, :]\n",
        "    out1 = self.fcn1(torch.cat([self.resNet1rgb(rgb), self.depthEncoder1(depth)], dim = 1))\n",
        "    out2 = self.fcn2(torch.cat([self.resNet2rgb(rgb), self.depthEncoder2(depth)], dim = 1))\n",
        "    out3 = self.fcn3(torch.cat([self.resNet3rgb(rgb), self.depthEncoder3(depth)], dim = 1))\n",
        "    out4 = self.fcn4(torch.cat([self.resNet4rgb(rgb), self.depthEncoder4(depth)], dim = 1))\n",
        "    out5 = self.fcn5(torch.cat([self.resNet5rgb(rgb), self.depthEncoder5(depth)], dim = 1))\n",
        "    outs = torch.cat([out1, out2, out3, out4, out5], dim = 1)\n",
        "    return outs\n"
      ],
      "metadata": {
        "id": "WFKgKCXrrNe_"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MidFusionSubnet()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EryoPktnMl9w",
        "outputId": "6fa16156-d16c-41b1-e3db-57e0f0986f19",
        "execution": {
          "iopub.status.busy": "2023-12-18T08:14:20.085716Z",
          "iopub.execute_input": "2023-12-18T08:14:20.086384Z",
          "iopub.status.idle": "2023-12-18T08:14:23.066113Z",
          "shell.execute_reply.started": "2023-12-18T08:14:20.086348Z",
          "shell.execute_reply": "2023-12-18T08:14:23.065058Z"
        },
        "trusted": true
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameter"
      ],
      "metadata": {
        "id": "jgW2kfIZMs_1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 0.0005\n",
        "epochs = 400\n",
        "batch_size = 32"
      ],
      "metadata": {
        "id": "RftkG91CMu32",
        "execution": {
          "iopub.status.busy": "2023-12-18T08:14:33.253788Z",
          "iopub.execute_input": "2023-12-18T08:14:33.254180Z",
          "iopub.status.idle": "2023-12-18T08:14:33.258686Z",
          "shell.execute_reply.started": "2023-12-18T08:14:33.254147Z",
          "shell.execute_reply": "2023-12-18T08:14:33.257772Z"
        },
        "trusted": true
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NMSE loss"
      ],
      "metadata": {
        "id": "XPEmPNzLNG6k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NMSELoss(nn.Module):\n",
        "    def __init__(self):\n",
        "          # super(diceloss, self).init()\n",
        "        super(NMSELoss, self).__init__()\n",
        "          # print('HI')\n",
        "    def forward(self, pred, target):\n",
        "        if target.size() != pred.size():\n",
        "              raise ValueError(\"Target size ({}) must be the same as input size ({})\".format(target.size(), pred.size()))\n",
        "\n",
        "        num=torch.sum((target-pred)**2,0)\n",
        "        den=torch.sum(target**2,0)\n",
        "\n",
        "        return torch.sum(num/den)"
      ],
      "metadata": {
        "id": "W7B_B5Y9NKbH",
        "execution": {
          "iopub.status.busy": "2023-12-18T08:14:35.907035Z",
          "iopub.execute_input": "2023-12-18T08:14:35.907378Z",
          "iopub.status.idle": "2023-12-18T08:14:35.913930Z",
          "shell.execute_reply.started": "2023-12-18T08:14:35.907354Z",
          "shell.execute_reply": "2023-12-18T08:14:35.912871Z"
        },
        "trusted": true
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loss and optimizer"
      ],
      "metadata": {
        "id": "dbPA1SYpNAqi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = NMSELoss()\n",
        "# optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "optimizer = torch.optim.Adam(model.parameters(),\n",
        "                            lr=lr,\n",
        "                            betas=(0.9, 0.999),\n",
        "                            eps=1e-08,\n",
        "                            weight_decay = 0,\n",
        "                            amsgrad = False)"
      ],
      "metadata": {
        "id": "QgiJS3F_NCXm",
        "execution": {
          "iopub.status.busy": "2023-12-18T08:14:37.715799Z",
          "iopub.execute_input": "2023-12-18T08:14:37.716185Z",
          "iopub.status.idle": "2023-12-18T08:14:37.723086Z",
          "shell.execute_reply.started": "2023-12-18T08:14:37.716151Z",
          "shell.execute_reply": "2023-12-18T08:14:37.722024Z"
        },
        "trusted": true
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train"
      ],
      "metadata": {
        "id": "W3mXlgx1M3th"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_single_epoch(model, dataset, device,\n",
        "                       criterion, optimizer,\n",
        "                       writer, epoch, train_loader):\n",
        "    model.train()\n",
        "\n",
        "    dataset.rgb_transforms = get_transforms(train=True, means=dataset.means[:3], stds=dataset.stds[:3])\n",
        "    dataset.d_transforms = get_transforms(train=True, means=dataset.means[3:], stds=dataset.stds[3:])\n",
        "\n",
        "    for i, (rgb, depth, label) in enumerate(train_loader):\n",
        "        rgb = rgb.to(device)\n",
        "        depth = depth.to(device)\n",
        "        label = label.to(device)  # ['FreshWeightShoot', 'DryWeightShoot', 'Height', 'Diameter', 'LeafArea']\n",
        "\n",
        "        # Forward pass\n",
        "        pred = model(rgb, depth)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = criterion(pred, label)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{epochs}, Batch {i+1}/{len(train_loader)}, Loss: {loss.item()}')\n",
        "        with open('run.txt', 'a') as f:\n",
        "            f.write('\\n')\n",
        "            f.write('Train MSE: '+ str(loss.tolist()))\n"
      ],
      "metadata": {
        "id": "atHQ5B1XM5NP",
        "execution": {
          "iopub.status.busy": "2023-12-18T08:14:39.664702Z",
          "iopub.execute_input": "2023-12-18T08:14:39.665418Z",
          "iopub.status.idle": "2023-12-18T08:14:39.674811Z",
          "shell.execute_reply.started": "2023-12-18T08:14:39.665380Z",
          "shell.execute_reply": "2023-12-18T08:14:39.673806Z"
        },
        "trusted": true
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(model, dataset, device, sav_dir, criterion, writer, epoch, val_loader, best_val_loss):\n",
        "    current_val_loss = 0\n",
        "    # training_val_loss=0s\n",
        "\n",
        "    model.eval()\n",
        "    print('Validating and Checkpointing!')\n",
        "\n",
        "    dataset.rgb_transforms = get_transforms(train=True, means=dataset.means[:3], stds=dataset.stds[:3])\n",
        "    dataset.d_transforms = get_transforms(train=True, means=dataset.means[3:], stds=dataset.stds[3:])\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (rgb, depth, label) in enumerate(val_loader):\n",
        "            rgb = rgb.to(device)\n",
        "            depth = depth.to(device)\n",
        "            label = label.to(device)\n",
        "\n",
        "            pred = model(rgb, depth)\n",
        "\n",
        "            loss = criterion(pred, label)\n",
        "            # acc=nmse(preds.detach(), targets)\n",
        "            current_val_loss = current_val_loss + loss.item()\n",
        "            # training_val_loss=training_val_loss+loss.detach().cpu().numpy()\n",
        "\n",
        "        # writer.add_scalar(\"MSE Loss/val\", training_val_loss, epoch)\n",
        "        writer.add_scalar(\"MSE Loss/val\", current_val_loss, epoch)\n",
        "\n",
        "    if current_val_loss < best_val_loss or epoch == 0:\n",
        "        best_val_loss = current_val_loss\n",
        "        torch.save(model.state_dict(), sav_dir+'bestmodel' + '.pth')\n",
        "        print('Best model Saved! Val MSE: ', str(best_val_loss))\n",
        "        with open('run.txt', 'a') as f:\n",
        "            f.write('\\n')\n",
        "            f.write('Best model Saved! Val MSE: '+ str(best_val_loss))\n",
        "\n",
        "    else:\n",
        "        print('Model is not good (might be overfitting)! Current val MSE: ', str(current_val_loss), 'Best Val MSE: ', str(best_val_loss))\n",
        "        with open('run.txt', 'a') as f:\n",
        "            f.write('\\n')\n",
        "            f.write('Model is not good (might be overfitting)! Current val MSE: '+ str(current_val_loss)+ 'Best Val MSE: '+ str(best_val_loss))\n",
        "    return best_val_loss"
      ],
      "metadata": {
        "id": "ieE0dxwgP3F0",
        "execution": {
          "iopub.status.busy": "2023-12-18T08:14:40.210955Z",
          "iopub.execute_input": "2023-12-18T08:14:40.211334Z",
          "iopub.status.idle": "2023-12-18T08:14:40.223403Z",
          "shell.execute_reply.started": "2023-12-18T08:14:40.211302Z",
          "shell.execute_reply": "2023-12-18T08:14:40.222428Z"
        },
        "trusted": true
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)\n",
        "\n",
        "best_val_loss = 9999999 # initial dummy value\n",
        "current_val_loss = 0\n",
        "\n",
        "writer = SummaryWriter()\n",
        "start = time.time()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    with open('run.txt', 'a') as f:\n",
        "                f.write('\\n')\n",
        "                f.write('Epoch: '+ str(epoch + 1) + ', Time Elapsed: '+ str((time.time()-start)/60) + ' mins')\n",
        "    print('Epoch: ', str(epoch + 1), ', Time Elapsed: ', str((time.time()-start)/60), ' mins')\n",
        "    train_single_epoch(model, dataset, device,\n",
        "                        criterion, optimizer,\n",
        "                        writer, epoch, train_loader)\n",
        "    best_val_loss = validate(model, dataset, device, sav_dir,\n",
        "                                criterion, writer, epoch, val_loader, best_val_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "id": "B38GTKhJP6rJ",
        "outputId": "52c92ed2-4814-428a-bc70-bb275cd1641b",
        "execution": {
          "iopub.status.busy": "2023-12-18T08:14:43.115392Z",
          "iopub.execute_input": "2023-12-18T08:14:43.116068Z",
          "iopub.status.idle": "2023-12-18T08:46:44.793672Z",
          "shell.execute_reply.started": "2023-12-18T08:14:43.116032Z",
          "shell.execute_reply": "2023-12-18T08:46:44.792474Z"
        },
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "trusted": true
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-8bc979ee32a0>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mbest_val_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m9999999\u001b[0m \u001b[0;31m# initial dummy value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcurrent_val_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1158\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1160\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m     def register_full_backward_pre_hook(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 810\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 810\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 810\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 810\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    831\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1156\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m   1157\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m-> 1158\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 14.75 GiB of which 11.06 MiB is free. Process 2005 has 14.73 GiB memory in use. Of the allocated memory 14.48 GiB is allocated by PyTorch, and 124.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation"
      ],
      "metadata": {
        "id": "vo196DDGKgtS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the test dataset"
      ],
      "metadata": {
        "id": "5bBqkPJUKgtS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the PyTorch datalaoder the autonomous greenhouse dataset.\n",
        "testset = GreenhouseDataset(rgb_dir = RGB_Data_Dir,\n",
        "                            d_dir = Depth_Data_Dir,\n",
        "                            jsonfile_dir = JSON_Files_Dir,\n",
        "                            rgb_transforms = get_transforms(train=False, means=dataset.means[:3], stds=dataset.stds[:3]),\n",
        "                            d_transforms = get_transforms(train=False, means=dataset.means[3:], stds=dataset.stds[3:]))\n",
        "\n",
        "# Grab last 50 images as test dataset\n",
        "testset.df = testset.df[-50:]\n",
        "\n",
        "# Get testset_size\n",
        "testset_size = testset.df.shape[0]\n",
        "\n",
        "# Create test dataloader\n",
        "test_loader = torch.utils.data.DataLoader(testset,\n",
        "                                          batch_size = 50,\n",
        "                                          num_workers = 0,\n",
        "                                          shuffle = False)"
      ],
      "metadata": {
        "id": "LNiS2urzSs2j",
        "execution": {
          "iopub.status.busy": "2023-12-18T08:50:14.905955Z",
          "iopub.execute_input": "2023-12-18T08:50:14.906787Z",
          "iopub.status.idle": "2023-12-18T08:50:14.923799Z",
          "shell.execute_reply.started": "2023-12-18T08:50:14.906746Z",
          "shell.execute_reply": "2023-12-18T08:50:14.923058Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define loss functions for model evaluation"
      ],
      "metadata": {
        "id": "gKt_khHVKgtU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cri = NMSELoss()\n",
        "mse = nn.MSELoss()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-18T08:50:18.493320Z",
          "iopub.execute_input": "2023-12-18T08:50:18.494274Z",
          "iopub.status.idle": "2023-12-18T08:50:18.498774Z",
          "shell.execute_reply.started": "2023-12-18T08:50:18.494233Z",
          "shell.execute_reply": "2023-12-18T08:50:18.497796Z"
        },
        "trusted": true,
        "id": "h4_LV8DdKgtU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the evaluation Loop"
      ],
      "metadata": {
        "id": "hCJLyzwyKgtn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation loop\n",
        "device=torch.device('cuda')\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "\n",
        "    device=torch.device('cuda')\n",
        "    model = FirstStageModel()\n",
        "    model.to(device)\n",
        "    model.load_state_dict(torch.load(sav_dir + 'bestmodel.pth'))\n",
        "    model.eval()\n",
        "\n",
        "    ap=torch.zeros((0,4))\n",
        "    at=torch.zeros((0,4))\n",
        "\n",
        "    for rgb, depth, targets in test_loader:\n",
        "        rgb = rgb.to(device)\n",
        "        depth = depth.to(device)\n",
        "        targets = targets.to(device)\n",
        "        targets = targets[:, :4]\n",
        "        pred1, pred2 = model(rgb, depth)\n",
        "        pred = torch.cat([pred1[:, :2], pred2], dim=1)\n",
        "        pred = torch.cat([pred, pred1[:, 2:]], dim=1) # fresh weight, dry weight, height, diameter\n",
        "        # mse_loss=mse(preds, targets)\n",
        "        # nmse=criterion(preds, targets)\n",
        "        # nmse, pred=cri(preds, targets)\n",
        "        ap=torch.cat((ap, pred.detach().cpu()), 0)\n",
        "        at=torch.cat((at, targets.detach().cpu()), 0)\n",
        "\n",
        "\n",
        "    print('FW MSE: ', str(mse(ap[:,0],at[:,0]).tolist()))\n",
        "    print('DW MSE: ', str(mse(ap[:,1],at[:,1]).tolist()))\n",
        "    print('H MSE: ', str(mse(ap[:,2],at[:,2]).tolist()))\n",
        "    print('D MSE: ', str(mse(ap[:,3],at[:,3]).tolist()))\n",
        "#     print('LA MSE: ', str(mse(ap[:,4],at[:,4]).tolist()))\n",
        "    print('Overall NMSE: ', str(cri(ap,at).tolist()))\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-18T08:55:19.566507Z",
          "iopub.execute_input": "2023-12-18T08:55:19.566878Z",
          "iopub.status.idle": "2023-12-18T08:55:22.787607Z",
          "shell.execute_reply.started": "2023-12-18T08:55:19.566839Z",
          "shell.execute_reply": "2023-12-18T08:55:22.786744Z"
        },
        "trusted": true,
        "id": "DkNd7JkvKgto",
        "outputId": "15efb756-e004-4e52-dae4-57715b09eda7"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "FW MSE:  1406.3551025390625\nDW MSE:  2.855520248413086\nH MSE:  30.06348419189453\nD MSE:  13.6848783493042\nOverall NMSE:  0.2702009975910187\n",
          "output_type": "stream"
        }
      ]
    }
  ]
}