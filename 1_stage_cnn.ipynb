{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOWxE4QE/55NVUPcA//ZlAe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/giyeongyoon/3rd_AGC/blob/master/1_stage_cnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "P7YSvf_oKlPr"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install albumentations==1.1.0\n",
        "!pip install agml"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import libraries"
      ],
      "metadata": {
        "id": "XeVhzZvgLCt7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torchvision import models, transforms\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "import albumentations as A\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import time"
      ],
      "metadata": {
        "id": "6BZ4j2lzK6wQ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download 2021 Autonomous Greenhouse Challenge dataset"
      ],
      "metadata": {
        "id": "JhSMln48K_iJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import agml\n",
        "loader = agml.data.AgMLDataLoader('autonomous_greenhouse_regression', dataset_path = './')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "T3-oDY-ZLBdJ",
        "outputId": "c70a52e1-8544-41ba-d56b-aeb64dca1d07"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading autonomous_greenhouse_regression (size = 887.2 MB): 887226368it [00:10, 84835135.11it/s]                                \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[AgML Download]: Extracting files for autonomous_greenhouse_regression... Done!\n",
            "\n",
            "====================================================================================================\n",
            "You have just downloaded \u001b[1mautonomous_greenhouse_regression\u001b[0m.\n",
            "\n",
            "This dataset is licensed under the \u001b[1mCC BY-SA 4.0\u001b[0m license.\n",
            "To learn more, visit: https://creativecommons.org/licenses/by-sa/4.0/\n",
            "\n",
            "When using this dataset, please cite the following:\n",
            "\n",
            "@misc{https://doi.org/10.4121/15023088.v1,\n",
            "  doi = {10.4121/15023088.V1},\n",
            "  url = {https://data.4tu.nl/articles/_/15023088/1},\n",
            "  author = {Hemming,  S. (Silke) and de Zwart,  H.F. (Feije) and Elings,  A. (Anne) and bijlaard,  monique and Marrewijk,  van,  Bart and Petropoulou,  Anna},\n",
            "  keywords = {Horticultural Crops,  Mechanical Engineering,  FOS: Mechanical engineering,  Artificial Intelligence and Image Processing,  FOS: Computer and information sciences,  Horticultural Production,  FOS: Agriculture,  forestry and fisheries,  Autonomous Greenhouse Challenge,  autonomous greenhouse,  Artificial Intelligence,  image processing,  computer vision,  Horticulture,  Lettuce,  sensors,  non-destructive sensing},\n",
            "  title = {3rd Autonomous Greenhouse Challenge: Online Challenge Lettuce Images},\n",
            "  publisher = {4TU.ResearchData},\n",
            "  year = {2021},\n",
            "  copyright = {Creative Commons Attribution 4.0 International}\n",
            "}\n",
            "\n",
            "You can find additional information about this dataset at:\n",
            "https://data.4tu.nl/articles/dataset/3rd_Autonomous_Greenhouse_Challenge_Online_Challenge_Lettuce_Images/15023088/1\n",
            "\n",
            "This message will \u001b[1mnot\u001b[0m be automatically shown\n",
            "again. To view this message again, in an AgMLDataLoader\n",
            "run `loader.info.citation_summary()`. Otherwise, you\n",
            "can use `agml.data.source(<name>).citation_summary().`\n",
            "\n",
            "You can find your dataset at /content/autonomous_greenhouse_regression.\n",
            "====================================================================================================\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/agml/data/metadata.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0;31m# Some weird behavior with lookups can happen.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/agml/data/metadata.py\u001b[0m in \u001b[0;36mnum_to_class\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    213\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclass_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m                     \u001b[0mnums\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclass_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m                     \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclass_type\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnums\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclass_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'keys'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-2fbc67e4142a>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0magml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAgMLDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'autonomous_greenhouse_regression'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/agml/data/loader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0;34m'classes'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0;34m'num_classes'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m             \u001b[0;34m'num_to_class'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_to_class\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m             \u001b[0;34m'class_to_num'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_to_num\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m             'data_distributions': {self.name: self._info.num_images}}\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/agml/data/metadata.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_metadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m             raise AttributeError(\n\u001b[0m\u001b[1;32m    158\u001b[0m                 maybe_you_meant(\n\u001b[1;32m    159\u001b[0m                     \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"Received invalid info parameter: '{key}'.\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: Received invalid info parameter: 'num_to_class'."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define data and output directories"
      ],
      "metadata": {
        "id": "D5kHbdg3LMBF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sav_dir='model_weights/'\n",
        "if not os.path.exists(sav_dir):\n",
        "    os.mkdir(sav_dir)\n",
        "# Comment these two lines and uncomment the next two if you've already croppped the images to another directory\n",
        "RGB_Data_Dir   = './autonomous_greenhouse_regression/images/'\n",
        "Depth_Data_Dir = './autonomous_greenhouse_regression/depth_images/'\n",
        "\n",
        "\n",
        "# RGB_Data_Dir='./autonomous_greenhouse_regression/cropped_images/'\n",
        "# Depth_Data_Dir='./autonomous_greenhouse_regression/cropped_depth_images/'\n",
        "\n",
        "\n",
        "JSON_Files_Dir = './autonomous_greenhouse_regression/annotations.json'"
      ],
      "metadata": {
        "id": "8BTdADxwLLH0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Crop"
      ],
      "metadata": {
        "id": "Xwp2Qb1KLPNm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "min_x=650\n",
        "max_x=1450\n",
        "min_y=200\n",
        "max_y=900\n",
        "cropped_img_dir='./autonomous_greenhouse_regression/cropped_images/'\n",
        "\n",
        "cropped_depth_img_dir='./autonomous_greenhouse_regression/cropped_depth_images/'\n",
        "\n",
        "if not os.path.exists(cropped_img_dir):\n",
        "    os.mkdir(cropped_img_dir)\n",
        "\n",
        "if not os.path.exists(cropped_depth_img_dir):\n",
        "    os.mkdir(cropped_depth_img_dir)\n",
        "\n",
        "for im in os.listdir(RGB_Data_Dir):\n",
        "    img = cv2.imread(RGB_Data_Dir+im)\n",
        "    crop_img = img[min_y:max_y,min_x:max_x]\n",
        "    cv2.imwrite(cropped_img_dir+im, crop_img)\n",
        "\n",
        "for depth_im in os.listdir(Depth_Data_Dir):\n",
        "    depth_img = cv2.imread(Depth_Data_Dir+depth_im, 0)\n",
        "    crop_depth_img = depth_img[min_y:max_y,min_x:max_x]\n",
        "    cv2.imwrite(cropped_depth_img_dir+depth_im, crop_depth_img)\n",
        "\n",
        "RGB_Data_Dir   = cropped_img_dir\n",
        "Depth_Data_Dir = cropped_depth_img_dir"
      ],
      "metadata": {
        "id": "5P0jSdmTLPyW"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create PyTorch dataset, create PyTorch dataloader, and split train/val/test"
      ],
      "metadata": {
        "id": "ZIGY6TuxLSs-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "split_seed = 12\n",
        "num_epochs = 400"
      ],
      "metadata": {
        "id": "Lfl_4s8ILTMC"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GreenhouseDataset(Dataset):\n",
        "    def __init__(self, rgb_dir, d_dir, jsonfile_dir, rgb_transforms=None, d_transforms=None):\n",
        "\n",
        "        self.df= pd.read_json(jsonfile_dir)\n",
        "        # flatten_json is a custom function to flat the nested json files!\n",
        "\n",
        "        self.rgb_transforms = rgb_transforms\n",
        "        self.d_transforms = d_transforms\n",
        "        self.rgb_dir = rgb_dir\n",
        "        self.d_dir = d_dir\n",
        "        self.num_outputs = len(self.df.iloc[0]['outputs']['regression'])\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # load images\n",
        "        row=self.df.iloc[idx]\n",
        "\n",
        "        rgb = plt.imread(self.rgb_dir+row['image'])\n",
        "        depth = plt.imread(self.d_dir+row['depth_image'])\n",
        "        depth = np.expand_dims(depth, 2)\n",
        "\n",
        "        target = list(row['outputs']['regression'].values())\n",
        "\n",
        "        #make sure your img and mask array are in this format before passing into albumentations transforms, img.shape=[H, W, C]\n",
        "        if self.rgb_transforms is not None:\n",
        "            aug_rgb = self.rgb_transforms(image=rgb)\n",
        "            rgb = aug_rgb['image']\n",
        "        elif self.d_transforms is not None:\n",
        "            aug_depth = self.d_transforms(image=depth)\n",
        "            depth = aug_depth['image']\n",
        "\n",
        "        rgb = np.transpose(rgb, (2,0,1))\n",
        "        depth = np.transpose(depth, (2,0,1))\n",
        "\n",
        "        #pytorch wants a different format for the image ([C, H, W])\n",
        "        rgb = torch.as_tensor(rgb, dtype=torch.float32)\n",
        "        depth = torch.as_tensor(depth, dtype=torch.float32)\n",
        "        target=torch.as_tensor(target, dtype=torch.float32)\n",
        "\n",
        "        return rgb, depth, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)"
      ],
      "metadata": {
        "id": "ZXdeZfsiLZIJ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## FIGURE OUT HOW TO CROP ALL THE IMAGES TO GET RID OF EXTRANIOUS PIXELS\n",
        "def get_transforms(train, means, stds):\n",
        "    if train:\n",
        "        transforms = A.Compose([\n",
        "        # A.Crop(x_min=650, y_min=200, x_max=1450, y_max=900, always_apply=False, p=1.0),\n",
        "        A.Flip(p=0.5),\n",
        "        A.ShiftScaleRotate(always_apply=False, p=0.5, shift_limit=(-0.06, 0.06), scale_limit=(-0.1, 0.1), rotate_limit=(-5, 5), interpolation=0, border_mode=0, value=means, mask_value=None),\n",
        "        A.Normalize(mean=means, std=stds, max_pixel_value=1.0, always_apply=False, p=1.0)\n",
        "        ])\n",
        "    else:\n",
        "        transforms =  A.Compose([\n",
        "        # A.Crop(x_min=650, y_min=200, x_max=1450, y_max=900, always_apply=False, p=1.0),\n",
        "        A.Normalize(mean=means, std=stds, max_pixel_value=1.0, always_apply=False, p=1.0)\n",
        "        ])\n",
        "    return transforms"
      ],
      "metadata": {
        "id": "__m1vXj4LcDy"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the PyTorch datalaoder the autonomous greenhouse dataset.\n",
        "dataset = GreenhouseDataset(rgb_dir = RGB_Data_Dir,\n",
        "                            d_dir = Depth_Data_Dir,\n",
        "                            jsonfile_dir = JSON_Files_Dir,\n",
        "                            rgb_transforms = get_transforms(train=False, means=[0,0,0],stds=[1,1,1]),\n",
        "                            d_transforms = get_transforms(train=False, means=[0,0,0],stds=[1,1,1]))\n",
        "\n",
        "# Remove last 50 images from training/validation set. These are the test set.\n",
        "dataset.df= dataset.df.iloc[:-50]\n",
        "\n",
        "# Split train and validation set. Stratify based on variety.\n",
        "train_split, val_split = train_test_split(dataset.df,\n",
        "                                          test_size = 0.2,\n",
        "                                          random_state = split_seed,\n",
        "                                          stratify = dataset.df['outputs'].str['classification']) #change to None if you don't have class info\n",
        "train = torch.utils.data.Subset(dataset, train_split.index.tolist())\n",
        "val   = torch.utils.data.Subset(dataset, val_split.index.tolist())\n",
        "\n",
        "# Create train and validation dataloaders\n",
        "train_loader = torch.utils.data.DataLoader(train, batch_size=6, num_workers=6, shuffle=True)\n",
        "val_loader   = torch.utils.data.DataLoader(val,   batch_size=6, shuffle=False, num_workers=6)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zjw72weiLdB-",
        "outputId": "5e6497cd-6a28-49fa-c7df-3909877db513"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Determine the mean and standard deviation of images for normalization (Only need to do once for a new dataset)"
      ],
      "metadata": {
        "id": "vGXtUu6LLiHV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# this part is just to check the MEAN and STD of the dataset (dont run unless you need mu and sigma)\n",
        "\n",
        "n_rgb = 0\n",
        "n_depth = 0\n",
        "mean_rgb = 0.\n",
        "std_rgb = 0.\n",
        "mean_depth = 0.\n",
        "std_depth = 0.\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=5, shuffle=False, num_workers=12)\n",
        "for rgb, depth, _ in dataloader:\n",
        "\n",
        "    # Rearrange batch to be the shape of [B, C, W * H]\n",
        "    rgb = rgb.view(rgb.size(0), rgb.size(1), -1)\n",
        "    depth = depth.view(depth.size(0), depth.size(1), -1)\n",
        "    # Update total number of images\n",
        "    n_rgb += rgb.size(0)\n",
        "    n_depth += depth.size(0)\n",
        "    # Compute mean and std here\n",
        "    mean_rgb += rgb.mean(2).sum(0)\n",
        "    std_rgb += rgb.std(2).sum(0)\n",
        "    mean_depth += depth.mean(2).sum(0)\n",
        "    std_depth += depth.std(2).sum(0)\n",
        "\n",
        "# Final step\n",
        "mean_rgb /= n_rgb\n",
        "std_rgb /= n_rgb\n",
        "mean_depth /= n_depth\n",
        "std_depth /= n_depth\n",
        "\n",
        "print('Mean of RGB: '+ str(mean_rgb))\n",
        "print('Standard Deviation of RGB', str(std_rgb))\n",
        "print('Mean of Depth: '+ str(mean_depth))\n",
        "print('Standard Deviation of Depth', str(std_depth))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OHD3FI6ULioT",
        "outputId": "b8becdb2-f5d2-4947-fae5-2a48d46ef9d0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 12 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean of RGB: tensor([0.5482, 0.4620, 0.3602])\n",
            "Standard Deviation of RGB tensor([0.1639, 0.1761, 0.2659])\n",
            "Mean of Depth: tensor([0.0127])\n",
            "Standard Deviation of Depth tensor([0.0035])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Copy the output of the previous cells into here to avoid needing to redetermine mean and std every time"
      ],
      "metadata": {
        "id": "6eveLzxiLlUm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.means = [0.5482, 0.4620, 0.3602, 0.0127]  #these values were copied from the previous cell\n",
        "dataset.stds = [0.1639, 0.1761, 0.2659, 0.0035]   #copy and paste the values to avoid having\n",
        "                                                  # to rerun the previous cell for every iteration"
      ],
      "metadata": {
        "id": "qAp_KbHvLl0P"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set device"
      ],
      "metadata": {
        "id": "B9is_me-LndB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available else 'cpu')"
      ],
      "metadata": {
        "id": "AhORwK3MLo4y"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model"
      ],
      "metadata": {
        "id": "7dMdA76JLuEK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FirstStageModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FirstStageModel, self).__init__()\n",
        "        # RGB Model\n",
        "        self.rgb_processing_block = nn.Sequential(nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1),\n",
        "                                                  nn.Conv2d(32, 3, kernel_size=1),\n",
        "                                                  nn.AdaptiveAvgPool2d((224, 224)))\n",
        "        self.rgb_encoder = models.resnet18(pretrained=True)\n",
        "        self.rgb_regressor = nn.Sequential(nn.Dropout(0.5),\n",
        "                                           nn.Linear(1000, 256),\n",
        "                                           nn.ReLU(),\n",
        "                                           nn.Dropout(0.5),\n",
        "                                           nn.Linear(256, 3),\n",
        "                                           nn.ReLU())\n",
        "\n",
        "\n",
        "        # Depth Model\n",
        "        self.depth_processing_block = nn.Sequential(nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1),\n",
        "                                         nn.Conv2d(32, 1, kernel_size=1),\n",
        "                                         nn.AdaptiveAvgPool2d((224, 224)))\n",
        "        self.depth_encoder = models.resnet50(pretrained=False)\n",
        "        self.depth_encoder.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.depth_regressor = nn.Sequential(nn.Dropout(0.5),\n",
        "                                             nn.Linear(1000, 256),\n",
        "                                             nn.ReLU(),\n",
        "                                             nn.Dropout(0.5),\n",
        "                                             nn.Linear(256, 1),\n",
        "                                             nn.ReLU())\n",
        "\n",
        "        self.final = nn.Sequential(nn.Dropout(0.5),\n",
        "                                   nn.Linear(4, 2048),\n",
        "                                   nn.ReLU(),\n",
        "                                   nn.Dropout(0.5),\n",
        "                                   nn.Linear(2048, 2048),\n",
        "                                   nn.ReLU(),\n",
        "                                   nn.Dropout(0.5),\n",
        "                                   nn.Linear(2048, 3),\n",
        "                                   nn.ReLU())\n",
        "\n",
        "    def forward(self, rgb, depth):\n",
        "        rgb_out = self.rgb_processing_block(rgb)\n",
        "        rgb_out = self.rgb_encoder(rgb_out)\n",
        "        rgb_out = self.rgb_regressor(rgb_out)\n",
        "\n",
        "        depth_out = self.depth_processing_block(depth)\n",
        "        depth_out = self.depth_encoder(depth_out)\n",
        "        output2 = self.depth_regressor(depth_out)  # height\n",
        "\n",
        "        output1 = torch.cat([rgb_out, output2], dim=1)\n",
        "        output1 = self.final(output1)  # fresh weight, dry weight, diameter\n",
        "\n",
        "        return output1, output2"
      ],
      "metadata": {
        "id": "aeLgAf9vLvPc"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = FirstStageModel()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EryoPktnMl9w",
        "outputId": "2eeffd75-8abf-459b-d708-e0e390ab7226"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:01<00:00, 57.6MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameter"
      ],
      "metadata": {
        "id": "jgW2kfIZMs_1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 0.0005\n",
        "epochs = 200\n",
        "batch_size = 32"
      ],
      "metadata": {
        "id": "RftkG91CMu32"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NMSE loss"
      ],
      "metadata": {
        "id": "XPEmPNzLNG6k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NMSELoss(nn.Module):\n",
        "    def __init__(self):\n",
        "          # super(diceloss, self).init()\n",
        "        super(NMSELoss, self).__init__()\n",
        "          # print('HI')\n",
        "    def forward(self, pred, target):\n",
        "        if target.size() != pred.size():\n",
        "              raise ValueError(\"Target size ({}) must be the same as input size ({})\".format(target.size(), pred.size()))\n",
        "\n",
        "        num=torch.sum((target-pred)**2,0)\n",
        "        den=torch.sum(target**2,0)\n",
        "\n",
        "        return torch.sum(num/den)"
      ],
      "metadata": {
        "id": "W7B_B5Y9NKbH"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loss and optimizer"
      ],
      "metadata": {
        "id": "dbPA1SYpNAqi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = NMSELoss()\n",
        "# optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "optimizer = torch.optim.Adam(model.parameters(),\n",
        "                            lr=lr,\n",
        "                            betas=(0.9, 0.999),\n",
        "                            eps=1e-08,\n",
        "                            weight_decay = 0,\n",
        "                            amsgrad = False)"
      ],
      "metadata": {
        "id": "QgiJS3F_NCXm"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train"
      ],
      "metadata": {
        "id": "W3mXlgx1M3th"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_single_epoch(model, dataset, device,\n",
        "                       criterion, optimizer,\n",
        "                       writer, epoch, train_loader):\n",
        "    model.train()\n",
        "\n",
        "    dataset.rgb_transforms = get_transforms(train=True, means=dataset.means[:3], stds=dataset.stds[:3])\n",
        "    dataset.d_transforms = get_transforms(train=True, means=dataset.means[3:], stds=dataset.stds[3:])\n",
        "\n",
        "    for i, (rgb, depth, label) in enumerate(train_loader):\n",
        "        rgb = rgb.to(device)\n",
        "        depth = depth.to(device)\n",
        "        label = label.to(device)  # ['FreshWeightShoot', 'DryWeightShoot', 'Height', 'Diameter', 'LeafArea']\n",
        "\n",
        "        # Forward pass - First stage\n",
        "        pred1, pred2 = model(rgb, depth)  # pred1: fresh weight, dry weight, diameter\n",
        "                                                      # pred2: height\n",
        "\n",
        "        # Calculate loss\n",
        "        pred = torch.cat([pred1[:, :2], pred2], dim=1)\n",
        "        pred = torch.cat([pred, pred1[:, 2:]], dim=1)\n",
        "        loss = criterion(pred, label[:, :4])\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{epochs}, Batch {i+1}/{len(train_loader)}, Loss: {loss.item()}')\n",
        "        with open('run.txt', 'a') as f:\n",
        "            f.write('\\n')\n",
        "            f.write('Train MSE: '+ str(loss.tolist()))\n",
        ""
      ],
      "metadata": {
        "id": "atHQ5B1XM5NP"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(model, dataset, device, sav_dir, criterion, writer, epoch, val_loader, best_val_loss):\n",
        "    current_val_loss = 0\n",
        "    # training_val_loss=0s\n",
        "\n",
        "    model.eval()\n",
        "    print('Validating and Checkpointing!')\n",
        "\n",
        "    dataset.rgb_transforms = get_transforms(train=True, means=dataset.means[:3], stds=dataset.stds[:3])\n",
        "    dataset.d_transforms = get_transforms(train=True, means=dataset.means[3:], stds=dataset.stds[3:])\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (rgb, depth, label) in enumerate(val_loader):\n",
        "            rgb = rgb.to(device)\n",
        "            depth = depth.to(device)\n",
        "            label = label.to(device)\n",
        "\n",
        "            pred1, pred2 = model(rgb, depth)\n",
        "\n",
        "            pred = torch.cat([pred1[:, :2], pred2], dim=1)\n",
        "            pred = torch.cat([pred, pred1[:, 2:]], dim=1)\n",
        "            loss = criterion(pred, label[:, :4])\n",
        "            # acc=nmse(preds.detach(), targets)\n",
        "            current_val_loss = current_val_loss + loss.item()\n",
        "            # training_val_loss=training_val_loss+loss.detach().cpu().numpy()\n",
        "\n",
        "        # writer.add_scalar(\"MSE Loss/val\", training_val_loss, epoch)\n",
        "        writer.add_scalar(\"MSE Loss/val\", current_val_loss, epoch)\n",
        "\n",
        "    if current_val_loss < best_val_loss or epoch == 0:\n",
        "        best_val_loss = current_val_loss\n",
        "        torch.save(model.state_dict(), sav_dir+'bestmodel' + '.pth')\n",
        "        print('Best model Saved! Val MSE: ', str(best_val_loss))\n",
        "        with open('run.txt', 'a') as f:\n",
        "            f.write('\\n')\n",
        "            f.write('Best model Saved! Val MSE: '+ str(best_val_loss))\n",
        "\n",
        "    else:\n",
        "        print('Model is not good (might be overfitting)! Current val MSE: ', str(current_val_loss), 'Best Val MSE: ', str(best_val_loss))\n",
        "        with open('run.txt', 'a') as f:\n",
        "            f.write('\\n')\n",
        "            f.write('Model is not good (might be overfitting)! Current val MSE: '+ str(current_val_loss)+ 'Best Val MSE: '+ str(best_val_loss))\n",
        "    return best_val_loss"
      ],
      "metadata": {
        "id": "ieE0dxwgP3F0"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)\n",
        "\n",
        "best_val_loss = 9999999 # initial dummy value\n",
        "current_val_loss = 0\n",
        "\n",
        "writer = SummaryWriter()\n",
        "start = time.time()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    with open('run.txt', 'a') as f:\n",
        "                f.write('\\n')\n",
        "                f.write('Epoch: '+ str(epoch + 1) + ', Time Elapsed: '+ str((time.time()-start)/60) + ' mins')\n",
        "    print('Epoch: ', str(epoch + 1), ', Time Elapsed: ', str((time.time()-start)/60), ' mins')\n",
        "    train_single_epoch(model, dataset, device,\n",
        "                        criterion, optimizer,\n",
        "                        writer, epoch, train_loader)\n",
        "    best_val_loss = validate(model, dataset, device, sav_dir,\n",
        "                                criterion, writer, epoch, val_loader, best_val_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B38GTKhJP6rJ",
        "outputId": "bffef5e6-8a87-4cef-8539-0225c7be0655"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:  1 , Time Elapsed:  6.2068303426106775e-06  mins\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43m스트리밍 출력 내용이 길어서 마지막 5000줄이 삭제되었습니다.\u001b[0m\n",
            "Epoch 96/200, Batch 40/45, Loss: 1.7116940021514893\n",
            "Epoch 96/200, Batch 41/45, Loss: 2.3889505863189697\n",
            "Epoch 96/200, Batch 42/45, Loss: 3.0428314208984375\n",
            "Epoch 96/200, Batch 43/45, Loss: 2.5612220764160156\n",
            "Epoch 96/200, Batch 44/45, Loss: 1.2447409629821777\n",
            "Epoch 96/200, Batch 45/45, Loss: 34.119869232177734\n",
            "Validating and Checkpointing!\n",
            "Model is not good (might be overfitting)! Current val MSE:  28.008179664611816 Best Val MSE:  25.10259437561035\n",
            "Epoch:  97 , Time Elapsed:  31.394042102495828  mins\n",
            "Epoch 97/200, Batch 1/45, Loss: 2.6198554039001465\n",
            "Epoch 97/200, Batch 2/45, Loss: 3.1290123462677\n",
            "Epoch 97/200, Batch 3/45, Loss: 3.134857177734375\n",
            "Epoch 97/200, Batch 4/45, Loss: 3.4151227474212646\n",
            "Epoch 97/200, Batch 5/45, Loss: 2.3956820964813232\n",
            "Epoch 97/200, Batch 6/45, Loss: 2.9125547409057617\n",
            "Epoch 97/200, Batch 7/45, Loss: 3.142975330352783\n",
            "Epoch 97/200, Batch 8/45, Loss: 3.1159141063690186\n",
            "Epoch 97/200, Batch 9/45, Loss: 2.716146230697632\n",
            "Epoch 97/200, Batch 10/45, Loss: 2.8178834915161133\n",
            "Epoch 97/200, Batch 11/45, Loss: 3.318174362182617\n",
            "Epoch 97/200, Batch 12/45, Loss: 3.1025185585021973\n",
            "Epoch 97/200, Batch 13/45, Loss: 3.197406530380249\n",
            "Epoch 97/200, Batch 14/45, Loss: 2.7684316635131836\n",
            "Epoch 97/200, Batch 15/45, Loss: 2.517228364944458\n",
            "Epoch 97/200, Batch 16/45, Loss: 2.655975580215454\n",
            "Epoch 97/200, Batch 17/45, Loss: 3.3781394958496094\n",
            "Epoch 97/200, Batch 18/45, Loss: 2.9043021202087402\n",
            "Epoch 97/200, Batch 19/45, Loss: 2.6806674003601074\n",
            "Epoch 97/200, Batch 20/45, Loss: 2.479123115539551\n",
            "Epoch 97/200, Batch 21/45, Loss: 2.8851261138916016\n",
            "Epoch 97/200, Batch 22/45, Loss: 2.736499309539795\n",
            "Epoch 97/200, Batch 23/45, Loss: 2.6805038452148438\n",
            "Epoch 97/200, Batch 24/45, Loss: 2.8959569931030273\n",
            "Epoch 97/200, Batch 25/45, Loss: 2.550471305847168\n",
            "Epoch 97/200, Batch 26/45, Loss: 2.4961299896240234\n",
            "Epoch 97/200, Batch 27/45, Loss: 2.8898775577545166\n",
            "Epoch 97/200, Batch 28/45, Loss: 2.147834300994873\n",
            "Epoch 97/200, Batch 29/45, Loss: 2.70442795753479\n",
            "Epoch 97/200, Batch 30/45, Loss: 2.592463731765747\n",
            "Epoch 97/200, Batch 31/45, Loss: 2.4576199054718018\n",
            "Epoch 97/200, Batch 32/45, Loss: 2.1497507095336914\n",
            "Epoch 97/200, Batch 33/45, Loss: 2.320298433303833\n",
            "Epoch 97/200, Batch 34/45, Loss: 2.4174304008483887\n",
            "Epoch 97/200, Batch 35/45, Loss: 2.707698345184326\n",
            "Epoch 97/200, Batch 36/45, Loss: 2.4079201221466064\n",
            "Epoch 97/200, Batch 37/45, Loss: 2.841134786605835\n",
            "Epoch 97/200, Batch 38/45, Loss: 2.18904972076416\n",
            "Epoch 97/200, Batch 39/45, Loss: 2.4642984867095947\n",
            "Epoch 97/200, Batch 40/45, Loss: 2.802476406097412\n",
            "Epoch 97/200, Batch 41/45, Loss: 2.37276029586792\n",
            "Epoch 97/200, Batch 42/45, Loss: 2.27614426612854\n",
            "Epoch 97/200, Batch 43/45, Loss: 2.901001453399658\n",
            "Epoch 97/200, Batch 44/45, Loss: 2.61370587348938\n",
            "Epoch 97/200, Batch 45/45, Loss: 2.6102817058563232\n",
            "Validating and Checkpointing!\n",
            "Model is not good (might be overfitting)! Current val MSE:  26.92062211036682 Best Val MSE:  25.10259437561035\n",
            "Epoch:  98 , Time Elapsed:  31.733195610841115  mins\n",
            "Epoch 98/200, Batch 1/45, Loss: 2.3615269660949707\n",
            "Epoch 98/200, Batch 2/45, Loss: 2.2173361778259277\n",
            "Epoch 98/200, Batch 3/45, Loss: 2.4314160346984863\n",
            "Epoch 98/200, Batch 4/45, Loss: 2.2909326553344727\n",
            "Epoch 98/200, Batch 5/45, Loss: 2.431417942047119\n",
            "Epoch 98/200, Batch 6/45, Loss: 2.17844295501709\n",
            "Epoch 98/200, Batch 7/45, Loss: 2.83880615234375\n",
            "Epoch 98/200, Batch 8/45, Loss: 2.344845771789551\n",
            "Epoch 98/200, Batch 9/45, Loss: 2.2926435470581055\n",
            "Epoch 98/200, Batch 10/45, Loss: 2.7345199584960938\n",
            "Epoch 98/200, Batch 11/45, Loss: 2.7405693531036377\n",
            "Epoch 98/200, Batch 12/45, Loss: 2.7436676025390625\n",
            "Epoch 98/200, Batch 13/45, Loss: 2.746382236480713\n",
            "Epoch 98/200, Batch 14/45, Loss: 2.5980911254882812\n",
            "Epoch 98/200, Batch 15/45, Loss: 2.4956259727478027\n",
            "Epoch 98/200, Batch 16/45, Loss: 2.8506765365600586\n",
            "Epoch 98/200, Batch 17/45, Loss: 2.3875157833099365\n",
            "Epoch 98/200, Batch 18/45, Loss: 2.62620210647583\n",
            "Epoch 98/200, Batch 19/45, Loss: 2.4132909774780273\n",
            "Epoch 98/200, Batch 20/45, Loss: 2.68110728263855\n",
            "Epoch 98/200, Batch 21/45, Loss: 2.033956289291382\n",
            "Epoch 98/200, Batch 22/45, Loss: 2.1822588443756104\n",
            "Epoch 98/200, Batch 23/45, Loss: 2.0816781520843506\n",
            "Epoch 98/200, Batch 24/45, Loss: 2.84352445602417\n",
            "Epoch 98/200, Batch 25/45, Loss: 2.1027891635894775\n",
            "Epoch 98/200, Batch 26/45, Loss: 2.4893994331359863\n",
            "Epoch 98/200, Batch 27/45, Loss: 3.206303119659424\n",
            "Epoch 98/200, Batch 28/45, Loss: 2.144857168197632\n",
            "Epoch 98/200, Batch 29/45, Loss: 2.000013589859009\n",
            "Epoch 98/200, Batch 30/45, Loss: 2.183091163635254\n",
            "Epoch 98/200, Batch 31/45, Loss: 1.7361397743225098\n",
            "Epoch 98/200, Batch 32/45, Loss: 1.5615838766098022\n",
            "Epoch 98/200, Batch 33/45, Loss: 2.174647331237793\n",
            "Epoch 98/200, Batch 34/45, Loss: 2.9680328369140625\n",
            "Epoch 98/200, Batch 35/45, Loss: 2.236638307571411\n",
            "Epoch 98/200, Batch 36/45, Loss: 2.5632991790771484\n",
            "Epoch 98/200, Batch 37/45, Loss: 2.885485887527466\n",
            "Epoch 98/200, Batch 38/45, Loss: 1.8766794204711914\n",
            "Epoch 98/200, Batch 39/45, Loss: 2.0306499004364014\n",
            "Epoch 98/200, Batch 40/45, Loss: 2.3654847145080566\n",
            "Epoch 98/200, Batch 41/45, Loss: 2.5030274391174316\n",
            "Epoch 98/200, Batch 42/45, Loss: 2.183516502380371\n",
            "Epoch 98/200, Batch 43/45, Loss: 2.4358108043670654\n",
            "Epoch 98/200, Batch 44/45, Loss: 2.017995595932007\n",
            "Epoch 98/200, Batch 45/45, Loss: 2.0866141319274902\n",
            "Validating and Checkpointing!\n",
            "Model is not good (might be overfitting)! Current val MSE:  27.291505336761475 Best Val MSE:  25.10259437561035\n",
            "Epoch:  99 , Time Elapsed:  32.04779710769653  mins\n",
            "Epoch 99/200, Batch 1/45, Loss: 2.481382369995117\n",
            "Epoch 99/200, Batch 2/45, Loss: 2.0820953845977783\n",
            "Epoch 99/200, Batch 3/45, Loss: 2.052873373031616\n",
            "Epoch 99/200, Batch 4/45, Loss: 2.00881290435791\n",
            "Epoch 99/200, Batch 5/45, Loss: 2.497009754180908\n",
            "Epoch 99/200, Batch 6/45, Loss: 2.3500072956085205\n",
            "Epoch 99/200, Batch 7/45, Loss: 2.331965923309326\n",
            "Epoch 99/200, Batch 8/45, Loss: 3.0471231937408447\n",
            "Epoch 99/200, Batch 9/45, Loss: 2.1959245204925537\n",
            "Epoch 99/200, Batch 10/45, Loss: 2.1795461177825928\n",
            "Epoch 99/200, Batch 11/45, Loss: 1.7530794143676758\n",
            "Epoch 99/200, Batch 12/45, Loss: 1.797995924949646\n",
            "Epoch 99/200, Batch 13/45, Loss: 2.577286720275879\n",
            "Epoch 99/200, Batch 14/45, Loss: 2.796060085296631\n",
            "Epoch 99/200, Batch 15/45, Loss: 2.609131097793579\n",
            "Epoch 99/200, Batch 16/45, Loss: 2.0880494117736816\n",
            "Epoch 99/200, Batch 17/45, Loss: 2.3536949157714844\n",
            "Epoch 99/200, Batch 18/45, Loss: 2.227597713470459\n",
            "Epoch 99/200, Batch 19/45, Loss: 1.837922215461731\n",
            "Epoch 99/200, Batch 20/45, Loss: 1.9384970664978027\n",
            "Epoch 99/200, Batch 21/45, Loss: 2.221808433532715\n",
            "Epoch 99/200, Batch 22/45, Loss: 2.8197929859161377\n",
            "Epoch 99/200, Batch 23/45, Loss: 2.4120473861694336\n",
            "Epoch 99/200, Batch 24/45, Loss: 2.4294302463531494\n",
            "Epoch 99/200, Batch 25/45, Loss: 2.3915836811065674\n",
            "Epoch 99/200, Batch 26/45, Loss: 1.5938657522201538\n",
            "Epoch 99/200, Batch 27/45, Loss: 1.8652175664901733\n",
            "Epoch 99/200, Batch 28/45, Loss: 1.9372837543487549\n",
            "Epoch 99/200, Batch 29/45, Loss: 2.6104865074157715\n",
            "Epoch 99/200, Batch 30/45, Loss: 2.334465742111206\n",
            "Epoch 99/200, Batch 31/45, Loss: 1.8551173210144043\n",
            "Epoch 99/200, Batch 32/45, Loss: 1.5824966430664062\n",
            "Epoch 99/200, Batch 33/45, Loss: 2.335935115814209\n",
            "Epoch 99/200, Batch 34/45, Loss: 2.203737258911133\n",
            "Epoch 99/200, Batch 35/45, Loss: 1.879895806312561\n",
            "Epoch 99/200, Batch 36/45, Loss: 2.6954147815704346\n",
            "Epoch 99/200, Batch 37/45, Loss: 1.8663763999938965\n",
            "Epoch 99/200, Batch 38/45, Loss: 2.2335684299468994\n",
            "Epoch 99/200, Batch 39/45, Loss: 2.3103229999542236\n",
            "Epoch 99/200, Batch 40/45, Loss: 2.758772134780884\n",
            "Epoch 99/200, Batch 41/45, Loss: 2.3973288536071777\n",
            "Epoch 99/200, Batch 42/45, Loss: 2.2716755867004395\n",
            "Epoch 99/200, Batch 43/45, Loss: 1.560900092124939\n",
            "Epoch 99/200, Batch 44/45, Loss: 2.5804190635681152\n",
            "Epoch 99/200, Batch 45/45, Loss: 2.1873679161071777\n",
            "Validating and Checkpointing!\n",
            "Model is not good (might be overfitting)! Current val MSE:  25.345254778862 Best Val MSE:  25.10259437561035\n",
            "Epoch:  100 , Time Elapsed:  32.4069170554479  mins\n",
            "Epoch 100/200, Batch 1/45, Loss: 1.949082851409912\n",
            "Epoch 100/200, Batch 2/45, Loss: 2.4268901348114014\n",
            "Epoch 100/200, Batch 3/45, Loss: 2.556417942047119\n",
            "Epoch 100/200, Batch 4/45, Loss: 2.3098111152648926\n",
            "Epoch 100/200, Batch 5/45, Loss: 2.0200607776641846\n",
            "Epoch 100/200, Batch 6/45, Loss: 2.1846652030944824\n",
            "Epoch 100/200, Batch 7/45, Loss: 2.739595413208008\n",
            "Epoch 100/200, Batch 8/45, Loss: 2.4325461387634277\n",
            "Epoch 100/200, Batch 9/45, Loss: 2.3618783950805664\n",
            "Epoch 100/200, Batch 10/45, Loss: 1.9807422161102295\n",
            "Epoch 100/200, Batch 11/45, Loss: 2.2209670543670654\n",
            "Epoch 100/200, Batch 12/45, Loss: 2.1567399501800537\n",
            "Epoch 100/200, Batch 13/45, Loss: 2.154615640640259\n",
            "Epoch 100/200, Batch 14/45, Loss: 2.781452178955078\n",
            "Epoch 100/200, Batch 15/45, Loss: 2.083874464035034\n",
            "Epoch 100/200, Batch 16/45, Loss: 1.70054292678833\n",
            "Epoch 100/200, Batch 17/45, Loss: 2.381197452545166\n",
            "Epoch 100/200, Batch 18/45, Loss: 2.4834744930267334\n",
            "Epoch 100/200, Batch 19/45, Loss: 2.20957612991333\n",
            "Epoch 100/200, Batch 20/45, Loss: 2.1616082191467285\n",
            "Epoch 100/200, Batch 21/45, Loss: 2.3369078636169434\n",
            "Epoch 100/200, Batch 22/45, Loss: 2.0604679584503174\n",
            "Epoch 100/200, Batch 23/45, Loss: 2.2956454753875732\n",
            "Epoch 100/200, Batch 24/45, Loss: 2.7075514793395996\n",
            "Epoch 100/200, Batch 25/45, Loss: 2.3998146057128906\n",
            "Epoch 100/200, Batch 26/45, Loss: 2.0851211547851562\n",
            "Epoch 100/200, Batch 27/45, Loss: 2.186504364013672\n",
            "Epoch 100/200, Batch 28/45, Loss: 1.871455192565918\n",
            "Epoch 100/200, Batch 29/45, Loss: 2.261612892150879\n",
            "Epoch 100/200, Batch 30/45, Loss: 1.8725066184997559\n",
            "Epoch 100/200, Batch 31/45, Loss: 3.14821457862854\n",
            "Epoch 100/200, Batch 32/45, Loss: 2.0874485969543457\n",
            "Epoch 100/200, Batch 33/45, Loss: 1.7411041259765625\n",
            "Epoch 100/200, Batch 34/45, Loss: 1.8039116859436035\n",
            "Epoch 100/200, Batch 35/45, Loss: 2.6564602851867676\n",
            "Epoch 100/200, Batch 36/45, Loss: 1.9925209283828735\n",
            "Epoch 100/200, Batch 37/45, Loss: 2.1926052570343018\n",
            "Epoch 100/200, Batch 38/45, Loss: 2.325429677963257\n",
            "Epoch 100/200, Batch 39/45, Loss: 2.8466906547546387\n",
            "Epoch 100/200, Batch 40/45, Loss: 2.147073984146118\n",
            "Epoch 100/200, Batch 41/45, Loss: 2.236299514770508\n",
            "Epoch 100/200, Batch 42/45, Loss: 1.6075109243392944\n",
            "Epoch 100/200, Batch 43/45, Loss: 2.828545331954956\n",
            "Epoch 100/200, Batch 44/45, Loss: 2.941957950592041\n",
            "Epoch 100/200, Batch 45/45, Loss: 2.139247417449951\n",
            "Validating and Checkpointing!\n",
            "Model is not good (might be overfitting)! Current val MSE:  25.827193021774292 Best Val MSE:  25.10259437561035\n",
            "Epoch:  101 , Time Elapsed:  32.74971999724706  mins\n",
            "Epoch 101/200, Batch 1/45, Loss: 2.3915843963623047\n",
            "Epoch 101/200, Batch 2/45, Loss: 2.0712196826934814\n",
            "Epoch 101/200, Batch 3/45, Loss: 2.494576930999756\n",
            "Epoch 101/200, Batch 4/45, Loss: 1.7577937841415405\n",
            "Epoch 101/200, Batch 5/45, Loss: 1.8976078033447266\n",
            "Epoch 101/200, Batch 6/45, Loss: 2.195809841156006\n",
            "Epoch 101/200, Batch 7/45, Loss: 2.1159749031066895\n",
            "Epoch 101/200, Batch 8/45, Loss: 2.307577133178711\n",
            "Epoch 101/200, Batch 9/45, Loss: 1.8206367492675781\n",
            "Epoch 101/200, Batch 10/45, Loss: 1.9571858644485474\n",
            "Epoch 101/200, Batch 11/45, Loss: 2.061936378479004\n",
            "Epoch 101/200, Batch 12/45, Loss: 2.414508581161499\n",
            "Epoch 101/200, Batch 13/45, Loss: 2.6369521617889404\n",
            "Epoch 101/200, Batch 14/45, Loss: 2.141786575317383\n",
            "Epoch 101/200, Batch 15/45, Loss: 1.9963068962097168\n",
            "Epoch 101/200, Batch 16/45, Loss: 2.4387383460998535\n",
            "Epoch 101/200, Batch 17/45, Loss: 1.6699090003967285\n",
            "Epoch 101/200, Batch 18/45, Loss: 1.8818068504333496\n",
            "Epoch 101/200, Batch 19/45, Loss: 2.1410820484161377\n",
            "Epoch 101/200, Batch 20/45, Loss: 2.216172933578491\n",
            "Epoch 101/200, Batch 21/45, Loss: 2.193005323410034\n",
            "Epoch 101/200, Batch 22/45, Loss: 2.26558256149292\n",
            "Epoch 101/200, Batch 23/45, Loss: 2.5052037239074707\n",
            "Epoch 101/200, Batch 24/45, Loss: 2.554574966430664\n",
            "Epoch 101/200, Batch 25/45, Loss: 2.5037457942962646\n",
            "Epoch 101/200, Batch 26/45, Loss: 2.215794563293457\n",
            "Epoch 101/200, Batch 27/45, Loss: 2.3787596225738525\n",
            "Epoch 101/200, Batch 28/45, Loss: 1.9477214813232422\n",
            "Epoch 101/200, Batch 29/45, Loss: 2.632401466369629\n",
            "Epoch 101/200, Batch 30/45, Loss: 2.1067535877227783\n",
            "Epoch 101/200, Batch 31/45, Loss: 1.748297095298767\n",
            "Epoch 101/200, Batch 32/45, Loss: 1.9558279514312744\n",
            "Epoch 101/200, Batch 33/45, Loss: 1.7670164108276367\n",
            "Epoch 101/200, Batch 34/45, Loss: 2.2590746879577637\n",
            "Epoch 101/200, Batch 35/45, Loss: 2.7453017234802246\n",
            "Epoch 101/200, Batch 36/45, Loss: 2.3078505992889404\n",
            "Epoch 101/200, Batch 37/45, Loss: 1.7496230602264404\n",
            "Epoch 101/200, Batch 38/45, Loss: 1.4763035774230957\n",
            "Epoch 101/200, Batch 39/45, Loss: 2.3421835899353027\n",
            "Epoch 101/200, Batch 40/45, Loss: 1.9739247560501099\n",
            "Epoch 101/200, Batch 41/45, Loss: 2.7872886657714844\n",
            "Epoch 101/200, Batch 42/45, Loss: 2.3770108222961426\n",
            "Epoch 101/200, Batch 43/45, Loss: 4.7079758644104\n",
            "Epoch 101/200, Batch 44/45, Loss: 2.5128164291381836\n",
            "Epoch 101/200, Batch 45/45, Loss: 3.3141887187957764\n",
            "Validating and Checkpointing!\n",
            "Model is not good (might be overfitting)! Current val MSE:  31.087876319885254 Best Val MSE:  25.10259437561035\n",
            "Epoch:  102 , Time Elapsed:  33.06938035090764  mins\n",
            "Epoch 102/200, Batch 1/45, Loss: 2.3683664798736572\n",
            "Epoch 102/200, Batch 2/45, Loss: 2.1884207725524902\n",
            "Epoch 102/200, Batch 3/45, Loss: 2.6195976734161377\n",
            "Epoch 102/200, Batch 4/45, Loss: 2.509136438369751\n",
            "Epoch 102/200, Batch 5/45, Loss: 2.2020645141601562\n",
            "Epoch 102/200, Batch 6/45, Loss: 3.0947303771972656\n",
            "Epoch 102/200, Batch 7/45, Loss: 2.6552858352661133\n",
            "Epoch 102/200, Batch 8/45, Loss: 2.0490903854370117\n",
            "Epoch 102/200, Batch 9/45, Loss: 1.8591604232788086\n",
            "Epoch 102/200, Batch 10/45, Loss: 2.451814651489258\n",
            "Epoch 102/200, Batch 11/45, Loss: 3.2690463066101074\n",
            "Epoch 102/200, Batch 12/45, Loss: 2.197375774383545\n",
            "Epoch 102/200, Batch 13/45, Loss: 2.1616902351379395\n",
            "Epoch 102/200, Batch 14/45, Loss: 2.079489231109619\n",
            "Epoch 102/200, Batch 15/45, Loss: 2.4186768531799316\n",
            "Epoch 102/200, Batch 16/45, Loss: 2.284090280532837\n",
            "Epoch 102/200, Batch 17/45, Loss: 1.741248369216919\n",
            "Epoch 102/200, Batch 18/45, Loss: 2.424894332885742\n",
            "Epoch 102/200, Batch 19/45, Loss: 2.2668371200561523\n",
            "Epoch 102/200, Batch 20/45, Loss: 1.4940370321273804\n",
            "Epoch 102/200, Batch 21/45, Loss: 2.3743746280670166\n",
            "Epoch 102/200, Batch 22/45, Loss: 2.091856002807617\n",
            "Epoch 102/200, Batch 23/45, Loss: 2.3429412841796875\n",
            "Epoch 102/200, Batch 24/45, Loss: 2.528594493865967\n",
            "Epoch 102/200, Batch 25/45, Loss: 2.4419970512390137\n",
            "Epoch 102/200, Batch 26/45, Loss: 2.267531633377075\n",
            "Epoch 102/200, Batch 27/45, Loss: 2.2883896827697754\n",
            "Epoch 102/200, Batch 28/45, Loss: 2.295609712600708\n",
            "Epoch 102/200, Batch 29/45, Loss: 2.185244560241699\n",
            "Epoch 102/200, Batch 30/45, Loss: 2.147195816040039\n",
            "Epoch 102/200, Batch 31/45, Loss: 1.4582364559173584\n",
            "Epoch 102/200, Batch 32/45, Loss: 2.587554454803467\n",
            "Epoch 102/200, Batch 33/45, Loss: 2.3965134620666504\n",
            "Epoch 102/200, Batch 34/45, Loss: 1.880743145942688\n",
            "Epoch 102/200, Batch 35/45, Loss: 2.004216432571411\n",
            "Epoch 102/200, Batch 36/45, Loss: 1.745701789855957\n",
            "Epoch 102/200, Batch 37/45, Loss: 2.2353501319885254\n",
            "Epoch 102/200, Batch 38/45, Loss: 2.288093328475952\n",
            "Epoch 102/200, Batch 39/45, Loss: 2.517799139022827\n",
            "Epoch 102/200, Batch 40/45, Loss: 1.8747262954711914\n",
            "Epoch 102/200, Batch 41/45, Loss: 1.8971989154815674\n",
            "Epoch 102/200, Batch 42/45, Loss: 1.8479735851287842\n",
            "Epoch 102/200, Batch 43/45, Loss: 1.8462498188018799\n",
            "Epoch 102/200, Batch 44/45, Loss: 1.7365729808807373\n",
            "Epoch 102/200, Batch 45/45, Loss: 2.3885951042175293\n",
            "Validating and Checkpointing!\n",
            "Model is not good (might be overfitting)! Current val MSE:  25.680355072021484 Best Val MSE:  25.10259437561035\n",
            "Epoch:  103 , Time Elapsed:  33.39185785849889  mins\n",
            "Epoch 103/200, Batch 1/45, Loss: 2.0640711784362793\n",
            "Epoch 103/200, Batch 2/45, Loss: 2.045222520828247\n",
            "Epoch 103/200, Batch 3/45, Loss: 2.6395678520202637\n",
            "Epoch 103/200, Batch 4/45, Loss: 2.78505277633667\n",
            "Epoch 103/200, Batch 5/45, Loss: 2.086215019226074\n",
            "Epoch 103/200, Batch 6/45, Loss: 2.6232385635375977\n",
            "Epoch 103/200, Batch 7/45, Loss: 2.0973644256591797\n",
            "Epoch 103/200, Batch 8/45, Loss: 1.8689920902252197\n",
            "Epoch 103/200, Batch 9/45, Loss: 1.7894103527069092\n",
            "Epoch 103/200, Batch 10/45, Loss: 4.222102165222168\n",
            "Epoch 103/200, Batch 11/45, Loss: 1.8639607429504395\n",
            "Epoch 103/200, Batch 12/45, Loss: 3.5485262870788574\n",
            "Epoch 103/200, Batch 13/45, Loss: 1.9653747081756592\n",
            "Epoch 103/200, Batch 14/45, Loss: 2.8934381008148193\n",
            "Epoch 103/200, Batch 15/45, Loss: 2.520242929458618\n",
            "Epoch 103/200, Batch 16/45, Loss: 2.104823589324951\n",
            "Epoch 103/200, Batch 17/45, Loss: 2.570733070373535\n",
            "Epoch 103/200, Batch 18/45, Loss: 2.390632152557373\n",
            "Epoch 103/200, Batch 19/45, Loss: 2.0288515090942383\n",
            "Epoch 103/200, Batch 20/45, Loss: 1.5148255825042725\n",
            "Epoch 103/200, Batch 21/45, Loss: 2.171088695526123\n",
            "Epoch 103/200, Batch 22/45, Loss: 2.432955026626587\n",
            "Epoch 103/200, Batch 23/45, Loss: 2.0193381309509277\n",
            "Epoch 103/200, Batch 24/45, Loss: 2.1621036529541016\n",
            "Epoch 103/200, Batch 25/45, Loss: 1.6950271129608154\n",
            "Epoch 103/200, Batch 26/45, Loss: 1.7056913375854492\n",
            "Epoch 103/200, Batch 27/45, Loss: 2.0940990447998047\n",
            "Epoch 103/200, Batch 28/45, Loss: 2.4576168060302734\n",
            "Epoch 103/200, Batch 29/45, Loss: 2.6766185760498047\n",
            "Epoch 103/200, Batch 30/45, Loss: 1.8586554527282715\n",
            "Epoch 103/200, Batch 31/45, Loss: 2.040691614151001\n",
            "Epoch 103/200, Batch 32/45, Loss: 2.3451333045959473\n",
            "Epoch 103/200, Batch 33/45, Loss: 1.9994399547576904\n",
            "Epoch 103/200, Batch 34/45, Loss: 1.5787713527679443\n",
            "Epoch 103/200, Batch 35/45, Loss: 2.106117010116577\n",
            "Epoch 103/200, Batch 36/45, Loss: 2.31596302986145\n",
            "Epoch 103/200, Batch 37/45, Loss: 1.8413913249969482\n",
            "Epoch 103/200, Batch 38/45, Loss: 2.358081340789795\n",
            "Epoch 103/200, Batch 39/45, Loss: 2.059372663497925\n",
            "Epoch 103/200, Batch 40/45, Loss: 2.0004611015319824\n",
            "Epoch 103/200, Batch 41/45, Loss: 1.9124133586883545\n",
            "Epoch 103/200, Batch 42/45, Loss: 1.931386113166809\n",
            "Epoch 103/200, Batch 43/45, Loss: 2.73616361618042\n",
            "Epoch 103/200, Batch 44/45, Loss: 1.568676233291626\n",
            "Epoch 103/200, Batch 45/45, Loss: 2.5712876319885254\n",
            "Validating and Checkpointing!\n",
            "Model is not good (might be overfitting)! Current val MSE:  31.56059718132019 Best Val MSE:  25.10259437561035\n",
            "Epoch:  104 , Time Elapsed:  33.74059776465098  mins\n",
            "Epoch 104/200, Batch 1/45, Loss: 1.83174729347229\n",
            "Epoch 104/200, Batch 2/45, Loss: 2.8915512561798096\n",
            "Epoch 104/200, Batch 3/45, Loss: 1.9950196743011475\n",
            "Epoch 104/200, Batch 4/45, Loss: 2.254426956176758\n",
            "Epoch 104/200, Batch 5/45, Loss: 1.7761387825012207\n",
            "Epoch 104/200, Batch 6/45, Loss: 2.1693830490112305\n",
            "Epoch 104/200, Batch 7/45, Loss: 2.1214187145233154\n",
            "Epoch 104/200, Batch 8/45, Loss: 2.639310359954834\n",
            "Epoch 104/200, Batch 9/45, Loss: 2.8859341144561768\n",
            "Epoch 104/200, Batch 10/45, Loss: 1.9121520519256592\n",
            "Epoch 104/200, Batch 11/45, Loss: 2.1619763374328613\n",
            "Epoch 104/200, Batch 12/45, Loss: 2.846142530441284\n",
            "Epoch 104/200, Batch 13/45, Loss: 2.3909807205200195\n",
            "Epoch 104/200, Batch 14/45, Loss: 2.640331506729126\n",
            "Epoch 104/200, Batch 15/45, Loss: 1.725965976715088\n",
            "Epoch 104/200, Batch 16/45, Loss: 2.4085018634796143\n",
            "Epoch 104/200, Batch 17/45, Loss: 2.6525309085845947\n",
            "Epoch 104/200, Batch 18/45, Loss: 1.839780330657959\n",
            "Epoch 104/200, Batch 19/45, Loss: 1.7938933372497559\n",
            "Epoch 104/200, Batch 20/45, Loss: 2.1493139266967773\n",
            "Epoch 104/200, Batch 21/45, Loss: 2.044123888015747\n",
            "Epoch 104/200, Batch 22/45, Loss: 2.3104264736175537\n",
            "Epoch 104/200, Batch 23/45, Loss: 1.8899238109588623\n",
            "Epoch 104/200, Batch 24/45, Loss: 1.5534343719482422\n",
            "Epoch 104/200, Batch 25/45, Loss: 1.9781930446624756\n",
            "Epoch 104/200, Batch 26/45, Loss: 2.4222910404205322\n",
            "Epoch 104/200, Batch 27/45, Loss: 2.1806724071502686\n",
            "Epoch 104/200, Batch 28/45, Loss: 2.277881145477295\n",
            "Epoch 104/200, Batch 29/45, Loss: 2.127732276916504\n",
            "Epoch 104/200, Batch 30/45, Loss: 2.2624258995056152\n",
            "Epoch 104/200, Batch 31/45, Loss: 8.858312606811523\n",
            "Epoch 104/200, Batch 32/45, Loss: 1.6584279537200928\n",
            "Epoch 104/200, Batch 33/45, Loss: 2.3947126865386963\n",
            "Epoch 104/200, Batch 34/45, Loss: 2.609405040740967\n",
            "Epoch 104/200, Batch 35/45, Loss: 2.8027095794677734\n",
            "Epoch 104/200, Batch 36/45, Loss: 2.182251453399658\n",
            "Epoch 104/200, Batch 37/45, Loss: 2.3319971561431885\n",
            "Epoch 104/200, Batch 38/45, Loss: 2.997316837310791\n",
            "Epoch 104/200, Batch 39/45, Loss: 2.2102584838867188\n",
            "Epoch 104/200, Batch 40/45, Loss: 2.4806020259857178\n",
            "Epoch 104/200, Batch 41/45, Loss: 2.6827802658081055\n",
            "Epoch 104/200, Batch 42/45, Loss: 2.293802499771118\n",
            "Epoch 104/200, Batch 43/45, Loss: 1.9055566787719727\n",
            "Epoch 104/200, Batch 44/45, Loss: 2.2623844146728516\n",
            "Epoch 104/200, Batch 45/45, Loss: 2.4361989498138428\n",
            "Validating and Checkpointing!\n",
            "Model is not good (might be overfitting)! Current val MSE:  27.294626712799072 Best Val MSE:  25.10259437561035\n",
            "Epoch:  105 , Time Elapsed:  34.0496587395668  mins\n",
            "Epoch 105/200, Batch 1/45, Loss: 2.071106433868408\n",
            "Epoch 105/200, Batch 2/45, Loss: 2.353386878967285\n",
            "Epoch 105/200, Batch 3/45, Loss: 2.3922975063323975\n",
            "Epoch 105/200, Batch 4/45, Loss: 2.236618757247925\n",
            "Epoch 105/200, Batch 5/45, Loss: 2.044574022293091\n",
            "Epoch 105/200, Batch 6/45, Loss: 2.4092891216278076\n",
            "Epoch 105/200, Batch 7/45, Loss: 1.9563547372817993\n",
            "Epoch 105/200, Batch 8/45, Loss: 2.360830307006836\n",
            "Epoch 105/200, Batch 9/45, Loss: 2.4608774185180664\n",
            "Epoch 105/200, Batch 10/45, Loss: 2.513362407684326\n",
            "Epoch 105/200, Batch 11/45, Loss: 3.021240711212158\n",
            "Epoch 105/200, Batch 12/45, Loss: 2.3899102210998535\n",
            "Epoch 105/200, Batch 13/45, Loss: 2.5460455417633057\n",
            "Epoch 105/200, Batch 14/45, Loss: 1.9894317388534546\n",
            "Epoch 105/200, Batch 15/45, Loss: 1.928228735923767\n",
            "Epoch 105/200, Batch 16/45, Loss: 2.4574666023254395\n",
            "Epoch 105/200, Batch 17/45, Loss: 2.291383743286133\n",
            "Epoch 105/200, Batch 18/45, Loss: 2.6594724655151367\n",
            "Epoch 105/200, Batch 19/45, Loss: 1.7885022163391113\n",
            "Epoch 105/200, Batch 20/45, Loss: 2.515383720397949\n",
            "Epoch 105/200, Batch 21/45, Loss: 2.422069549560547\n",
            "Epoch 105/200, Batch 22/45, Loss: 1.844474196434021\n",
            "Epoch 105/200, Batch 23/45, Loss: 2.153043746948242\n",
            "Epoch 105/200, Batch 24/45, Loss: 2.4354753494262695\n",
            "Epoch 105/200, Batch 25/45, Loss: 2.4281387329101562\n",
            "Epoch 105/200, Batch 26/45, Loss: 2.3789637088775635\n",
            "Epoch 105/200, Batch 27/45, Loss: 2.8974835872650146\n",
            "Epoch 105/200, Batch 28/45, Loss: 2.2032413482666016\n",
            "Epoch 105/200, Batch 29/45, Loss: 2.507657766342163\n",
            "Epoch 105/200, Batch 30/45, Loss: 1.2245103120803833\n",
            "Epoch 105/200, Batch 31/45, Loss: 1.9488457441329956\n",
            "Epoch 105/200, Batch 32/45, Loss: 2.3949484825134277\n",
            "Epoch 105/200, Batch 33/45, Loss: 2.2831058502197266\n",
            "Epoch 105/200, Batch 34/45, Loss: 1.8572430610656738\n",
            "Epoch 105/200, Batch 35/45, Loss: 1.8632562160491943\n",
            "Epoch 105/200, Batch 36/45, Loss: 1.843773365020752\n",
            "Epoch 105/200, Batch 37/45, Loss: 1.7438868284225464\n",
            "Epoch 105/200, Batch 38/45, Loss: 2.398073196411133\n",
            "Epoch 105/200, Batch 39/45, Loss: 2.0717623233795166\n",
            "Epoch 105/200, Batch 40/45, Loss: 1.5574064254760742\n",
            "Epoch 105/200, Batch 41/45, Loss: 1.908363938331604\n",
            "Epoch 105/200, Batch 42/45, Loss: 2.2565512657165527\n",
            "Epoch 105/200, Batch 43/45, Loss: 2.437485933303833\n",
            "Epoch 105/200, Batch 44/45, Loss: 2.0838217735290527\n",
            "Epoch 105/200, Batch 45/45, Loss: 2.602201461791992\n",
            "Validating and Checkpointing!\n",
            "Model is not good (might be overfitting)! Current val MSE:  28.247649669647217 Best Val MSE:  25.10259437561035\n",
            "Epoch:  106 , Time Elapsed:  34.3750440398852  mins\n",
            "Epoch 106/200, Batch 1/45, Loss: 1.8617432117462158\n",
            "Epoch 106/200, Batch 2/45, Loss: 2.352072238922119\n",
            "Epoch 106/200, Batch 3/45, Loss: 2.1045005321502686\n",
            "Epoch 106/200, Batch 4/45, Loss: 2.327454090118408\n",
            "Epoch 106/200, Batch 5/45, Loss: 2.421283721923828\n",
            "Epoch 106/200, Batch 6/45, Loss: 2.057990550994873\n",
            "Epoch 106/200, Batch 7/45, Loss: 2.4267563819885254\n",
            "Epoch 106/200, Batch 8/45, Loss: 1.8571586608886719\n",
            "Epoch 106/200, Batch 9/45, Loss: 2.3462626934051514\n",
            "Epoch 106/200, Batch 10/45, Loss: 2.285733461380005\n",
            "Epoch 106/200, Batch 11/45, Loss: 2.0831477642059326\n",
            "Epoch 106/200, Batch 12/45, Loss: 2.5454397201538086\n",
            "Epoch 106/200, Batch 13/45, Loss: 2.236583709716797\n",
            "Epoch 106/200, Batch 14/45, Loss: 2.845677614212036\n",
            "Epoch 106/200, Batch 15/45, Loss: 2.4732871055603027\n",
            "Epoch 106/200, Batch 16/45, Loss: 2.2065019607543945\n",
            "Epoch 106/200, Batch 17/45, Loss: 2.7290096282958984\n",
            "Epoch 106/200, Batch 18/45, Loss: 2.046762466430664\n",
            "Epoch 106/200, Batch 19/45, Loss: 1.1800092458724976\n",
            "Epoch 106/200, Batch 20/45, Loss: 2.4064559936523438\n",
            "Epoch 106/200, Batch 21/45, Loss: 2.535501003265381\n",
            "Epoch 106/200, Batch 22/45, Loss: 1.7300969362258911\n",
            "Epoch 106/200, Batch 23/45, Loss: 1.8350646495819092\n",
            "Epoch 106/200, Batch 24/45, Loss: 2.431150197982788\n",
            "Epoch 106/200, Batch 25/45, Loss: 1.8130383491516113\n",
            "Epoch 106/200, Batch 26/45, Loss: 2.122323989868164\n",
            "Epoch 106/200, Batch 27/45, Loss: 2.0484797954559326\n",
            "Epoch 106/200, Batch 28/45, Loss: 3.9177985191345215\n",
            "Epoch 106/200, Batch 29/45, Loss: 2.42232084274292\n",
            "Epoch 106/200, Batch 30/45, Loss: 1.9225993156433105\n",
            "Epoch 106/200, Batch 31/45, Loss: 2.0305843353271484\n",
            "Epoch 106/200, Batch 32/45, Loss: 2.8501412868499756\n",
            "Epoch 106/200, Batch 33/45, Loss: 1.7583839893341064\n",
            "Epoch 106/200, Batch 34/45, Loss: 2.193523645401001\n",
            "Epoch 106/200, Batch 35/45, Loss: 1.776781439781189\n",
            "Epoch 106/200, Batch 36/45, Loss: 2.3322794437408447\n",
            "Epoch 106/200, Batch 37/45, Loss: 2.3771040439605713\n",
            "Epoch 106/200, Batch 38/45, Loss: 2.4205322265625\n",
            "Epoch 106/200, Batch 39/45, Loss: 1.946555256843567\n",
            "Epoch 106/200, Batch 40/45, Loss: 2.2781479358673096\n",
            "Epoch 106/200, Batch 41/45, Loss: 1.9548349380493164\n",
            "Epoch 106/200, Batch 42/45, Loss: 2.2728450298309326\n",
            "Epoch 106/200, Batch 43/45, Loss: 2.3114404678344727\n",
            "Epoch 106/200, Batch 44/45, Loss: 1.6227085590362549\n",
            "Epoch 106/200, Batch 45/45, Loss: 1.8112682104110718\n",
            "Validating and Checkpointing!\n",
            "Model is not good (might be overfitting)! Current val MSE:  32.41359877586365 Best Val MSE:  25.10259437561035\n",
            "Epoch:  107 , Time Elapsed:  34.71812495787938  mins\n",
            "Epoch 107/200, Batch 1/45, Loss: 2.1931405067443848\n",
            "Epoch 107/200, Batch 2/45, Loss: 2.3129994869232178\n",
            "Epoch 107/200, Batch 3/45, Loss: 2.474940538406372\n",
            "Epoch 107/200, Batch 4/45, Loss: 2.223705291748047\n",
            "Epoch 107/200, Batch 5/45, Loss: 2.299311876296997\n",
            "Epoch 107/200, Batch 6/45, Loss: 1.799250602722168\n",
            "Epoch 107/200, Batch 7/45, Loss: 2.121281385421753\n",
            "Epoch 107/200, Batch 8/45, Loss: 2.132452964782715\n",
            "Epoch 107/200, Batch 9/45, Loss: 2.22605562210083\n",
            "Epoch 107/200, Batch 10/45, Loss: 2.3518996238708496\n",
            "Epoch 107/200, Batch 11/45, Loss: 2.263566017150879\n",
            "Epoch 107/200, Batch 12/45, Loss: 1.737380027770996\n",
            "Epoch 107/200, Batch 13/45, Loss: 2.189781665802002\n",
            "Epoch 107/200, Batch 14/45, Loss: 2.7414889335632324\n",
            "Epoch 107/200, Batch 15/45, Loss: 2.1966848373413086\n",
            "Epoch 107/200, Batch 16/45, Loss: 1.7986106872558594\n",
            "Epoch 107/200, Batch 17/45, Loss: 1.5897661447525024\n",
            "Epoch 107/200, Batch 18/45, Loss: 2.164155960083008\n",
            "Epoch 107/200, Batch 19/45, Loss: 2.04753041267395\n",
            "Epoch 107/200, Batch 20/45, Loss: 2.079620361328125\n",
            "Epoch 107/200, Batch 21/45, Loss: 3.0059032440185547\n",
            "Epoch 107/200, Batch 22/45, Loss: 2.2466847896575928\n",
            "Epoch 107/200, Batch 23/45, Loss: 2.0083491802215576\n",
            "Epoch 107/200, Batch 24/45, Loss: 2.303619861602783\n",
            "Epoch 107/200, Batch 25/45, Loss: 1.8608828783035278\n",
            "Epoch 107/200, Batch 26/45, Loss: 1.6125656366348267\n",
            "Epoch 107/200, Batch 27/45, Loss: 2.402250289916992\n",
            "Epoch 107/200, Batch 28/45, Loss: 2.4068121910095215\n",
            "Epoch 107/200, Batch 29/45, Loss: 2.0805273056030273\n",
            "Epoch 107/200, Batch 30/45, Loss: 2.070070266723633\n",
            "Epoch 107/200, Batch 31/45, Loss: 3.2019622325897217\n",
            "Epoch 107/200, Batch 32/45, Loss: 2.1248135566711426\n",
            "Epoch 107/200, Batch 33/45, Loss: 1.9503557682037354\n",
            "Epoch 107/200, Batch 34/45, Loss: 2.8524279594421387\n",
            "Epoch 107/200, Batch 35/45, Loss: 2.064028024673462\n",
            "Epoch 107/200, Batch 36/45, Loss: 2.322504758834839\n",
            "Epoch 107/200, Batch 37/45, Loss: 2.113619327545166\n",
            "Epoch 107/200, Batch 38/45, Loss: 2.0059103965759277\n",
            "Epoch 107/200, Batch 39/45, Loss: 2.124279499053955\n",
            "Epoch 107/200, Batch 40/45, Loss: 2.1375386714935303\n",
            "Epoch 107/200, Batch 41/45, Loss: 2.274038553237915\n",
            "Epoch 107/200, Batch 42/45, Loss: 2.707681179046631\n",
            "Epoch 107/200, Batch 43/45, Loss: 2.8242030143737793\n",
            "Epoch 107/200, Batch 44/45, Loss: 2.48093843460083\n",
            "Epoch 107/200, Batch 45/45, Loss: 2.6363589763641357\n",
            "Validating and Checkpointing!\n",
            "Model is not good (might be overfitting)! Current val MSE:  101.54059314727783 Best Val MSE:  25.10259437561035\n",
            "Epoch:  108 , Time Elapsed:  35.03830066521962  mins\n",
            "Epoch 108/200, Batch 1/45, Loss: 2.2016990184783936\n",
            "Epoch 108/200, Batch 2/45, Loss: 2.1609249114990234\n",
            "Epoch 108/200, Batch 3/45, Loss: 2.437150239944458\n",
            "Epoch 108/200, Batch 4/45, Loss: 2.462815761566162\n",
            "Epoch 108/200, Batch 5/45, Loss: 2.553959846496582\n",
            "Epoch 108/200, Batch 6/45, Loss: 2.4125914573669434\n",
            "Epoch 108/200, Batch 7/45, Loss: 2.658175230026245\n",
            "Epoch 108/200, Batch 8/45, Loss: 2.0079269409179688\n",
            "Epoch 108/200, Batch 9/45, Loss: 1.7198426723480225\n",
            "Epoch 108/200, Batch 10/45, Loss: 2.32871675491333\n",
            "Epoch 108/200, Batch 11/45, Loss: 2.285128116607666\n",
            "Epoch 108/200, Batch 12/45, Loss: 2.009974241256714\n",
            "Epoch 108/200, Batch 13/45, Loss: 2.21866512298584\n",
            "Epoch 108/200, Batch 14/45, Loss: 2.114882707595825\n",
            "Epoch 108/200, Batch 15/45, Loss: 2.6704678535461426\n",
            "Epoch 108/200, Batch 16/45, Loss: 2.1412267684936523\n",
            "Epoch 108/200, Batch 17/45, Loss: 1.6949597597122192\n",
            "Epoch 108/200, Batch 18/45, Loss: 2.565887451171875\n",
            "Epoch 108/200, Batch 19/45, Loss: 2.4417412281036377\n",
            "Epoch 108/200, Batch 20/45, Loss: 2.0622072219848633\n",
            "Epoch 108/200, Batch 21/45, Loss: 2.415050506591797\n",
            "Epoch 108/200, Batch 22/45, Loss: 2.4431211948394775\n",
            "Epoch 108/200, Batch 23/45, Loss: 2.133589744567871\n",
            "Epoch 108/200, Batch 24/45, Loss: 1.9736882448196411\n",
            "Epoch 108/200, Batch 25/45, Loss: 2.3672635555267334\n",
            "Epoch 108/200, Batch 26/45, Loss: 1.8920533657073975\n",
            "Epoch 108/200, Batch 27/45, Loss: 2.735565662384033\n",
            "Epoch 108/200, Batch 28/45, Loss: 2.882448196411133\n",
            "Epoch 108/200, Batch 29/45, Loss: 2.0500683784484863\n",
            "Epoch 108/200, Batch 30/45, Loss: 1.7258210182189941\n",
            "Epoch 108/200, Batch 31/45, Loss: 2.6194918155670166\n",
            "Epoch 108/200, Batch 32/45, Loss: 2.241180419921875\n",
            "Epoch 108/200, Batch 33/45, Loss: 2.1420090198516846\n",
            "Epoch 108/200, Batch 34/45, Loss: 2.8204195499420166\n",
            "Epoch 108/200, Batch 35/45, Loss: 2.222388505935669\n",
            "Epoch 108/200, Batch 36/45, Loss: 2.099893093109131\n",
            "Epoch 108/200, Batch 37/45, Loss: 2.454916477203369\n",
            "Epoch 108/200, Batch 38/45, Loss: 1.8455920219421387\n",
            "Epoch 108/200, Batch 39/45, Loss: 2.265972137451172\n",
            "Epoch 108/200, Batch 40/45, Loss: 1.5206794738769531\n",
            "Epoch 108/200, Batch 41/45, Loss: 2.440797805786133\n",
            "Epoch 108/200, Batch 42/45, Loss: 2.8502092361450195\n",
            "Epoch 108/200, Batch 43/45, Loss: 2.1153664588928223\n",
            "Epoch 108/200, Batch 44/45, Loss: 2.086771011352539\n",
            "Epoch 108/200, Batch 45/45, Loss: 1.9674113988876343\n",
            "Validating and Checkpointing!\n",
            "Model is not good (might be overfitting)! Current val MSE:  30.68168807029724 Best Val MSE:  25.10259437561035\n",
            "Epoch:  109 , Time Elapsed:  35.363374431928  mins\n",
            "Epoch 109/200, Batch 1/45, Loss: 1.8340303897857666\n",
            "Epoch 109/200, Batch 2/45, Loss: 2.0319604873657227\n",
            "Epoch 109/200, Batch 3/45, Loss: 1.849629521369934\n",
            "Epoch 109/200, Batch 4/45, Loss: 2.174041509628296\n",
            "Epoch 109/200, Batch 5/45, Loss: 2.4236388206481934\n",
            "Epoch 109/200, Batch 6/45, Loss: 2.1081337928771973\n",
            "Epoch 109/200, Batch 7/45, Loss: 1.6992466449737549\n",
            "Epoch 109/200, Batch 8/45, Loss: 2.2477128505706787\n",
            "Epoch 109/200, Batch 9/45, Loss: 1.9589570760726929\n",
            "Epoch 109/200, Batch 10/45, Loss: 5.975696086883545\n",
            "Epoch 109/200, Batch 11/45, Loss: 2.605100631713867\n",
            "Epoch 109/200, Batch 12/45, Loss: 2.218040943145752\n",
            "Epoch 109/200, Batch 13/45, Loss: 1.9709205627441406\n",
            "Epoch 109/200, Batch 14/45, Loss: 2.3016531467437744\n",
            "Epoch 109/200, Batch 15/45, Loss: 1.8089513778686523\n",
            "Epoch 109/200, Batch 16/45, Loss: 2.2765469551086426\n",
            "Epoch 109/200, Batch 17/45, Loss: 2.3822128772735596\n",
            "Epoch 109/200, Batch 18/45, Loss: 2.78110671043396\n",
            "Epoch 109/200, Batch 19/45, Loss: 2.9648971557617188\n",
            "Epoch 109/200, Batch 20/45, Loss: 2.889953136444092\n",
            "Epoch 109/200, Batch 21/45, Loss: 3.0889029502868652\n",
            "Epoch 109/200, Batch 22/45, Loss: 3.1368794441223145\n",
            "Epoch 109/200, Batch 23/45, Loss: 2.6388771533966064\n",
            "Epoch 109/200, Batch 24/45, Loss: 3.0237698554992676\n",
            "Epoch 109/200, Batch 25/45, Loss: 2.809750556945801\n",
            "Epoch 109/200, Batch 26/45, Loss: 2.743694305419922\n",
            "Epoch 109/200, Batch 27/45, Loss: 2.7464261054992676\n",
            "Epoch 109/200, Batch 28/45, Loss: 2.2547833919525146\n",
            "Epoch 109/200, Batch 29/45, Loss: 2.0614662170410156\n",
            "Epoch 109/200, Batch 30/45, Loss: 2.578171730041504\n",
            "Epoch 109/200, Batch 31/45, Loss: 3.0165598392486572\n",
            "Epoch 109/200, Batch 32/45, Loss: 2.4301228523254395\n",
            "Epoch 109/200, Batch 33/45, Loss: 2.2223267555236816\n",
            "Epoch 109/200, Batch 34/45, Loss: 2.5396828651428223\n",
            "Epoch 109/200, Batch 35/45, Loss: 2.716435432434082\n",
            "Epoch 109/200, Batch 36/45, Loss: 2.5102524757385254\n",
            "Epoch 109/200, Batch 37/45, Loss: 2.6088881492614746\n",
            "Epoch 109/200, Batch 38/45, Loss: 2.1392202377319336\n",
            "Epoch 109/200, Batch 39/45, Loss: 2.710068941116333\n",
            "Epoch 109/200, Batch 40/45, Loss: 2.43471622467041\n",
            "Epoch 109/200, Batch 41/45, Loss: 2.151038885116577\n",
            "Epoch 109/200, Batch 42/45, Loss: 2.395104169845581\n",
            "Epoch 109/200, Batch 43/45, Loss: 2.6955862045288086\n",
            "Epoch 109/200, Batch 44/45, Loss: 2.0512866973876953\n",
            "Epoch 109/200, Batch 45/45, Loss: 2.0492334365844727\n",
            "Validating and Checkpointing!\n",
            "Model is not good (might be overfitting)! Current val MSE:  28.507675886154175 Best Val MSE:  25.10259437561035\n",
            "Epoch:  110 , Time Elapsed:  35.693222216765086  mins\n",
            "Epoch 110/200, Batch 1/45, Loss: 2.4351444244384766\n",
            "Epoch 110/200, Batch 2/45, Loss: 2.054736852645874\n",
            "Epoch 110/200, Batch 3/45, Loss: 2.3226993083953857\n",
            "Epoch 110/200, Batch 4/45, Loss: 2.648256540298462\n",
            "Epoch 110/200, Batch 5/45, Loss: 1.945211410522461\n",
            "Epoch 110/200, Batch 6/45, Loss: 2.2497334480285645\n",
            "Epoch 110/200, Batch 7/45, Loss: 1.9487918615341187\n",
            "Epoch 110/200, Batch 8/45, Loss: 1.9163216352462769\n",
            "Epoch 110/200, Batch 9/45, Loss: 2.196822166442871\n",
            "Epoch 110/200, Batch 10/45, Loss: 1.4678876399993896\n",
            "Epoch 110/200, Batch 11/45, Loss: 2.2416601181030273\n",
            "Epoch 110/200, Batch 12/45, Loss: 2.430577039718628\n",
            "Epoch 110/200, Batch 13/45, Loss: 2.1034388542175293\n",
            "Epoch 110/200, Batch 14/45, Loss: 2.613926887512207\n",
            "Epoch 110/200, Batch 15/45, Loss: 2.044666051864624\n",
            "Epoch 110/200, Batch 16/45, Loss: 2.1088008880615234\n",
            "Epoch 110/200, Batch 17/45, Loss: 2.0705819129943848\n",
            "Epoch 110/200, Batch 18/45, Loss: 2.4899301528930664\n",
            "Epoch 110/200, Batch 19/45, Loss: 1.4964654445648193\n",
            "Epoch 110/200, Batch 20/45, Loss: 1.6143416166305542\n",
            "Epoch 110/200, Batch 21/45, Loss: 2.3520703315734863\n",
            "Epoch 110/200, Batch 22/45, Loss: 1.5799740552902222\n",
            "Epoch 110/200, Batch 23/45, Loss: 2.1856040954589844\n",
            "Epoch 110/200, Batch 24/45, Loss: 2.083125114440918\n",
            "Epoch 110/200, Batch 25/45, Loss: 2.72324800491333\n",
            "Epoch 110/200, Batch 26/45, Loss: 1.7343871593475342\n",
            "Epoch 110/200, Batch 27/45, Loss: 2.2739176750183105\n",
            "Epoch 110/200, Batch 28/45, Loss: 2.4324123859405518\n",
            "Epoch 110/200, Batch 29/45, Loss: 2.386866331100464\n",
            "Epoch 110/200, Batch 30/45, Loss: 1.8049583435058594\n",
            "Epoch 110/200, Batch 31/45, Loss: 2.4541561603546143\n",
            "Epoch 110/200, Batch 32/45, Loss: 2.319833755493164\n",
            "Epoch 110/200, Batch 33/45, Loss: 2.600534200668335\n",
            "Epoch 110/200, Batch 34/45, Loss: 1.8177452087402344\n",
            "Epoch 110/200, Batch 35/45, Loss: 2.7056305408477783\n",
            "Epoch 110/200, Batch 36/45, Loss: 2.345566987991333\n",
            "Epoch 110/200, Batch 37/45, Loss: 1.9616461992263794\n",
            "Epoch 110/200, Batch 38/45, Loss: 2.435950756072998\n",
            "Epoch 110/200, Batch 39/45, Loss: 2.094621181488037\n",
            "Epoch 110/200, Batch 40/45, Loss: 2.4485456943511963\n",
            "Epoch 110/200, Batch 41/45, Loss: 2.2350881099700928\n",
            "Epoch 110/200, Batch 42/45, Loss: 2.2484211921691895\n",
            "Epoch 110/200, Batch 43/45, Loss: 2.5997564792633057\n",
            "Epoch 110/200, Batch 44/45, Loss: 1.6534833908081055\n",
            "Epoch 110/200, Batch 45/45, Loss: 2.396259307861328\n",
            "Validating and Checkpointing!\n",
            "Model is not good (might be overfitting)! Current val MSE:  27.924325942993164 Best Val MSE:  25.10259437561035\n",
            "Epoch:  111 , Time Elapsed:  36.01307152112325  mins\n",
            "Epoch 111/200, Batch 1/45, Loss: 2.3665521144866943\n",
            "Epoch 111/200, Batch 2/45, Loss: 2.8929576873779297\n",
            "Epoch 111/200, Batch 3/45, Loss: 2.2138774394989014\n",
            "Epoch 111/200, Batch 4/45, Loss: 1.695533037185669\n",
            "Epoch 111/200, Batch 5/45, Loss: 2.042215347290039\n",
            "Epoch 111/200, Batch 6/45, Loss: 2.1642956733703613\n",
            "Epoch 111/200, Batch 7/45, Loss: 1.533855676651001\n",
            "Epoch 111/200, Batch 8/45, Loss: 2.5839266777038574\n",
            "Epoch 111/200, Batch 9/45, Loss: 2.678557872772217\n",
            "Epoch 111/200, Batch 10/45, Loss: 2.971100330352783\n",
            "Epoch 111/200, Batch 11/45, Loss: 2.4454095363616943\n",
            "Epoch 111/200, Batch 12/45, Loss: 2.4617419242858887\n",
            "Epoch 111/200, Batch 13/45, Loss: 2.608241081237793\n",
            "Epoch 111/200, Batch 14/45, Loss: 3.391226291656494\n",
            "Epoch 111/200, Batch 15/45, Loss: 1.5585925579071045\n",
            "Epoch 111/200, Batch 16/45, Loss: 2.118577003479004\n",
            "Epoch 111/200, Batch 17/45, Loss: 2.251749038696289\n",
            "Epoch 111/200, Batch 18/45, Loss: 2.0591821670532227\n",
            "Epoch 111/200, Batch 19/45, Loss: 1.5728471279144287\n",
            "Epoch 111/200, Batch 20/45, Loss: 3.545936107635498\n",
            "Epoch 111/200, Batch 21/45, Loss: 2.9090538024902344\n",
            "Epoch 111/200, Batch 22/45, Loss: 2.4299917221069336\n",
            "Epoch 111/200, Batch 23/45, Loss: 2.0832574367523193\n",
            "Epoch 111/200, Batch 24/45, Loss: 2.159031391143799\n",
            "Epoch 111/200, Batch 25/45, Loss: 2.1997933387756348\n",
            "Epoch 111/200, Batch 26/45, Loss: 2.0555171966552734\n",
            "Epoch 111/200, Batch 27/45, Loss: 2.515798807144165\n",
            "Epoch 111/200, Batch 28/45, Loss: 2.3836147785186768\n",
            "Epoch 111/200, Batch 29/45, Loss: 1.8345383405685425\n",
            "Epoch 111/200, Batch 30/45, Loss: 2.264723300933838\n",
            "Epoch 111/200, Batch 31/45, Loss: 2.1429238319396973\n",
            "Epoch 111/200, Batch 32/45, Loss: 2.327563762664795\n",
            "Epoch 111/200, Batch 33/45, Loss: 1.9303233623504639\n",
            "Epoch 111/200, Batch 34/45, Loss: 2.015380859375\n",
            "Epoch 111/200, Batch 35/45, Loss: 2.4529128074645996\n",
            "Epoch 111/200, Batch 36/45, Loss: 2.3298840522766113\n",
            "Epoch 111/200, Batch 37/45, Loss: 2.5813522338867188\n",
            "Epoch 111/200, Batch 38/45, Loss: 2.6335768699645996\n",
            "Epoch 111/200, Batch 39/45, Loss: 2.0774850845336914\n",
            "Epoch 111/200, Batch 40/45, Loss: 2.941164016723633\n",
            "Epoch 111/200, Batch 41/45, Loss: 1.3131897449493408\n",
            "Epoch 111/200, Batch 42/45, Loss: 2.0316240787506104\n",
            "Epoch 111/200, Batch 43/45, Loss: 2.0948972702026367\n",
            "Epoch 111/200, Batch 44/45, Loss: 2.309396982192993\n",
            "Epoch 111/200, Batch 45/45, Loss: 2.7201945781707764\n",
            "Validating and Checkpointing!\n",
            "Model is not good (might be overfitting)! Current val MSE:  26.102654576301575 Best Val MSE:  25.10259437561035\n",
            "Epoch:  112 , Time Elapsed:  36.35324862798055  mins\n",
            "Epoch 112/200, Batch 1/45, Loss: 2.393582582473755\n",
            "Epoch 112/200, Batch 2/45, Loss: 1.8661173582077026\n",
            "Epoch 112/200, Batch 3/45, Loss: 2.0379393100738525\n",
            "Epoch 112/200, Batch 4/45, Loss: 1.7821300029754639\n",
            "Epoch 112/200, Batch 5/45, Loss: 2.491326093673706\n",
            "Epoch 112/200, Batch 6/45, Loss: 1.9617459774017334\n",
            "Epoch 112/200, Batch 7/45, Loss: 1.7896316051483154\n",
            "Epoch 112/200, Batch 8/45, Loss: 1.8552722930908203\n",
            "Epoch 112/200, Batch 9/45, Loss: 1.532607913017273\n",
            "Epoch 112/200, Batch 10/45, Loss: 2.1448895931243896\n",
            "Epoch 112/200, Batch 11/45, Loss: 2.352353096008301\n",
            "Epoch 112/200, Batch 12/45, Loss: 2.379478931427002\n",
            "Epoch 112/200, Batch 13/45, Loss: 1.6053121089935303\n",
            "Epoch 112/200, Batch 14/45, Loss: 2.6900248527526855\n",
            "Epoch 112/200, Batch 15/45, Loss: 1.9088146686553955\n",
            "Epoch 112/200, Batch 16/45, Loss: 2.636162757873535\n",
            "Epoch 112/200, Batch 17/45, Loss: 2.3882033824920654\n",
            "Epoch 112/200, Batch 18/45, Loss: 2.8180158138275146\n",
            "Epoch 112/200, Batch 19/45, Loss: 2.089660406112671\n",
            "Epoch 112/200, Batch 20/45, Loss: 2.2546772956848145\n",
            "Epoch 112/200, Batch 21/45, Loss: 2.356653928756714\n",
            "Epoch 112/200, Batch 22/45, Loss: 2.2363200187683105\n",
            "Epoch 112/200, Batch 23/45, Loss: 2.529971122741699\n",
            "Epoch 112/200, Batch 24/45, Loss: 2.549755573272705\n",
            "Epoch 112/200, Batch 25/45, Loss: 2.649355173110962\n",
            "Epoch 112/200, Batch 26/45, Loss: 2.102710723876953\n",
            "Epoch 112/200, Batch 27/45, Loss: 2.257378101348877\n",
            "Epoch 112/200, Batch 28/45, Loss: 2.395848274230957\n",
            "Epoch 112/200, Batch 29/45, Loss: 2.279411792755127\n",
            "Epoch 112/200, Batch 30/45, Loss: 2.5576090812683105\n",
            "Epoch 112/200, Batch 31/45, Loss: 1.492950677871704\n",
            "Epoch 112/200, Batch 32/45, Loss: 2.580160617828369\n",
            "Epoch 112/200, Batch 33/45, Loss: 2.562175750732422\n",
            "Epoch 112/200, Batch 34/45, Loss: 1.8833481073379517\n",
            "Epoch 112/200, Batch 35/45, Loss: 2.239898681640625\n",
            "Epoch 112/200, Batch 36/45, Loss: 2.1635234355926514\n",
            "Epoch 112/200, Batch 37/45, Loss: 2.6725313663482666\n",
            "Epoch 112/200, Batch 38/45, Loss: 2.52345609664917\n",
            "Epoch 112/200, Batch 39/45, Loss: 2.809760570526123\n",
            "Epoch 112/200, Batch 40/45, Loss: 1.8812373876571655\n",
            "Epoch 112/200, Batch 41/45, Loss: 1.8641871213912964\n",
            "Epoch 112/200, Batch 42/45, Loss: 1.8210570812225342\n",
            "Epoch 112/200, Batch 43/45, Loss: 1.8966941833496094\n",
            "Epoch 112/200, Batch 44/45, Loss: 1.9761781692504883\n",
            "Epoch 112/200, Batch 45/45, Loss: 1.8531655073165894\n",
            "Validating and Checkpointing!\n",
            "Model is not good (might be overfitting)! Current val MSE:  26.592010140419006 Best Val MSE:  25.10259437561035\n",
            "Epoch:  113 , Time Elapsed:  36.67225881020228  mins\n",
            "Epoch 113/200, Batch 1/45, Loss: 2.5145931243896484\n",
            "Epoch 113/200, Batch 2/45, Loss: 2.1544339656829834\n",
            "Epoch 113/200, Batch 3/45, Loss: 2.0134470462799072\n",
            "Epoch 113/200, Batch 4/45, Loss: 1.7580175399780273\n",
            "Epoch 113/200, Batch 5/45, Loss: 1.6751590967178345\n",
            "Epoch 113/200, Batch 6/45, Loss: 2.1760239601135254\n",
            "Epoch 113/200, Batch 7/45, Loss: 2.187263250350952\n",
            "Epoch 113/200, Batch 8/45, Loss: 2.47739315032959\n",
            "Epoch 113/200, Batch 9/45, Loss: 2.011777877807617\n",
            "Epoch 113/200, Batch 10/45, Loss: 2.3471415042877197\n",
            "Epoch 113/200, Batch 11/45, Loss: 2.2871131896972656\n",
            "Epoch 113/200, Batch 12/45, Loss: 2.2602291107177734\n",
            "Epoch 113/200, Batch 13/45, Loss: 2.304568290710449\n",
            "Epoch 113/200, Batch 14/45, Loss: 1.6701083183288574\n",
            "Epoch 113/200, Batch 15/45, Loss: 2.2069671154022217\n",
            "Epoch 113/200, Batch 16/45, Loss: 1.8620277643203735\n",
            "Epoch 113/200, Batch 17/45, Loss: 2.1154093742370605\n",
            "Epoch 113/200, Batch 18/45, Loss: 2.2906906604766846\n",
            "Epoch 113/200, Batch 19/45, Loss: 2.0817604064941406\n",
            "Epoch 113/200, Batch 20/45, Loss: 2.44230580329895\n",
            "Epoch 113/200, Batch 21/45, Loss: 1.4960566759109497\n",
            "Epoch 113/200, Batch 22/45, Loss: 1.9739266633987427\n",
            "Epoch 113/200, Batch 23/45, Loss: 1.5758471488952637\n",
            "Epoch 113/200, Batch 24/45, Loss: 1.6360019445419312\n",
            "Epoch 113/200, Batch 25/45, Loss: 2.087892532348633\n",
            "Epoch 113/200, Batch 26/45, Loss: 1.8140102624893188\n",
            "Epoch 113/200, Batch 27/45, Loss: 1.4196168184280396\n",
            "Epoch 113/200, Batch 28/45, Loss: 2.7662034034729004\n",
            "Epoch 113/200, Batch 29/45, Loss: 2.5700807571411133\n",
            "Epoch 113/200, Batch 30/45, Loss: 2.0770883560180664\n",
            "Epoch 113/200, Batch 31/45, Loss: 1.494808316230774\n",
            "Epoch 113/200, Batch 32/45, Loss: 2.3914198875427246\n",
            "Epoch 113/200, Batch 33/45, Loss: 2.9332594871520996\n",
            "Epoch 113/200, Batch 34/45, Loss: 2.186113119125366\n",
            "Epoch 113/200, Batch 35/45, Loss: 2.301116466522217\n",
            "Epoch 113/200, Batch 36/45, Loss: 2.428387403488159\n",
            "Epoch 113/200, Batch 37/45, Loss: 2.759449005126953\n",
            "Epoch 113/200, Batch 38/45, Loss: 1.7478290796279907\n",
            "Epoch 113/200, Batch 39/45, Loss: 2.1303300857543945\n",
            "Epoch 113/200, Batch 40/45, Loss: 1.5651730298995972\n",
            "Epoch 113/200, Batch 41/45, Loss: 2.441349983215332\n",
            "Epoch 113/200, Batch 42/45, Loss: 2.1828019618988037\n",
            "Epoch 113/200, Batch 43/45, Loss: 2.52640962600708\n",
            "Epoch 113/200, Batch 44/45, Loss: 2.342683792114258\n",
            "Epoch 113/200, Batch 45/45, Loss: 2.3913612365722656\n",
            "Validating and Checkpointing!\n",
            "Model is not good (might be overfitting)! Current val MSE:  26.649388194084167 Best Val MSE:  25.10259437561035\n",
            "Epoch:  114 , Time Elapsed:  36.994074404239655  mins\n",
            "Epoch 114/200, Batch 1/45, Loss: 2.507296562194824\n",
            "Epoch 114/200, Batch 2/45, Loss: 1.9828474521636963\n",
            "Epoch 114/200, Batch 3/45, Loss: 2.097787380218506\n",
            "Epoch 114/200, Batch 4/45, Loss: 2.7624497413635254\n",
            "Epoch 114/200, Batch 5/45, Loss: 2.417670249938965\n",
            "Epoch 114/200, Batch 6/45, Loss: 2.4764106273651123\n",
            "Epoch 114/200, Batch 7/45, Loss: 1.9855046272277832\n",
            "Epoch 114/200, Batch 8/45, Loss: 2.6161656379699707\n",
            "Epoch 114/200, Batch 9/45, Loss: 1.9470266103744507\n",
            "Epoch 114/200, Batch 10/45, Loss: 2.2765777111053467\n",
            "Epoch 114/200, Batch 11/45, Loss: 1.9998469352722168\n",
            "Epoch 114/200, Batch 12/45, Loss: 2.297030448913574\n",
            "Epoch 114/200, Batch 13/45, Loss: 1.8900599479675293\n",
            "Epoch 114/200, Batch 14/45, Loss: 1.6902958154678345\n",
            "Epoch 114/200, Batch 15/45, Loss: 2.5860533714294434\n",
            "Epoch 114/200, Batch 16/45, Loss: 2.182224988937378\n",
            "Epoch 114/200, Batch 17/45, Loss: 2.1742517948150635\n",
            "Epoch 114/200, Batch 18/45, Loss: 1.6919116973876953\n",
            "Epoch 114/200, Batch 19/45, Loss: 1.9688019752502441\n",
            "Epoch 114/200, Batch 20/45, Loss: 1.8707596063613892\n",
            "Epoch 114/200, Batch 21/45, Loss: 2.244450569152832\n",
            "Epoch 114/200, Batch 22/45, Loss: 2.022679090499878\n",
            "Epoch 114/200, Batch 23/45, Loss: 3.1494109630584717\n",
            "Epoch 114/200, Batch 24/45, Loss: 2.4329466819763184\n",
            "Epoch 114/200, Batch 25/45, Loss: 2.2656149864196777\n",
            "Epoch 114/200, Batch 26/45, Loss: 1.9939075708389282\n",
            "Epoch 114/200, Batch 27/45, Loss: 1.8796085119247437\n",
            "Epoch 114/200, Batch 28/45, Loss: 2.0336825847625732\n",
            "Epoch 114/200, Batch 29/45, Loss: 2.225341796875\n",
            "Epoch 114/200, Batch 30/45, Loss: 2.5737524032592773\n",
            "Epoch 114/200, Batch 31/45, Loss: 1.5494730472564697\n",
            "Epoch 114/200, Batch 32/45, Loss: 2.104952573776245\n",
            "Epoch 114/200, Batch 33/45, Loss: 2.660429000854492\n",
            "Epoch 114/200, Batch 34/45, Loss: 2.1195483207702637\n",
            "Epoch 114/200, Batch 35/45, Loss: 1.732619047164917\n",
            "Epoch 114/200, Batch 36/45, Loss: 2.3420519828796387\n",
            "Epoch 114/200, Batch 37/45, Loss: 2.5664327144622803\n",
            "Epoch 114/200, Batch 38/45, Loss: 1.8068146705627441\n",
            "Epoch 114/200, Batch 39/45, Loss: 2.334218978881836\n",
            "Epoch 114/200, Batch 40/45, Loss: 2.0862269401550293\n",
            "Epoch 114/200, Batch 41/45, Loss: 2.125731945037842\n",
            "Epoch 114/200, Batch 42/45, Loss: 1.7534371614456177\n",
            "Epoch 114/200, Batch 43/45, Loss: 2.6908254623413086\n",
            "Epoch 114/200, Batch 44/45, Loss: 1.8230035305023193\n",
            "Epoch 114/200, Batch 45/45, Loss: 1.5930618047714233\n",
            "Validating and Checkpointing!\n",
            "Model is not good (might be overfitting)! Current val MSE:  25.937230110168457 Best Val MSE:  25.10259437561035\n",
            "Epoch:  115 , Time Elapsed:  37.33785389264425  mins\n",
            "Epoch 115/200, Batch 1/45, Loss: 2.1513664722442627\n",
            "Epoch 115/200, Batch 2/45, Loss: 1.8571527004241943\n",
            "Epoch 115/200, Batch 3/45, Loss: 2.8862380981445312\n",
            "Epoch 115/200, Batch 4/45, Loss: 1.8562040328979492\n",
            "Epoch 115/200, Batch 5/45, Loss: 2.6910791397094727\n",
            "Epoch 115/200, Batch 6/45, Loss: 1.8002119064331055\n",
            "Epoch 115/200, Batch 7/45, Loss: 3.641883373260498\n",
            "Epoch 115/200, Batch 8/45, Loss: 2.438373327255249\n",
            "Epoch 115/200, Batch 9/45, Loss: 1.803798794746399\n",
            "Epoch 115/200, Batch 10/45, Loss: 2.195449113845825\n",
            "Epoch 115/200, Batch 11/45, Loss: 1.867721676826477\n",
            "Epoch 115/200, Batch 12/45, Loss: 2.0552265644073486\n",
            "Epoch 115/200, Batch 13/45, Loss: 2.3278400897979736\n",
            "Epoch 115/200, Batch 14/45, Loss: 1.580772042274475\n",
            "Epoch 115/200, Batch 15/45, Loss: 1.838512897491455\n",
            "Epoch 115/200, Batch 16/45, Loss: 17.04176902770996\n",
            "Epoch 115/200, Batch 17/45, Loss: 2.2872304916381836\n",
            "Epoch 115/200, Batch 18/45, Loss: 2.5172557830810547\n",
            "Epoch 115/200, Batch 19/45, Loss: 2.13346004486084\n",
            "Epoch 115/200, Batch 20/45, Loss: 2.437105894088745\n",
            "Epoch 115/200, Batch 21/45, Loss: 1.6115453243255615\n",
            "Epoch 115/200, Batch 22/45, Loss: 3.1303787231445312\n",
            "Epoch 115/200, Batch 23/45, Loss: 2.895742416381836\n",
            "Epoch 115/200, Batch 24/45, Loss: 2.2400405406951904\n",
            "Epoch 115/200, Batch 25/45, Loss: 2.335988998413086\n",
            "Epoch 115/200, Batch 26/45, Loss: 2.77199649810791\n",
            "Epoch 115/200, Batch 27/45, Loss: 2.728853702545166\n",
            "Epoch 115/200, Batch 28/45, Loss: 2.7138853073120117\n",
            "Epoch 115/200, Batch 29/45, Loss: 2.8308162689208984\n",
            "Epoch 115/200, Batch 30/45, Loss: 3.24019718170166\n",
            "Epoch 115/200, Batch 31/45, Loss: 2.3153929710388184\n",
            "Epoch 115/200, Batch 32/45, Loss: 2.819582939147949\n",
            "Epoch 115/200, Batch 33/45, Loss: 2.4662702083587646\n",
            "Epoch 115/200, Batch 34/45, Loss: 2.1957757472991943\n",
            "Epoch 115/200, Batch 35/45, Loss: 2.3674497604370117\n",
            "Epoch 115/200, Batch 36/45, Loss: 1.9315156936645508\n",
            "Epoch 115/200, Batch 37/45, Loss: 2.740596294403076\n",
            "Epoch 115/200, Batch 38/45, Loss: 1.5944095849990845\n",
            "Epoch 115/200, Batch 39/45, Loss: 2.3062126636505127\n",
            "Epoch 115/200, Batch 40/45, Loss: 1.633585810661316\n",
            "Epoch 115/200, Batch 41/45, Loss: 2.0065062046051025\n",
            "Epoch 115/200, Batch 42/45, Loss: 1.9786654710769653\n",
            "Epoch 115/200, Batch 43/45, Loss: 2.5873336791992188\n",
            "Epoch 115/200, Batch 44/45, Loss: 2.4842705726623535\n",
            "Epoch 115/200, Batch 45/45, Loss: 2.048959493637085\n",
            "Validating and Checkpointing!\n",
            "Model is not good (might be overfitting)! Current val MSE:  32.85493803024292 Best Val MSE:  25.10259437561035\n",
            "Epoch:  116 , Time Elapsed:  37.66479003429413  mins\n",
            "Epoch 116/200, Batch 1/45, Loss: 2.0090832710266113\n",
            "Epoch 116/200, Batch 2/45, Loss: 2.331615686416626\n",
            "Epoch 116/200, Batch 3/45, Loss: 1.939660906791687\n",
            "Epoch 116/200, Batch 4/45, Loss: 2.8205833435058594\n",
            "Epoch 116/200, Batch 5/45, Loss: 2.5498156547546387\n",
            "Epoch 116/200, Batch 6/45, Loss: 2.883101224899292\n",
            "Epoch 116/200, Batch 7/45, Loss: 2.7062320709228516\n",
            "Epoch 116/200, Batch 8/45, Loss: 2.1351301670074463\n",
            "Epoch 116/200, Batch 9/45, Loss: 1.7567579746246338\n",
            "Epoch 116/200, Batch 10/45, Loss: 3.172247886657715\n",
            "Epoch 116/200, Batch 11/45, Loss: 1.678117275238037\n",
            "Epoch 116/200, Batch 12/45, Loss: 2.24556565284729\n",
            "Epoch 116/200, Batch 13/45, Loss: 1.8690261840820312\n",
            "Epoch 116/200, Batch 14/45, Loss: 2.0587403774261475\n",
            "Epoch 116/200, Batch 15/45, Loss: 2.517040967941284\n",
            "Epoch 116/200, Batch 16/45, Loss: 2.263739585876465\n",
            "Epoch 116/200, Batch 17/45, Loss: 2.8305349349975586\n",
            "Epoch 116/200, Batch 18/45, Loss: 2.2677109241485596\n",
            "Epoch 116/200, Batch 19/45, Loss: 2.2698826789855957\n",
            "Epoch 116/200, Batch 20/45, Loss: 2.088622808456421\n",
            "Epoch 116/200, Batch 21/45, Loss: 1.8716901540756226\n",
            "Epoch 116/200, Batch 22/45, Loss: 2.011362314224243\n",
            "Epoch 116/200, Batch 23/45, Loss: 1.8857519626617432\n",
            "Epoch 116/200, Batch 24/45, Loss: 2.409515142440796\n",
            "Epoch 116/200, Batch 25/45, Loss: 2.8277111053466797\n",
            "Epoch 116/200, Batch 26/45, Loss: 1.610819935798645\n",
            "Epoch 116/200, Batch 27/45, Loss: 2.3412184715270996\n",
            "Epoch 116/200, Batch 28/45, Loss: 3.413024663925171\n",
            "Epoch 116/200, Batch 29/45, Loss: 2.029863119125366\n",
            "Epoch 116/200, Batch 30/45, Loss: 2.2684483528137207\n",
            "Epoch 116/200, Batch 31/45, Loss: 2.44265079498291\n",
            "Epoch 116/200, Batch 32/45, Loss: 2.114396333694458\n",
            "Epoch 116/200, Batch 33/45, Loss: 2.5881428718566895\n",
            "Epoch 116/200, Batch 34/45, Loss: 2.3480172157287598\n",
            "Epoch 116/200, Batch 35/45, Loss: 2.0573785305023193\n",
            "Epoch 116/200, Batch 36/45, Loss: 2.0391829013824463\n",
            "Epoch 116/200, Batch 37/45, Loss: 2.560835123062134\n",
            "Epoch 116/200, Batch 38/45, Loss: 3.8597328662872314\n",
            "Epoch 116/200, Batch 39/45, Loss: 1.9973349571228027\n",
            "Epoch 116/200, Batch 40/45, Loss: 1.8995097875595093\n",
            "Epoch 116/200, Batch 41/45, Loss: 1.4164164066314697\n",
            "Epoch 116/200, Batch 42/45, Loss: 1.5210968255996704\n",
            "Epoch 116/200, Batch 43/45, Loss: 2.0708351135253906\n",
            "Epoch 116/200, Batch 44/45, Loss: 2.448080539703369\n",
            "Epoch 116/200, Batch 45/45, Loss: 2.2658097743988037\n",
            "Validating and Checkpointing!\n",
            "Model is not good (might be overfitting)! Current val MSE:  28.8335440158844 Best Val MSE:  25.10259437561035\n",
            "Epoch:  117 , Time Elapsed:  38.009293528397876  mins\n",
            "Epoch 117/200, Batch 1/45, Loss: 2.5582430362701416\n",
            "Epoch 117/200, Batch 2/45, Loss: 2.133024215698242\n",
            "Epoch 117/200, Batch 3/45, Loss: 2.39111328125\n",
            "Epoch 117/200, Batch 4/45, Loss: 2.1562819480895996\n",
            "Epoch 117/200, Batch 5/45, Loss: 2.204568862915039\n",
            "Epoch 117/200, Batch 6/45, Loss: 2.817323684692383\n",
            "Epoch 117/200, Batch 7/45, Loss: 2.0999019145965576\n",
            "Epoch 117/200, Batch 8/45, Loss: 2.11966872215271\n",
            "Epoch 117/200, Batch 9/45, Loss: 2.2925000190734863\n",
            "Epoch 117/200, Batch 10/45, Loss: 1.7247179746627808\n",
            "Epoch 117/200, Batch 11/45, Loss: 2.295539617538452\n",
            "Epoch 117/200, Batch 12/45, Loss: 2.124499797821045\n",
            "Epoch 117/200, Batch 13/45, Loss: 2.3529253005981445\n",
            "Epoch 117/200, Batch 14/45, Loss: 2.3201825618743896\n",
            "Epoch 117/200, Batch 15/45, Loss: 2.333411931991577\n",
            "Epoch 117/200, Batch 16/45, Loss: 1.6220237016677856\n",
            "Epoch 117/200, Batch 17/45, Loss: 2.6657893657684326\n",
            "Epoch 117/200, Batch 18/45, Loss: 2.0967154502868652\n",
            "Epoch 117/200, Batch 19/45, Loss: 2.2288055419921875\n",
            "Epoch 117/200, Batch 20/45, Loss: 2.4454076290130615\n",
            "Epoch 117/200, Batch 21/45, Loss: 2.2198712825775146\n",
            "Epoch 117/200, Batch 22/45, Loss: 2.986603260040283\n",
            "Epoch 117/200, Batch 23/45, Loss: 1.8583946228027344\n",
            "Epoch 117/200, Batch 24/45, Loss: 2.6071813106536865\n",
            "Epoch 117/200, Batch 25/45, Loss: 2.3904905319213867\n",
            "Epoch 117/200, Batch 26/45, Loss: 2.2770676612854004\n",
            "Epoch 117/200, Batch 27/45, Loss: 2.087010145187378\n",
            "Epoch 117/200, Batch 28/45, Loss: 1.5991685390472412\n",
            "Epoch 117/200, Batch 29/45, Loss: 2.469874382019043\n",
            "Epoch 117/200, Batch 30/45, Loss: 2.1173949241638184\n",
            "Epoch 117/200, Batch 31/45, Loss: 1.9634130001068115\n",
            "Epoch 117/200, Batch 32/45, Loss: 2.534900426864624\n",
            "Epoch 117/200, Batch 33/45, Loss: 1.7329989671707153\n",
            "Epoch 117/200, Batch 34/45, Loss: 2.3682732582092285\n",
            "Epoch 117/200, Batch 35/45, Loss: 1.9585981369018555\n",
            "Epoch 117/200, Batch 36/45, Loss: 2.5959880352020264\n",
            "Epoch 117/200, Batch 37/45, Loss: 2.0281982421875\n",
            "Epoch 117/200, Batch 38/45, Loss: 2.75321888923645\n",
            "Epoch 117/200, Batch 39/45, Loss: 2.695502519607544\n",
            "Epoch 117/200, Batch 40/45, Loss: 1.7989623546600342\n",
            "Epoch 117/200, Batch 41/45, Loss: 2.032247304916382\n",
            "Epoch 117/200, Batch 42/45, Loss: 2.439948320388794\n",
            "Epoch 117/200, Batch 43/45, Loss: 2.3021748065948486\n",
            "Epoch 117/200, Batch 44/45, Loss: 1.460033655166626\n",
            "Epoch 117/200, Batch 45/45, Loss: 2.5531458854675293\n",
            "Validating and Checkpointing!\n",
            "Model is not good (might be overfitting)! Current val MSE:  26.541586875915527 Best Val MSE:  25.10259437561035\n",
            "Epoch:  118 , Time Elapsed:  38.37282339334488  mins\n",
            "Epoch 118/200, Batch 1/45, Loss: 2.4826087951660156\n",
            "Epoch 118/200, Batch 2/45, Loss: 1.9256623983383179\n",
            "Epoch 118/200, Batch 3/45, Loss: 2.5200400352478027\n",
            "Epoch 118/200, Batch 4/45, Loss: 1.7619009017944336\n",
            "Epoch 118/200, Batch 5/45, Loss: 2.0524404048919678\n",
            "Epoch 118/200, Batch 6/45, Loss: 1.6612939834594727\n",
            "Epoch 118/200, Batch 7/45, Loss: 2.367586612701416\n",
            "Epoch 118/200, Batch 8/45, Loss: 2.0785250663757324\n",
            "Epoch 118/200, Batch 9/45, Loss: 2.3578600883483887\n",
            "Epoch 118/200, Batch 10/45, Loss: 1.6037416458129883\n",
            "Epoch 118/200, Batch 11/45, Loss: 1.7773360013961792\n",
            "Epoch 118/200, Batch 12/45, Loss: 2.249812126159668\n",
            "Epoch 118/200, Batch 13/45, Loss: 2.333591938018799\n",
            "Epoch 118/200, Batch 14/45, Loss: 2.396674633026123\n",
            "Epoch 118/200, Batch 15/45, Loss: 2.6144425868988037\n",
            "Epoch 118/200, Batch 16/45, Loss: 2.7431819438934326\n",
            "Epoch 118/200, Batch 17/45, Loss: 2.5947885513305664\n",
            "Epoch 118/200, Batch 18/45, Loss: 1.6706949472427368\n",
            "Epoch 118/200, Batch 19/45, Loss: 1.7473942041397095\n",
            "Epoch 118/200, Batch 20/45, Loss: 2.858290672302246\n",
            "Epoch 118/200, Batch 21/45, Loss: 2.5129144191741943\n",
            "Epoch 118/200, Batch 22/45, Loss: 2.818819761276245\n",
            "Epoch 118/200, Batch 23/45, Loss: 4.138935089111328\n",
            "Epoch 118/200, Batch 24/45, Loss: 2.1975855827331543\n",
            "Epoch 118/200, Batch 25/45, Loss: 2.2036874294281006\n",
            "Epoch 118/200, Batch 26/45, Loss: 2.317190170288086\n",
            "Epoch 118/200, Batch 27/45, Loss: 2.3566231727600098\n",
            "Epoch 118/200, Batch 28/45, Loss: 1.8860409259796143\n",
            "Epoch 118/200, Batch 29/45, Loss: 1.8785239458084106\n",
            "Epoch 118/200, Batch 30/45, Loss: 2.28263258934021\n",
            "Epoch 118/200, Batch 31/45, Loss: 2.3757028579711914\n",
            "Epoch 118/200, Batch 32/45, Loss: 2.1142139434814453\n",
            "Epoch 118/200, Batch 33/45, Loss: 1.7720752954483032\n",
            "Epoch 118/200, Batch 34/45, Loss: 2.723568916320801\n",
            "Epoch 118/200, Batch 35/45, Loss: 2.292764902114868\n",
            "Epoch 118/200, Batch 36/45, Loss: 2.1878983974456787\n",
            "Epoch 118/200, Batch 37/45, Loss: 2.372079372406006\n",
            "Epoch 118/200, Batch 38/45, Loss: 1.4161008596420288\n",
            "Epoch 118/200, Batch 39/45, Loss: 2.594329595565796\n",
            "Epoch 118/200, Batch 40/45, Loss: 2.788469076156616\n",
            "Epoch 118/200, Batch 41/45, Loss: 2.2974660396575928\n",
            "Epoch 118/200, Batch 42/45, Loss: 2.3675568103790283\n",
            "Epoch 118/200, Batch 43/45, Loss: 1.6999889612197876\n",
            "Epoch 118/200, Batch 44/45, Loss: 2.6854074001312256\n",
            "Epoch 118/200, Batch 45/45, Loss: 2.678523063659668\n",
            "Validating and Checkpointing!\n",
            "Model is not good (might be overfitting)! Current val MSE:  26.260817527770996 Best Val MSE:  25.10259437561035\n",
            "Epoch:  119 , Time Elapsed:  38.688282469908394  mins\n",
            "Epoch 119/200, Batch 1/45, Loss: 2.026219606399536\n",
            "Epoch 119/200, Batch 2/45, Loss: 2.6608474254608154\n",
            "Epoch 119/200, Batch 3/45, Loss: 2.0195353031158447\n",
            "Epoch 119/200, Batch 4/45, Loss: 1.869271993637085\n",
            "Epoch 119/200, Batch 5/45, Loss: 1.5167313814163208\n",
            "Epoch 119/200, Batch 6/45, Loss: 2.124265193939209\n",
            "Epoch 119/200, Batch 7/45, Loss: 1.940910816192627\n",
            "Epoch 119/200, Batch 8/45, Loss: 2.663191318511963\n",
            "Epoch 119/200, Batch 9/45, Loss: 2.291170120239258\n",
            "Epoch 119/200, Batch 10/45, Loss: 2.099562644958496\n",
            "Epoch 119/200, Batch 11/45, Loss: 2.686220169067383\n",
            "Epoch 119/200, Batch 12/45, Loss: 1.7679479122161865\n",
            "Epoch 119/200, Batch 13/45, Loss: 1.8159620761871338\n",
            "Epoch 119/200, Batch 14/45, Loss: 2.1167240142822266\n",
            "Epoch 119/200, Batch 15/45, Loss: 2.0424201488494873\n",
            "Epoch 119/200, Batch 16/45, Loss: 2.283313751220703\n",
            "Epoch 119/200, Batch 17/45, Loss: 1.5385938882827759\n",
            "Epoch 119/200, Batch 18/45, Loss: 1.405168890953064\n",
            "Epoch 119/200, Batch 19/45, Loss: 2.251579761505127\n",
            "Epoch 119/200, Batch 20/45, Loss: 2.5326051712036133\n",
            "Epoch 119/200, Batch 21/45, Loss: 1.8628093004226685\n",
            "Epoch 119/200, Batch 22/45, Loss: 2.238577127456665\n",
            "Epoch 119/200, Batch 23/45, Loss: 2.4449243545532227\n",
            "Epoch 119/200, Batch 24/45, Loss: 1.947941541671753\n",
            "Epoch 119/200, Batch 25/45, Loss: 2.4911389350891113\n",
            "Epoch 119/200, Batch 26/45, Loss: 2.4833173751831055\n",
            "Epoch 119/200, Batch 27/45, Loss: 1.6599175930023193\n",
            "Epoch 119/200, Batch 28/45, Loss: 2.3975720405578613\n",
            "Epoch 119/200, Batch 29/45, Loss: 2.0700111389160156\n",
            "Epoch 119/200, Batch 30/45, Loss: 2.3569958209991455\n",
            "Epoch 119/200, Batch 31/45, Loss: 2.4931740760803223\n",
            "Epoch 119/200, Batch 32/45, Loss: 2.433384418487549\n",
            "Epoch 119/200, Batch 33/45, Loss: 2.167778491973877\n",
            "Epoch 119/200, Batch 34/45, Loss: 2.5438168048858643\n",
            "Epoch 119/200, Batch 35/45, Loss: 2.0109829902648926\n",
            "Epoch 119/200, Batch 36/45, Loss: 2.387777805328369\n",
            "Epoch 119/200, Batch 37/45, Loss: 1.8910549879074097\n",
            "Epoch 119/200, Batch 38/45, Loss: 2.243762969970703\n",
            "Epoch 119/200, Batch 39/45, Loss: 2.2996041774749756\n",
            "Epoch 119/200, Batch 40/45, Loss: 2.260040521621704\n",
            "Epoch 119/200, Batch 41/45, Loss: 2.2136311531066895\n",
            "Epoch 119/200, Batch 42/45, Loss: 2.3548519611358643\n",
            "Epoch 119/200, Batch 43/45, Loss: 1.5574252605438232\n",
            "Epoch 119/200, Batch 44/45, Loss: 1.7375514507293701\n",
            "Epoch 119/200, Batch 45/45, Loss: 2.3563849925994873\n",
            "Validating and Checkpointing!\n",
            "Model is not good (might be overfitting)! Current val MSE:  26.85099506378174 Best Val MSE:  25.10259437561035\n",
            "Epoch:  120 , Time Elapsed:  39.023474276065826  mins\n",
            "Epoch 120/200, Batch 1/45, Loss: 2.0930542945861816\n",
            "Epoch 120/200, Batch 2/45, Loss: 2.6286070346832275\n",
            "Epoch 120/200, Batch 3/45, Loss: 1.6123608350753784\n",
            "Epoch 120/200, Batch 4/45, Loss: 1.6644160747528076\n",
            "Epoch 120/200, Batch 5/45, Loss: 2.3701181411743164\n",
            "Epoch 120/200, Batch 6/45, Loss: 1.9284684658050537\n",
            "Epoch 120/200, Batch 7/45, Loss: 1.8421093225479126\n",
            "Epoch 120/200, Batch 8/45, Loss: 2.479276418685913\n",
            "Epoch 120/200, Batch 9/45, Loss: 1.6033655405044556\n",
            "Epoch 120/200, Batch 10/45, Loss: 2.4200332164764404\n",
            "Epoch 120/200, Batch 11/45, Loss: 2.152367115020752\n",
            "Epoch 120/200, Batch 12/45, Loss: 2.2621097564697266\n",
            "Epoch 120/200, Batch 13/45, Loss: 2.5832974910736084\n",
            "Epoch 120/200, Batch 14/45, Loss: 2.754960536956787\n",
            "Epoch 120/200, Batch 15/45, Loss: 2.20275616645813\n",
            "Epoch 120/200, Batch 16/45, Loss: 2.370093822479248\n",
            "Epoch 120/200, Batch 17/45, Loss: 2.4413857460021973\n",
            "Epoch 120/200, Batch 18/45, Loss: 2.4645602703094482\n",
            "Epoch 120/200, Batch 19/45, Loss: 2.8185107707977295\n",
            "Epoch 120/200, Batch 20/45, Loss: 1.997231125831604\n",
            "Epoch 120/200, Batch 21/45, Loss: 2.2198166847229004\n",
            "Epoch 120/200, Batch 22/45, Loss: 1.7148023843765259\n",
            "Epoch 120/200, Batch 23/45, Loss: 2.351674795150757\n",
            "Epoch 120/200, Batch 24/45, Loss: 2.0016393661499023\n",
            "Epoch 120/200, Batch 25/45, Loss: 1.7521841526031494\n",
            "Epoch 120/200, Batch 26/45, Loss: 1.7597180604934692\n",
            "Epoch 120/200, Batch 27/45, Loss: 1.7412381172180176\n",
            "Epoch 120/200, Batch 28/45, Loss: 1.5879074335098267\n",
            "Epoch 120/200, Batch 29/45, Loss: 2.3815319538116455\n",
            "Epoch 120/200, Batch 30/45, Loss: 2.3223016262054443\n",
            "Epoch 120/200, Batch 31/45, Loss: 3.3101646900177\n",
            "Epoch 120/200, Batch 32/45, Loss: 2.6388099193573\n",
            "Epoch 120/200, Batch 33/45, Loss: 2.394557237625122\n",
            "Epoch 120/200, Batch 34/45, Loss: 2.01460599899292\n",
            "Epoch 120/200, Batch 35/45, Loss: 1.9028795957565308\n",
            "Epoch 120/200, Batch 36/45, Loss: 2.6707422733306885\n",
            "Epoch 120/200, Batch 37/45, Loss: 2.3015713691711426\n",
            "Epoch 120/200, Batch 38/45, Loss: 2.4152441024780273\n",
            "Epoch 120/200, Batch 39/45, Loss: 2.765256643295288\n",
            "Epoch 120/200, Batch 40/45, Loss: 2.156250476837158\n",
            "Epoch 120/200, Batch 41/45, Loss: 2.2957763671875\n",
            "Epoch 120/200, Batch 42/45, Loss: 2.588608741760254\n",
            "Epoch 120/200, Batch 43/45, Loss: 1.871511459350586\n",
            "Epoch 120/200, Batch 44/45, Loss: 2.3494038581848145\n",
            "Epoch 120/200, Batch 45/45, Loss: 2.545743942260742\n",
            "Validating and Checkpointing!\n",
            "Model is not good (might be overfitting)! Current val MSE:  27.854859590530396 Best Val MSE:  25.10259437561035\n",
            "Epoch:  121 , Time Elapsed:  39.37486297686895  mins\n",
            "Epoch 121/200, Batch 1/45, Loss: 1.602118968963623\n",
            "Epoch 121/200, Batch 2/45, Loss: 2.2879490852355957\n",
            "Epoch 121/200, Batch 3/45, Loss: 1.9937427043914795\n",
            "Epoch 121/200, Batch 4/45, Loss: 1.6090558767318726\n",
            "Epoch 121/200, Batch 5/45, Loss: 2.496807336807251\n",
            "Epoch 121/200, Batch 6/45, Loss: 2.733996868133545\n",
            "Epoch 121/200, Batch 7/45, Loss: 1.6912496089935303\n",
            "Epoch 121/200, Batch 8/45, Loss: 1.9404377937316895\n",
            "Epoch 121/200, Batch 9/45, Loss: 1.3561469316482544\n",
            "Epoch 121/200, Batch 10/45, Loss: 2.4728708267211914\n",
            "Epoch 121/200, Batch 11/45, Loss: 2.154109477996826\n",
            "Epoch 121/200, Batch 12/45, Loss: 1.9693844318389893\n",
            "Epoch 121/200, Batch 13/45, Loss: 2.5547285079956055\n",
            "Epoch 121/200, Batch 14/45, Loss: 2.215989112854004\n",
            "Epoch 121/200, Batch 15/45, Loss: 2.3122057914733887\n",
            "Epoch 121/200, Batch 16/45, Loss: 2.185704469680786\n",
            "Epoch 121/200, Batch 17/45, Loss: 1.9037243127822876\n",
            "Epoch 121/200, Batch 18/45, Loss: 1.956608533859253\n",
            "Epoch 121/200, Batch 19/45, Loss: 2.007521152496338\n",
            "Epoch 121/200, Batch 20/45, Loss: 2.29703688621521\n",
            "Epoch 121/200, Batch 21/45, Loss: 2.5152170658111572\n",
            "Epoch 121/200, Batch 22/45, Loss: 1.9833533763885498\n",
            "Epoch 121/200, Batch 23/45, Loss: 2.3836894035339355\n",
            "Epoch 121/200, Batch 24/45, Loss: 2.582582473754883\n",
            "Epoch 121/200, Batch 25/45, Loss: 2.2447128295898438\n",
            "Epoch 121/200, Batch 26/45, Loss: 2.4389195442199707\n",
            "Epoch 121/200, Batch 27/45, Loss: 2.5458297729492188\n",
            "Epoch 121/200, Batch 28/45, Loss: 2.1695187091827393\n",
            "Epoch 121/200, Batch 29/45, Loss: 1.5594110488891602\n",
            "Epoch 121/200, Batch 30/45, Loss: 2.301313638687134\n",
            "Epoch 121/200, Batch 31/45, Loss: 2.643502712249756\n",
            "Epoch 121/200, Batch 32/45, Loss: 1.9752402305603027\n",
            "Epoch 121/200, Batch 33/45, Loss: 2.101083278656006\n",
            "Epoch 121/200, Batch 34/45, Loss: 1.8617258071899414\n",
            "Epoch 121/200, Batch 35/45, Loss: 2.474682331085205\n",
            "Epoch 121/200, Batch 36/45, Loss: 2.3381943702697754\n",
            "Epoch 121/200, Batch 37/45, Loss: 2.2724573612213135\n",
            "Epoch 121/200, Batch 38/45, Loss: 2.2894182205200195\n",
            "Epoch 121/200, Batch 39/45, Loss: 2.796811580657959\n",
            "Epoch 121/200, Batch 40/45, Loss: 2.487450361251831\n",
            "Epoch 121/200, Batch 41/45, Loss: 2.2334046363830566\n",
            "Epoch 121/200, Batch 42/45, Loss: 2.294180393218994\n",
            "Epoch 121/200, Batch 43/45, Loss: 2.2715389728546143\n",
            "Epoch 121/200, Batch 44/45, Loss: 1.912848711013794\n",
            "Epoch 121/200, Batch 45/45, Loss: 2.034701108932495\n",
            "Validating and Checkpointing!\n",
            "Model is not good (might be overfitting)! Current val MSE:  25.49998641014099 Best Val MSE:  25.10259437561035\n",
            "Epoch:  122 , Time Elapsed:  39.70876866181691  mins\n",
            "Epoch 122/200, Batch 1/45, Loss: 2.267458915710449\n",
            "Epoch 122/200, Batch 2/45, Loss: 2.396766424179077\n",
            "Epoch 122/200, Batch 3/45, Loss: 2.4201512336730957\n",
            "Epoch 122/200, Batch 4/45, Loss: 2.6324150562286377\n",
            "Epoch 122/200, Batch 5/45, Loss: 1.9689502716064453\n",
            "Epoch 122/200, Batch 6/45, Loss: 2.169299364089966\n",
            "Epoch 122/200, Batch 7/45, Loss: 2.3568167686462402\n",
            "Epoch 122/200, Batch 8/45, Loss: 2.162264347076416\n",
            "Epoch 122/200, Batch 9/45, Loss: 1.9701426029205322\n",
            "Epoch 122/200, Batch 10/45, Loss: 1.6586246490478516\n",
            "Epoch 122/200, Batch 11/45, Loss: 2.071964740753174\n",
            "Epoch 122/200, Batch 12/45, Loss: 2.38704776763916\n",
            "Epoch 122/200, Batch 13/45, Loss: 1.7442477941513062\n",
            "Epoch 122/200, Batch 14/45, Loss: 2.782287836074829\n",
            "Epoch 122/200, Batch 15/45, Loss: 2.1991100311279297\n",
            "Epoch 122/200, Batch 16/45, Loss: 1.8992540836334229\n",
            "Epoch 122/200, Batch 17/45, Loss: 2.200796604156494\n",
            "Epoch 122/200, Batch 18/45, Loss: 2.54074764251709\n",
            "Epoch 122/200, Batch 19/45, Loss: 2.4991960525512695\n",
            "Epoch 122/200, Batch 20/45, Loss: 2.5028791427612305\n",
            "Epoch 122/200, Batch 21/45, Loss: 2.3990252017974854\n",
            "Epoch 122/200, Batch 22/45, Loss: 2.381002426147461\n",
            "Epoch 122/200, Batch 23/45, Loss: 2.1414051055908203\n",
            "Epoch 122/200, Batch 24/45, Loss: 2.6071293354034424\n",
            "Epoch 122/200, Batch 25/45, Loss: 1.6857247352600098\n",
            "Epoch 122/200, Batch 26/45, Loss: 2.3323891162872314\n",
            "Epoch 122/200, Batch 27/45, Loss: 1.8577790260314941\n",
            "Epoch 122/200, Batch 28/45, Loss: 1.7979525327682495\n",
            "Epoch 122/200, Batch 29/45, Loss: 2.3490612506866455\n",
            "Epoch 122/200, Batch 30/45, Loss: 2.3870038986206055\n",
            "Epoch 122/200, Batch 31/45, Loss: 2.46781325340271\n",
            "Epoch 122/200, Batch 32/45, Loss: 2.1809277534484863\n",
            "Epoch 122/200, Batch 33/45, Loss: 2.316990852355957\n",
            "Epoch 122/200, Batch 34/45, Loss: 2.897562026977539\n",
            "Epoch 122/200, Batch 35/45, Loss: 2.4166619777679443\n",
            "Epoch 122/200, Batch 36/45, Loss: 1.8586900234222412\n",
            "Epoch 122/200, Batch 37/45, Loss: 1.933562159538269\n",
            "Epoch 122/200, Batch 38/45, Loss: 2.413749933242798\n",
            "Epoch 122/200, Batch 39/45, Loss: 2.031223773956299\n",
            "Epoch 122/200, Batch 40/45, Loss: 1.4919285774230957\n",
            "Epoch 122/200, Batch 41/45, Loss: 2.182919979095459\n",
            "Epoch 122/200, Batch 42/45, Loss: 2.730668783187866\n",
            "Epoch 122/200, Batch 43/45, Loss: 2.4178173542022705\n",
            "Epoch 122/200, Batch 44/45, Loss: 1.7169263362884521\n",
            "Epoch 122/200, Batch 45/45, Loss: 2.560291290283203\n",
            "Validating and Checkpointing!\n",
            "Model is not good (might be overfitting)! Current val MSE:  28.03693401813507 Best Val MSE:  25.10259437561035\n",
            "Epoch:  123 , Time Elapsed:  40.05924630959829  mins\n",
            "Epoch 123/200, Batch 1/45, Loss: 2.0472471714019775\n",
            "Epoch 123/200, Batch 2/45, Loss: 1.9129796028137207\n",
            "Epoch 123/200, Batch 3/45, Loss: 2.503957986831665\n",
            "Epoch 123/200, Batch 4/45, Loss: 2.1687488555908203\n",
            "Epoch 123/200, Batch 5/45, Loss: 2.2409591674804688\n",
            "Epoch 123/200, Batch 6/45, Loss: 4.383596897125244\n",
            "Epoch 123/200, Batch 7/45, Loss: 2.2033848762512207\n",
            "Epoch 123/200, Batch 8/45, Loss: 2.221756935119629\n",
            "Epoch 123/200, Batch 9/45, Loss: 2.353559970855713\n",
            "Epoch 123/200, Batch 10/45, Loss: 1.892350196838379\n",
            "Epoch 123/200, Batch 11/45, Loss: 2.657809019088745\n",
            "Epoch 123/200, Batch 12/45, Loss: 2.592402935028076\n",
            "Epoch 123/200, Batch 13/45, Loss: 1.6889257431030273\n",
            "Epoch 123/200, Batch 14/45, Loss: 2.5235047340393066\n",
            "Epoch 123/200, Batch 15/45, Loss: 2.397028923034668\n",
            "Epoch 123/200, Batch 16/45, Loss: 1.9727199077606201\n",
            "Epoch 123/200, Batch 17/45, Loss: 2.0928523540496826\n",
            "Epoch 123/200, Batch 18/45, Loss: 2.0302541255950928\n",
            "Epoch 123/200, Batch 19/45, Loss: 1.8974536657333374\n",
            "Epoch 123/200, Batch 20/45, Loss: 2.338174343109131\n",
            "Epoch 123/200, Batch 21/45, Loss: 2.0493268966674805\n",
            "Epoch 123/200, Batch 22/45, Loss: 2.2940492630004883\n",
            "Epoch 123/200, Batch 23/45, Loss: 2.098130941390991\n",
            "Epoch 123/200, Batch 24/45, Loss: 2.295436382293701\n",
            "Epoch 123/200, Batch 25/45, Loss: 2.37089467048645\n",
            "Epoch 123/200, Batch 26/45, Loss: 1.4612399339675903\n",
            "Epoch 123/200, Batch 27/45, Loss: 1.9244229793548584\n",
            "Epoch 123/200, Batch 28/45, Loss: 2.6173949241638184\n",
            "Epoch 123/200, Batch 29/45, Loss: 2.5062084197998047\n",
            "Epoch 123/200, Batch 30/45, Loss: 2.663325309753418\n",
            "Epoch 123/200, Batch 31/45, Loss: 1.9269113540649414\n",
            "Epoch 123/200, Batch 32/45, Loss: 2.816063165664673\n",
            "Epoch 123/200, Batch 33/45, Loss: 2.211560010910034\n",
            "Epoch 123/200, Batch 34/45, Loss: 3.184938430786133\n",
            "Epoch 123/200, Batch 35/45, Loss: 2.686161518096924\n",
            "Epoch 123/200, Batch 36/45, Loss: 2.792180061340332\n",
            "Epoch 123/200, Batch 37/45, Loss: 2.792543411254883\n",
            "Epoch 123/200, Batch 38/45, Loss: 2.022761821746826\n",
            "Epoch 123/200, Batch 39/45, Loss: 2.6164770126342773\n",
            "Epoch 123/200, Batch 40/45, Loss: 2.771071434020996\n",
            "Epoch 123/200, Batch 41/45, Loss: 2.2387542724609375\n",
            "Epoch 123/200, Batch 42/45, Loss: 2.210338830947876\n",
            "Epoch 123/200, Batch 43/45, Loss: 2.595815658569336\n",
            "Epoch 123/200, Batch 44/45, Loss: 1.9638078212738037\n",
            "Epoch 123/200, Batch 45/45, Loss: 2.3568599224090576\n",
            "Validating and Checkpointing!\n",
            "Model is not good (might be overfitting)! Current val MSE:  25.339564442634583 Best Val MSE:  25.10259437561035\n",
            "Epoch:  124 , Time Elapsed:  40.38041393756866  mins\n",
            "Epoch 124/200, Batch 1/45, Loss: 1.9110276699066162\n",
            "Epoch 124/200, Batch 2/45, Loss: 2.357344627380371\n",
            "Epoch 124/200, Batch 3/45, Loss: 2.938145399093628\n",
            "Epoch 124/200, Batch 4/45, Loss: 3.614408493041992\n",
            "Epoch 124/200, Batch 5/45, Loss: 2.5213685035705566\n",
            "Epoch 124/200, Batch 6/45, Loss: 2.130675792694092\n",
            "Epoch 124/200, Batch 7/45, Loss: 2.8329174518585205\n",
            "Epoch 124/200, Batch 8/45, Loss: 2.102602958679199\n",
            "Epoch 124/200, Batch 9/45, Loss: 1.989274501800537\n",
            "Epoch 124/200, Batch 10/45, Loss: 2.3510260581970215\n",
            "Epoch 124/200, Batch 11/45, Loss: 2.459761142730713\n",
            "Epoch 124/200, Batch 12/45, Loss: 2.388918399810791\n",
            "Epoch 124/200, Batch 13/45, Loss: 2.5642521381378174\n",
            "Epoch 124/200, Batch 14/45, Loss: 2.3548495769500732\n",
            "Epoch 124/200, Batch 15/45, Loss: 2.3590078353881836\n",
            "Epoch 124/200, Batch 16/45, Loss: 2.3224494457244873\n",
            "Epoch 124/200, Batch 17/45, Loss: 2.321077346801758\n",
            "Epoch 124/200, Batch 18/45, Loss: 2.5585618019104004\n",
            "Epoch 124/200, Batch 19/45, Loss: 1.594868540763855\n",
            "Epoch 124/200, Batch 20/45, Loss: 1.5823272466659546\n",
            "Epoch 124/200, Batch 21/45, Loss: 2.462597131729126\n",
            "Epoch 124/200, Batch 22/45, Loss: 1.757361888885498\n",
            "Epoch 124/200, Batch 23/45, Loss: 2.5767757892608643\n",
            "Epoch 124/200, Batch 24/45, Loss: 1.6390659809112549\n",
            "Epoch 124/200, Batch 25/45, Loss: 2.155860185623169\n",
            "Epoch 124/200, Batch 26/45, Loss: 3.33031964302063\n",
            "Epoch 124/200, Batch 27/45, Loss: 2.38688063621521\n",
            "Epoch 124/200, Batch 28/45, Loss: 2.518646001815796\n",
            "Epoch 124/200, Batch 29/45, Loss: 1.8488281965255737\n",
            "Epoch 124/200, Batch 30/45, Loss: 2.3453290462493896\n",
            "Epoch 124/200, Batch 31/45, Loss: 2.5062503814697266\n",
            "Epoch 124/200, Batch 32/45, Loss: 2.372159481048584\n",
            "Epoch 124/200, Batch 33/45, Loss: 2.0149033069610596\n",
            "Epoch 124/200, Batch 34/45, Loss: 2.710421085357666\n",
            "Epoch 124/200, Batch 35/45, Loss: 2.0057644844055176\n",
            "Epoch 124/200, Batch 36/45, Loss: 2.6060478687286377\n",
            "Epoch 124/200, Batch 37/45, Loss: 2.15146541595459\n",
            "Epoch 124/200, Batch 38/45, Loss: 1.9340388774871826\n",
            "Epoch 124/200, Batch 39/45, Loss: 2.5750951766967773\n",
            "Epoch 124/200, Batch 40/45, Loss: 2.510315179824829\n",
            "Epoch 124/200, Batch 41/45, Loss: 2.9632937908172607\n",
            "Epoch 124/200, Batch 42/45, Loss: 1.921065092086792\n",
            "Epoch 124/200, Batch 43/45, Loss: 1.9073697328567505\n",
            "Epoch 124/200, Batch 44/45, Loss: 2.2117271423339844\n",
            "Epoch 124/200, Batch 45/45, Loss: 1.6537847518920898\n",
            "Validating and Checkpointing!\n",
            "Model is not good (might be overfitting)! Current val MSE:  27.17105269432068 Best Val MSE:  25.10259437561035\n",
            "Epoch:  125 , Time Elapsed:  40.704156970977785  mins\n",
            "Epoch 125/200, Batch 1/45, Loss: 2.153167724609375\n",
            "Epoch 125/200, Batch 2/45, Loss: 2.376472234725952\n",
            "Epoch 125/200, Batch 3/45, Loss: 1.8103549480438232\n",
            "Epoch 125/200, Batch 4/45, Loss: 1.8756096363067627\n",
            "Epoch 125/200, Batch 5/45, Loss: 1.7037203311920166\n",
            "Epoch 125/200, Batch 6/45, Loss: 2.1432278156280518\n",
            "Epoch 125/200, Batch 7/45, Loss: 1.9065021276474\n",
            "Epoch 125/200, Batch 8/45, Loss: 2.5144829750061035\n",
            "Epoch 125/200, Batch 9/45, Loss: 1.8897086381912231\n",
            "Epoch 125/200, Batch 10/45, Loss: 2.1052422523498535\n",
            "Epoch 125/200, Batch 11/45, Loss: 2.355020523071289\n",
            "Epoch 125/200, Batch 12/45, Loss: 1.4318500757217407\n",
            "Epoch 125/200, Batch 13/45, Loss: 2.3153481483459473\n",
            "Epoch 125/200, Batch 14/45, Loss: 2.635246753692627\n",
            "Epoch 125/200, Batch 15/45, Loss: 2.3638315200805664\n",
            "Epoch 125/200, Batch 16/45, Loss: 1.8187973499298096\n",
            "Epoch 125/200, Batch 17/45, Loss: 2.3938910961151123\n",
            "Epoch 125/200, Batch 18/45, Loss: 2.4890847206115723\n",
            "Epoch 125/200, Batch 19/45, Loss: 2.4420626163482666\n",
            "Epoch 125/200, Batch 20/45, Loss: 2.1672539710998535\n",
            "Epoch 125/200, Batch 21/45, Loss: 2.353360652923584\n",
            "Epoch 125/200, Batch 22/45, Loss: 2.9179892539978027\n",
            "Epoch 125/200, Batch 23/45, Loss: 2.202848434448242\n",
            "Epoch 125/200, Batch 24/45, Loss: 1.908576250076294\n",
            "Epoch 125/200, Batch 25/45, Loss: 2.587581157684326\n",
            "Epoch 125/200, Batch 26/45, Loss: 2.409346580505371\n",
            "Epoch 125/200, Batch 27/45, Loss: 2.5175275802612305\n",
            "Epoch 125/200, Batch 28/45, Loss: 1.734778881072998\n",
            "Epoch 125/200, Batch 29/45, Loss: 1.5155220031738281\n",
            "Epoch 125/200, Batch 30/45, Loss: 2.2995479106903076\n",
            "Epoch 125/200, Batch 31/45, Loss: 2.104825973510742\n",
            "Epoch 125/200, Batch 32/45, Loss: 2.398317575454712\n",
            "Epoch 125/200, Batch 33/45, Loss: 1.899679183959961\n",
            "Epoch 125/200, Batch 34/45, Loss: 2.033726692199707\n",
            "Epoch 125/200, Batch 35/45, Loss: 1.998185157775879\n",
            "Epoch 125/200, Batch 36/45, Loss: 1.9496344327926636\n",
            "Epoch 125/200, Batch 37/45, Loss: 2.1500182151794434\n",
            "Epoch 125/200, Batch 38/45, Loss: 2.3921070098876953\n",
            "Epoch 125/200, Batch 39/45, Loss: 2.5219805240631104\n",
            "Epoch 125/200, Batch 40/45, Loss: 2.346973180770874\n",
            "Epoch 125/200, Batch 41/45, Loss: 2.4117343425750732\n",
            "Epoch 125/200, Batch 42/45, Loss: 2.508222818374634\n",
            "Epoch 125/200, Batch 43/45, Loss: 2.2250020503997803\n",
            "Epoch 125/200, Batch 44/45, Loss: 1.6267226934432983\n",
            "Epoch 125/200, Batch 45/45, Loss: 2.527759552001953\n",
            "Validating and Checkpointing!\n",
            "Model is not good (might be overfitting)! Current val MSE:  26.479718327522278 Best Val MSE:  25.10259437561035\n",
            "Epoch:  126 , Time Elapsed:  41.05980018377304  mins\n",
            "Epoch 126/200, Batch 1/45, Loss: 2.4160866737365723\n",
            "Epoch 126/200, Batch 2/45, Loss: 2.2985143661499023\n",
            "Epoch 126/200, Batch 3/45, Loss: 1.7650084495544434\n",
            "Epoch 126/200, Batch 4/45, Loss: 2.0587291717529297\n",
            "Epoch 126/200, Batch 5/45, Loss: 2.158020257949829\n",
            "Epoch 126/200, Batch 6/45, Loss: 2.6279098987579346\n",
            "Epoch 126/200, Batch 7/45, Loss: 2.5120432376861572\n",
            "Epoch 126/200, Batch 8/45, Loss: 2.118807077407837\n",
            "Epoch 126/200, Batch 9/45, Loss: 2.679598331451416\n",
            "Epoch 126/200, Batch 10/45, Loss: 2.5597424507141113\n",
            "Epoch 126/200, Batch 11/45, Loss: 2.456210136413574\n",
            "Epoch 126/200, Batch 12/45, Loss: 2.1213202476501465\n",
            "Epoch 126/200, Batch 13/45, Loss: 1.9468388557434082\n",
            "Epoch 126/200, Batch 14/45, Loss: 2.119910955429077\n",
            "Epoch 126/200, Batch 15/45, Loss: 1.7717156410217285\n",
            "Epoch 126/200, Batch 16/45, Loss: 2.397709846496582\n",
            "Epoch 126/200, Batch 17/45, Loss: 3.6426072120666504\n",
            "Epoch 126/200, Batch 18/45, Loss: 2.692518711090088\n",
            "Epoch 126/200, Batch 19/45, Loss: 2.058231830596924\n",
            "Epoch 126/200, Batch 20/45, Loss: 1.2120921611785889\n",
            "Epoch 126/200, Batch 21/45, Loss: 2.0294995307922363\n",
            "Epoch 126/200, Batch 22/45, Loss: 2.0461020469665527\n",
            "Epoch 126/200, Batch 23/45, Loss: 1.9991624355316162\n",
            "Epoch 126/200, Batch 24/45, Loss: 2.40818190574646\n",
            "Epoch 126/200, Batch 25/45, Loss: 2.011260509490967\n",
            "Epoch 126/200, Batch 26/45, Loss: 2.7910685539245605\n",
            "Epoch 126/200, Batch 27/45, Loss: 2.447099447250366\n",
            "Epoch 126/200, Batch 28/45, Loss: 2.3336050510406494\n",
            "Epoch 126/200, Batch 29/45, Loss: 1.31415593624115\n",
            "Epoch 126/200, Batch 30/45, Loss: 2.02043080329895\n",
            "Epoch 126/200, Batch 31/45, Loss: 2.5259642601013184\n",
            "Epoch 126/200, Batch 32/45, Loss: 2.231102705001831\n",
            "Epoch 126/200, Batch 33/45, Loss: 2.7300612926483154\n",
            "Epoch 126/200, Batch 34/45, Loss: 2.1175882816314697\n",
            "Epoch 126/200, Batch 35/45, Loss: 2.1773881912231445\n",
            "Epoch 126/200, Batch 36/45, Loss: 2.047973155975342\n",
            "Epoch 126/200, Batch 37/45, Loss: 2.1059207916259766\n",
            "Epoch 126/200, Batch 38/45, Loss: 2.1941850185394287\n",
            "Epoch 126/200, Batch 39/45, Loss: 2.6809639930725098\n",
            "Epoch 126/200, Batch 40/45, Loss: 2.4780924320220947\n",
            "Epoch 126/200, Batch 41/45, Loss: 2.2899272441864014\n",
            "Epoch 126/200, Batch 42/45, Loss: 1.9461681842803955\n",
            "Epoch 126/200, Batch 43/45, Loss: 1.8540849685668945\n",
            "Epoch 126/200, Batch 44/45, Loss: 1.224101185798645\n",
            "Epoch 126/200, Batch 45/45, Loss: 2.464763641357422\n",
            "Validating and Checkpointing!\n",
            "Model is not good (might be overfitting)! Current val MSE:  28.197133421897888 Best Val MSE:  25.10259437561035\n",
            "Epoch:  127 , Time Elapsed:  41.3841415365537  mins\n",
            "Epoch 127/200, Batch 1/45, Loss: 2.127298593521118\n",
            "Epoch 127/200, Batch 2/45, Loss: 1.6979385614395142\n",
            "Epoch 127/200, Batch 3/45, Loss: 2.2268424034118652\n",
            "Epoch 127/200, Batch 4/45, Loss: 1.7388737201690674\n",
            "Epoch 127/200, Batch 5/45, Loss: 2.530897378921509\n",
            "Epoch 127/200, Batch 6/45, Loss: 4.225390434265137\n",
            "Epoch 127/200, Batch 7/45, Loss: 2.6714277267456055\n",
            "Epoch 127/200, Batch 8/45, Loss: 2.9105429649353027\n",
            "Epoch 127/200, Batch 9/45, Loss: 1.8939366340637207\n",
            "Epoch 127/200, Batch 10/45, Loss: 1.7935707569122314\n",
            "Epoch 127/200, Batch 11/45, Loss: 4.013253688812256\n",
            "Epoch 127/200, Batch 12/45, Loss: 1.772252082824707\n",
            "Epoch 127/200, Batch 13/45, Loss: 1.9411349296569824\n",
            "Epoch 127/200, Batch 14/45, Loss: 3.5587549209594727\n",
            "Epoch 127/200, Batch 15/45, Loss: 1.721441626548767\n",
            "Epoch 127/200, Batch 16/45, Loss: 2.041187286376953\n",
            "Epoch 127/200, Batch 17/45, Loss: 1.881534457206726\n",
            "Epoch 127/200, Batch 18/45, Loss: 2.319553852081299\n",
            "Epoch 127/200, Batch 19/45, Loss: 2.284153461456299\n",
            "Epoch 127/200, Batch 20/45, Loss: 1.64817476272583\n",
            "Epoch 127/200, Batch 21/45, Loss: 2.1933138370513916\n",
            "Epoch 127/200, Batch 22/45, Loss: 1.7932140827178955\n",
            "Epoch 127/200, Batch 23/45, Loss: 1.9164921045303345\n",
            "Epoch 127/200, Batch 24/45, Loss: 2.0930933952331543\n",
            "Epoch 127/200, Batch 25/45, Loss: 2.4235501289367676\n",
            "Epoch 127/200, Batch 26/45, Loss: 2.4082131385803223\n",
            "Epoch 127/200, Batch 27/45, Loss: 1.8810169696807861\n",
            "Epoch 127/200, Batch 28/45, Loss: 1.9836047887802124\n",
            "Epoch 127/200, Batch 29/45, Loss: 2.359755754470825\n",
            "Epoch 127/200, Batch 30/45, Loss: 2.3792595863342285\n",
            "Epoch 127/200, Batch 31/45, Loss: 1.9859583377838135\n",
            "Epoch 127/200, Batch 32/45, Loss: 2.770862579345703\n",
            "Epoch 127/200, Batch 33/45, Loss: 1.9849671125411987\n",
            "Epoch 127/200, Batch 34/45, Loss: 2.5268490314483643\n",
            "Epoch 127/200, Batch 35/45, Loss: 2.118873119354248\n",
            "Epoch 127/200, Batch 36/45, Loss: 2.6476526260375977\n",
            "Epoch 127/200, Batch 37/45, Loss: 1.6069278717041016\n",
            "Epoch 127/200, Batch 38/45, Loss: 2.3290088176727295\n",
            "Epoch 127/200, Batch 39/45, Loss: 2.3780710697174072\n",
            "Epoch 127/200, Batch 40/45, Loss: 2.04093074798584\n",
            "Epoch 127/200, Batch 41/45, Loss: 2.0376086235046387\n",
            "Epoch 127/200, Batch 42/45, Loss: 2.4129176139831543\n",
            "Epoch 127/200, Batch 43/45, Loss: 2.292348623275757\n",
            "Epoch 127/200, Batch 44/45, Loss: 2.11655330657959\n",
            "Epoch 127/200, Batch 45/45, Loss: 1.7677738666534424\n",
            "Validating and Checkpointing!\n",
            "Model is not good (might be overfitting)! Current val MSE:  27.552257895469666 Best Val MSE:  25.10259437561035\n",
            "Epoch:  128 , Time Elapsed:  41.7148757259051  mins\n",
            "Epoch 128/200, Batch 1/45, Loss: 2.124403953552246\n",
            "Epoch 128/200, Batch 2/45, Loss: 2.761359453201294\n",
            "Epoch 128/200, Batch 3/45, Loss: 2.0261383056640625\n",
            "Epoch 128/200, Batch 4/45, Loss: 2.360133171081543\n",
            "Epoch 128/200, Batch 5/45, Loss: 2.701521635055542\n",
            "Epoch 128/200, Batch 6/45, Loss: 2.491753339767456\n",
            "Epoch 128/200, Batch 7/45, Loss: 2.739759922027588\n",
            "Epoch 128/200, Batch 8/45, Loss: 2.3518900871276855\n",
            "Epoch 128/200, Batch 9/45, Loss: 2.3471198081970215\n",
            "Epoch 128/200, Batch 10/45, Loss: 1.931950569152832\n",
            "Epoch 128/200, Batch 11/45, Loss: 2.339766502380371\n",
            "Epoch 128/200, Batch 12/45, Loss: 2.1355843544006348\n",
            "Epoch 128/200, Batch 13/45, Loss: 1.9310524463653564\n",
            "Epoch 128/200, Batch 14/45, Loss: 2.8870463371276855\n",
            "Epoch 128/200, Batch 15/45, Loss: 2.206294536590576\n",
            "Epoch 128/200, Batch 16/45, Loss: 2.092082977294922\n",
            "Epoch 128/200, Batch 17/45, Loss: 2.52215838432312\n",
            "Epoch 128/200, Batch 18/45, Loss: 2.3835301399230957\n",
            "Epoch 128/200, Batch 19/45, Loss: 2.3373260498046875\n",
            "Epoch 128/200, Batch 20/45, Loss: 1.9193427562713623\n",
            "Epoch 128/200, Batch 21/45, Loss: 1.861573576927185\n",
            "Epoch 128/200, Batch 22/45, Loss: 2.0277624130249023\n",
            "Epoch 128/200, Batch 23/45, Loss: 2.183910369873047\n",
            "Epoch 128/200, Batch 24/45, Loss: 1.6096622943878174\n",
            "Epoch 128/200, Batch 25/45, Loss: 4.215118408203125\n",
            "Epoch 128/200, Batch 26/45, Loss: 2.279573917388916\n",
            "Epoch 128/200, Batch 27/45, Loss: 2.042856454849243\n",
            "Epoch 128/200, Batch 28/45, Loss: 2.734600067138672\n",
            "Epoch 128/200, Batch 29/45, Loss: 1.6431901454925537\n",
            "Epoch 128/200, Batch 30/45, Loss: 1.9225666522979736\n",
            "Epoch 128/200, Batch 31/45, Loss: 2.345730781555176\n",
            "Epoch 128/200, Batch 32/45, Loss: 2.2529280185699463\n",
            "Epoch 128/200, Batch 33/45, Loss: 2.434082508087158\n",
            "Epoch 128/200, Batch 34/45, Loss: 1.691516399383545\n",
            "Epoch 128/200, Batch 35/45, Loss: 2.2853081226348877\n",
            "Epoch 128/200, Batch 36/45, Loss: 2.1941709518432617\n",
            "Epoch 128/200, Batch 37/45, Loss: 1.8507020473480225\n",
            "Epoch 128/200, Batch 38/45, Loss: 3.119128942489624\n",
            "Epoch 128/200, Batch 39/45, Loss: 2.0795037746429443\n",
            "Epoch 128/200, Batch 40/45, Loss: 2.110100030899048\n",
            "Epoch 128/200, Batch 41/45, Loss: 2.0047690868377686\n",
            "Epoch 128/200, Batch 42/45, Loss: 2.2482337951660156\n",
            "Epoch 128/200, Batch 43/45, Loss: 2.6841931343078613\n",
            "Epoch 128/200, Batch 44/45, Loss: 2.4509212970733643\n",
            "Epoch 128/200, Batch 45/45, Loss: 2.570181131362915\n",
            "Validating and Checkpointing!\n",
            "Model is not good (might be overfitting)! Current val MSE:  29.58735752105713 Best Val MSE:  25.10259437561035\n",
            "Epoch:  129 , Time Elapsed:  42.111277842521666  mins\n",
            "Epoch 129/200, Batch 1/45, Loss: 2.189460277557373\n",
            "Epoch 129/200, Batch 2/45, Loss: 2.8149051666259766\n",
            "Epoch 129/200, Batch 3/45, Loss: 2.189286708831787\n",
            "Epoch 129/200, Batch 4/45, Loss: 2.2049248218536377\n",
            "Epoch 129/200, Batch 5/45, Loss: 2.072662353515625\n",
            "Epoch 129/200, Batch 6/45, Loss: 2.4949450492858887\n",
            "Epoch 129/200, Batch 7/45, Loss: 2.392709493637085\n",
            "Epoch 129/200, Batch 8/45, Loss: 2.640416383743286\n",
            "Epoch 129/200, Batch 9/45, Loss: 2.038109540939331\n",
            "Epoch 129/200, Batch 10/45, Loss: 1.645710825920105\n",
            "Epoch 129/200, Batch 11/45, Loss: 2.400801181793213\n",
            "Epoch 129/200, Batch 12/45, Loss: 2.272904872894287\n",
            "Epoch 129/200, Batch 13/45, Loss: 4.887269020080566\n",
            "Epoch 129/200, Batch 14/45, Loss: 1.9504753351211548\n",
            "Epoch 129/200, Batch 15/45, Loss: 2.098820924758911\n",
            "Epoch 129/200, Batch 16/45, Loss: 2.746654748916626\n",
            "Epoch 129/200, Batch 17/45, Loss: 1.4491844177246094\n",
            "Epoch 129/200, Batch 18/45, Loss: 2.3196146488189697\n",
            "Epoch 129/200, Batch 19/45, Loss: 2.775674343109131\n",
            "Epoch 129/200, Batch 20/45, Loss: 1.5288498401641846\n",
            "Epoch 129/200, Batch 21/45, Loss: 2.306751251220703\n",
            "Epoch 129/200, Batch 22/45, Loss: 2.4754624366760254\n",
            "Epoch 129/200, Batch 23/45, Loss: 2.7662744522094727\n",
            "Epoch 129/200, Batch 24/45, Loss: 1.5747601985931396\n",
            "Epoch 129/200, Batch 25/45, Loss: 2.311734199523926\n",
            "Epoch 129/200, Batch 26/45, Loss: 2.7862138748168945\n",
            "Epoch 129/200, Batch 27/45, Loss: 2.755709648132324\n",
            "Epoch 129/200, Batch 28/45, Loss: 1.848283290863037\n",
            "Epoch 129/200, Batch 29/45, Loss: 2.0171396732330322\n",
            "Epoch 129/200, Batch 30/45, Loss: 2.0427019596099854\n",
            "Epoch 129/200, Batch 31/45, Loss: 2.615441083908081\n",
            "Epoch 129/200, Batch 32/45, Loss: 2.1028709411621094\n",
            "Epoch 129/200, Batch 33/45, Loss: 2.313771963119507\n",
            "Epoch 129/200, Batch 34/45, Loss: 2.239210844039917\n",
            "Epoch 129/200, Batch 35/45, Loss: 2.879485607147217\n",
            "Epoch 129/200, Batch 36/45, Loss: 2.1679575443267822\n",
            "Epoch 129/200, Batch 37/45, Loss: 2.222127676010132\n",
            "Epoch 129/200, Batch 38/45, Loss: 2.1472995281219482\n",
            "Epoch 129/200, Batch 39/45, Loss: 2.5829391479492188\n",
            "Epoch 129/200, Batch 40/45, Loss: 1.990970253944397\n",
            "Epoch 129/200, Batch 41/45, Loss: 2.2526204586029053\n",
            "Epoch 129/200, Batch 42/45, Loss: 2.213496208190918\n",
            "Epoch 129/200, Batch 43/45, Loss: 2.125429391860962\n",
            "Epoch 129/200, Batch 44/45, Loss: 1.7243151664733887\n",
            "Epoch 129/200, Batch 45/45, Loss: 2.1511168479919434\n",
            "Validating and Checkpointing!\n",
            "Model is not good (might be overfitting)! Current val MSE:  27.498665928840637 Best Val MSE:  25.10259437561035\n",
            "Epoch:  130 , Time Elapsed:  42.4497793396314  mins\n",
            "Epoch 130/200, Batch 1/45, Loss: 2.2097573280334473\n",
            "Epoch 130/200, Batch 2/45, Loss: 2.918825149536133\n",
            "Epoch 130/200, Batch 3/45, Loss: 2.3372979164123535\n",
            "Epoch 130/200, Batch 4/45, Loss: 2.2574617862701416\n",
            "Epoch 130/200, Batch 5/45, Loss: 1.9028747081756592\n",
            "Epoch 130/200, Batch 6/45, Loss: 2.1653616428375244\n",
            "Epoch 130/200, Batch 7/45, Loss: 1.6803733110427856\n",
            "Epoch 130/200, Batch 8/45, Loss: 1.9496066570281982\n",
            "Epoch 130/200, Batch 9/45, Loss: 2.409109115600586\n",
            "Epoch 130/200, Batch 10/45, Loss: 2.147285223007202\n",
            "Epoch 130/200, Batch 11/45, Loss: 2.0233888626098633\n",
            "Epoch 130/200, Batch 12/45, Loss: 1.856886386871338\n",
            "Epoch 130/200, Batch 13/45, Loss: 2.1921355724334717\n",
            "Epoch 130/200, Batch 14/45, Loss: 1.7310665845870972\n",
            "Epoch 130/200, Batch 15/45, Loss: 2.773474931716919\n",
            "Epoch 130/200, Batch 16/45, Loss: 1.6004102230072021\n",
            "Epoch 130/200, Batch 17/45, Loss: 1.9259381294250488\n",
            "Epoch 130/200, Batch 18/45, Loss: 2.684220790863037\n",
            "Epoch 130/200, Batch 19/45, Loss: 2.208453893661499\n",
            "Epoch 130/200, Batch 20/45, Loss: 1.6384358406066895\n",
            "Epoch 130/200, Batch 21/45, Loss: 2.3722357749938965\n",
            "Epoch 130/200, Batch 22/45, Loss: 2.1067845821380615\n",
            "Epoch 130/200, Batch 23/45, Loss: 2.182121753692627\n",
            "Epoch 130/200, Batch 24/45, Loss: 1.6122592687606812\n",
            "Epoch 130/200, Batch 25/45, Loss: 1.2078065872192383\n",
            "Epoch 130/200, Batch 26/45, Loss: 2.0871944427490234\n",
            "Epoch 130/200, Batch 27/45, Loss: 1.8332841396331787\n",
            "Epoch 130/200, Batch 28/45, Loss: 1.90903639793396\n",
            "Epoch 130/200, Batch 29/45, Loss: 2.719259738922119\n",
            "Epoch 130/200, Batch 30/45, Loss: 2.8243112564086914\n",
            "Epoch 130/200, Batch 31/45, Loss: 2.2488303184509277\n",
            "Epoch 130/200, Batch 32/45, Loss: 2.1688976287841797\n",
            "Epoch 130/200, Batch 33/45, Loss: 2.643467903137207\n",
            "Epoch 130/200, Batch 34/45, Loss: 1.441507339477539\n",
            "Epoch 130/200, Batch 35/45, Loss: 2.2784721851348877\n",
            "Epoch 130/200, Batch 36/45, Loss: 2.042328357696533\n",
            "Epoch 130/200, Batch 37/45, Loss: 1.7486882209777832\n",
            "Epoch 130/200, Batch 38/45, Loss: 2.316230297088623\n",
            "Epoch 130/200, Batch 39/45, Loss: 2.309396743774414\n",
            "Epoch 130/200, Batch 40/45, Loss: 1.9857527017593384\n",
            "Epoch 130/200, Batch 41/45, Loss: 2.8535685539245605\n",
            "Epoch 130/200, Batch 42/45, Loss: 1.999679446220398\n",
            "Epoch 130/200, Batch 43/45, Loss: 1.5702407360076904\n",
            "Epoch 130/200, Batch 44/45, Loss: 2.0316030979156494\n",
            "Epoch 130/200, Batch 45/45, Loss: 2.0932908058166504\n",
            "Validating and Checkpointing!\n",
            "Model is not good (might be overfitting)! Current val MSE:  27.688831210136414 Best Val MSE:  25.10259437561035\n",
            "Epoch:  131 , Time Elapsed:  42.795197224617006  mins\n",
            "Epoch 131/200, Batch 1/45, Loss: 2.349752902984619\n",
            "Epoch 131/200, Batch 2/45, Loss: 2.1307709217071533\n",
            "Epoch 131/200, Batch 3/45, Loss: 2.160921096801758\n",
            "Epoch 131/200, Batch 4/45, Loss: 1.7606369256973267\n",
            "Epoch 131/200, Batch 5/45, Loss: 2.259772300720215\n",
            "Epoch 131/200, Batch 6/45, Loss: 2.22396183013916\n",
            "Epoch 131/200, Batch 7/45, Loss: 2.263362407684326\n",
            "Epoch 131/200, Batch 8/45, Loss: 2.4989821910858154\n",
            "Epoch 131/200, Batch 9/45, Loss: 1.5169347524642944\n",
            "Epoch 131/200, Batch 10/45, Loss: 2.159409523010254\n",
            "Epoch 131/200, Batch 11/45, Loss: 1.4305753707885742\n",
            "Epoch 131/200, Batch 12/45, Loss: 1.6028138399124146\n",
            "Epoch 131/200, Batch 13/45, Loss: 2.465371608734131\n",
            "Epoch 131/200, Batch 14/45, Loss: 2.213991165161133\n",
            "Epoch 131/200, Batch 15/45, Loss: 3.245483636856079\n",
            "Epoch 131/200, Batch 16/45, Loss: 2.7639644145965576\n",
            "Epoch 131/200, Batch 17/45, Loss: 2.6174938678741455\n",
            "Epoch 131/200, Batch 18/45, Loss: 2.509281635284424\n",
            "Epoch 131/200, Batch 19/45, Loss: 2.2885313034057617\n",
            "Epoch 131/200, Batch 20/45, Loss: 1.7552094459533691\n",
            "Epoch 131/200, Batch 21/45, Loss: 2.0075490474700928\n",
            "Epoch 131/200, Batch 22/45, Loss: 1.9766790866851807\n",
            "Epoch 131/200, Batch 23/45, Loss: 2.4253592491149902\n",
            "Epoch 131/200, Batch 24/45, Loss: 2.353700637817383\n",
            "Epoch 131/200, Batch 25/45, Loss: 1.970591425895691\n",
            "Epoch 131/200, Batch 26/45, Loss: 1.7864543199539185\n",
            "Epoch 131/200, Batch 27/45, Loss: 2.1289398670196533\n",
            "Epoch 131/200, Batch 28/45, Loss: 2.0024619102478027\n",
            "Epoch 131/200, Batch 29/45, Loss: 2.006063938140869\n",
            "Epoch 131/200, Batch 30/45, Loss: 2.5187196731567383\n",
            "Epoch 131/200, Batch 31/45, Loss: 2.470548152923584\n",
            "Epoch 131/200, Batch 32/45, Loss: 2.4793503284454346\n",
            "Epoch 131/200, Batch 33/45, Loss: 2.455981731414795\n",
            "Epoch 131/200, Batch 34/45, Loss: 2.56992506980896\n",
            "Epoch 131/200, Batch 35/45, Loss: 1.6273837089538574\n",
            "Epoch 131/200, Batch 36/45, Loss: 1.793541669845581\n",
            "Epoch 131/200, Batch 37/45, Loss: 2.3190460205078125\n",
            "Epoch 131/200, Batch 38/45, Loss: 2.0738399028778076\n",
            "Epoch 131/200, Batch 39/45, Loss: 2.249915361404419\n",
            "Epoch 131/200, Batch 40/45, Loss: 2.3943545818328857\n",
            "Epoch 131/200, Batch 41/45, Loss: 2.3775501251220703\n",
            "Epoch 131/200, Batch 42/45, Loss: 1.5527849197387695\n",
            "Epoch 131/200, Batch 43/45, Loss: 1.9374933242797852\n",
            "Epoch 131/200, Batch 44/45, Loss: 2.3227767944335938\n",
            "Epoch 131/200, Batch 45/45, Loss: 2.3228650093078613\n",
            "Validating and Checkpointing!\n",
            "Model is not good (might be overfitting)! Current val MSE:  29.19546413421631 Best Val MSE:  25.10259437561035\n",
            "Epoch:  132 , Time Elapsed:  43.131441223621366  mins\n",
            "Epoch 132/200, Batch 1/45, Loss: 2.215102195739746\n",
            "Epoch 132/200, Batch 2/45, Loss: 2.310487985610962\n",
            "Epoch 132/200, Batch 3/45, Loss: 2.303114414215088\n",
            "Epoch 132/200, Batch 4/45, Loss: 2.289693832397461\n",
            "Epoch 132/200, Batch 5/45, Loss: 2.5621256828308105\n",
            "Epoch 132/200, Batch 6/45, Loss: 2.269423484802246\n",
            "Epoch 132/200, Batch 7/45, Loss: 2.417522430419922\n",
            "Epoch 132/200, Batch 8/45, Loss: 2.386960983276367\n",
            "Epoch 132/200, Batch 9/45, Loss: 1.9781392812728882\n",
            "Epoch 132/200, Batch 10/45, Loss: 2.7972571849823\n",
            "Epoch 132/200, Batch 11/45, Loss: 1.4751479625701904\n",
            "Epoch 132/200, Batch 12/45, Loss: 1.8135885000228882\n",
            "Epoch 132/200, Batch 13/45, Loss: 2.767735481262207\n",
            "Epoch 132/200, Batch 14/45, Loss: 2.239921808242798\n",
            "Epoch 132/200, Batch 15/45, Loss: 1.9142107963562012\n",
            "Epoch 132/200, Batch 16/45, Loss: 2.0582597255706787\n",
            "Epoch 132/200, Batch 17/45, Loss: 2.265043020248413\n",
            "Epoch 132/200, Batch 18/45, Loss: 2.2216196060180664\n",
            "Epoch 132/200, Batch 19/45, Loss: 2.775852680206299\n",
            "Epoch 132/200, Batch 20/45, Loss: 1.9637221097946167\n",
            "Epoch 132/200, Batch 21/45, Loss: 2.394444227218628\n",
            "Epoch 132/200, Batch 22/45, Loss: 1.693354606628418\n",
            "Epoch 132/200, Batch 23/45, Loss: 2.535968065261841\n",
            "Epoch 132/200, Batch 24/45, Loss: 1.4446923732757568\n",
            "Epoch 132/200, Batch 25/45, Loss: 1.7883057594299316\n",
            "Epoch 132/200, Batch 26/45, Loss: 2.925088405609131\n",
            "Epoch 132/200, Batch 27/45, Loss: 1.4557708501815796\n",
            "Epoch 132/200, Batch 28/45, Loss: 1.8564274311065674\n",
            "Epoch 132/200, Batch 29/45, Loss: 2.832456111907959\n",
            "Epoch 132/200, Batch 30/45, Loss: 2.2752468585968018\n",
            "Epoch 132/200, Batch 31/45, Loss: 1.9079960584640503\n",
            "Epoch 132/200, Batch 32/45, Loss: 2.9242048263549805\n",
            "Epoch 132/200, Batch 33/45, Loss: 2.0914812088012695\n",
            "Epoch 132/200, Batch 34/45, Loss: 2.2788803577423096\n",
            "Epoch 132/200, Batch 35/45, Loss: 2.3497791290283203\n",
            "Epoch 132/200, Batch 36/45, Loss: 2.171647310256958\n",
            "Epoch 132/200, Batch 37/45, Loss: 2.4841222763061523\n",
            "Epoch 132/200, Batch 38/45, Loss: 1.6214513778686523\n",
            "Epoch 132/200, Batch 39/45, Loss: 1.8217850923538208\n",
            "Epoch 132/200, Batch 40/45, Loss: 2.3651885986328125\n",
            "Epoch 132/200, Batch 41/45, Loss: 1.542020559310913\n",
            "Epoch 132/200, Batch 42/45, Loss: 2.2484664916992188\n",
            "Epoch 132/200, Batch 43/45, Loss: 1.6524076461791992\n",
            "Epoch 132/200, Batch 44/45, Loss: 2.0338547229766846\n",
            "Epoch 132/200, Batch 45/45, Loss: 1.8313326835632324\n",
            "Validating and Checkpointing!\n",
            "Model is not good (might be overfitting)! Current val MSE:  25.991825580596924 Best Val MSE:  25.10259437561035\n",
            "Epoch:  133 , Time Elapsed:  43.469811769326526  mins\n",
            "Epoch 133/200, Batch 1/45, Loss: 2.2096478939056396\n",
            "Epoch 133/200, Batch 2/45, Loss: 1.9543122053146362\n",
            "Epoch 133/200, Batch 3/45, Loss: 1.8744202852249146\n",
            "Epoch 133/200, Batch 4/45, Loss: 2.458491325378418\n",
            "Epoch 133/200, Batch 5/45, Loss: 2.564851760864258\n",
            "Epoch 133/200, Batch 6/45, Loss: 1.6582777500152588\n",
            "Epoch 133/200, Batch 7/45, Loss: 2.1167044639587402\n",
            "Epoch 133/200, Batch 8/45, Loss: 1.501498818397522\n",
            "Epoch 133/200, Batch 9/45, Loss: 1.868415117263794\n",
            "Epoch 133/200, Batch 10/45, Loss: 1.7727185487747192\n",
            "Epoch 133/200, Batch 11/45, Loss: 2.1038804054260254\n",
            "Epoch 133/200, Batch 12/45, Loss: 2.337552785873413\n",
            "Epoch 133/200, Batch 13/45, Loss: 2.280545711517334\n",
            "Epoch 133/200, Batch 14/45, Loss: 2.3841938972473145\n",
            "Epoch 133/200, Batch 15/45, Loss: 3.5131754875183105\n",
            "Epoch 133/200, Batch 16/45, Loss: 2.0243139266967773\n",
            "Epoch 133/200, Batch 17/45, Loss: 2.7251038551330566\n",
            "Epoch 133/200, Batch 18/45, Loss: 1.3738434314727783\n",
            "Epoch 133/200, Batch 19/45, Loss: 2.005352020263672\n",
            "Epoch 133/200, Batch 20/45, Loss: 2.532630443572998\n",
            "Epoch 133/200, Batch 21/45, Loss: 1.8527648448944092\n",
            "Epoch 133/200, Batch 22/45, Loss: 2.202043056488037\n",
            "Epoch 133/200, Batch 23/45, Loss: 2.2619822025299072\n",
            "Epoch 133/200, Batch 24/45, Loss: 2.165421962738037\n",
            "Epoch 133/200, Batch 25/45, Loss: 2.749270439147949\n",
            "Epoch 133/200, Batch 26/45, Loss: 2.745014190673828\n",
            "Epoch 133/200, Batch 27/45, Loss: 2.174931764602661\n",
            "Epoch 133/200, Batch 28/45, Loss: 2.176701307296753\n",
            "Epoch 133/200, Batch 29/45, Loss: 2.013213872909546\n",
            "Epoch 133/200, Batch 30/45, Loss: 2.3628809452056885\n",
            "Epoch 133/200, Batch 31/45, Loss: 2.2394189834594727\n",
            "Epoch 133/200, Batch 32/45, Loss: 2.062389373779297\n",
            "Epoch 133/200, Batch 33/45, Loss: 2.0791420936584473\n",
            "Epoch 133/200, Batch 34/45, Loss: 2.067927598953247\n",
            "Epoch 133/200, Batch 35/45, Loss: 1.8002688884735107\n",
            "Epoch 133/200, Batch 36/45, Loss: 1.8890607357025146\n",
            "Epoch 133/200, Batch 37/45, Loss: 2.217499256134033\n",
            "Epoch 133/200, Batch 38/45, Loss: 2.311692714691162\n",
            "Epoch 133/200, Batch 39/45, Loss: 1.9151400327682495\n",
            "Epoch 133/200, Batch 40/45, Loss: 1.9567487239837646\n",
            "Epoch 133/200, Batch 41/45, Loss: 2.1684515476226807\n",
            "Epoch 133/200, Batch 42/45, Loss: 2.321305751800537\n",
            "Epoch 133/200, Batch 43/45, Loss: 2.194141149520874\n",
            "Epoch 133/200, Batch 44/45, Loss: 2.6179397106170654\n",
            "Epoch 133/200, Batch 45/45, Loss: 1.8818769454956055\n",
            "Validating and Checkpointing!\n",
            "Model is not good (might be overfitting)! Current val MSE:  27.81408929824829 Best Val MSE:  25.10259437561035\n",
            "Epoch:  134 , Time Elapsed:  43.83586728175481  mins\n",
            "Epoch 134/200, Batch 1/45, Loss: 2.1530518531799316\n",
            "Epoch 134/200, Batch 2/45, Loss: 1.9562305212020874\n",
            "Epoch 134/200, Batch 3/45, Loss: 2.9002206325531006\n",
            "Epoch 134/200, Batch 4/45, Loss: 2.070922613143921\n",
            "Epoch 134/200, Batch 5/45, Loss: 2.027891159057617\n",
            "Epoch 134/200, Batch 6/45, Loss: 1.9357256889343262\n",
            "Epoch 134/200, Batch 7/45, Loss: 2.0609078407287598\n",
            "Epoch 134/200, Batch 8/45, Loss: 1.898196816444397\n",
            "Epoch 134/200, Batch 9/45, Loss: 2.660529851913452\n",
            "Epoch 134/200, Batch 10/45, Loss: 2.2675282955169678\n",
            "Epoch 134/200, Batch 11/45, Loss: 2.181589365005493\n",
            "Epoch 134/200, Batch 12/45, Loss: 1.6066663265228271\n",
            "Epoch 134/200, Batch 13/45, Loss: 2.4564976692199707\n",
            "Epoch 134/200, Batch 14/45, Loss: 1.5802396535873413\n",
            "Epoch 134/200, Batch 15/45, Loss: 1.7192449569702148\n",
            "Epoch 134/200, Batch 16/45, Loss: 2.644602060317993\n",
            "Epoch 134/200, Batch 17/45, Loss: 1.9265425205230713\n",
            "Epoch 134/200, Batch 18/45, Loss: 2.3212385177612305\n",
            "Epoch 134/200, Batch 19/45, Loss: 2.4190316200256348\n",
            "Epoch 134/200, Batch 20/45, Loss: 2.083191156387329\n",
            "Epoch 134/200, Batch 21/45, Loss: 3.5009469985961914\n",
            "Epoch 134/200, Batch 22/45, Loss: 2.2254927158355713\n",
            "Epoch 134/200, Batch 23/45, Loss: 1.8054909706115723\n",
            "Epoch 134/200, Batch 24/45, Loss: 2.1456456184387207\n",
            "Epoch 134/200, Batch 25/45, Loss: 2.380479574203491\n",
            "Epoch 134/200, Batch 26/45, Loss: 2.279062271118164\n",
            "Epoch 134/200, Batch 27/45, Loss: 2.07456111907959\n",
            "Epoch 134/200, Batch 28/45, Loss: 2.5850932598114014\n",
            "Epoch 134/200, Batch 29/45, Loss: 2.5885064601898193\n",
            "Epoch 134/200, Batch 30/45, Loss: 2.635577440261841\n",
            "Epoch 134/200, Batch 31/45, Loss: 1.9972901344299316\n",
            "Epoch 134/200, Batch 32/45, Loss: 2.235708236694336\n",
            "Epoch 134/200, Batch 33/45, Loss: 1.668348789215088\n",
            "Epoch 134/200, Batch 34/45, Loss: 2.0714993476867676\n",
            "Epoch 134/200, Batch 35/45, Loss: 1.903598427772522\n",
            "Epoch 134/200, Batch 36/45, Loss: 2.1040141582489014\n",
            "Epoch 134/200, Batch 37/45, Loss: 2.2220969200134277\n",
            "Epoch 134/200, Batch 38/45, Loss: 1.9976319074630737\n",
            "Epoch 134/200, Batch 39/45, Loss: 2.673593044281006\n",
            "Epoch 134/200, Batch 40/45, Loss: 2.095979690551758\n",
            "Epoch 134/200, Batch 41/45, Loss: 2.409069538116455\n",
            "Epoch 134/200, Batch 42/45, Loss: 2.148498296737671\n",
            "Epoch 134/200, Batch 43/45, Loss: 2.013951063156128\n",
            "Epoch 134/200, Batch 44/45, Loss: 1.8754539489746094\n",
            "Epoch 134/200, Batch 45/45, Loss: 1.97480046749115\n",
            "Validating and Checkpointing!\n",
            "Model is not good (might be overfitting)! Current val MSE:  26.260462760925293 Best Val MSE:  25.10259437561035\n",
            "Epoch:  135 , Time Elapsed:  44.16336911519368  mins\n",
            "Epoch 135/200, Batch 1/45, Loss: 2.247347593307495\n",
            "Epoch 135/200, Batch 2/45, Loss: 2.738109588623047\n",
            "Epoch 135/200, Batch 3/45, Loss: 1.514673113822937\n",
            "Epoch 135/200, Batch 4/45, Loss: 1.7248215675354004\n",
            "Epoch 135/200, Batch 5/45, Loss: 2.339275360107422\n",
            "Epoch 135/200, Batch 6/45, Loss: 1.808232069015503\n",
            "Epoch 135/200, Batch 7/45, Loss: 1.5441768169403076\n",
            "Epoch 135/200, Batch 8/45, Loss: 2.7588281631469727\n",
            "Epoch 135/200, Batch 9/45, Loss: 1.938429594039917\n",
            "Epoch 135/200, Batch 10/45, Loss: 3.0276620388031006\n",
            "Epoch 135/200, Batch 11/45, Loss: 2.4281232357025146\n",
            "Epoch 135/200, Batch 12/45, Loss: 2.2442290782928467\n",
            "Epoch 135/200, Batch 13/45, Loss: 2.001345634460449\n",
            "Epoch 135/200, Batch 14/45, Loss: 2.6138956546783447\n",
            "Epoch 135/200, Batch 15/45, Loss: 2.6973824501037598\n",
            "Epoch 135/200, Batch 16/45, Loss: 2.679603099822998\n",
            "Epoch 135/200, Batch 17/45, Loss: 1.5055917501449585\n",
            "Epoch 135/200, Batch 18/45, Loss: 2.0949337482452393\n",
            "Epoch 135/200, Batch 19/45, Loss: 2.0129101276397705\n",
            "Epoch 135/200, Batch 20/45, Loss: 1.8438398838043213\n",
            "Epoch 135/200, Batch 21/45, Loss: 2.2960376739501953\n",
            "Epoch 135/200, Batch 22/45, Loss: 1.9974007606506348\n",
            "Epoch 135/200, Batch 23/45, Loss: 1.947063684463501\n",
            "Epoch 135/200, Batch 24/45, Loss: 1.6885831356048584\n",
            "Epoch 135/200, Batch 25/45, Loss: 2.300588607788086\n",
            "Epoch 135/200, Batch 26/45, Loss: 1.9361611604690552\n",
            "Epoch 135/200, Batch 27/45, Loss: 2.0975680351257324\n",
            "Epoch 135/200, Batch 28/45, Loss: 1.9927263259887695\n",
            "Epoch 135/200, Batch 29/45, Loss: 1.8275091648101807\n",
            "Epoch 135/200, Batch 30/45, Loss: 1.9688315391540527\n",
            "Epoch 135/200, Batch 31/45, Loss: 1.747463345527649\n",
            "Epoch 135/200, Batch 32/45, Loss: 1.867571473121643\n",
            "Epoch 135/200, Batch 33/45, Loss: 2.4301960468292236\n",
            "Epoch 135/200, Batch 34/45, Loss: 1.4654299020767212\n",
            "Epoch 135/200, Batch 35/45, Loss: 2.29111909866333\n",
            "Epoch 135/200, Batch 36/45, Loss: 2.0837979316711426\n",
            "Epoch 135/200, Batch 37/45, Loss: 1.8070210218429565\n",
            "Epoch 135/200, Batch 38/45, Loss: 1.9273269176483154\n",
            "Epoch 135/200, Batch 39/45, Loss: 1.9404504299163818\n",
            "Epoch 135/200, Batch 40/45, Loss: 2.04011607170105\n",
            "Epoch 135/200, Batch 41/45, Loss: 1.535649061203003\n",
            "Epoch 135/200, Batch 42/45, Loss: 2.3410091400146484\n",
            "Epoch 135/200, Batch 43/45, Loss: 2.3781168460845947\n",
            "Epoch 135/200, Batch 44/45, Loss: 2.6095337867736816\n",
            "Epoch 135/200, Batch 45/45, Loss: 2.8158695697784424\n",
            "Validating and Checkpointing!\n",
            "Model is not good (might be overfitting)! Current val MSE:  28.291499495506287 Best Val MSE:  25.10259437561035\n",
            "Epoch:  136 , Time Elapsed:  44.50188762346904  mins\n",
            "Epoch 136/200, Batch 1/45, Loss: 2.1645307540893555\n",
            "Epoch 136/200, Batch 2/45, Loss: 1.6346144676208496\n",
            "Epoch 136/200, Batch 3/45, Loss: 2.5630321502685547\n",
            "Epoch 136/200, Batch 4/45, Loss: 2.070688247680664\n",
            "Epoch 136/200, Batch 5/45, Loss: 1.9380308389663696\n",
            "Epoch 136/200, Batch 6/45, Loss: 2.4062187671661377\n",
            "Epoch 136/200, Batch 7/45, Loss: 1.908957839012146\n",
            "Epoch 136/200, Batch 8/45, Loss: 1.6526637077331543\n",
            "Epoch 136/200, Batch 9/45, Loss: 1.6871733665466309\n",
            "Epoch 136/200, Batch 10/45, Loss: 2.074248790740967\n",
            "Epoch 136/200, Batch 11/45, Loss: 2.274722099304199\n",
            "Epoch 136/200, Batch 12/45, Loss: 1.9480020999908447\n",
            "Epoch 136/200, Batch 13/45, Loss: 2.235529661178589\n",
            "Epoch 136/200, Batch 14/45, Loss: 2.0382165908813477\n",
            "Epoch 136/200, Batch 15/45, Loss: 2.1519458293914795\n",
            "Epoch 136/200, Batch 16/45, Loss: 1.9356380701065063\n",
            "Epoch 136/200, Batch 17/45, Loss: 2.272320032119751\n",
            "Epoch 136/200, Batch 18/45, Loss: 3.000825881958008\n",
            "Epoch 136/200, Batch 19/45, Loss: 1.861652135848999\n",
            "Epoch 136/200, Batch 20/45, Loss: 2.8879926204681396\n",
            "Epoch 136/200, Batch 21/45, Loss: 2.5512046813964844\n",
            "Epoch 136/200, Batch 22/45, Loss: 2.2018113136291504\n",
            "Epoch 136/200, Batch 23/45, Loss: 3.306694746017456\n",
            "Epoch 136/200, Batch 24/45, Loss: 2.7154788970947266\n",
            "Epoch 136/200, Batch 25/45, Loss: 1.8149492740631104\n",
            "Epoch 136/200, Batch 26/45, Loss: 2.3798298835754395\n",
            "Epoch 136/200, Batch 27/45, Loss: 2.1237196922302246\n",
            "Epoch 136/200, Batch 28/45, Loss: 1.9506744146347046\n",
            "Epoch 136/200, Batch 29/45, Loss: 1.8884131908416748\n",
            "Epoch 136/200, Batch 30/45, Loss: 2.0923781394958496\n",
            "Epoch 136/200, Batch 31/45, Loss: 2.096871852874756\n",
            "Epoch 136/200, Batch 32/45, Loss: 2.6947405338287354\n",
            "Epoch 136/200, Batch 33/45, Loss: 2.465578556060791\n",
            "Epoch 136/200, Batch 34/45, Loss: 2.9953179359436035\n",
            "Epoch 136/200, Batch 35/45, Loss: 2.5935001373291016\n",
            "Epoch 136/200, Batch 36/45, Loss: 2.2883737087249756\n",
            "Epoch 136/200, Batch 37/45, Loss: 1.637382984161377\n",
            "Epoch 136/200, Batch 38/45, Loss: 1.8870201110839844\n",
            "Epoch 136/200, Batch 39/45, Loss: 1.6764824390411377\n",
            "Epoch 136/200, Batch 40/45, Loss: 2.4688456058502197\n",
            "Epoch 136/200, Batch 41/45, Loss: 2.3459084033966064\n",
            "Epoch 136/200, Batch 42/45, Loss: 1.831465244293213\n",
            "Epoch 136/200, Batch 43/45, Loss: 2.5430638790130615\n",
            "Epoch 136/200, Batch 44/45, Loss: 2.678760528564453\n",
            "Epoch 136/200, Batch 45/45, Loss: 2.56119966506958\n",
            "Validating and Checkpointing!\n",
            "Model is not good (might be overfitting)! Current val MSE:  27.392996549606323 Best Val MSE:  25.10259437561035\n",
            "Epoch:  137 , Time Elapsed:  44.864356656869255  mins\n",
            "Epoch 137/200, Batch 1/45, Loss: 1.8336186408996582\n",
            "Epoch 137/200, Batch 2/45, Loss: 1.9471038579940796\n",
            "Epoch 137/200, Batch 3/45, Loss: 2.565234422683716\n",
            "Epoch 137/200, Batch 4/45, Loss: 2.2273778915405273\n",
            "Epoch 137/200, Batch 5/45, Loss: 2.2249112129211426\n",
            "Epoch 137/200, Batch 6/45, Loss: 2.6085457801818848\n",
            "Epoch 137/200, Batch 7/45, Loss: 1.9020675420761108\n",
            "Epoch 137/200, Batch 8/45, Loss: 1.8585225343704224\n",
            "Epoch 137/200, Batch 9/45, Loss: 1.9119774103164673\n",
            "Epoch 137/200, Batch 10/45, Loss: 1.6907074451446533\n",
            "Epoch 137/200, Batch 11/45, Loss: 2.269380569458008\n",
            "Epoch 137/200, Batch 12/45, Loss: 2.3029048442840576\n",
            "Epoch 137/200, Batch 13/45, Loss: 2.938999652862549\n",
            "Epoch 137/200, Batch 14/45, Loss: 2.0369997024536133\n",
            "Epoch 137/200, Batch 15/45, Loss: 1.3749637603759766\n",
            "Epoch 137/200, Batch 16/45, Loss: 2.2278892993927\n",
            "Epoch 137/200, Batch 17/45, Loss: 1.9075541496276855\n",
            "Epoch 137/200, Batch 18/45, Loss: 2.825937271118164\n",
            "Epoch 137/200, Batch 19/45, Loss: 1.5188884735107422\n",
            "Epoch 137/200, Batch 20/45, Loss: 2.416224956512451\n",
            "Epoch 137/200, Batch 21/45, Loss: 2.2232723236083984\n",
            "Epoch 137/200, Batch 22/45, Loss: 2.123851776123047\n",
            "Epoch 137/200, Batch 23/45, Loss: 2.5683610439300537\n",
            "Epoch 137/200, Batch 24/45, Loss: 2.6135501861572266\n",
            "Epoch 137/200, Batch 25/45, Loss: 2.450122594833374\n",
            "Epoch 137/200, Batch 26/45, Loss: 1.8086750507354736\n",
            "Epoch 137/200, Batch 27/45, Loss: 1.329590082168579\n",
            "Epoch 137/200, Batch 28/45, Loss: 2.8868837356567383\n",
            "Epoch 137/200, Batch 29/45, Loss: 2.318390130996704\n",
            "Epoch 137/200, Batch 30/45, Loss: 1.8573310375213623\n",
            "Epoch 137/200, Batch 31/45, Loss: 2.4165072441101074\n",
            "Epoch 137/200, Batch 32/45, Loss: 2.3893954753875732\n",
            "Epoch 137/200, Batch 33/45, Loss: 1.964108943939209\n",
            "Epoch 137/200, Batch 34/45, Loss: 2.0830531120300293\n",
            "Epoch 137/200, Batch 35/45, Loss: 1.9116404056549072\n",
            "Epoch 137/200, Batch 36/45, Loss: 2.302203893661499\n",
            "Epoch 137/200, Batch 37/45, Loss: 2.7161574363708496\n",
            "Epoch 137/200, Batch 38/45, Loss: 2.2973368167877197\n",
            "Epoch 137/200, Batch 39/45, Loss: 2.217639923095703\n",
            "Epoch 137/200, Batch 40/45, Loss: 1.6964871883392334\n",
            "Epoch 137/200, Batch 41/45, Loss: 2.4363222122192383\n",
            "Epoch 137/200, Batch 42/45, Loss: 1.8738640546798706\n",
            "Epoch 137/200, Batch 43/45, Loss: 3.399545669555664\n",
            "Epoch 137/200, Batch 44/45, Loss: 1.622243881225586\n",
            "Epoch 137/200, Batch 45/45, Loss: 2.1924846172332764\n",
            "Validating and Checkpointing!\n",
            "Model is not good (might be overfitting)! Current val MSE:  26.566402196884155 Best Val MSE:  25.10259437561035\n",
            "Epoch:  138 , Time Elapsed:  45.190410725275676  mins\n",
            "Epoch 138/200, Batch 1/45, Loss: 2.3725452423095703\n",
            "Epoch 138/200, Batch 2/45, Loss: 2.7942237854003906\n",
            "Epoch 138/200, Batch 3/45, Loss: 2.059134006500244\n",
            "Epoch 138/200, Batch 4/45, Loss: 2.077254295349121\n",
            "Epoch 138/200, Batch 5/45, Loss: 1.2418012619018555\n",
            "Epoch 138/200, Batch 6/45, Loss: 2.15087890625\n",
            "Epoch 138/200, Batch 7/45, Loss: 2.2852745056152344\n",
            "Epoch 138/200, Batch 8/45, Loss: 1.6346347332000732\n",
            "Epoch 138/200, Batch 9/45, Loss: 2.3473124504089355\n",
            "Epoch 138/200, Batch 10/45, Loss: 2.1449098587036133\n",
            "Epoch 138/200, Batch 11/45, Loss: 2.5009069442749023\n",
            "Epoch 138/200, Batch 12/45, Loss: 2.0850815773010254\n",
            "Epoch 138/200, Batch 13/45, Loss: 1.314367413520813\n",
            "Epoch 138/200, Batch 14/45, Loss: 2.507578134536743\n",
            "Epoch 138/200, Batch 15/45, Loss: 1.893190622329712\n",
            "Epoch 138/200, Batch 16/45, Loss: 1.9600465297698975\n",
            "Epoch 138/200, Batch 17/45, Loss: 2.548597812652588\n",
            "Epoch 138/200, Batch 18/45, Loss: 2.248504638671875\n",
            "Epoch 138/200, Batch 19/45, Loss: 2.240088939666748\n",
            "Epoch 138/200, Batch 20/45, Loss: 2.2969915866851807\n",
            "Epoch 138/200, Batch 21/45, Loss: 2.0266027450561523\n",
            "Epoch 138/200, Batch 22/45, Loss: 2.6097898483276367\n",
            "Epoch 138/200, Batch 23/45, Loss: 2.0209221839904785\n",
            "Epoch 138/200, Batch 24/45, Loss: 2.2804040908813477\n",
            "Epoch 138/200, Batch 25/45, Loss: 1.7753639221191406\n",
            "Epoch 138/200, Batch 26/45, Loss: 2.217740535736084\n",
            "Epoch 138/200, Batch 27/45, Loss: 1.9943690299987793\n",
            "Epoch 138/200, Batch 28/45, Loss: 1.9117047786712646\n",
            "Epoch 138/200, Batch 29/45, Loss: 2.1317975521087646\n",
            "Epoch 138/200, Batch 30/45, Loss: 2.1915974617004395\n",
            "Epoch 138/200, Batch 31/45, Loss: 2.4630472660064697\n",
            "Epoch 138/200, Batch 32/45, Loss: 2.5147511959075928\n",
            "Epoch 138/200, Batch 33/45, Loss: 2.036494493484497\n",
            "Epoch 138/200, Batch 34/45, Loss: 1.9621502161026\n",
            "Epoch 138/200, Batch 35/45, Loss: 2.0910444259643555\n",
            "Epoch 138/200, Batch 36/45, Loss: 2.3724606037139893\n",
            "Epoch 138/200, Batch 37/45, Loss: 1.8562180995941162\n",
            "Epoch 138/200, Batch 38/45, Loss: 1.438838005065918\n",
            "Epoch 138/200, Batch 39/45, Loss: 2.234184741973877\n",
            "Epoch 138/200, Batch 40/45, Loss: 1.5279666185379028\n",
            "Epoch 138/200, Batch 41/45, Loss: 2.0754811763763428\n",
            "Epoch 138/200, Batch 42/45, Loss: 2.3547134399414062\n",
            "Epoch 138/200, Batch 43/45, Loss: 1.8977028131484985\n",
            "Epoch 138/200, Batch 44/45, Loss: 2.3364052772521973\n",
            "Epoch 138/200, Batch 45/45, Loss: 1.4595768451690674\n",
            "Validating and Checkpointing!\n",
            "Model is not good (might be overfitting)! Current val MSE:  29.333298921585083 Best Val MSE:  25.10259437561035\n",
            "Epoch:  139 , Time Elapsed:  45.531740574042004  mins\n",
            "Epoch 139/200, Batch 1/45, Loss: 1.8278149366378784\n",
            "Epoch 139/200, Batch 2/45, Loss: 2.766834259033203\n",
            "Epoch 139/200, Batch 3/45, Loss: 1.979676365852356\n",
            "Epoch 139/200, Batch 4/45, Loss: 1.4261806011199951\n",
            "Epoch 139/200, Batch 5/45, Loss: 2.1324620246887207\n",
            "Epoch 139/200, Batch 6/45, Loss: 2.7848963737487793\n",
            "Epoch 139/200, Batch 7/45, Loss: 2.748849391937256\n",
            "Epoch 139/200, Batch 8/45, Loss: 2.3510074615478516\n",
            "Epoch 139/200, Batch 9/45, Loss: 2.401844024658203\n",
            "Epoch 139/200, Batch 10/45, Loss: 2.5776941776275635\n",
            "Epoch 139/200, Batch 11/45, Loss: 2.4086291790008545\n",
            "Epoch 139/200, Batch 12/45, Loss: 2.370150566101074\n",
            "Epoch 139/200, Batch 13/45, Loss: 2.255124568939209\n",
            "Epoch 139/200, Batch 14/45, Loss: 1.5869160890579224\n",
            "Epoch 139/200, Batch 15/45, Loss: 2.465773344039917\n",
            "Epoch 139/200, Batch 16/45, Loss: 2.8198540210723877\n",
            "Epoch 139/200, Batch 17/45, Loss: 2.7281312942504883\n",
            "Epoch 139/200, Batch 18/45, Loss: 1.9931621551513672\n",
            "Epoch 139/200, Batch 19/45, Loss: 2.664384603500366\n",
            "Epoch 139/200, Batch 20/45, Loss: 1.9333882331848145\n",
            "Epoch 139/200, Batch 21/45, Loss: 2.531061887741089\n",
            "Epoch 139/200, Batch 22/45, Loss: 2.395484447479248\n",
            "Epoch 139/200, Batch 23/45, Loss: 2.307438850402832\n",
            "Epoch 139/200, Batch 24/45, Loss: 1.7843916416168213\n",
            "Epoch 139/200, Batch 25/45, Loss: 2.2658181190490723\n",
            "Epoch 139/200, Batch 26/45, Loss: 1.7103848457336426\n",
            "Epoch 139/200, Batch 27/45, Loss: 2.5796291828155518\n",
            "Epoch 139/200, Batch 28/45, Loss: 2.2877819538116455\n",
            "Epoch 139/200, Batch 29/45, Loss: 1.6531479358673096\n",
            "Epoch 139/200, Batch 30/45, Loss: 1.733720064163208\n",
            "Epoch 139/200, Batch 31/45, Loss: 1.6626076698303223\n",
            "Epoch 139/200, Batch 32/45, Loss: 2.3236587047576904\n",
            "Epoch 139/200, Batch 33/45, Loss: 2.852055072784424\n",
            "Epoch 139/200, Batch 34/45, Loss: 2.1682355403900146\n",
            "Epoch 139/200, Batch 35/45, Loss: 2.349693536758423\n",
            "Epoch 139/200, Batch 36/45, Loss: 2.0885770320892334\n",
            "Epoch 139/200, Batch 37/45, Loss: 2.7001123428344727\n",
            "Epoch 139/200, Batch 38/45, Loss: 2.3667187690734863\n",
            "Epoch 139/200, Batch 39/45, Loss: 2.233982563018799\n",
            "Epoch 139/200, Batch 40/45, Loss: 2.206514358520508\n",
            "Epoch 139/200, Batch 41/45, Loss: 2.302063465118408\n",
            "Epoch 139/200, Batch 42/45, Loss: 2.175877571105957\n",
            "Epoch 139/200, Batch 43/45, Loss: 2.3808372020721436\n",
            "Epoch 139/200, Batch 44/45, Loss: 2.545403242111206\n",
            "Epoch 139/200, Batch 45/45, Loss: 2.0681824684143066\n",
            "Validating and Checkpointing!\n",
            "Model is not good (might be overfitting)! Current val MSE:  27.43643808364868 Best Val MSE:  25.10259437561035\n",
            "Epoch:  140 , Time Elapsed:  45.863642207781474  mins\n",
            "Epoch 140/200, Batch 1/45, Loss: 1.132866621017456\n",
            "Epoch 140/200, Batch 2/45, Loss: 1.9943757057189941\n",
            "Epoch 140/200, Batch 3/45, Loss: 2.1893348693847656\n",
            "Epoch 140/200, Batch 4/45, Loss: 2.376861810684204\n",
            "Epoch 140/200, Batch 5/45, Loss: 2.5906333923339844\n",
            "Epoch 140/200, Batch 6/45, Loss: 1.6853338479995728\n",
            "Epoch 140/200, Batch 7/45, Loss: 2.2292537689208984\n",
            "Epoch 140/200, Batch 8/45, Loss: 2.2246134281158447\n",
            "Epoch 140/200, Batch 9/45, Loss: 2.0678367614746094\n",
            "Epoch 140/200, Batch 10/45, Loss: 1.9725213050842285\n",
            "Epoch 140/200, Batch 11/45, Loss: 2.389944076538086\n",
            "Epoch 140/200, Batch 12/45, Loss: 2.0438265800476074\n",
            "Epoch 140/200, Batch 13/45, Loss: 2.423701286315918\n",
            "Epoch 140/200, Batch 14/45, Loss: 2.0583343505859375\n",
            "Epoch 140/200, Batch 15/45, Loss: 2.3047127723693848\n",
            "Epoch 140/200, Batch 16/45, Loss: 2.5917370319366455\n",
            "Epoch 140/200, Batch 17/45, Loss: 1.716949701309204\n",
            "Epoch 140/200, Batch 18/45, Loss: 2.434710741043091\n",
            "Epoch 140/200, Batch 19/45, Loss: 1.4120359420776367\n",
            "Epoch 140/200, Batch 20/45, Loss: 1.8540024757385254\n",
            "Epoch 140/200, Batch 21/45, Loss: 1.6521611213684082\n",
            "Epoch 140/200, Batch 22/45, Loss: 1.5054965019226074\n",
            "Epoch 140/200, Batch 23/45, Loss: 2.0423643589019775\n",
            "Epoch 140/200, Batch 24/45, Loss: 2.200010061264038\n",
            "Epoch 140/200, Batch 25/45, Loss: 1.6475145816802979\n",
            "Epoch 140/200, Batch 26/45, Loss: 2.783686637878418\n",
            "Epoch 140/200, Batch 27/45, Loss: 1.7014915943145752\n",
            "Epoch 140/200, Batch 28/45, Loss: 2.318042278289795\n",
            "Epoch 140/200, Batch 29/45, Loss: 1.5276446342468262\n",
            "Epoch 140/200, Batch 30/45, Loss: 3.090467929840088\n",
            "Epoch 140/200, Batch 31/45, Loss: 1.788816213607788\n",
            "Epoch 140/200, Batch 32/45, Loss: 2.0642318725585938\n",
            "Epoch 140/200, Batch 33/45, Loss: 2.138775587081909\n",
            "Epoch 140/200, Batch 34/45, Loss: 2.694711208343506\n",
            "Epoch 140/200, Batch 35/45, Loss: 2.43939471244812\n",
            "Epoch 140/200, Batch 36/45, Loss: 2.055662155151367\n",
            "Epoch 140/200, Batch 37/45, Loss: 1.4800459146499634\n",
            "Epoch 140/200, Batch 38/45, Loss: 2.0313289165496826\n",
            "Epoch 140/200, Batch 39/45, Loss: 2.7229714393615723\n",
            "Epoch 140/200, Batch 40/45, Loss: 1.9939894676208496\n",
            "Epoch 140/200, Batch 41/45, Loss: 1.6040010452270508\n",
            "Epoch 140/200, Batch 42/45, Loss: 2.3445804119110107\n",
            "Epoch 140/200, Batch 43/45, Loss: 2.536820650100708\n",
            "Epoch 140/200, Batch 44/45, Loss: 2.4094555377960205\n",
            "Epoch 140/200, Batch 45/45, Loss: 2.16416072845459\n",
            "Validating and Checkpointing!\n",
            "Model is not good (might be overfitting)! Current val MSE:  27.9347665309906 Best Val MSE:  25.10259437561035\n",
            "Epoch:  141 , Time Elapsed:  46.19673231840134  mins\n",
            "Epoch 141/200, Batch 1/45, Loss: 2.3842155933380127\n",
            "Epoch 141/200, Batch 2/45, Loss: 2.811798095703125\n",
            "Epoch 141/200, Batch 3/45, Loss: 2.066079616546631\n",
            "Epoch 141/200, Batch 4/45, Loss: 2.4420127868652344\n",
            "Epoch 141/200, Batch 5/45, Loss: 1.7240228652954102\n",
            "Epoch 141/200, Batch 6/45, Loss: 2.286449432373047\n",
            "Epoch 141/200, Batch 7/45, Loss: 2.531569004058838\n",
            "Epoch 141/200, Batch 8/45, Loss: 2.1788339614868164\n",
            "Epoch 141/200, Batch 9/45, Loss: 2.567429304122925\n",
            "Epoch 141/200, Batch 10/45, Loss: 1.928730845451355\n",
            "Epoch 141/200, Batch 11/45, Loss: 2.3641438484191895\n",
            "Epoch 141/200, Batch 12/45, Loss: 2.448019504547119\n",
            "Epoch 141/200, Batch 13/45, Loss: 2.1286227703094482\n",
            "Epoch 141/200, Batch 14/45, Loss: 2.08721661567688\n",
            "Epoch 141/200, Batch 15/45, Loss: 1.8814327716827393\n",
            "Epoch 141/200, Batch 16/45, Loss: 3.1842844486236572\n",
            "Epoch 141/200, Batch 17/45, Loss: 2.277703285217285\n",
            "Epoch 141/200, Batch 18/45, Loss: 2.405141830444336\n",
            "Epoch 141/200, Batch 19/45, Loss: 1.5024100542068481\n",
            "Epoch 141/200, Batch 20/45, Loss: 1.7246687412261963\n",
            "Epoch 141/200, Batch 21/45, Loss: 1.9563287496566772\n",
            "Epoch 141/200, Batch 22/45, Loss: 2.40940260887146\n",
            "Epoch 141/200, Batch 23/45, Loss: 1.667895793914795\n",
            "Epoch 141/200, Batch 24/45, Loss: 2.128535509109497\n",
            "Epoch 141/200, Batch 25/45, Loss: 2.187389612197876\n",
            "Epoch 141/200, Batch 26/45, Loss: 1.5808945894241333\n",
            "Epoch 141/200, Batch 27/45, Loss: 2.665081739425659\n",
            "Epoch 141/200, Batch 28/45, Loss: 2.816187620162964\n",
            "Epoch 141/200, Batch 29/45, Loss: 2.116166114807129\n",
            "Epoch 141/200, Batch 30/45, Loss: 2.1653671264648438\n",
            "Epoch 141/200, Batch 31/45, Loss: 1.6059552431106567\n",
            "Epoch 141/200, Batch 32/45, Loss: 2.514761447906494\n",
            "Epoch 141/200, Batch 33/45, Loss: 1.9052727222442627\n",
            "Epoch 141/200, Batch 34/45, Loss: 2.3712985515594482\n",
            "Epoch 141/200, Batch 35/45, Loss: 2.3375136852264404\n",
            "Epoch 141/200, Batch 36/45, Loss: 2.4204111099243164\n",
            "Epoch 141/200, Batch 37/45, Loss: 2.117851495742798\n",
            "Epoch 141/200, Batch 38/45, Loss: 1.9501323699951172\n",
            "Epoch 141/200, Batch 39/45, Loss: 2.4880990982055664\n",
            "Epoch 141/200, Batch 40/45, Loss: 2.1261496543884277\n",
            "Epoch 141/200, Batch 41/45, Loss: 2.445025682449341\n",
            "Epoch 141/200, Batch 42/45, Loss: 2.2813827991485596\n",
            "Epoch 141/200, Batch 43/45, Loss: 4.936471462249756\n",
            "Epoch 141/200, Batch 44/45, Loss: 1.5414278507232666\n",
            "Epoch 141/200, Batch 45/45, Loss: 2.762528657913208\n",
            "Validating and Checkpointing!\n",
            "Best model Saved! Val MSE:  24.81772005558014\n",
            "Epoch:  142 , Time Elapsed:  46.570387128988905  mins\n",
            "Epoch 142/200, Batch 1/45, Loss: 1.6879435777664185\n",
            "Epoch 142/200, Batch 2/45, Loss: 2.554090738296509\n",
            "Epoch 142/200, Batch 3/45, Loss: 1.92363440990448\n",
            "Epoch 142/200, Batch 4/45, Loss: 2.698200225830078\n",
            "Epoch 142/200, Batch 5/45, Loss: 2.0749804973602295\n",
            "Epoch 142/200, Batch 6/45, Loss: 4.839962959289551\n",
            "Epoch 142/200, Batch 7/45, Loss: 2.3282551765441895\n",
            "Epoch 142/200, Batch 8/45, Loss: 1.9483399391174316\n",
            "Epoch 142/200, Batch 9/45, Loss: 2.3489229679107666\n",
            "Epoch 142/200, Batch 10/45, Loss: 1.7843046188354492\n",
            "Epoch 142/200, Batch 11/45, Loss: 1.52452552318573\n",
            "Epoch 142/200, Batch 12/45, Loss: 2.627182722091675\n",
            "Epoch 142/200, Batch 13/45, Loss: 1.8386645317077637\n",
            "Epoch 142/200, Batch 14/45, Loss: 2.491849422454834\n",
            "Epoch 142/200, Batch 15/45, Loss: 2.095825433731079\n",
            "Epoch 142/200, Batch 16/45, Loss: 2.4147517681121826\n",
            "Epoch 142/200, Batch 17/45, Loss: 1.85916268825531\n",
            "Epoch 142/200, Batch 18/45, Loss: 2.6124720573425293\n",
            "Epoch 142/200, Batch 19/45, Loss: 2.058682680130005\n",
            "Epoch 142/200, Batch 20/45, Loss: 1.9952833652496338\n",
            "Epoch 142/200, Batch 21/45, Loss: 2.295045852661133\n",
            "Epoch 142/200, Batch 22/45, Loss: 1.9340770244598389\n",
            "Epoch 142/200, Batch 23/45, Loss: 2.2546582221984863\n",
            "Epoch 142/200, Batch 24/45, Loss: 1.8346788883209229\n",
            "Epoch 142/200, Batch 25/45, Loss: 2.791293144226074\n",
            "Epoch 142/200, Batch 26/45, Loss: 2.642183780670166\n",
            "Epoch 142/200, Batch 27/45, Loss: 2.052687168121338\n",
            "Epoch 142/200, Batch 28/45, Loss: 2.19120454788208\n",
            "Epoch 142/200, Batch 29/45, Loss: 2.3467698097229004\n",
            "Epoch 142/200, Batch 30/45, Loss: 2.2990000247955322\n",
            "Epoch 142/200, Batch 31/45, Loss: 2.0778586864471436\n",
            "Epoch 142/200, Batch 32/45, Loss: 2.5433342456817627\n",
            "Epoch 142/200, Batch 33/45, Loss: 2.288208484649658\n",
            "Epoch 142/200, Batch 34/45, Loss: 2.283054828643799\n",
            "Epoch 142/200, Batch 35/45, Loss: 1.6091029644012451\n",
            "Epoch 142/200, Batch 36/45, Loss: 2.0838358402252197\n",
            "Epoch 142/200, Batch 37/45, Loss: 2.682033061981201\n",
            "Epoch 142/200, Batch 38/45, Loss: 2.1264493465423584\n",
            "Epoch 142/200, Batch 39/45, Loss: 2.698185920715332\n",
            "Epoch 142/200, Batch 40/45, Loss: 2.727670669555664\n",
            "Epoch 142/200, Batch 41/45, Loss: 2.429452419281006\n",
            "Epoch 142/200, Batch 42/45, Loss: 2.0906262397766113\n",
            "Epoch 142/200, Batch 43/45, Loss: 1.7036404609680176\n",
            "Epoch 142/200, Batch 44/45, Loss: 2.4228014945983887\n",
            "Epoch 142/200, Batch 45/45, Loss: 1.9009971618652344\n",
            "Validating and Checkpointing!\n",
            "Model is not good (might be overfitting)! Current val MSE:  26.662545919418335 Best Val MSE:  24.81772005558014\n",
            "Epoch:  143 , Time Elapsed:  46.90228658914566  mins\n",
            "Epoch 143/200, Batch 1/45, Loss: 2.752326488494873\n",
            "Epoch 143/200, Batch 2/45, Loss: 1.953091025352478\n",
            "Epoch 143/200, Batch 3/45, Loss: 2.307708978652954\n",
            "Epoch 143/200, Batch 4/45, Loss: 1.8695590496063232\n",
            "Epoch 143/200, Batch 5/45, Loss: 2.0424892902374268\n",
            "Epoch 143/200, Batch 6/45, Loss: 2.5721211433410645\n",
            "Epoch 143/200, Batch 7/45, Loss: 2.2646241188049316\n",
            "Epoch 143/200, Batch 8/45, Loss: 2.5975635051727295\n",
            "Epoch 143/200, Batch 9/45, Loss: 2.6391189098358154\n",
            "Epoch 143/200, Batch 10/45, Loss: 1.9316810369491577\n",
            "Epoch 143/200, Batch 11/45, Loss: 2.3324508666992188\n",
            "Epoch 143/200, Batch 12/45, Loss: 2.019343137741089\n",
            "Epoch 143/200, Batch 13/45, Loss: 2.1407647132873535\n",
            "Epoch 143/200, Batch 14/45, Loss: 1.7750152349472046\n",
            "Epoch 143/200, Batch 15/45, Loss: 1.8086882829666138\n",
            "Epoch 143/200, Batch 16/45, Loss: 2.2684366703033447\n",
            "Epoch 143/200, Batch 17/45, Loss: 1.856360912322998\n",
            "Epoch 143/200, Batch 18/45, Loss: 2.0559144020080566\n",
            "Epoch 143/200, Batch 19/45, Loss: 1.6374175548553467\n",
            "Epoch 143/200, Batch 20/45, Loss: 2.692620277404785\n",
            "Epoch 143/200, Batch 21/45, Loss: 1.743781566619873\n",
            "Epoch 143/200, Batch 22/45, Loss: 1.6866450309753418\n",
            "Epoch 143/200, Batch 23/45, Loss: 2.4589457511901855\n",
            "Epoch 143/200, Batch 24/45, Loss: 2.64347243309021\n",
            "Epoch 143/200, Batch 25/45, Loss: 2.322154998779297\n",
            "Epoch 143/200, Batch 26/45, Loss: 1.6646363735198975\n",
            "Epoch 143/200, Batch 27/45, Loss: 2.519221305847168\n",
            "Epoch 143/200, Batch 28/45, Loss: 2.3493893146514893\n",
            "Epoch 143/200, Batch 29/45, Loss: 1.307774543762207\n",
            "Epoch 143/200, Batch 30/45, Loss: 2.481228828430176\n",
            "Epoch 143/200, Batch 31/45, Loss: 2.155566453933716\n",
            "Epoch 143/200, Batch 32/45, Loss: 1.5801563262939453\n",
            "Epoch 143/200, Batch 33/45, Loss: 2.488405227661133\n",
            "Epoch 143/200, Batch 34/45, Loss: 1.7774100303649902\n",
            "Epoch 143/200, Batch 35/45, Loss: 1.8968946933746338\n",
            "Epoch 143/200, Batch 36/45, Loss: 2.128953695297241\n",
            "Epoch 143/200, Batch 37/45, Loss: 1.9849947690963745\n",
            "Epoch 143/200, Batch 38/45, Loss: 1.627559781074524\n",
            "Epoch 143/200, Batch 39/45, Loss: 1.9478230476379395\n",
            "Epoch 143/200, Batch 40/45, Loss: 1.4481562376022339\n",
            "Epoch 143/200, Batch 41/45, Loss: 2.40106463432312\n",
            "Epoch 143/200, Batch 42/45, Loss: 1.5384942293167114\n",
            "Epoch 143/200, Batch 43/45, Loss: 1.9722262620925903\n",
            "Epoch 143/200, Batch 44/45, Loss: 1.818953037261963\n",
            "Epoch 143/200, Batch 45/45, Loss: 1.9871296882629395\n",
            "Validating and Checkpointing!\n",
            "Model is not good (might be overfitting)! Current val MSE:  27.447606325149536 Best Val MSE:  24.81772005558014\n",
            "Epoch:  144 , Time Elapsed:  47.22457914749781  mins\n",
            "Epoch 144/200, Batch 1/45, Loss: 2.3631467819213867\n",
            "Epoch 144/200, Batch 2/45, Loss: 1.9274070262908936\n",
            "Epoch 144/200, Batch 3/45, Loss: 1.443364143371582\n",
            "Epoch 144/200, Batch 4/45, Loss: 2.176915407180786\n",
            "Epoch 144/200, Batch 5/45, Loss: 1.933080792427063\n",
            "Epoch 144/200, Batch 6/45, Loss: 4.846956253051758\n",
            "Epoch 144/200, Batch 7/45, Loss: 1.6497881412506104\n",
            "Epoch 144/200, Batch 8/45, Loss: 2.199707508087158\n",
            "Epoch 144/200, Batch 9/45, Loss: 1.8756556510925293\n",
            "Epoch 144/200, Batch 10/45, Loss: 1.894753098487854\n",
            "Epoch 144/200, Batch 11/45, Loss: 2.2021751403808594\n",
            "Epoch 144/200, Batch 12/45, Loss: 2.0639235973358154\n",
            "Epoch 144/200, Batch 13/45, Loss: 2.029148817062378\n",
            "Epoch 144/200, Batch 14/45, Loss: 1.3689017295837402\n",
            "Epoch 144/200, Batch 15/45, Loss: 1.745134949684143\n",
            "Epoch 144/200, Batch 16/45, Loss: 2.3040366172790527\n",
            "Epoch 144/200, Batch 17/45, Loss: 2.154918670654297\n",
            "Epoch 144/200, Batch 18/45, Loss: 2.6314382553100586\n",
            "Epoch 144/200, Batch 19/45, Loss: 2.3702239990234375\n",
            "Epoch 144/200, Batch 20/45, Loss: 2.429302453994751\n",
            "Epoch 144/200, Batch 21/45, Loss: 1.9114383459091187\n",
            "Epoch 144/200, Batch 22/45, Loss: 2.0768017768859863\n",
            "Epoch 144/200, Batch 23/45, Loss: 1.3449337482452393\n",
            "Epoch 144/200, Batch 24/45, Loss: 2.683969736099243\n",
            "Epoch 144/200, Batch 25/45, Loss: 2.138402223587036\n",
            "Epoch 144/200, Batch 26/45, Loss: 2.1184730529785156\n",
            "Epoch 144/200, Batch 27/45, Loss: 2.300187110900879\n",
            "Epoch 144/200, Batch 28/45, Loss: 2.388836622238159\n",
            "Epoch 144/200, Batch 29/45, Loss: 2.149044990539551\n",
            "Epoch 144/200, Batch 30/45, Loss: 2.2399160861968994\n",
            "Epoch 144/200, Batch 31/45, Loss: 1.7807188034057617\n",
            "Epoch 144/200, Batch 32/45, Loss: 2.413074254989624\n",
            "Epoch 144/200, Batch 33/45, Loss: 3.008227586746216\n",
            "Epoch 144/200, Batch 34/45, Loss: 1.8183226585388184\n",
            "Epoch 144/200, Batch 35/45, Loss: 1.9324458837509155\n",
            "Epoch 144/200, Batch 36/45, Loss: 2.1517531871795654\n",
            "Epoch 144/200, Batch 37/45, Loss: 2.199629783630371\n",
            "Epoch 144/200, Batch 38/45, Loss: 2.106630802154541\n",
            "Epoch 144/200, Batch 39/45, Loss: 1.8381202220916748\n",
            "Epoch 144/200, Batch 40/45, Loss: 2.221240758895874\n",
            "Epoch 144/200, Batch 41/45, Loss: 2.880721092224121\n",
            "Epoch 144/200, Batch 42/45, Loss: 2.482908248901367\n",
            "Epoch 144/200, Batch 43/45, Loss: 2.5479648113250732\n",
            "Epoch 144/200, Batch 44/45, Loss: 2.790130138397217\n",
            "Epoch 144/200, Batch 45/45, Loss: 1.8752864599227905\n",
            "Validating and Checkpointing!\n",
            "Model is not good (might be overfitting)! Current val MSE:  28.174885749816895 Best Val MSE:  24.81772005558014\n",
            "Epoch:  145 , Time Elapsed:  47.57545259793599  mins\n",
            "Epoch 145/200, Batch 1/45, Loss: 1.844172477722168\n",
            "Epoch 145/200, Batch 2/45, Loss: 1.7788753509521484\n",
            "Epoch 145/200, Batch 3/45, Loss: 2.210291624069214\n",
            "Epoch 145/200, Batch 4/45, Loss: 2.3338193893432617\n",
            "Epoch 145/200, Batch 5/45, Loss: 2.704193115234375\n",
            "Epoch 145/200, Batch 6/45, Loss: 1.3236268758773804\n",
            "Epoch 145/200, Batch 7/45, Loss: 2.2659871578216553\n",
            "Epoch 145/200, Batch 8/45, Loss: 2.765138626098633\n",
            "Epoch 145/200, Batch 9/45, Loss: 2.2563791275024414\n",
            "Epoch 145/200, Batch 10/45, Loss: 1.8639860153198242\n",
            "Epoch 145/200, Batch 11/45, Loss: 2.0517938137054443\n",
            "Epoch 145/200, Batch 12/45, Loss: 2.3423142433166504\n",
            "Epoch 145/200, Batch 13/45, Loss: 2.284377336502075\n",
            "Epoch 145/200, Batch 14/45, Loss: 1.8909165859222412\n",
            "Epoch 145/200, Batch 15/45, Loss: 2.088419198989868\n",
            "Epoch 145/200, Batch 16/45, Loss: 2.3363516330718994\n",
            "Epoch 145/200, Batch 17/45, Loss: 2.7447478771209717\n",
            "Epoch 145/200, Batch 18/45, Loss: 2.820305585861206\n",
            "Epoch 145/200, Batch 19/45, Loss: 2.2060182094573975\n",
            "Epoch 145/200, Batch 20/45, Loss: 2.3063199520111084\n",
            "Epoch 145/200, Batch 21/45, Loss: 2.47265625\n",
            "Epoch 145/200, Batch 22/45, Loss: 2.602904796600342\n",
            "Epoch 145/200, Batch 23/45, Loss: 1.669279932975769\n",
            "Epoch 145/200, Batch 24/45, Loss: 2.1336607933044434\n",
            "Epoch 145/200, Batch 25/45, Loss: 2.1973352432250977\n",
            "Epoch 145/200, Batch 26/45, Loss: 2.3372113704681396\n",
            "Epoch 145/200, Batch 27/45, Loss: 2.739802837371826\n",
            "Epoch 145/200, Batch 28/45, Loss: 2.1598737239837646\n",
            "Epoch 145/200, Batch 29/45, Loss: 2.5628843307495117\n",
            "Epoch 145/200, Batch 30/45, Loss: 2.770953893661499\n",
            "Epoch 145/200, Batch 31/45, Loss: 2.3614883422851562\n",
            "Epoch 145/200, Batch 32/45, Loss: 2.503793716430664\n",
            "Epoch 145/200, Batch 33/45, Loss: 2.3389575481414795\n",
            "Epoch 145/200, Batch 34/45, Loss: 2.173612117767334\n",
            "Epoch 145/200, Batch 35/45, Loss: 1.7595241069793701\n",
            "Epoch 145/200, Batch 36/45, Loss: 2.4920642375946045\n",
            "Epoch 145/200, Batch 37/45, Loss: 2.2118210792541504\n",
            "Epoch 145/200, Batch 38/45, Loss: 2.040482997894287\n",
            "Epoch 145/200, Batch 39/45, Loss: 1.540452003479004\n",
            "Epoch 145/200, Batch 40/45, Loss: 1.7039697170257568\n",
            "Epoch 145/200, Batch 41/45, Loss: 2.2915070056915283\n",
            "Epoch 145/200, Batch 42/45, Loss: 1.8363810777664185\n",
            "Epoch 145/200, Batch 43/45, Loss: 1.9677259922027588\n",
            "Epoch 145/200, Batch 44/45, Loss: 1.9487698078155518\n",
            "Epoch 145/200, Batch 45/45, Loss: 2.0696144104003906\n",
            "Validating and Checkpointing!\n",
            "Model is not good (might be overfitting)! Current val MSE:  33.44711673259735 Best Val MSE:  24.81772005558014\n",
            "Epoch:  146 , Time Elapsed:  47.89288179079691  mins\n",
            "Epoch 146/200, Batch 1/45, Loss: 2.4890713691711426\n",
            "Epoch 146/200, Batch 2/45, Loss: 2.288640022277832\n",
            "Epoch 146/200, Batch 3/45, Loss: 2.6285552978515625\n",
            "Epoch 146/200, Batch 4/45, Loss: 1.4862446784973145\n",
            "Epoch 146/200, Batch 5/45, Loss: 2.136183738708496\n",
            "Epoch 146/200, Batch 6/45, Loss: 5.312050819396973\n",
            "Epoch 146/200, Batch 7/45, Loss: 2.123039722442627\n",
            "Epoch 146/200, Batch 8/45, Loss: 1.9415044784545898\n",
            "Epoch 146/200, Batch 9/45, Loss: 2.2305026054382324\n",
            "Epoch 146/200, Batch 10/45, Loss: 1.8516366481781006\n",
            "Epoch 146/200, Batch 11/45, Loss: 2.412978410720825\n",
            "Epoch 146/200, Batch 12/45, Loss: 3.0498273372650146\n",
            "Epoch 146/200, Batch 13/45, Loss: 2.875448703765869\n",
            "Epoch 146/200, Batch 14/45, Loss: 2.434361457824707\n",
            "Epoch 146/200, Batch 15/45, Loss: 2.254425287246704\n",
            "Epoch 146/200, Batch 16/45, Loss: 2.387242555618286\n",
            "Epoch 146/200, Batch 17/45, Loss: 2.153164863586426\n",
            "Epoch 146/200, Batch 18/45, Loss: 2.475703001022339\n",
            "Epoch 146/200, Batch 19/45, Loss: 1.8063693046569824\n",
            "Epoch 146/200, Batch 20/45, Loss: 2.6073029041290283\n",
            "Epoch 146/200, Batch 21/45, Loss: 2.248383045196533\n",
            "Epoch 146/200, Batch 22/45, Loss: 1.998152732849121\n",
            "Epoch 146/200, Batch 23/45, Loss: 2.147933006286621\n",
            "Epoch 146/200, Batch 24/45, Loss: 2.498072624206543\n",
            "Epoch 146/200, Batch 25/45, Loss: 2.2361743450164795\n",
            "Epoch 146/200, Batch 26/45, Loss: 2.1935298442840576\n",
            "Epoch 146/200, Batch 27/45, Loss: 2.535834312438965\n",
            "Epoch 146/200, Batch 28/45, Loss: 2.148560047149658\n",
            "Epoch 146/200, Batch 29/45, Loss: 1.8919472694396973\n",
            "Epoch 146/200, Batch 30/45, Loss: 2.440265655517578\n",
            "Epoch 146/200, Batch 31/45, Loss: 2.7747621536254883\n",
            "Epoch 146/200, Batch 32/45, Loss: 1.6824288368225098\n",
            "Epoch 146/200, Batch 33/45, Loss: 2.037227153778076\n",
            "Epoch 146/200, Batch 34/45, Loss: 1.8507413864135742\n",
            "Epoch 146/200, Batch 35/45, Loss: 2.1270227432250977\n",
            "Epoch 146/200, Batch 36/45, Loss: 1.569495439529419\n",
            "Epoch 146/200, Batch 37/45, Loss: 1.8353192806243896\n",
            "Epoch 146/200, Batch 38/45, Loss: 1.6363314390182495\n",
            "Epoch 146/200, Batch 39/45, Loss: 2.066953659057617\n",
            "Epoch 146/200, Batch 40/45, Loss: 2.4061388969421387\n",
            "Epoch 146/200, Batch 41/45, Loss: 2.404256582260132\n",
            "Epoch 146/200, Batch 42/45, Loss: 2.3657784461975098\n",
            "Epoch 146/200, Batch 43/45, Loss: 1.743345022201538\n",
            "Epoch 146/200, Batch 44/45, Loss: 3.8505358695983887\n",
            "Epoch 146/200, Batch 45/45, Loss: 1.6967167854309082\n",
            "Validating and Checkpointing!\n",
            "Model is not good (might be overfitting)! Current val MSE:  26.475247621536255 Best Val MSE:  24.81772005558014\n",
            "Epoch:  147 , Time Elapsed:  48.21355632146199  mins\n",
            "Epoch 147/200, Batch 1/45, Loss: 2.3505301475524902\n",
            "Epoch 147/200, Batch 2/45, Loss: 2.8836472034454346\n",
            "Epoch 147/200, Batch 3/45, Loss: 2.3916192054748535\n",
            "Epoch 147/200, Batch 4/45, Loss: 4.840341091156006\n",
            "Epoch 147/200, Batch 5/45, Loss: 1.972683310508728\n",
            "Epoch 147/200, Batch 6/45, Loss: 2.1057331562042236\n",
            "Epoch 147/200, Batch 7/45, Loss: 1.9602888822555542\n",
            "Epoch 147/200, Batch 8/45, Loss: 2.46628737449646\n",
            "Epoch 147/200, Batch 9/45, Loss: 1.7872129678726196\n",
            "Epoch 147/200, Batch 10/45, Loss: 3.001486301422119\n",
            "Epoch 147/200, Batch 11/45, Loss: 2.5967507362365723\n",
            "Epoch 147/200, Batch 12/45, Loss: 2.8766257762908936\n",
            "Epoch 147/200, Batch 13/45, Loss: 2.2226431369781494\n",
            "Epoch 147/200, Batch 14/45, Loss: 2.0805270671844482\n",
            "Epoch 147/200, Batch 15/45, Loss: 1.8295111656188965\n",
            "Epoch 147/200, Batch 16/45, Loss: 1.7987329959869385\n",
            "Epoch 147/200, Batch 17/45, Loss: 2.309746742248535\n",
            "Epoch 147/200, Batch 18/45, Loss: 2.3877274990081787\n",
            "Epoch 147/200, Batch 19/45, Loss: 2.1645500659942627\n",
            "Epoch 147/200, Batch 20/45, Loss: 2.221548080444336\n",
            "Epoch 147/200, Batch 21/45, Loss: 2.6673359870910645\n",
            "Epoch 147/200, Batch 22/45, Loss: 2.5524473190307617\n",
            "Epoch 147/200, Batch 23/45, Loss: 2.1857893466949463\n",
            "Epoch 147/200, Batch 24/45, Loss: 2.4380743503570557\n",
            "Epoch 147/200, Batch 25/45, Loss: 2.44669246673584\n",
            "Epoch 147/200, Batch 26/45, Loss: 2.6147377490997314\n",
            "Epoch 147/200, Batch 27/45, Loss: 2.459287166595459\n",
            "Epoch 147/200, Batch 28/45, Loss: 1.9058611392974854\n",
            "Epoch 147/200, Batch 29/45, Loss: 2.5323262214660645\n",
            "Epoch 147/200, Batch 30/45, Loss: 2.049579620361328\n",
            "Epoch 147/200, Batch 31/45, Loss: 2.549257516860962\n",
            "Epoch 147/200, Batch 32/45, Loss: 1.881532907485962\n",
            "Epoch 147/200, Batch 33/45, Loss: 2.126398801803589\n",
            "Epoch 147/200, Batch 34/45, Loss: 2.3228759765625\n",
            "Epoch 147/200, Batch 35/45, Loss: 1.9261924028396606\n",
            "Epoch 147/200, Batch 36/45, Loss: 2.65259051322937\n",
            "Epoch 147/200, Batch 37/45, Loss: 2.5679736137390137\n",
            "Epoch 147/200, Batch 38/45, Loss: 2.120142698287964\n",
            "Epoch 147/200, Batch 39/45, Loss: 1.8048831224441528\n",
            "Epoch 147/200, Batch 40/45, Loss: 1.9447790384292603\n",
            "Epoch 147/200, Batch 41/45, Loss: 2.982800006866455\n",
            "Epoch 147/200, Batch 42/45, Loss: 2.022573947906494\n",
            "Epoch 147/200, Batch 43/45, Loss: 2.402073860168457\n",
            "Epoch 147/200, Batch 44/45, Loss: 1.2181999683380127\n",
            "Epoch 147/200, Batch 45/45, Loss: 2.0701801776885986\n",
            "Validating and Checkpointing!\n",
            "Model is not good (might be overfitting)! Current val MSE:  28.485389471054077 Best Val MSE:  24.81772005558014\n",
            "Epoch:  148 , Time Elapsed:  48.5476326028506  mins\n",
            "Epoch 148/200, Batch 1/45, Loss: 2.2534844875335693\n",
            "Epoch 148/200, Batch 2/45, Loss: 2.055950164794922\n",
            "Epoch 148/200, Batch 3/45, Loss: 1.8640705347061157\n",
            "Epoch 148/200, Batch 4/45, Loss: 2.3274893760681152\n",
            "Epoch 148/200, Batch 5/45, Loss: 2.042240619659424\n",
            "Epoch 148/200, Batch 6/45, Loss: 1.9372193813323975\n",
            "Epoch 148/200, Batch 7/45, Loss: 2.0342583656311035\n",
            "Epoch 148/200, Batch 8/45, Loss: 2.344139814376831\n",
            "Epoch 148/200, Batch 9/45, Loss: 1.898924469947815\n",
            "Epoch 148/200, Batch 10/45, Loss: 2.4995357990264893\n",
            "Epoch 148/200, Batch 11/45, Loss: 2.339237928390503\n",
            "Epoch 148/200, Batch 12/45, Loss: 2.4441094398498535\n",
            "Epoch 148/200, Batch 13/45, Loss: 2.187464475631714\n",
            "Epoch 148/200, Batch 14/45, Loss: 1.7304339408874512\n",
            "Epoch 148/200, Batch 15/45, Loss: 1.882068157196045\n",
            "Epoch 148/200, Batch 16/45, Loss: 2.5482940673828125\n",
            "Epoch 148/200, Batch 17/45, Loss: 2.1292314529418945\n",
            "Epoch 148/200, Batch 18/45, Loss: 2.2898831367492676\n",
            "Epoch 148/200, Batch 19/45, Loss: 2.382772445678711\n",
            "Epoch 148/200, Batch 20/45, Loss: 1.917039155960083\n",
            "Epoch 148/200, Batch 21/45, Loss: 1.5733884572982788\n",
            "Epoch 148/200, Batch 22/45, Loss: 2.390718936920166\n",
            "Epoch 148/200, Batch 23/45, Loss: 2.4250552654266357\n",
            "Epoch 148/200, Batch 24/45, Loss: 2.1258623600006104\n",
            "Epoch 148/200, Batch 25/45, Loss: 2.1117448806762695\n",
            "Epoch 148/200, Batch 26/45, Loss: 2.2174715995788574\n",
            "Epoch 148/200, Batch 27/45, Loss: 2.2828714847564697\n",
            "Epoch 148/200, Batch 28/45, Loss: 1.8164827823638916\n",
            "Epoch 148/200, Batch 29/45, Loss: 2.8363609313964844\n",
            "Epoch 148/200, Batch 30/45, Loss: 2.0945112705230713\n",
            "Epoch 148/200, Batch 31/45, Loss: 2.5278453826904297\n",
            "Epoch 148/200, Batch 32/45, Loss: 2.6205861568450928\n",
            "Epoch 148/200, Batch 33/45, Loss: 2.2080936431884766\n",
            "Epoch 148/200, Batch 34/45, Loss: 2.7879512310028076\n",
            "Epoch 148/200, Batch 35/45, Loss: 2.404886245727539\n",
            "Epoch 148/200, Batch 36/45, Loss: 2.41141939163208\n",
            "Epoch 148/200, Batch 37/45, Loss: 1.3068578243255615\n",
            "Epoch 148/200, Batch 38/45, Loss: 2.1955161094665527\n",
            "Epoch 148/200, Batch 39/45, Loss: 2.167119026184082\n",
            "Epoch 148/200, Batch 40/45, Loss: 1.413986325263977\n",
            "Epoch 148/200, Batch 41/45, Loss: 1.7178494930267334\n",
            "Epoch 148/200, Batch 42/45, Loss: 1.7277785539627075\n",
            "Epoch 148/200, Batch 43/45, Loss: 2.0418508052825928\n",
            "Epoch 148/200, Batch 44/45, Loss: 1.681297779083252\n",
            "Epoch 148/200, Batch 45/45, Loss: 1.9701064825057983\n",
            "Validating and Checkpointing!\n",
            "Model is not good (might be overfitting)! Current val MSE:  42.619330286979675 Best Val MSE:  24.81772005558014\n",
            "Epoch:  149 , Time Elapsed:  48.87610591252645  mins\n",
            "Epoch 149/200, Batch 1/45, Loss: 2.611095666885376\n",
            "Epoch 149/200, Batch 2/45, Loss: 2.1217408180236816\n",
            "Epoch 149/200, Batch 3/45, Loss: 2.5953316688537598\n",
            "Epoch 149/200, Batch 4/45, Loss: 2.138857841491699\n",
            "Epoch 149/200, Batch 5/45, Loss: 1.6391057968139648\n",
            "Epoch 149/200, Batch 6/45, Loss: 2.4123239517211914\n",
            "Epoch 149/200, Batch 7/45, Loss: 2.122544050216675\n",
            "Epoch 149/200, Batch 8/45, Loss: 2.372293472290039\n",
            "Epoch 149/200, Batch 9/45, Loss: 2.1992239952087402\n",
            "Epoch 149/200, Batch 10/45, Loss: 2.226703643798828\n",
            "Epoch 149/200, Batch 11/45, Loss: 1.5078867673873901\n",
            "Epoch 149/200, Batch 12/45, Loss: 2.1123409271240234\n",
            "Epoch 149/200, Batch 13/45, Loss: 2.8176026344299316\n",
            "Epoch 149/200, Batch 14/45, Loss: 2.423915147781372\n",
            "Epoch 149/200, Batch 15/45, Loss: 2.337359666824341\n",
            "Epoch 149/200, Batch 16/45, Loss: 2.679062843322754\n",
            "Epoch 149/200, Batch 17/45, Loss: 1.4621790647506714\n",
            "Epoch 149/200, Batch 18/45, Loss: 2.321793794631958\n",
            "Epoch 149/200, Batch 19/45, Loss: 2.0835206508636475\n",
            "Epoch 149/200, Batch 20/45, Loss: 2.155935287475586\n",
            "Epoch 149/200, Batch 21/45, Loss: 2.3555707931518555\n",
            "Epoch 149/200, Batch 22/45, Loss: 2.4190921783447266\n",
            "Epoch 149/200, Batch 23/45, Loss: 2.691321849822998\n",
            "Epoch 149/200, Batch 24/45, Loss: 2.355116844177246\n",
            "Epoch 149/200, Batch 25/45, Loss: 2.4625630378723145\n",
            "Epoch 149/200, Batch 26/45, Loss: 2.511528491973877\n",
            "Epoch 149/200, Batch 27/45, Loss: 2.0232810974121094\n",
            "Epoch 149/200, Batch 28/45, Loss: 2.177865505218506\n",
            "Epoch 149/200, Batch 29/45, Loss: 2.6450014114379883\n",
            "Epoch 149/200, Batch 30/45, Loss: 2.290762424468994\n",
            "Epoch 149/200, Batch 31/45, Loss: 1.8843083381652832\n",
            "Epoch 149/200, Batch 32/45, Loss: 2.2442402839660645\n",
            "Epoch 149/200, Batch 33/45, Loss: 2.7827019691467285\n",
            "Epoch 149/200, Batch 34/45, Loss: 0.9608521461486816\n",
            "Epoch 149/200, Batch 35/45, Loss: 1.87712562084198\n",
            "Epoch 149/200, Batch 36/45, Loss: 2.626079559326172\n",
            "Epoch 149/200, Batch 37/45, Loss: 2.2575747966766357\n",
            "Epoch 149/200, Batch 38/45, Loss: 2.3288111686706543\n",
            "Epoch 149/200, Batch 39/45, Loss: 2.473271369934082\n",
            "Epoch 149/200, Batch 40/45, Loss: 1.9706720113754272\n",
            "Epoch 149/200, Batch 41/45, Loss: 1.4672319889068604\n",
            "Epoch 149/200, Batch 42/45, Loss: 1.98794686794281\n",
            "Epoch 149/200, Batch 43/45, Loss: 2.7935991287231445\n",
            "Epoch 149/200, Batch 44/45, Loss: 2.213026285171509\n",
            "Epoch 149/200, Batch 45/45, Loss: 2.5390067100524902\n",
            "Validating and Checkpointing!\n",
            "Model is not good (might be overfitting)! Current val MSE:  26.042232275009155 Best Val MSE:  24.81772005558014\n",
            "Epoch:  150 , Time Elapsed:  49.20475962162018  mins\n",
            "Epoch 150/200, Batch 1/45, Loss: 2.1644320487976074\n",
            "Epoch 150/200, Batch 2/45, Loss: 2.3489108085632324\n",
            "Epoch 150/200, Batch 3/45, Loss: 1.8993995189666748\n",
            "Epoch 150/200, Batch 4/45, Loss: 1.875284194946289\n",
            "Epoch 150/200, Batch 5/45, Loss: 2.2444281578063965\n",
            "Epoch 150/200, Batch 6/45, Loss: 2.0360968112945557\n",
            "Epoch 150/200, Batch 7/45, Loss: 2.7361371517181396\n",
            "Epoch 150/200, Batch 8/45, Loss: 2.4412050247192383\n",
            "Epoch 150/200, Batch 9/45, Loss: 2.3948442935943604\n",
            "Epoch 150/200, Batch 10/45, Loss: 2.396106719970703\n",
            "Epoch 150/200, Batch 11/45, Loss: 1.8445231914520264\n",
            "Epoch 150/200, Batch 12/45, Loss: 2.2525501251220703\n",
            "Epoch 150/200, Batch 13/45, Loss: 1.9908769130706787\n",
            "Epoch 150/200, Batch 14/45, Loss: 2.1689796447753906\n",
            "Epoch 150/200, Batch 15/45, Loss: 1.8326847553253174\n",
            "Epoch 150/200, Batch 16/45, Loss: 1.6565685272216797\n",
            "Epoch 150/200, Batch 17/45, Loss: 1.4980270862579346\n",
            "Epoch 150/200, Batch 18/45, Loss: 2.3143157958984375\n",
            "Epoch 150/200, Batch 19/45, Loss: 2.3722753524780273\n",
            "Epoch 150/200, Batch 20/45, Loss: 1.7911522388458252\n",
            "Epoch 150/200, Batch 21/45, Loss: 1.9293471574783325\n",
            "Epoch 150/200, Batch 22/45, Loss: 2.2574212551116943\n",
            "Epoch 150/200, Batch 23/45, Loss: 1.5838786363601685\n",
            "Epoch 150/200, Batch 24/45, Loss: 2.267815589904785\n",
            "Epoch 150/200, Batch 25/45, Loss: 1.7883718013763428\n",
            "Epoch 150/200, Batch 26/45, Loss: 2.2498927116394043\n",
            "Epoch 150/200, Batch 27/45, Loss: 1.9746463298797607\n",
            "Epoch 150/200, Batch 28/45, Loss: 2.6626381874084473\n",
            "Epoch 150/200, Batch 29/45, Loss: 2.505053997039795\n",
            "Epoch 150/200, Batch 30/45, Loss: 1.95438551902771\n",
            "Epoch 150/200, Batch 31/45, Loss: 2.200648784637451\n",
            "Epoch 150/200, Batch 32/45, Loss: 2.0636494159698486\n",
            "Epoch 150/200, Batch 33/45, Loss: 2.45804762840271\n",
            "Epoch 150/200, Batch 34/45, Loss: 1.9974076747894287\n",
            "Epoch 150/200, Batch 35/45, Loss: 2.313382625579834\n",
            "Epoch 150/200, Batch 36/45, Loss: 1.3910168409347534\n",
            "Epoch 150/200, Batch 37/45, Loss: 1.8682031631469727\n",
            "Epoch 150/200, Batch 38/45, Loss: 1.3925974369049072\n",
            "Epoch 150/200, Batch 39/45, Loss: 1.7925679683685303\n",
            "Epoch 150/200, Batch 40/45, Loss: 2.297017812728882\n",
            "Epoch 150/200, Batch 41/45, Loss: 2.5093374252319336\n",
            "Epoch 150/200, Batch 42/45, Loss: 2.6261205673217773\n",
            "Epoch 150/200, Batch 43/45, Loss: 2.335480213165283\n",
            "Epoch 150/200, Batch 44/45, Loss: 2.3908324241638184\n",
            "Epoch 150/200, Batch 45/45, Loss: 2.060041904449463\n",
            "Validating and Checkpointing!\n",
            "Model is not good (might be overfitting)! Current val MSE:  28.27491068840027 Best Val MSE:  24.81772005558014\n",
            "Epoch:  151 , Time Elapsed:  49.527450188001  mins\n",
            "Epoch 151/200, Batch 1/45, Loss: 2.1244170665740967\n",
            "Epoch 151/200, Batch 2/45, Loss: 2.1571974754333496\n",
            "Epoch 151/200, Batch 3/45, Loss: 2.723424196243286\n",
            "Epoch 151/200, Batch 4/45, Loss: 1.6594032049179077\n",
            "Epoch 151/200, Batch 5/45, Loss: 1.7610349655151367\n",
            "Epoch 151/200, Batch 6/45, Loss: 2.8006174564361572\n",
            "Epoch 151/200, Batch 7/45, Loss: 2.099903106689453\n",
            "Epoch 151/200, Batch 8/45, Loss: 2.003953218460083\n",
            "Epoch 151/200, Batch 9/45, Loss: 2.0493385791778564\n",
            "Epoch 151/200, Batch 10/45, Loss: 1.904634714126587\n",
            "Epoch 151/200, Batch 11/45, Loss: 2.0936191082000732\n",
            "Epoch 151/200, Batch 12/45, Loss: 2.1308398246765137\n",
            "Epoch 151/200, Batch 13/45, Loss: 2.785295248031616\n",
            "Epoch 151/200, Batch 14/45, Loss: 1.821211338043213\n",
            "Epoch 151/200, Batch 15/45, Loss: 1.6346793174743652\n",
            "Epoch 151/200, Batch 16/45, Loss: 1.6886332035064697\n",
            "Epoch 151/200, Batch 17/45, Loss: 1.9626507759094238\n",
            "Epoch 151/200, Batch 18/45, Loss: 2.117539405822754\n",
            "Epoch 151/200, Batch 19/45, Loss: 1.840055227279663\n",
            "Epoch 151/200, Batch 20/45, Loss: 2.03414249420166\n",
            "Epoch 151/200, Batch 21/45, Loss: 2.5828306674957275\n",
            "Epoch 151/200, Batch 22/45, Loss: 2.433241367340088\n",
            "Epoch 151/200, Batch 23/45, Loss: 1.9003732204437256\n",
            "Epoch 151/200, Batch 24/45, Loss: 2.040153980255127\n",
            "Epoch 151/200, Batch 25/45, Loss: 2.3275644779205322\n",
            "Epoch 151/200, Batch 26/45, Loss: 2.039302110671997\n",
            "Epoch 151/200, Batch 27/45, Loss: 2.004805564880371\n",
            "Epoch 151/200, Batch 28/45, Loss: 2.785477638244629\n",
            "Epoch 151/200, Batch 29/45, Loss: 1.8498162031173706\n",
            "Epoch 151/200, Batch 30/45, Loss: 2.2300429344177246\n",
            "Epoch 151/200, Batch 31/45, Loss: 1.621955156326294\n",
            "Epoch 151/200, Batch 32/45, Loss: 1.4000110626220703\n",
            "Epoch 151/200, Batch 33/45, Loss: 1.7896400690078735\n",
            "Epoch 151/200, Batch 34/45, Loss: 2.1145284175872803\n",
            "Epoch 151/200, Batch 35/45, Loss: 2.1422438621520996\n",
            "Epoch 151/200, Batch 36/45, Loss: 2.7380857467651367\n",
            "Epoch 151/200, Batch 37/45, Loss: 3.048623561859131\n",
            "Epoch 151/200, Batch 38/45, Loss: 2.1909847259521484\n",
            "Epoch 151/200, Batch 39/45, Loss: 2.500316619873047\n",
            "Epoch 151/200, Batch 40/45, Loss: 2.1280155181884766\n",
            "Epoch 151/200, Batch 41/45, Loss: 2.9358224868774414\n",
            "Epoch 151/200, Batch 42/45, Loss: 1.7879294157028198\n",
            "Epoch 151/200, Batch 43/45, Loss: 2.294020175933838\n",
            "Epoch 151/200, Batch 44/45, Loss: 1.5433743000030518\n",
            "Epoch 151/200, Batch 45/45, Loss: 1.950162410736084\n",
            "Validating and Checkpointing!\n",
            "Model is not good (might be overfitting)! Current val MSE:  26.425049662590027 Best Val MSE:  24.81772005558014\n",
            "Epoch:  152 , Time Elapsed:  49.84561829566955  mins\n",
            "Epoch 152/200, Batch 1/45, Loss: 1.9872236251831055\n",
            "Epoch 152/200, Batch 2/45, Loss: 2.3672261238098145\n",
            "Epoch 152/200, Batch 3/45, Loss: 2.3116326332092285\n",
            "Epoch 152/200, Batch 4/45, Loss: 3.0694797039031982\n",
            "Epoch 152/200, Batch 5/45, Loss: 2.440667152404785\n",
            "Epoch 152/200, Batch 6/45, Loss: 1.9453848600387573\n",
            "Epoch 152/200, Batch 7/45, Loss: 2.37495493888855\n",
            "Epoch 152/200, Batch 8/45, Loss: 2.5475714206695557\n",
            "Epoch 152/200, Batch 9/45, Loss: 2.558462619781494\n",
            "Epoch 152/200, Batch 10/45, Loss: 2.1257545948028564\n",
            "Epoch 152/200, Batch 11/45, Loss: 1.7569235563278198\n",
            "Epoch 152/200, Batch 12/45, Loss: 1.8553762435913086\n",
            "Epoch 152/200, Batch 13/45, Loss: 3.1198830604553223\n",
            "Epoch 152/200, Batch 14/45, Loss: 2.1844961643218994\n",
            "Epoch 152/200, Batch 15/45, Loss: 2.1286418437957764\n",
            "Epoch 152/200, Batch 16/45, Loss: 2.5114736557006836\n",
            "Epoch 152/200, Batch 17/45, Loss: 2.3532168865203857\n",
            "Epoch 152/200, Batch 18/45, Loss: 2.2259905338287354\n",
            "Epoch 152/200, Batch 19/45, Loss: 1.5266470909118652\n",
            "Epoch 152/200, Batch 20/45, Loss: 2.183382511138916\n",
            "Epoch 152/200, Batch 21/45, Loss: 2.3301870822906494\n",
            "Epoch 152/200, Batch 22/45, Loss: 4.256048679351807\n",
            "Epoch 152/200, Batch 23/45, Loss: 2.732271194458008\n",
            "Epoch 152/200, Batch 24/45, Loss: 1.800572156906128\n",
            "Epoch 152/200, Batch 25/45, Loss: 2.037656545639038\n",
            "Epoch 152/200, Batch 26/45, Loss: 2.192732810974121\n",
            "Epoch 152/200, Batch 27/45, Loss: 2.5770914554595947\n",
            "Epoch 152/200, Batch 28/45, Loss: 1.7940030097961426\n",
            "Epoch 152/200, Batch 29/45, Loss: 1.9147040843963623\n",
            "Epoch 152/200, Batch 30/45, Loss: 2.1113383769989014\n",
            "Epoch 152/200, Batch 31/45, Loss: 2.1038968563079834\n",
            "Epoch 152/200, Batch 32/45, Loss: 2.3436529636383057\n",
            "Epoch 152/200, Batch 33/45, Loss: 2.2541306018829346\n",
            "Epoch 152/200, Batch 34/45, Loss: 2.2174642086029053\n",
            "Epoch 152/200, Batch 35/45, Loss: 2.1421947479248047\n",
            "Epoch 152/200, Batch 36/45, Loss: 2.84548020362854\n",
            "Epoch 152/200, Batch 37/45, Loss: 2.336791515350342\n",
            "Epoch 152/200, Batch 38/45, Loss: 2.6025474071502686\n",
            "Epoch 152/200, Batch 39/45, Loss: 2.5389575958251953\n",
            "Epoch 152/200, Batch 40/45, Loss: 1.7220488786697388\n",
            "Epoch 152/200, Batch 41/45, Loss: 2.5844318866729736\n",
            "Epoch 152/200, Batch 42/45, Loss: 2.17284893989563\n",
            "Epoch 152/200, Batch 43/45, Loss: 2.1318228244781494\n",
            "Epoch 152/200, Batch 44/45, Loss: 2.1546149253845215\n",
            "Epoch 152/200, Batch 45/45, Loss: 1.9537672996520996\n",
            "Validating and Checkpointing!\n",
            "Model is not good (might be overfitting)! Current val MSE:  26.78301501274109 Best Val MSE:  24.81772005558014\n",
            "Epoch:  153 , Time Elapsed:  50.18725729783376  mins\n",
            "Epoch 153/200, Batch 1/45, Loss: 2.695380926132202\n",
            "Epoch 153/200, Batch 2/45, Loss: 1.8357688188552856\n",
            "Epoch 153/200, Batch 3/45, Loss: 1.5666099786758423\n",
            "Epoch 153/200, Batch 4/45, Loss: 2.1700119972229004\n",
            "Epoch 153/200, Batch 5/45, Loss: 2.433361291885376\n",
            "Epoch 153/200, Batch 6/45, Loss: 2.5503151416778564\n",
            "Epoch 153/200, Batch 7/45, Loss: 1.993485689163208\n",
            "Epoch 153/200, Batch 8/45, Loss: 2.4429144859313965\n",
            "Epoch 153/200, Batch 9/45, Loss: 1.6578574180603027\n",
            "Epoch 153/200, Batch 10/45, Loss: 1.674293041229248\n",
            "Epoch 153/200, Batch 11/45, Loss: 2.7837882041931152\n",
            "Epoch 153/200, Batch 12/45, Loss: 2.140765905380249\n",
            "Epoch 153/200, Batch 13/45, Loss: 2.1845881938934326\n",
            "Epoch 153/200, Batch 14/45, Loss: 1.7044665813446045\n",
            "Epoch 153/200, Batch 15/45, Loss: 1.418918251991272\n",
            "Epoch 153/200, Batch 16/45, Loss: 2.366598606109619\n",
            "Epoch 153/200, Batch 17/45, Loss: 2.2457456588745117\n",
            "Epoch 153/200, Batch 18/45, Loss: 2.0335617065429688\n",
            "Epoch 153/200, Batch 19/45, Loss: 2.5843300819396973\n",
            "Epoch 153/200, Batch 20/45, Loss: 1.7263190746307373\n",
            "Epoch 153/200, Batch 21/45, Loss: 2.88686203956604\n",
            "Epoch 153/200, Batch 22/45, Loss: 1.3761024475097656\n",
            "Epoch 153/200, Batch 23/45, Loss: 2.6719000339508057\n",
            "Epoch 153/200, Batch 24/45, Loss: 1.5088512897491455\n",
            "Epoch 153/200, Batch 25/45, Loss: 2.103120803833008\n",
            "Epoch 153/200, Batch 26/45, Loss: 2.6115002632141113\n",
            "Epoch 153/200, Batch 27/45, Loss: 1.6900240182876587\n",
            "Epoch 153/200, Batch 28/45, Loss: 2.0607948303222656\n",
            "Epoch 153/200, Batch 29/45, Loss: 1.8868989944458008\n",
            "Epoch 153/200, Batch 30/45, Loss: 2.3475942611694336\n",
            "Epoch 153/200, Batch 31/45, Loss: 2.371415615081787\n",
            "Epoch 153/200, Batch 32/45, Loss: 1.9818445444107056\n",
            "Epoch 153/200, Batch 33/45, Loss: 6.234679222106934\n",
            "Epoch 153/200, Batch 34/45, Loss: 1.7444864511489868\n",
            "Epoch 153/200, Batch 35/45, Loss: 2.124281883239746\n",
            "Epoch 153/200, Batch 36/45, Loss: 2.895564556121826\n",
            "Epoch 153/200, Batch 37/45, Loss: 2.2010297775268555\n",
            "Epoch 153/200, Batch 38/45, Loss: 1.525024652481079\n",
            "Epoch 153/200, Batch 39/45, Loss: 3.0581302642822266\n",
            "Epoch 153/200, Batch 40/45, Loss: 2.490664482116699\n",
            "Epoch 153/200, Batch 41/45, Loss: 2.24015212059021\n",
            "Epoch 153/200, Batch 42/45, Loss: 2.1177377700805664\n",
            "Epoch 153/200, Batch 43/45, Loss: 2.2479567527770996\n",
            "Epoch 153/200, Batch 44/45, Loss: 2.0918264389038086\n",
            "Epoch 153/200, Batch 45/45, Loss: 1.9280543327331543\n",
            "Validating and Checkpointing!\n",
            "Model is not good (might be overfitting)! Current val MSE:  25.748632192611694 Best Val MSE:  24.81772005558014\n",
            "Epoch:  154 , Time Elapsed:  50.500865821043654  mins\n",
            "Epoch 154/200, Batch 1/45, Loss: 1.7473899126052856\n",
            "Epoch 154/200, Batch 2/45, Loss: 2.4759068489074707\n",
            "Epoch 154/200, Batch 3/45, Loss: 2.41770076751709\n",
            "Epoch 154/200, Batch 4/45, Loss: 2.4285378456115723\n",
            "Epoch 154/200, Batch 5/45, Loss: 1.837451696395874\n",
            "Epoch 154/200, Batch 6/45, Loss: 2.596527099609375\n",
            "Epoch 154/200, Batch 7/45, Loss: 2.4073221683502197\n",
            "Epoch 154/200, Batch 8/45, Loss: 2.5826590061187744\n",
            "Epoch 154/200, Batch 9/45, Loss: 1.659043312072754\n",
            "Epoch 154/200, Batch 10/45, Loss: 2.0664267539978027\n",
            "Epoch 154/200, Batch 11/45, Loss: 2.6567211151123047\n",
            "Epoch 154/200, Batch 12/45, Loss: 2.1851587295532227\n",
            "Epoch 154/200, Batch 13/45, Loss: 2.35327410697937\n",
            "Epoch 154/200, Batch 14/45, Loss: 2.7474112510681152\n",
            "Epoch 154/200, Batch 15/45, Loss: 2.592508316040039\n",
            "Epoch 154/200, Batch 16/45, Loss: 2.5853137969970703\n",
            "Epoch 154/200, Batch 17/45, Loss: 2.1088027954101562\n",
            "Epoch 154/200, Batch 18/45, Loss: 2.541393756866455\n",
            "Epoch 154/200, Batch 19/45, Loss: 2.5465891361236572\n",
            "Epoch 154/200, Batch 20/45, Loss: 2.501927614212036\n",
            "Epoch 154/200, Batch 21/45, Loss: 1.7137495279312134\n",
            "Epoch 154/200, Batch 22/45, Loss: 1.6163729429244995\n",
            "Epoch 154/200, Batch 23/45, Loss: 1.8644976615905762\n",
            "Epoch 154/200, Batch 24/45, Loss: 2.2379398345947266\n",
            "Epoch 154/200, Batch 25/45, Loss: 2.0557126998901367\n",
            "Epoch 154/200, Batch 26/45, Loss: 2.3435134887695312\n",
            "Epoch 154/200, Batch 27/45, Loss: 1.8249870538711548\n",
            "Epoch 154/200, Batch 28/45, Loss: 2.469118595123291\n",
            "Epoch 154/200, Batch 29/45, Loss: 1.9341907501220703\n",
            "Epoch 154/200, Batch 30/45, Loss: 2.75302791595459\n",
            "Epoch 154/200, Batch 31/45, Loss: 2.535578966140747\n",
            "Epoch 154/200, Batch 32/45, Loss: 2.7099006175994873\n",
            "Epoch 154/200, Batch 33/45, Loss: 2.119093894958496\n",
            "Epoch 154/200, Batch 34/45, Loss: 2.5335469245910645\n",
            "Epoch 154/200, Batch 35/45, Loss: 2.3498308658599854\n",
            "Epoch 154/200, Batch 36/45, Loss: 2.347353935241699\n",
            "Epoch 154/200, Batch 37/45, Loss: 1.7701749801635742\n",
            "Epoch 154/200, Batch 38/45, Loss: 1.882962703704834\n",
            "Epoch 154/200, Batch 39/45, Loss: 2.970348596572876\n",
            "Epoch 154/200, Batch 40/45, Loss: 2.1572113037109375\n",
            "Epoch 154/200, Batch 41/45, Loss: 2.4280261993408203\n",
            "Epoch 154/200, Batch 42/45, Loss: 1.7866517305374146\n",
            "Epoch 154/200, Batch 43/45, Loss: 1.9147199392318726\n",
            "Epoch 154/200, Batch 44/45, Loss: 2.0803096294403076\n",
            "Epoch 154/200, Batch 45/45, Loss: 2.089571475982666\n",
            "Validating and Checkpointing!\n",
            "Model is not good (might be overfitting)! Current val MSE:  27.13125205039978 Best Val MSE:  24.81772005558014\n",
            "Epoch:  155 , Time Elapsed:  50.819536145528154  mins\n",
            "Epoch 155/200, Batch 1/45, Loss: 1.485771656036377\n",
            "Epoch 155/200, Batch 2/45, Loss: 2.0425219535827637\n",
            "Epoch 155/200, Batch 3/45, Loss: 2.8329153060913086\n",
            "Epoch 155/200, Batch 4/45, Loss: 1.883872151374817\n",
            "Epoch 155/200, Batch 5/45, Loss: 1.811967372894287\n",
            "Epoch 155/200, Batch 6/45, Loss: 1.6396209001541138\n",
            "Epoch 155/200, Batch 7/45, Loss: 2.332986831665039\n",
            "Epoch 155/200, Batch 8/45, Loss: 1.5371077060699463\n",
            "Epoch 155/200, Batch 9/45, Loss: 2.315617084503174\n",
            "Epoch 155/200, Batch 10/45, Loss: 1.9681422710418701\n",
            "Epoch 155/200, Batch 11/45, Loss: 1.7689467668533325\n",
            "Epoch 155/200, Batch 12/45, Loss: 1.926971673965454\n",
            "Epoch 155/200, Batch 13/45, Loss: 1.610465168952942\n",
            "Epoch 155/200, Batch 14/45, Loss: 2.261384963989258\n",
            "Epoch 155/200, Batch 15/45, Loss: 2.6567435264587402\n",
            "Epoch 155/200, Batch 16/45, Loss: 2.362173080444336\n",
            "Epoch 155/200, Batch 17/45, Loss: 2.3766977787017822\n",
            "Epoch 155/200, Batch 18/45, Loss: 2.246403217315674\n",
            "Epoch 155/200, Batch 19/45, Loss: 1.6623940467834473\n",
            "Epoch 155/200, Batch 20/45, Loss: 2.3987247943878174\n",
            "Epoch 155/200, Batch 21/45, Loss: 2.85036301612854\n",
            "Epoch 155/200, Batch 22/45, Loss: 2.152385711669922\n",
            "Epoch 155/200, Batch 23/45, Loss: 2.074636697769165\n",
            "Epoch 155/200, Batch 24/45, Loss: 1.9321277141571045\n",
            "Epoch 155/200, Batch 25/45, Loss: 1.9131786823272705\n",
            "Epoch 155/200, Batch 26/45, Loss: 1.9687952995300293\n",
            "Epoch 155/200, Batch 27/45, Loss: 2.445502996444702\n",
            "Epoch 155/200, Batch 28/45, Loss: 1.8251408338546753\n",
            "Epoch 155/200, Batch 29/45, Loss: 2.1459295749664307\n",
            "Epoch 155/200, Batch 30/45, Loss: 2.8283746242523193\n",
            "Epoch 155/200, Batch 31/45, Loss: 2.2840054035186768\n",
            "Epoch 155/200, Batch 32/45, Loss: 2.5919296741485596\n",
            "Epoch 155/200, Batch 33/45, Loss: 2.4157965183258057\n",
            "Epoch 155/200, Batch 34/45, Loss: 6.429537773132324\n",
            "Epoch 155/200, Batch 35/45, Loss: 1.7790274620056152\n",
            "Epoch 155/200, Batch 36/45, Loss: 3.071866512298584\n",
            "Epoch 155/200, Batch 37/45, Loss: 2.7483391761779785\n",
            "Epoch 155/200, Batch 38/45, Loss: 2.052236557006836\n",
            "Epoch 155/200, Batch 39/45, Loss: 2.668419599533081\n",
            "Epoch 155/200, Batch 40/45, Loss: 2.5012667179107666\n",
            "Epoch 155/200, Batch 41/45, Loss: 2.5426528453826904\n",
            "Epoch 155/200, Batch 42/45, Loss: 2.1807641983032227\n",
            "Epoch 155/200, Batch 43/45, Loss: 2.366426944732666\n",
            "Epoch 155/200, Batch 44/45, Loss: 2.2792654037475586\n",
            "Epoch 155/200, Batch 45/45, Loss: 1.7082798480987549\n",
            "Validating and Checkpointing!\n",
            "Model is not good (might be overfitting)! Current val MSE:  151.74843883514404 Best Val MSE:  24.81772005558014\n",
            "Epoch:  156 , Time Elapsed:  51.18301910559337  mins\n",
            "Epoch 156/200, Batch 1/45, Loss: 1.1119837760925293\n",
            "Epoch 156/200, Batch 2/45, Loss: 2.4960339069366455\n",
            "Epoch 156/200, Batch 3/45, Loss: 1.6968390941619873\n",
            "Epoch 156/200, Batch 4/45, Loss: 1.8635605573654175\n",
            "Epoch 156/200, Batch 5/45, Loss: 2.528585433959961\n",
            "Epoch 156/200, Batch 6/45, Loss: 2.5500307083129883\n",
            "Epoch 156/200, Batch 7/45, Loss: 2.387695789337158\n",
            "Epoch 156/200, Batch 8/45, Loss: 2.4528841972351074\n",
            "Epoch 156/200, Batch 9/45, Loss: 2.7676656246185303\n",
            "Epoch 156/200, Batch 10/45, Loss: 1.501474142074585\n",
            "Epoch 156/200, Batch 11/45, Loss: 2.2328178882598877\n",
            "Epoch 156/200, Batch 12/45, Loss: 3.025416851043701\n",
            "Epoch 156/200, Batch 13/45, Loss: 2.2291107177734375\n",
            "Epoch 156/200, Batch 14/45, Loss: 2.417534828186035\n",
            "Epoch 156/200, Batch 15/45, Loss: 1.857553243637085\n",
            "Epoch 156/200, Batch 16/45, Loss: 2.4178338050842285\n",
            "Epoch 156/200, Batch 17/45, Loss: 1.6162856817245483\n",
            "Epoch 156/200, Batch 18/45, Loss: 2.880753755569458\n",
            "Epoch 156/200, Batch 19/45, Loss: 1.5592870712280273\n",
            "Epoch 156/200, Batch 20/45, Loss: 2.2914068698883057\n",
            "Epoch 156/200, Batch 21/45, Loss: 2.149214267730713\n",
            "Epoch 156/200, Batch 22/45, Loss: 2.168375253677368\n",
            "Epoch 156/200, Batch 23/45, Loss: 2.165414571762085\n",
            "Epoch 156/200, Batch 24/45, Loss: 1.5894821882247925\n",
            "Epoch 156/200, Batch 25/45, Loss: 2.0490403175354004\n",
            "Epoch 156/200, Batch 26/45, Loss: 2.4683544635772705\n",
            "Epoch 156/200, Batch 27/45, Loss: 3.186324119567871\n",
            "Epoch 156/200, Batch 28/45, Loss: 1.680229902267456\n",
            "Epoch 156/200, Batch 29/45, Loss: 1.7835896015167236\n",
            "Epoch 156/200, Batch 30/45, Loss: 2.8563809394836426\n",
            "Epoch 156/200, Batch 31/45, Loss: 2.0888383388519287\n",
            "Epoch 156/200, Batch 32/45, Loss: 2.3266959190368652\n",
            "Epoch 156/200, Batch 33/45, Loss: 1.9861998558044434\n",
            "Epoch 156/200, Batch 34/45, Loss: 2.818770408630371\n",
            "Epoch 156/200, Batch 35/45, Loss: 1.6941379308700562\n",
            "Epoch 156/200, Batch 36/45, Loss: 2.149847984313965\n",
            "Epoch 156/200, Batch 37/45, Loss: 1.9619921445846558\n",
            "Epoch 156/200, Batch 38/45, Loss: 2.2071993350982666\n",
            "Epoch 156/200, Batch 39/45, Loss: 2.2473301887512207\n",
            "Epoch 156/200, Batch 40/45, Loss: 2.6397361755371094\n",
            "Epoch 156/200, Batch 41/45, Loss: 2.440401077270508\n",
            "Epoch 156/200, Batch 42/45, Loss: 1.6270604133605957\n",
            "Epoch 156/200, Batch 43/45, Loss: 2.466827154159546\n",
            "Epoch 156/200, Batch 44/45, Loss: 2.0170834064483643\n",
            "Epoch 156/200, Batch 45/45, Loss: 1.618135690689087\n",
            "Validating and Checkpointing!\n",
            "Model is not good (might be overfitting)! Current val MSE:  54.8956139087677 Best Val MSE:  24.81772005558014\n",
            "Epoch:  157 , Time Elapsed:  51.495945274829865  mins\n",
            "Epoch 157/200, Batch 1/45, Loss: 9.276135444641113\n",
            "Epoch 157/200, Batch 2/45, Loss: 2.27352237701416\n",
            "Epoch 157/200, Batch 3/45, Loss: 2.7000770568847656\n",
            "Epoch 157/200, Batch 4/45, Loss: 2.508699655532837\n",
            "Epoch 157/200, Batch 5/45, Loss: 2.945228099822998\n",
            "Epoch 157/200, Batch 6/45, Loss: 2.7793431282043457\n",
            "Epoch 157/200, Batch 7/45, Loss: 3.149841070175171\n",
            "Epoch 157/200, Batch 8/45, Loss: 2.6729423999786377\n",
            "Epoch 157/200, Batch 9/45, Loss: 2.7622828483581543\n",
            "Epoch 157/200, Batch 10/45, Loss: 3.117443561553955\n",
            "Epoch 157/200, Batch 11/45, Loss: 2.6946256160736084\n",
            "Epoch 157/200, Batch 12/45, Loss: 2.5478110313415527\n",
            "Epoch 157/200, Batch 13/45, Loss: 2.6320457458496094\n",
            "Epoch 157/200, Batch 14/45, Loss: 1.9048447608947754\n",
            "Epoch 157/200, Batch 15/45, Loss: 2.258305549621582\n",
            "Epoch 157/200, Batch 16/45, Loss: 2.495351791381836\n",
            "Epoch 157/200, Batch 17/45, Loss: 2.2890191078186035\n",
            "Epoch 157/200, Batch 18/45, Loss: 1.9450745582580566\n",
            "Epoch 157/200, Batch 19/45, Loss: 1.9206032752990723\n",
            "Epoch 157/200, Batch 20/45, Loss: 2.300438642501831\n",
            "Epoch 157/200, Batch 21/45, Loss: 2.0158069133758545\n",
            "Epoch 157/200, Batch 22/45, Loss: 2.5811691284179688\n",
            "Epoch 157/200, Batch 23/45, Loss: 2.521078586578369\n",
            "Epoch 157/200, Batch 24/45, Loss: 2.5308022499084473\n",
            "Epoch 157/200, Batch 25/45, Loss: 2.270183563232422\n",
            "Epoch 157/200, Batch 26/45, Loss: 2.4855780601501465\n",
            "Epoch 157/200, Batch 27/45, Loss: 2.2006120681762695\n",
            "Epoch 157/200, Batch 28/45, Loss: 2.2360000610351562\n",
            "Epoch 157/200, Batch 29/45, Loss: 1.9808168411254883\n",
            "Epoch 157/200, Batch 30/45, Loss: 2.046032190322876\n",
            "Epoch 157/200, Batch 31/45, Loss: 2.4284610748291016\n",
            "Epoch 157/200, Batch 32/45, Loss: 1.9653453826904297\n",
            "Epoch 157/200, Batch 33/45, Loss: 2.008291006088257\n",
            "Epoch 157/200, Batch 34/45, Loss: 2.130690097808838\n",
            "Epoch 157/200, Batch 35/45, Loss: 2.2121286392211914\n",
            "Epoch 157/200, Batch 36/45, Loss: 2.601952075958252\n",
            "Epoch 157/200, Batch 37/45, Loss: 2.715269088745117\n",
            "Epoch 157/200, Batch 38/45, Loss: 2.52048921585083\n",
            "Epoch 157/200, Batch 39/45, Loss: 2.620481014251709\n",
            "Epoch 157/200, Batch 40/45, Loss: 2.1517333984375\n",
            "Epoch 157/200, Batch 41/45, Loss: 2.5385327339172363\n",
            "Epoch 157/200, Batch 42/45, Loss: 2.3576414585113525\n",
            "Epoch 157/200, Batch 43/45, Loss: 2.8603861331939697\n",
            "Epoch 157/200, Batch 44/45, Loss: 2.3465237617492676\n",
            "Epoch 157/200, Batch 45/45, Loss: 2.3328864574432373\n",
            "Validating and Checkpointing!\n",
            "Model is not good (might be overfitting)! Current val MSE:  27.87821102142334 Best Val MSE:  24.81772005558014\n",
            "Epoch:  158 , Time Elapsed:  51.81418540875117  mins\n",
            "Epoch 158/200, Batch 1/45, Loss: 1.9734618663787842\n",
            "Epoch 158/200, Batch 2/45, Loss: 2.002296209335327\n",
            "Epoch 158/200, Batch 3/45, Loss: 2.2325215339660645\n",
            "Epoch 158/200, Batch 4/45, Loss: 2.5895838737487793\n",
            "Epoch 158/200, Batch 5/45, Loss: 3.033810615539551\n",
            "Epoch 158/200, Batch 6/45, Loss: 2.345151662826538\n",
            "Epoch 158/200, Batch 7/45, Loss: 1.9886555671691895\n",
            "Epoch 158/200, Batch 8/45, Loss: 1.5687804222106934\n",
            "Epoch 158/200, Batch 9/45, Loss: 1.9214537143707275\n",
            "Epoch 158/200, Batch 10/45, Loss: 2.4472029209136963\n",
            "Epoch 158/200, Batch 11/45, Loss: 3.1007118225097656\n",
            "Epoch 158/200, Batch 12/45, Loss: 2.3413991928100586\n",
            "Epoch 158/200, Batch 13/45, Loss: 2.3576786518096924\n",
            "Epoch 158/200, Batch 14/45, Loss: 2.0215940475463867\n",
            "Epoch 158/200, Batch 15/45, Loss: 2.461343288421631\n",
            "Epoch 158/200, Batch 16/45, Loss: 2.003553628921509\n",
            "Epoch 158/200, Batch 17/45, Loss: 1.7036762237548828\n",
            "Epoch 158/200, Batch 18/45, Loss: 2.5141611099243164\n",
            "Epoch 158/200, Batch 19/45, Loss: 2.393444061279297\n",
            "Epoch 158/200, Batch 20/45, Loss: 2.2989883422851562\n",
            "Epoch 158/200, Batch 21/45, Loss: 2.726287364959717\n",
            "Epoch 158/200, Batch 22/45, Loss: 1.7933655977249146\n",
            "Epoch 158/200, Batch 23/45, Loss: 2.3874354362487793\n",
            "Epoch 158/200, Batch 24/45, Loss: 2.4893431663513184\n",
            "Epoch 158/200, Batch 25/45, Loss: 2.1833856105804443\n",
            "Epoch 158/200, Batch 26/45, Loss: 1.722536563873291\n",
            "Epoch 158/200, Batch 27/45, Loss: 1.9398367404937744\n",
            "Epoch 158/200, Batch 28/45, Loss: 2.252215623855591\n",
            "Epoch 158/200, Batch 29/45, Loss: 2.826549530029297\n",
            "Epoch 158/200, Batch 30/45, Loss: 1.8840973377227783\n",
            "Epoch 158/200, Batch 31/45, Loss: 2.19294810295105\n",
            "Epoch 158/200, Batch 32/45, Loss: 1.678153157234192\n",
            "Epoch 158/200, Batch 33/45, Loss: 2.1865158081054688\n",
            "Epoch 158/200, Batch 34/45, Loss: 2.2191107273101807\n",
            "Epoch 158/200, Batch 35/45, Loss: 1.3818519115447998\n",
            "Epoch 158/200, Batch 36/45, Loss: 1.6033402681350708\n",
            "Epoch 158/200, Batch 37/45, Loss: 2.27856183052063\n",
            "Epoch 158/200, Batch 38/45, Loss: 2.17812442779541\n",
            "Epoch 158/200, Batch 39/45, Loss: 2.2898168563842773\n",
            "Epoch 158/200, Batch 40/45, Loss: 2.014888048171997\n",
            "Epoch 158/200, Batch 41/45, Loss: 2.615231990814209\n",
            "Epoch 158/200, Batch 42/45, Loss: 1.6936877965927124\n",
            "Epoch 158/200, Batch 43/45, Loss: 2.7669248580932617\n",
            "Epoch 158/200, Batch 44/45, Loss: 1.5374300479888916\n",
            "Epoch 158/200, Batch 45/45, Loss: 2.636683702468872\n",
            "Validating and Checkpointing!\n",
            "Model is not good (might be overfitting)! Current val MSE:  26.602668523788452 Best Val MSE:  24.81772005558014\n",
            "Epoch:  159 , Time Elapsed:  52.15707460641861  mins\n",
            "Epoch 159/200, Batch 1/45, Loss: 2.210801601409912\n",
            "Epoch 159/200, Batch 2/45, Loss: 2.322664737701416\n",
            "Epoch 159/200, Batch 3/45, Loss: 1.6832106113433838\n",
            "Epoch 159/200, Batch 4/45, Loss: 1.8117257356643677\n",
            "Epoch 159/200, Batch 5/45, Loss: 2.245004415512085\n",
            "Epoch 159/200, Batch 6/45, Loss: 2.471391201019287\n",
            "Epoch 159/200, Batch 7/45, Loss: 1.5547153949737549\n",
            "Epoch 159/200, Batch 8/45, Loss: 1.685621738433838\n",
            "Epoch 159/200, Batch 9/45, Loss: 2.4940831661224365\n",
            "Epoch 159/200, Batch 10/45, Loss: 1.8187265396118164\n",
            "Epoch 159/200, Batch 11/45, Loss: 1.5853241682052612\n",
            "Epoch 159/200, Batch 12/45, Loss: 1.8835517168045044\n",
            "Epoch 159/200, Batch 13/45, Loss: 2.210920572280884\n",
            "Epoch 159/200, Batch 14/45, Loss: 2.185940742492676\n",
            "Epoch 159/200, Batch 15/45, Loss: 1.8378702402114868\n",
            "Epoch 159/200, Batch 16/45, Loss: 1.8463119268417358\n",
            "Epoch 159/200, Batch 17/45, Loss: 8.624351501464844\n",
            "Epoch 159/200, Batch 18/45, Loss: 2.0800271034240723\n",
            "Epoch 159/200, Batch 19/45, Loss: 1.5795114040374756\n",
            "Epoch 159/200, Batch 20/45, Loss: 2.1788957118988037\n",
            "Epoch 159/200, Batch 21/45, Loss: 2.362301826477051\n",
            "Epoch 159/200, Batch 22/45, Loss: 2.2334249019622803\n",
            "Epoch 159/200, Batch 23/45, Loss: 3.036655902862549\n",
            "Epoch 159/200, Batch 24/45, Loss: 1.9278117418289185\n",
            "Epoch 159/200, Batch 25/45, Loss: 1.707228183746338\n",
            "Epoch 159/200, Batch 26/45, Loss: 2.4711294174194336\n",
            "Epoch 159/200, Batch 27/45, Loss: 2.108130931854248\n",
            "Epoch 159/200, Batch 28/45, Loss: 2.3468732833862305\n",
            "Epoch 159/200, Batch 29/45, Loss: 2.3358941078186035\n",
            "Epoch 159/200, Batch 30/45, Loss: 1.6670962572097778\n",
            "Epoch 159/200, Batch 31/45, Loss: 1.4637346267700195\n",
            "Epoch 159/200, Batch 32/45, Loss: 1.7993872165679932\n",
            "Epoch 159/200, Batch 33/45, Loss: 1.8645684719085693\n",
            "Epoch 159/200, Batch 34/45, Loss: 2.3592586517333984\n",
            "Epoch 159/200, Batch 35/45, Loss: 2.2614314556121826\n",
            "Epoch 159/200, Batch 36/45, Loss: 1.9758260250091553\n",
            "Epoch 159/200, Batch 37/45, Loss: 1.9297256469726562\n",
            "Epoch 159/200, Batch 38/45, Loss: 2.1037521362304688\n",
            "Epoch 159/200, Batch 39/45, Loss: 2.60659122467041\n",
            "Epoch 159/200, Batch 40/45, Loss: 2.9826292991638184\n",
            "Epoch 159/200, Batch 41/45, Loss: 1.451229453086853\n",
            "Epoch 159/200, Batch 42/45, Loss: 1.5865437984466553\n",
            "Epoch 159/200, Batch 43/45, Loss: 2.185420513153076\n",
            "Epoch 159/200, Batch 44/45, Loss: 2.5539371967315674\n",
            "Epoch 159/200, Batch 45/45, Loss: 2.425227403640747\n",
            "Validating and Checkpointing!\n",
            "Model is not good (might be overfitting)! Current val MSE:  35.342082023620605 Best Val MSE:  24.81772005558014\n",
            "Epoch:  160 , Time Elapsed:  52.47565228541692  mins\n",
            "Epoch 160/200, Batch 1/45, Loss: 2.2183308601379395\n",
            "Epoch 160/200, Batch 2/45, Loss: 1.9510151147842407\n",
            "Epoch 160/200, Batch 3/45, Loss: 2.786090612411499\n",
            "Epoch 160/200, Batch 4/45, Loss: 1.9961758852005005\n",
            "Epoch 160/200, Batch 5/45, Loss: 2.125216007232666\n",
            "Epoch 160/200, Batch 6/45, Loss: 1.9879519939422607\n",
            "Epoch 160/200, Batch 7/45, Loss: 2.76519775390625\n",
            "Epoch 160/200, Batch 8/45, Loss: 1.709911584854126\n",
            "Epoch 160/200, Batch 9/45, Loss: 2.5725491046905518\n",
            "Epoch 160/200, Batch 10/45, Loss: 2.5080466270446777\n",
            "Epoch 160/200, Batch 11/45, Loss: 2.094146251678467\n",
            "Epoch 160/200, Batch 12/45, Loss: 2.3770503997802734\n",
            "Epoch 160/200, Batch 13/45, Loss: 2.258390188217163\n",
            "Epoch 160/200, Batch 14/45, Loss: 3.1969945430755615\n",
            "Epoch 160/200, Batch 15/45, Loss: 2.175659656524658\n",
            "Epoch 160/200, Batch 16/45, Loss: 2.106908082962036\n",
            "Epoch 160/200, Batch 17/45, Loss: 2.369844913482666\n",
            "Epoch 160/200, Batch 18/45, Loss: 1.9533017873764038\n",
            "Epoch 160/200, Batch 19/45, Loss: 2.0029196739196777\n",
            "Epoch 160/200, Batch 20/45, Loss: 2.5845680236816406\n",
            "Epoch 160/200, Batch 21/45, Loss: 2.0248889923095703\n",
            "Epoch 160/200, Batch 22/45, Loss: 1.93831467628479\n",
            "Epoch 160/200, Batch 23/45, Loss: 2.6434314250946045\n",
            "Epoch 160/200, Batch 24/45, Loss: 1.9635510444641113\n",
            "Epoch 160/200, Batch 25/45, Loss: 1.5459842681884766\n",
            "Epoch 160/200, Batch 26/45, Loss: 2.4747202396392822\n",
            "Epoch 160/200, Batch 27/45, Loss: 1.9639358520507812\n",
            "Epoch 160/200, Batch 28/45, Loss: 2.3661346435546875\n",
            "Epoch 160/200, Batch 29/45, Loss: 2.4223036766052246\n",
            "Epoch 160/200, Batch 30/45, Loss: 2.3008227348327637\n",
            "Epoch 160/200, Batch 31/45, Loss: 2.498093605041504\n",
            "Epoch 160/200, Batch 32/45, Loss: 2.181631088256836\n",
            "Epoch 160/200, Batch 33/45, Loss: 2.162473201751709\n",
            "Epoch 160/200, Batch 34/45, Loss: 2.3309898376464844\n",
            "Epoch 160/200, Batch 35/45, Loss: 2.641623020172119\n",
            "Epoch 160/200, Batch 36/45, Loss: 2.0368382930755615\n",
            "Epoch 160/200, Batch 37/45, Loss: 2.2621326446533203\n",
            "Epoch 160/200, Batch 38/45, Loss: 2.5386264324188232\n",
            "Epoch 160/200, Batch 39/45, Loss: 1.989760160446167\n",
            "Epoch 160/200, Batch 40/45, Loss: 2.217759609222412\n",
            "Epoch 160/200, Batch 41/45, Loss: 1.9742536544799805\n",
            "Epoch 160/200, Batch 42/45, Loss: 2.4517974853515625\n",
            "Epoch 160/200, Batch 43/45, Loss: 2.217547655105591\n",
            "Epoch 160/200, Batch 44/45, Loss: 2.041940689086914\n",
            "Epoch 160/200, Batch 45/45, Loss: 2.4985427856445312\n",
            "Validating and Checkpointing!\n",
            "Model is not good (might be overfitting)! Current val MSE:  29.96011459827423 Best Val MSE:  24.81772005558014\n",
            "Epoch:  161 , Time Elapsed:  52.7935329635938  mins\n",
            "Epoch 161/200, Batch 1/45, Loss: 2.1984148025512695\n",
            "Epoch 161/200, Batch 2/45, Loss: 2.103583812713623\n",
            "Epoch 161/200, Batch 3/45, Loss: 2.523625135421753\n",
            "Epoch 161/200, Batch 4/45, Loss: 2.3777432441711426\n",
            "Epoch 161/200, Batch 5/45, Loss: 2.205889940261841\n",
            "Epoch 161/200, Batch 6/45, Loss: 2.2377736568450928\n",
            "Epoch 161/200, Batch 7/45, Loss: 1.7225010395050049\n",
            "Epoch 161/200, Batch 8/45, Loss: 2.5043091773986816\n",
            "Epoch 161/200, Batch 9/45, Loss: 1.7321979999542236\n",
            "Epoch 161/200, Batch 10/45, Loss: 1.9790315628051758\n",
            "Epoch 161/200, Batch 11/45, Loss: 2.348217487335205\n",
            "Epoch 161/200, Batch 12/45, Loss: 1.9019005298614502\n",
            "Epoch 161/200, Batch 13/45, Loss: 2.5108559131622314\n",
            "Epoch 161/200, Batch 14/45, Loss: 2.2387897968292236\n",
            "Epoch 161/200, Batch 15/45, Loss: 2.2103490829467773\n",
            "Epoch 161/200, Batch 16/45, Loss: 2.3995044231414795\n",
            "Epoch 161/200, Batch 17/45, Loss: 1.9195988178253174\n",
            "Epoch 161/200, Batch 18/45, Loss: 1.8402408361434937\n",
            "Epoch 161/200, Batch 19/45, Loss: 2.4790663719177246\n",
            "Epoch 161/200, Batch 20/45, Loss: 2.526925563812256\n",
            "Epoch 161/200, Batch 21/45, Loss: 1.9657206535339355\n",
            "Epoch 161/200, Batch 22/45, Loss: 2.1159822940826416\n",
            "Epoch 161/200, Batch 23/45, Loss: 4.529433250427246\n",
            "Epoch 161/200, Batch 24/45, Loss: 2.0738821029663086\n",
            "Epoch 161/200, Batch 25/45, Loss: 1.7796552181243896\n",
            "Epoch 161/200, Batch 26/45, Loss: 2.82948637008667\n",
            "Epoch 161/200, Batch 27/45, Loss: 1.198286771774292\n",
            "Epoch 161/200, Batch 28/45, Loss: 2.2701058387756348\n",
            "Epoch 161/200, Batch 29/45, Loss: 2.2303109169006348\n",
            "Epoch 161/200, Batch 30/45, Loss: 1.9012796878814697\n",
            "Epoch 161/200, Batch 31/45, Loss: 2.2357137203216553\n",
            "Epoch 161/200, Batch 32/45, Loss: 1.514020562171936\n",
            "Epoch 161/200, Batch 33/45, Loss: 2.1050634384155273\n",
            "Epoch 161/200, Batch 34/45, Loss: 2.099025011062622\n",
            "Epoch 161/200, Batch 35/45, Loss: 2.9709513187408447\n",
            "Epoch 161/200, Batch 36/45, Loss: 2.1948795318603516\n",
            "Epoch 161/200, Batch 37/45, Loss: 2.1205544471740723\n",
            "Epoch 161/200, Batch 38/45, Loss: 1.8473896980285645\n",
            "Epoch 161/200, Batch 39/45, Loss: 2.5035572052001953\n",
            "Epoch 161/200, Batch 40/45, Loss: 2.2292251586914062\n",
            "Epoch 161/200, Batch 41/45, Loss: 2.1335408687591553\n",
            "Epoch 161/200, Batch 42/45, Loss: 2.836245536804199\n",
            "Epoch 161/200, Batch 43/45, Loss: 1.508345365524292\n",
            "Epoch 161/200, Batch 44/45, Loss: 1.5375127792358398\n",
            "Epoch 161/200, Batch 45/45, Loss: 1.8642263412475586\n",
            "Validating and Checkpointing!\n",
            "Model is not good (might be overfitting)! Current val MSE:  25.612274408340454 Best Val MSE:  24.81772005558014\n",
            "Epoch:  162 , Time Elapsed:  53.139937126636504  mins\n",
            "Epoch 162/200, Batch 1/45, Loss: 1.9691611528396606\n",
            "Epoch 162/200, Batch 2/45, Loss: 1.7968618869781494\n",
            "Epoch 162/200, Batch 3/45, Loss: 2.191926956176758\n",
            "Epoch 162/200, Batch 4/45, Loss: 2.073883533477783\n",
            "Epoch 162/200, Batch 5/45, Loss: 2.1813011169433594\n",
            "Epoch 162/200, Batch 6/45, Loss: 2.1942434310913086\n",
            "Epoch 162/200, Batch 7/45, Loss: 2.309134006500244\n",
            "Epoch 162/200, Batch 8/45, Loss: 2.8377127647399902\n",
            "Epoch 162/200, Batch 9/45, Loss: 2.4798786640167236\n",
            "Epoch 162/200, Batch 10/45, Loss: 1.9416309595108032\n",
            "Epoch 162/200, Batch 11/45, Loss: 1.5775796175003052\n",
            "Epoch 162/200, Batch 12/45, Loss: 2.828799247741699\n",
            "Epoch 162/200, Batch 13/45, Loss: 2.2454745769500732\n",
            "Epoch 162/200, Batch 14/45, Loss: 1.7835841178894043\n",
            "Epoch 162/200, Batch 15/45, Loss: 2.322941780090332\n",
            "Epoch 162/200, Batch 16/45, Loss: 2.0430402755737305\n",
            "Epoch 162/200, Batch 17/45, Loss: 2.640742063522339\n",
            "Epoch 162/200, Batch 18/45, Loss: 2.9945359230041504\n",
            "Epoch 162/200, Batch 19/45, Loss: 1.978124737739563\n",
            "Epoch 162/200, Batch 20/45, Loss: 2.8348283767700195\n",
            "Epoch 162/200, Batch 21/45, Loss: 2.8539631366729736\n",
            "Epoch 162/200, Batch 22/45, Loss: 2.204514980316162\n",
            "Epoch 162/200, Batch 23/45, Loss: 1.9954415559768677\n",
            "Epoch 162/200, Batch 24/45, Loss: 2.3195462226867676\n",
            "Epoch 162/200, Batch 25/45, Loss: 2.045583486557007\n",
            "Epoch 162/200, Batch 26/45, Loss: 1.7421462535858154\n",
            "Epoch 162/200, Batch 27/45, Loss: 2.3959739208221436\n",
            "Epoch 162/200, Batch 28/45, Loss: 3.113173484802246\n",
            "Epoch 162/200, Batch 29/45, Loss: 1.7174253463745117\n",
            "Epoch 162/200, Batch 30/45, Loss: 2.107356071472168\n",
            "Epoch 162/200, Batch 31/45, Loss: 2.6174402236938477\n",
            "Epoch 162/200, Batch 32/45, Loss: 1.7918055057525635\n",
            "Epoch 162/200, Batch 33/45, Loss: 2.0452194213867188\n",
            "Epoch 162/200, Batch 34/45, Loss: 2.387638807296753\n",
            "Epoch 162/200, Batch 35/45, Loss: 2.039677619934082\n",
            "Epoch 162/200, Batch 36/45, Loss: 2.114358901977539\n",
            "Epoch 162/200, Batch 37/45, Loss: 2.3633131980895996\n",
            "Epoch 162/200, Batch 38/45, Loss: 3.0233354568481445\n",
            "Epoch 162/200, Batch 39/45, Loss: 2.372779369354248\n",
            "Epoch 162/200, Batch 40/45, Loss: 1.518143653869629\n",
            "Epoch 162/200, Batch 41/45, Loss: 2.577042579650879\n",
            "Epoch 162/200, Batch 42/45, Loss: 2.214006185531616\n",
            "Epoch 162/200, Batch 43/45, Loss: 1.7111197710037231\n",
            "Epoch 162/200, Batch 44/45, Loss: 2.3171186447143555\n",
            "Epoch 162/200, Batch 45/45, Loss: 2.154226303100586\n",
            "Validating and Checkpointing!\n",
            "Best model Saved! Val MSE:  24.743369102478027\n",
            "Epoch:  163 , Time Elapsed:  53.457323237260184  mins\n",
            "Epoch 163/200, Batch 1/45, Loss: 2.5502426624298096\n",
            "Epoch 163/200, Batch 2/45, Loss: 2.5131943225860596\n",
            "Epoch 163/200, Batch 3/45, Loss: 2.2155418395996094\n",
            "Epoch 163/200, Batch 4/45, Loss: 2.1853723526000977\n",
            "Epoch 163/200, Batch 5/45, Loss: 2.1279807090759277\n",
            "Epoch 163/200, Batch 6/45, Loss: 1.6227134466171265\n",
            "Epoch 163/200, Batch 7/45, Loss: 1.988877296447754\n",
            "Epoch 163/200, Batch 8/45, Loss: 1.5146799087524414\n",
            "Epoch 163/200, Batch 9/45, Loss: 1.825500726699829\n",
            "Epoch 163/200, Batch 10/45, Loss: 1.849963665008545\n",
            "Epoch 163/200, Batch 11/45, Loss: 2.483297824859619\n",
            "Epoch 163/200, Batch 12/45, Loss: 1.7756633758544922\n",
            "Epoch 163/200, Batch 13/45, Loss: 2.9947092533111572\n",
            "Epoch 163/200, Batch 14/45, Loss: 1.8746278285980225\n",
            "Epoch 163/200, Batch 15/45, Loss: 2.7188100814819336\n",
            "Epoch 163/200, Batch 16/45, Loss: 2.2136600017547607\n",
            "Epoch 163/200, Batch 17/45, Loss: 2.869443416595459\n",
            "Epoch 163/200, Batch 18/45, Loss: 1.954751968383789\n",
            "Epoch 163/200, Batch 19/45, Loss: 2.8283309936523438\n",
            "Epoch 163/200, Batch 20/45, Loss: 2.49836802482605\n",
            "Epoch 163/200, Batch 21/45, Loss: 1.6617684364318848\n",
            "Epoch 163/200, Batch 22/45, Loss: 2.326375961303711\n",
            "Epoch 163/200, Batch 23/45, Loss: 2.837183713912964\n",
            "Epoch 163/200, Batch 24/45, Loss: 2.7678020000457764\n",
            "Epoch 163/200, Batch 25/45, Loss: 2.2139511108398438\n",
            "Epoch 163/200, Batch 26/45, Loss: 1.8140980005264282\n",
            "Epoch 163/200, Batch 27/45, Loss: 2.154386043548584\n",
            "Epoch 163/200, Batch 28/45, Loss: 1.97304368019104\n",
            "Epoch 163/200, Batch 29/45, Loss: 2.512794017791748\n",
            "Epoch 163/200, Batch 30/45, Loss: 1.971149206161499\n",
            "Epoch 163/200, Batch 31/45, Loss: 2.3977551460266113\n",
            "Epoch 163/200, Batch 32/45, Loss: 2.587599039077759\n",
            "Epoch 163/200, Batch 33/45, Loss: 2.670417070388794\n",
            "Epoch 163/200, Batch 34/45, Loss: 1.7597455978393555\n",
            "Epoch 163/200, Batch 35/45, Loss: 2.034501075744629\n",
            "Epoch 163/200, Batch 36/45, Loss: 1.956104040145874\n",
            "Epoch 163/200, Batch 37/45, Loss: 2.1503350734710693\n",
            "Epoch 163/200, Batch 38/45, Loss: 2.441437244415283\n",
            "Epoch 163/200, Batch 39/45, Loss: 2.5218722820281982\n",
            "Epoch 163/200, Batch 40/45, Loss: 2.0629067420959473\n",
            "Epoch 163/200, Batch 41/45, Loss: 2.3881800174713135\n",
            "Epoch 163/200, Batch 42/45, Loss: 1.9098803997039795\n",
            "Epoch 163/200, Batch 43/45, Loss: 1.9966411590576172\n",
            "Epoch 163/200, Batch 44/45, Loss: 2.111774444580078\n",
            "Epoch 163/200, Batch 45/45, Loss: 2.457674026489258\n",
            "Validating and Checkpointing!\n",
            "Model is not good (might be overfitting)! Current val MSE:  29.236768007278442 Best Val MSE:  24.743369102478027\n",
            "Epoch:  164 , Time Elapsed:  53.775898933410645  mins\n",
            "Epoch 164/200, Batch 1/45, Loss: 1.8383469581604004\n",
            "Epoch 164/200, Batch 2/45, Loss: 2.566824436187744\n",
            "Epoch 164/200, Batch 3/45, Loss: 2.600774049758911\n",
            "Epoch 164/200, Batch 4/45, Loss: 2.233797550201416\n",
            "Epoch 164/200, Batch 5/45, Loss: 2.3891756534576416\n",
            "Epoch 164/200, Batch 6/45, Loss: 2.385201930999756\n",
            "Epoch 164/200, Batch 7/45, Loss: 1.8134071826934814\n",
            "Epoch 164/200, Batch 8/45, Loss: 2.419485092163086\n",
            "Epoch 164/200, Batch 9/45, Loss: 2.498483896255493\n",
            "Epoch 164/200, Batch 10/45, Loss: 2.185021162033081\n",
            "Epoch 164/200, Batch 11/45, Loss: 1.937706708908081\n",
            "Epoch 164/200, Batch 12/45, Loss: 2.0781168937683105\n",
            "Epoch 164/200, Batch 13/45, Loss: 1.8368644714355469\n",
            "Epoch 164/200, Batch 14/45, Loss: 1.2494490146636963\n",
            "Epoch 164/200, Batch 15/45, Loss: 1.8508901596069336\n",
            "Epoch 164/200, Batch 16/45, Loss: 1.923957347869873\n",
            "Epoch 164/200, Batch 17/45, Loss: 2.148364543914795\n",
            "Epoch 164/200, Batch 18/45, Loss: 1.980639934539795\n",
            "Epoch 164/200, Batch 19/45, Loss: 1.654419183731079\n",
            "Epoch 164/200, Batch 20/45, Loss: 2.104112148284912\n",
            "Epoch 164/200, Batch 21/45, Loss: 2.148550510406494\n",
            "Epoch 164/200, Batch 22/45, Loss: 2.974947690963745\n",
            "Epoch 164/200, Batch 23/45, Loss: 2.237827777862549\n",
            "Epoch 164/200, Batch 24/45, Loss: 2.287325382232666\n",
            "Epoch 164/200, Batch 25/45, Loss: 1.877136468887329\n",
            "Epoch 164/200, Batch 26/45, Loss: 2.4908244609832764\n",
            "Epoch 164/200, Batch 27/45, Loss: 2.4493799209594727\n",
            "Epoch 164/200, Batch 28/45, Loss: 2.2342143058776855\n",
            "Epoch 164/200, Batch 29/45, Loss: 1.7720218896865845\n",
            "Epoch 164/200, Batch 30/45, Loss: 2.5262417793273926\n",
            "Epoch 164/200, Batch 31/45, Loss: 2.292973041534424\n",
            "Epoch 164/200, Batch 32/45, Loss: 2.236562728881836\n",
            "Epoch 164/200, Batch 33/45, Loss: 2.132781982421875\n",
            "Epoch 164/200, Batch 34/45, Loss: 1.9209805727005005\n",
            "Epoch 164/200, Batch 35/45, Loss: 2.3423354625701904\n",
            "Epoch 164/200, Batch 36/45, Loss: 1.416978120803833\n",
            "Epoch 164/200, Batch 37/45, Loss: 2.187922239303589\n",
            "Epoch 164/200, Batch 38/45, Loss: 1.755820631980896\n",
            "Epoch 164/200, Batch 39/45, Loss: 1.699864149093628\n",
            "Epoch 164/200, Batch 40/45, Loss: 2.3057849407196045\n",
            "Epoch 164/200, Batch 41/45, Loss: 2.2067577838897705\n",
            "Epoch 164/200, Batch 42/45, Loss: 1.9485511779785156\n",
            "Epoch 164/200, Batch 43/45, Loss: 1.717022180557251\n",
            "Epoch 164/200, Batch 44/45, Loss: 2.0483789443969727\n",
            "Epoch 164/200, Batch 45/45, Loss: 2.4354605674743652\n",
            "Validating and Checkpointing!\n",
            "Model is not good (might be overfitting)! Current val MSE:  25.09050726890564 Best Val MSE:  24.743369102478027\n",
            "Epoch:  165 , Time Elapsed:  54.117060883839926  mins\n",
            "Epoch 165/200, Batch 1/45, Loss: 1.9288184642791748\n",
            "Epoch 165/200, Batch 2/45, Loss: 2.1059582233428955\n",
            "Epoch 165/200, Batch 3/45, Loss: 1.9911226034164429\n",
            "Epoch 165/200, Batch 4/45, Loss: 1.6840827465057373\n",
            "Epoch 165/200, Batch 5/45, Loss: 2.571909189224243\n",
            "Epoch 165/200, Batch 6/45, Loss: 1.8932241201400757\n",
            "Epoch 165/200, Batch 7/45, Loss: 2.0516417026519775\n",
            "Epoch 165/200, Batch 8/45, Loss: 2.0370450019836426\n",
            "Epoch 165/200, Batch 9/45, Loss: 1.5773484706878662\n",
            "Epoch 165/200, Batch 10/45, Loss: 2.1107065677642822\n",
            "Epoch 165/200, Batch 11/45, Loss: 2.4792263507843018\n",
            "Epoch 165/200, Batch 12/45, Loss: 2.673633575439453\n",
            "Epoch 165/200, Batch 13/45, Loss: 2.176079273223877\n",
            "Epoch 165/200, Batch 14/45, Loss: 2.329969882965088\n",
            "Epoch 165/200, Batch 15/45, Loss: 2.6667842864990234\n",
            "Epoch 165/200, Batch 16/45, Loss: 2.801374912261963\n",
            "Epoch 165/200, Batch 17/45, Loss: 1.7418538331985474\n",
            "Epoch 165/200, Batch 18/45, Loss: 1.9908804893493652\n",
            "Epoch 165/200, Batch 19/45, Loss: 2.083371877670288\n",
            "Epoch 165/200, Batch 20/45, Loss: 2.363868474960327\n",
            "Epoch 165/200, Batch 21/45, Loss: 1.9736765623092651\n",
            "Epoch 165/200, Batch 22/45, Loss: 1.87465238571167\n",
            "Epoch 165/200, Batch 23/45, Loss: 1.6060147285461426\n",
            "Epoch 165/200, Batch 24/45, Loss: 2.001131534576416\n",
            "Epoch 165/200, Batch 25/45, Loss: 2.052419662475586\n",
            "Epoch 165/200, Batch 26/45, Loss: 2.857442855834961\n",
            "Epoch 165/200, Batch 27/45, Loss: 2.229250192642212\n",
            "Epoch 165/200, Batch 28/45, Loss: 1.949887990951538\n",
            "Epoch 165/200, Batch 29/45, Loss: 1.776559829711914\n",
            "Epoch 165/200, Batch 30/45, Loss: 2.1711158752441406\n",
            "Epoch 165/200, Batch 31/45, Loss: 2.6024205684661865\n",
            "Epoch 165/200, Batch 32/45, Loss: 1.9927940368652344\n",
            "Epoch 165/200, Batch 33/45, Loss: 2.247926950454712\n",
            "Epoch 165/200, Batch 34/45, Loss: 2.568922996520996\n",
            "Epoch 165/200, Batch 35/45, Loss: 1.831845760345459\n",
            "Epoch 165/200, Batch 36/45, Loss: 1.5415527820587158\n",
            "Epoch 165/200, Batch 37/45, Loss: 2.7416114807128906\n",
            "Epoch 165/200, Batch 38/45, Loss: 1.5919216871261597\n",
            "Epoch 165/200, Batch 39/45, Loss: 1.4941362142562866\n",
            "Epoch 165/200, Batch 40/45, Loss: 1.171196460723877\n",
            "Epoch 165/200, Batch 41/45, Loss: 2.0017688274383545\n",
            "Epoch 165/200, Batch 42/45, Loss: 2.5792036056518555\n",
            "Epoch 165/200, Batch 43/45, Loss: 1.7734787464141846\n",
            "Epoch 165/200, Batch 44/45, Loss: 2.65175199508667\n",
            "Epoch 165/200, Batch 45/45, Loss: 2.9453787803649902\n",
            "Validating and Checkpointing!\n",
            "Model is not good (might be overfitting)! Current val MSE:  26.93475604057312 Best Val MSE:  24.743369102478027\n",
            "Epoch:  166 , Time Elapsed:  54.436776876449585  mins\n",
            "Epoch 166/200, Batch 1/45, Loss: 1.4792978763580322\n",
            "Epoch 166/200, Batch 2/45, Loss: 1.7144248485565186\n",
            "Epoch 166/200, Batch 3/45, Loss: 1.6875722408294678\n",
            "Epoch 166/200, Batch 4/45, Loss: 1.8674283027648926\n",
            "Epoch 166/200, Batch 5/45, Loss: 1.6032001972198486\n",
            "Epoch 166/200, Batch 6/45, Loss: 2.128265857696533\n",
            "Epoch 166/200, Batch 7/45, Loss: 2.590130567550659\n",
            "Epoch 166/200, Batch 8/45, Loss: 1.8847553730010986\n",
            "Epoch 166/200, Batch 9/45, Loss: 1.7546671628952026\n",
            "Epoch 166/200, Batch 10/45, Loss: 2.2463812828063965\n",
            "Epoch 166/200, Batch 11/45, Loss: 1.6959705352783203\n",
            "Epoch 166/200, Batch 12/45, Loss: 2.365265369415283\n",
            "Epoch 166/200, Batch 13/45, Loss: 1.868504524230957\n",
            "Epoch 166/200, Batch 14/45, Loss: 2.686513900756836\n",
            "Epoch 166/200, Batch 15/45, Loss: 1.96040678024292\n",
            "Epoch 166/200, Batch 16/45, Loss: 2.547178268432617\n",
            "Epoch 166/200, Batch 17/45, Loss: 1.6943268775939941\n",
            "Epoch 166/200, Batch 18/45, Loss: 2.953709602355957\n",
            "Epoch 166/200, Batch 19/45, Loss: 2.086958885192871\n",
            "Epoch 166/200, Batch 20/45, Loss: 1.748242974281311\n",
            "Epoch 166/200, Batch 21/45, Loss: 1.9095852375030518\n",
            "Epoch 166/200, Batch 22/45, Loss: 2.4524998664855957\n",
            "Epoch 166/200, Batch 23/45, Loss: 2.223507881164551\n",
            "Epoch 166/200, Batch 24/45, Loss: 1.9692633152008057\n",
            "Epoch 166/200, Batch 25/45, Loss: 2.0372369289398193\n",
            "Epoch 166/200, Batch 26/45, Loss: 1.8663363456726074\n",
            "Epoch 166/200, Batch 27/45, Loss: 2.1826367378234863\n",
            "Epoch 166/200, Batch 28/45, Loss: 2.124572277069092\n",
            "Epoch 166/200, Batch 29/45, Loss: 2.7470579147338867\n",
            "Epoch 166/200, Batch 30/45, Loss: 1.8854666948318481\n",
            "Epoch 166/200, Batch 31/45, Loss: 1.781556248664856\n",
            "Epoch 166/200, Batch 32/45, Loss: 2.2876648902893066\n",
            "Epoch 166/200, Batch 33/45, Loss: 3.146319627761841\n",
            "Epoch 166/200, Batch 34/45, Loss: 2.202853202819824\n",
            "Epoch 166/200, Batch 35/45, Loss: 1.526252269744873\n",
            "Epoch 166/200, Batch 36/45, Loss: 2.028730869293213\n",
            "Epoch 166/200, Batch 37/45, Loss: 2.4195351600646973\n",
            "Epoch 166/200, Batch 38/45, Loss: 2.074507474899292\n",
            "Epoch 166/200, Batch 39/45, Loss: 1.6165248155593872\n",
            "Epoch 166/200, Batch 40/45, Loss: 1.91762113571167\n",
            "Epoch 166/200, Batch 41/45, Loss: 1.7962822914123535\n",
            "Epoch 166/200, Batch 42/45, Loss: 1.7838048934936523\n",
            "Epoch 166/200, Batch 43/45, Loss: 2.5227510929107666\n",
            "Epoch 166/200, Batch 44/45, Loss: 1.5912410020828247\n",
            "Epoch 166/200, Batch 45/45, Loss: 2.03551983833313\n",
            "Validating and Checkpointing!\n",
            "Model is not good (might be overfitting)! Current val MSE:  27.751662731170654 Best Val MSE:  24.743369102478027\n",
            "Epoch:  167 , Time Elapsed:  54.76347802082697  mins\n",
            "Epoch 167/200, Batch 1/45, Loss: 2.1824653148651123\n",
            "Epoch 167/200, Batch 2/45, Loss: 2.5609755516052246\n",
            "Epoch 167/200, Batch 3/45, Loss: 1.6868751049041748\n",
            "Epoch 167/200, Batch 4/45, Loss: 2.046980381011963\n",
            "Epoch 167/200, Batch 5/45, Loss: 1.957822322845459\n",
            "Epoch 167/200, Batch 6/45, Loss: 1.8144137859344482\n",
            "Epoch 167/200, Batch 7/45, Loss: 2.1867294311523438\n",
            "Epoch 167/200, Batch 8/45, Loss: 2.0815720558166504\n",
            "Epoch 167/200, Batch 9/45, Loss: 1.2881277799606323\n",
            "Epoch 167/200, Batch 10/45, Loss: 2.3377685546875\n",
            "Epoch 167/200, Batch 11/45, Loss: 2.283123016357422\n",
            "Epoch 167/200, Batch 12/45, Loss: 2.5774455070495605\n",
            "Epoch 167/200, Batch 13/45, Loss: 2.0755438804626465\n",
            "Epoch 167/200, Batch 14/45, Loss: 2.9566171169281006\n",
            "Epoch 167/200, Batch 15/45, Loss: 1.7753186225891113\n",
            "Epoch 167/200, Batch 16/45, Loss: 2.2432234287261963\n",
            "Epoch 167/200, Batch 17/45, Loss: 1.7105944156646729\n",
            "Epoch 167/200, Batch 18/45, Loss: 1.9638322591781616\n",
            "Epoch 167/200, Batch 19/45, Loss: 2.136960983276367\n",
            "Epoch 167/200, Batch 20/45, Loss: 2.7868688106536865\n",
            "Epoch 167/200, Batch 21/45, Loss: 2.2558469772338867\n",
            "Epoch 167/200, Batch 22/45, Loss: 2.4048714637756348\n",
            "Epoch 167/200, Batch 23/45, Loss: 2.67454195022583\n",
            "Epoch 167/200, Batch 24/45, Loss: 2.5893588066101074\n",
            "Epoch 167/200, Batch 25/45, Loss: 2.3326754570007324\n",
            "Epoch 167/200, Batch 26/45, Loss: 1.0965570211410522\n",
            "Epoch 167/200, Batch 27/45, Loss: 1.8914271593093872\n",
            "Epoch 167/200, Batch 28/45, Loss: 4.132094860076904\n",
            "Epoch 167/200, Batch 29/45, Loss: 2.4290313720703125\n",
            "Epoch 167/200, Batch 30/45, Loss: 2.200336456298828\n",
            "Epoch 167/200, Batch 31/45, Loss: 2.003983497619629\n",
            "Epoch 167/200, Batch 32/45, Loss: 2.0114073753356934\n",
            "Epoch 167/200, Batch 33/45, Loss: 2.9251699447631836\n",
            "Epoch 167/200, Batch 34/45, Loss: 2.5457401275634766\n",
            "Epoch 167/200, Batch 35/45, Loss: 1.9036445617675781\n",
            "Epoch 167/200, Batch 36/45, Loss: 2.734581470489502\n",
            "Epoch 167/200, Batch 37/45, Loss: 3.194120407104492\n",
            "Epoch 167/200, Batch 38/45, Loss: 2.455930233001709\n",
            "Epoch 167/200, Batch 39/45, Loss: 2.2644238471984863\n",
            "Epoch 167/200, Batch 40/45, Loss: 2.1697781085968018\n",
            "Epoch 167/200, Batch 41/45, Loss: 2.0487945079803467\n",
            "Epoch 167/200, Batch 42/45, Loss: 2.6202921867370605\n",
            "Epoch 167/200, Batch 43/45, Loss: 2.017291784286499\n",
            "Epoch 167/200, Batch 44/45, Loss: 2.1526498794555664\n",
            "Epoch 167/200, Batch 45/45, Loss: 2.2593140602111816\n",
            "Validating and Checkpointing!\n",
            "Model is not good (might be overfitting)! Current val MSE:  25.257537245750427 Best Val MSE:  24.743369102478027\n",
            "Epoch:  168 , Time Elapsed:  55.08326659202576  mins\n",
            "Epoch 168/200, Batch 1/45, Loss: 2.1531453132629395\n",
            "Epoch 168/200, Batch 2/45, Loss: 2.6018717288970947\n",
            "Epoch 168/200, Batch 3/45, Loss: 2.247938632965088\n",
            "Epoch 168/200, Batch 4/45, Loss: 2.0306003093719482\n",
            "Epoch 168/200, Batch 5/45, Loss: 1.796641230583191\n",
            "Epoch 168/200, Batch 6/45, Loss: 1.5711979866027832\n",
            "Epoch 168/200, Batch 7/45, Loss: 1.6190241575241089\n",
            "Epoch 168/200, Batch 8/45, Loss: 2.4246931076049805\n",
            "Epoch 168/200, Batch 9/45, Loss: 1.6848558187484741\n",
            "Epoch 168/200, Batch 10/45, Loss: 2.1063179969787598\n",
            "Epoch 168/200, Batch 11/45, Loss: 2.431121349334717\n",
            "Epoch 168/200, Batch 12/45, Loss: 2.63232421875\n",
            "Epoch 168/200, Batch 13/45, Loss: 1.8207262754440308\n",
            "Epoch 168/200, Batch 14/45, Loss: 2.097958564758301\n",
            "Epoch 168/200, Batch 15/45, Loss: 2.1050925254821777\n",
            "Epoch 168/200, Batch 16/45, Loss: 2.1492059230804443\n",
            "Epoch 168/200, Batch 17/45, Loss: 1.8080922365188599\n",
            "Epoch 168/200, Batch 18/45, Loss: 1.6274511814117432\n",
            "Epoch 168/200, Batch 19/45, Loss: 2.0606164932250977\n",
            "Epoch 168/200, Batch 20/45, Loss: 2.12581729888916\n",
            "Epoch 168/200, Batch 21/45, Loss: 2.4418485164642334\n",
            "Epoch 168/200, Batch 22/45, Loss: 2.1356194019317627\n",
            "Epoch 168/200, Batch 23/45, Loss: 2.2414751052856445\n",
            "Epoch 168/200, Batch 24/45, Loss: 2.253516674041748\n",
            "Epoch 168/200, Batch 25/45, Loss: 2.8169360160827637\n",
            "Epoch 168/200, Batch 26/45, Loss: 2.205447196960449\n",
            "Epoch 168/200, Batch 27/45, Loss: 2.235199451446533\n",
            "Epoch 168/200, Batch 28/45, Loss: 2.58695912361145\n",
            "Epoch 168/200, Batch 29/45, Loss: 1.891943097114563\n",
            "Epoch 168/200, Batch 30/45, Loss: 2.378370761871338\n",
            "Epoch 168/200, Batch 31/45, Loss: 2.240252733230591\n",
            "Epoch 168/200, Batch 32/45, Loss: 2.4281651973724365\n",
            "Epoch 168/200, Batch 33/45, Loss: 2.0701699256896973\n",
            "Epoch 168/200, Batch 34/45, Loss: 1.9528887271881104\n",
            "Epoch 168/200, Batch 35/45, Loss: 2.580568790435791\n",
            "Epoch 168/200, Batch 36/45, Loss: 2.1603269577026367\n",
            "Epoch 168/200, Batch 37/45, Loss: 2.6834447383880615\n",
            "Epoch 168/200, Batch 38/45, Loss: 2.1656155586242676\n",
            "Epoch 168/200, Batch 39/45, Loss: 2.5958070755004883\n",
            "Epoch 168/200, Batch 40/45, Loss: 1.8060098886489868\n",
            "Epoch 168/200, Batch 41/45, Loss: 2.523502826690674\n",
            "Epoch 168/200, Batch 42/45, Loss: 2.2465198040008545\n",
            "Epoch 168/200, Batch 43/45, Loss: 1.923376441001892\n",
            "Epoch 168/200, Batch 44/45, Loss: 1.688103437423706\n",
            "Epoch 168/200, Batch 45/45, Loss: 2.1198294162750244\n",
            "Validating and Checkpointing!\n",
            "Model is not good (might be overfitting)! Current val MSE:  25.60209608078003 Best Val MSE:  24.743369102478027\n",
            "Epoch:  169 , Time Elapsed:  55.39940801858902  mins\n",
            "Epoch 169/200, Batch 1/45, Loss: 1.9933276176452637\n",
            "Epoch 169/200, Batch 2/45, Loss: 2.407684564590454\n",
            "Epoch 169/200, Batch 3/45, Loss: 2.1373274326324463\n",
            "Epoch 169/200, Batch 4/45, Loss: 2.105059862136841\n",
            "Epoch 169/200, Batch 5/45, Loss: 2.1945817470550537\n",
            "Epoch 169/200, Batch 6/45, Loss: 2.080380439758301\n",
            "Epoch 169/200, Batch 7/45, Loss: 2.428138017654419\n",
            "Epoch 169/200, Batch 8/45, Loss: 2.285006523132324\n",
            "Epoch 169/200, Batch 9/45, Loss: 2.0514800548553467\n",
            "Epoch 169/200, Batch 10/45, Loss: 1.1108266115188599\n",
            "Epoch 169/200, Batch 11/45, Loss: 2.471660852432251\n",
            "Epoch 169/200, Batch 12/45, Loss: 1.760983943939209\n",
            "Epoch 169/200, Batch 13/45, Loss: 2.4099392890930176\n",
            "Epoch 169/200, Batch 14/45, Loss: 2.478107452392578\n",
            "Epoch 169/200, Batch 15/45, Loss: 2.0855586528778076\n",
            "Epoch 169/200, Batch 16/45, Loss: 1.5843888521194458\n",
            "Epoch 169/200, Batch 17/45, Loss: 1.9605960845947266\n",
            "Epoch 169/200, Batch 18/45, Loss: 2.293914318084717\n",
            "Epoch 169/200, Batch 19/45, Loss: 2.440605640411377\n",
            "Epoch 169/200, Batch 20/45, Loss: 1.741309404373169\n",
            "Epoch 169/200, Batch 21/45, Loss: 2.470491647720337\n",
            "Epoch 169/200, Batch 22/45, Loss: 2.4655022621154785\n",
            "Epoch 169/200, Batch 23/45, Loss: 2.1348347663879395\n",
            "Epoch 169/200, Batch 24/45, Loss: 2.116163730621338\n",
            "Epoch 169/200, Batch 25/45, Loss: 1.7870848178863525\n",
            "Epoch 169/200, Batch 26/45, Loss: 2.6042003631591797\n",
            "Epoch 169/200, Batch 27/45, Loss: 2.62332820892334\n",
            "Epoch 169/200, Batch 28/45, Loss: 1.3129889965057373\n",
            "Epoch 169/200, Batch 29/45, Loss: 2.894071102142334\n",
            "Epoch 169/200, Batch 30/45, Loss: 2.716360330581665\n",
            "Epoch 169/200, Batch 31/45, Loss: 1.7628529071807861\n",
            "Epoch 169/200, Batch 32/45, Loss: 1.821942687034607\n",
            "Epoch 169/200, Batch 33/45, Loss: 2.358600378036499\n",
            "Epoch 169/200, Batch 34/45, Loss: 2.3178248405456543\n",
            "Epoch 169/200, Batch 35/45, Loss: 2.1169960498809814\n",
            "Epoch 169/200, Batch 36/45, Loss: 2.1188995838165283\n",
            "Epoch 169/200, Batch 37/45, Loss: 1.6104400157928467\n",
            "Epoch 169/200, Batch 38/45, Loss: 3.144986152648926\n",
            "Epoch 169/200, Batch 39/45, Loss: 1.7417640686035156\n",
            "Epoch 169/200, Batch 40/45, Loss: 1.5889854431152344\n",
            "Epoch 169/200, Batch 41/45, Loss: 1.741981029510498\n",
            "Epoch 169/200, Batch 42/45, Loss: 4.829487323760986\n",
            "Epoch 169/200, Batch 43/45, Loss: 1.9936134815216064\n",
            "Epoch 169/200, Batch 44/45, Loss: 1.5964865684509277\n",
            "Epoch 169/200, Batch 45/45, Loss: 2.4123692512512207\n",
            "Validating and Checkpointing!\n",
            "Model is not good (might be overfitting)! Current val MSE:  39.6675066947937 Best Val MSE:  24.743369102478027\n",
            "Epoch:  170 , Time Elapsed:  55.72943073113759  mins\n",
            "Epoch 170/200, Batch 1/45, Loss: 2.3285117149353027\n",
            "Epoch 170/200, Batch 2/45, Loss: 2.8260154724121094\n",
            "Epoch 170/200, Batch 3/45, Loss: 1.739661693572998\n",
            "Epoch 170/200, Batch 4/45, Loss: 1.909181833267212\n",
            "Epoch 170/200, Batch 5/45, Loss: 2.3124709129333496\n",
            "Epoch 170/200, Batch 6/45, Loss: 1.4664186239242554\n",
            "Epoch 170/200, Batch 7/45, Loss: 2.573737621307373\n",
            "Epoch 170/200, Batch 8/45, Loss: 2.2398552894592285\n",
            "Epoch 170/200, Batch 9/45, Loss: 2.444645643234253\n",
            "Epoch 170/200, Batch 10/45, Loss: 2.343977928161621\n",
            "Epoch 170/200, Batch 11/45, Loss: 2.6219382286071777\n",
            "Epoch 170/200, Batch 12/45, Loss: 1.910751461982727\n",
            "Epoch 170/200, Batch 13/45, Loss: 1.6568844318389893\n",
            "Epoch 170/200, Batch 14/45, Loss: 2.1523900032043457\n",
            "Epoch 170/200, Batch 15/45, Loss: 2.6360318660736084\n",
            "Epoch 170/200, Batch 16/45, Loss: 2.561460018157959\n",
            "Epoch 170/200, Batch 17/45, Loss: 2.55188250541687\n",
            "Epoch 170/200, Batch 18/45, Loss: 2.0518746376037598\n",
            "Epoch 170/200, Batch 19/45, Loss: 1.9416813850402832\n",
            "Epoch 170/200, Batch 20/45, Loss: 1.981350064277649\n",
            "Epoch 170/200, Batch 21/45, Loss: 2.4561893939971924\n",
            "Epoch 170/200, Batch 22/45, Loss: 2.5158917903900146\n",
            "Epoch 170/200, Batch 23/45, Loss: 2.2123775482177734\n",
            "Epoch 170/200, Batch 24/45, Loss: 5.716996669769287\n",
            "Epoch 170/200, Batch 25/45, Loss: 1.5980119705200195\n",
            "Epoch 170/200, Batch 26/45, Loss: 1.7255723476409912\n",
            "Epoch 170/200, Batch 27/45, Loss: 2.6264524459838867\n",
            "Epoch 170/200, Batch 28/45, Loss: 2.5091466903686523\n",
            "Epoch 170/200, Batch 29/45, Loss: 1.3685693740844727\n",
            "Epoch 170/200, Batch 30/45, Loss: 1.7479197978973389\n",
            "Epoch 170/200, Batch 31/45, Loss: 2.0981380939483643\n",
            "Epoch 170/200, Batch 32/45, Loss: 2.212336301803589\n",
            "Epoch 170/200, Batch 33/45, Loss: 2.6274821758270264\n",
            "Epoch 170/200, Batch 34/45, Loss: 2.4375059604644775\n",
            "Epoch 170/200, Batch 35/45, Loss: 2.6615395545959473\n",
            "Epoch 170/200, Batch 36/45, Loss: 2.283013105392456\n",
            "Epoch 170/200, Batch 37/45, Loss: 1.9194107055664062\n",
            "Epoch 170/200, Batch 38/45, Loss: 2.5969693660736084\n",
            "Epoch 170/200, Batch 39/45, Loss: 2.388568639755249\n",
            "Epoch 170/200, Batch 40/45, Loss: 1.9039700031280518\n",
            "Epoch 170/200, Batch 41/45, Loss: 2.368997573852539\n",
            "Epoch 170/200, Batch 42/45, Loss: 2.3183846473693848\n",
            "Epoch 170/200, Batch 43/45, Loss: 1.8146963119506836\n",
            "Epoch 170/200, Batch 44/45, Loss: 1.8085200786590576\n",
            "Epoch 170/200, Batch 45/45, Loss: 2.2907562255859375\n",
            "Validating and Checkpointing!\n",
            "Model is not good (might be overfitting)! Current val MSE:  26.143640995025635 Best Val MSE:  24.743369102478027\n",
            "Epoch:  171 , Time Elapsed:  56.047680294513704  mins\n",
            "Epoch 171/200, Batch 1/45, Loss: 1.5264103412628174\n",
            "Epoch 171/200, Batch 2/45, Loss: 2.321774959564209\n",
            "Epoch 171/200, Batch 3/45, Loss: 1.765209674835205\n",
            "Epoch 171/200, Batch 4/45, Loss: 1.609689474105835\n",
            "Epoch 171/200, Batch 5/45, Loss: 2.668455123901367\n",
            "Epoch 171/200, Batch 6/45, Loss: 2.348506450653076\n",
            "Epoch 171/200, Batch 7/45, Loss: 2.4184136390686035\n",
            "Epoch 171/200, Batch 8/45, Loss: 2.3262779712677\n",
            "Epoch 171/200, Batch 9/45, Loss: 1.7798411846160889\n",
            "Epoch 171/200, Batch 10/45, Loss: 1.7807161808013916\n",
            "Epoch 171/200, Batch 11/45, Loss: 1.81648850440979\n",
            "Epoch 171/200, Batch 12/45, Loss: 2.258523941040039\n",
            "Epoch 171/200, Batch 13/45, Loss: 2.396914482116699\n",
            "Epoch 171/200, Batch 14/45, Loss: 1.8776748180389404\n",
            "Epoch 171/200, Batch 15/45, Loss: 2.1610469818115234\n",
            "Epoch 171/200, Batch 16/45, Loss: 2.252655267715454\n",
            "Epoch 171/200, Batch 17/45, Loss: 2.1414144039154053\n",
            "Epoch 171/200, Batch 18/45, Loss: 2.7690820693969727\n",
            "Epoch 171/200, Batch 19/45, Loss: 1.4514952898025513\n",
            "Epoch 171/200, Batch 20/45, Loss: 2.3218414783477783\n",
            "Epoch 171/200, Batch 21/45, Loss: 1.9107816219329834\n",
            "Epoch 171/200, Batch 22/45, Loss: 2.4984235763549805\n",
            "Epoch 171/200, Batch 23/45, Loss: 1.9182310104370117\n",
            "Epoch 171/200, Batch 24/45, Loss: 2.7103681564331055\n",
            "Epoch 171/200, Batch 25/45, Loss: 1.569193720817566\n",
            "Epoch 171/200, Batch 26/45, Loss: 1.9108870029449463\n",
            "Epoch 171/200, Batch 27/45, Loss: 2.735107898712158\n",
            "Epoch 171/200, Batch 28/45, Loss: 1.7696197032928467\n",
            "Epoch 171/200, Batch 29/45, Loss: 1.885376214981079\n",
            "Epoch 171/200, Batch 30/45, Loss: 2.9321322441101074\n",
            "Epoch 171/200, Batch 31/45, Loss: 1.9118757247924805\n",
            "Epoch 171/200, Batch 32/45, Loss: 2.591369152069092\n",
            "Epoch 171/200, Batch 33/45, Loss: 2.185391902923584\n",
            "Epoch 171/200, Batch 34/45, Loss: 1.9339882135391235\n",
            "Epoch 171/200, Batch 35/45, Loss: 1.8061747550964355\n",
            "Epoch 171/200, Batch 36/45, Loss: 2.2096033096313477\n",
            "Epoch 171/200, Batch 37/45, Loss: 2.2441394329071045\n",
            "Epoch 171/200, Batch 38/45, Loss: 2.4591012001037598\n",
            "Epoch 171/200, Batch 39/45, Loss: 2.292004346847534\n",
            "Epoch 171/200, Batch 40/45, Loss: 1.4645793437957764\n",
            "Epoch 171/200, Batch 41/45, Loss: 2.6884443759918213\n",
            "Epoch 171/200, Batch 42/45, Loss: 1.617055058479309\n",
            "Epoch 171/200, Batch 43/45, Loss: 2.488107442855835\n",
            "Epoch 171/200, Batch 44/45, Loss: 1.8978217840194702\n",
            "Epoch 171/200, Batch 45/45, Loss: 2.1970672607421875\n",
            "Validating and Checkpointing!\n",
            "Model is not good (might be overfitting)! Current val MSE:  68.46864628791809 Best Val MSE:  24.743369102478027\n",
            "Epoch:  172 , Time Elapsed:  56.36521209081014  mins\n",
            "Epoch 172/200, Batch 1/45, Loss: 1.8882372379302979\n",
            "Epoch 172/200, Batch 2/45, Loss: 1.5890837907791138\n",
            "Epoch 172/200, Batch 3/45, Loss: 2.332667827606201\n",
            "Epoch 172/200, Batch 4/45, Loss: 2.3697519302368164\n",
            "Epoch 172/200, Batch 5/45, Loss: 1.9726097583770752\n",
            "Epoch 172/200, Batch 6/45, Loss: 2.103511333465576\n",
            "Epoch 172/200, Batch 7/45, Loss: 2.359048843383789\n",
            "Epoch 172/200, Batch 8/45, Loss: 1.5161948204040527\n",
            "Epoch 172/200, Batch 9/45, Loss: 2.3313229084014893\n",
            "Epoch 172/200, Batch 10/45, Loss: 4.185605049133301\n",
            "Epoch 172/200, Batch 11/45, Loss: 2.4747068881988525\n",
            "Epoch 172/200, Batch 12/45, Loss: 1.570387363433838\n",
            "Epoch 172/200, Batch 13/45, Loss: 1.8916997909545898\n",
            "Epoch 172/200, Batch 14/45, Loss: 2.3627185821533203\n",
            "Epoch 172/200, Batch 15/45, Loss: 2.1039841175079346\n",
            "Epoch 172/200, Batch 16/45, Loss: 2.458651065826416\n",
            "Epoch 172/200, Batch 17/45, Loss: 1.5250132083892822\n",
            "Epoch 172/200, Batch 18/45, Loss: 1.7387880086898804\n",
            "Epoch 172/200, Batch 19/45, Loss: 2.297454833984375\n",
            "Epoch 172/200, Batch 20/45, Loss: 1.9752277135849\n",
            "Epoch 172/200, Batch 21/45, Loss: 2.0629615783691406\n",
            "Epoch 172/200, Batch 22/45, Loss: 2.6005430221557617\n",
            "Epoch 172/200, Batch 23/45, Loss: 2.2068710327148438\n",
            "Epoch 172/200, Batch 24/45, Loss: 2.03132700920105\n",
            "Epoch 172/200, Batch 25/45, Loss: 2.813716173171997\n",
            "Epoch 172/200, Batch 26/45, Loss: 2.4866607189178467\n",
            "Epoch 172/200, Batch 27/45, Loss: 2.5474727153778076\n",
            "Epoch 172/200, Batch 28/45, Loss: 2.3516337871551514\n",
            "Epoch 172/200, Batch 29/45, Loss: 1.767938494682312\n",
            "Epoch 172/200, Batch 30/45, Loss: 2.1315815448760986\n",
            "Epoch 172/200, Batch 31/45, Loss: 2.589179039001465\n",
            "Epoch 172/200, Batch 32/45, Loss: 1.4842894077301025\n",
            "Epoch 172/200, Batch 33/45, Loss: 1.9389197826385498\n",
            "Epoch 172/200, Batch 34/45, Loss: 2.3995306491851807\n",
            "Epoch 172/200, Batch 35/45, Loss: 1.5040427446365356\n",
            "Epoch 172/200, Batch 36/45, Loss: 2.277186393737793\n",
            "Epoch 172/200, Batch 37/45, Loss: 2.6875925064086914\n",
            "Epoch 172/200, Batch 38/45, Loss: 2.201552152633667\n",
            "Epoch 172/200, Batch 39/45, Loss: 1.706092119216919\n",
            "Epoch 172/200, Batch 40/45, Loss: 2.2155864238739014\n",
            "Epoch 172/200, Batch 41/45, Loss: 2.1326425075531006\n",
            "Epoch 172/200, Batch 42/45, Loss: 2.3858704566955566\n",
            "Epoch 172/200, Batch 43/45, Loss: 2.6340837478637695\n",
            "Epoch 172/200, Batch 44/45, Loss: 2.0428965091705322\n",
            "Epoch 172/200, Batch 45/45, Loss: 1.6635613441467285\n",
            "Validating and Checkpointing!\n",
            "Model is not good (might be overfitting)! Current val MSE:  26.411763072013855 Best Val MSE:  24.743369102478027\n",
            "Epoch:  173 , Time Elapsed:  56.70569671789805  mins\n",
            "Epoch 173/200, Batch 1/45, Loss: 1.2216665744781494\n",
            "Epoch 173/200, Batch 2/45, Loss: 2.130660057067871\n",
            "Epoch 173/200, Batch 3/45, Loss: 1.8439326286315918\n",
            "Epoch 173/200, Batch 4/45, Loss: 2.2750401496887207\n",
            "Epoch 173/200, Batch 5/45, Loss: 1.834688425064087\n",
            "Epoch 173/200, Batch 6/45, Loss: 2.962045431137085\n",
            "Epoch 173/200, Batch 7/45, Loss: 2.444622278213501\n",
            "Epoch 173/200, Batch 8/45, Loss: 1.9136360883712769\n",
            "Epoch 173/200, Batch 9/45, Loss: 2.3105552196502686\n",
            "Epoch 173/200, Batch 10/45, Loss: 2.076075792312622\n",
            "Epoch 173/200, Batch 11/45, Loss: 2.4488301277160645\n",
            "Epoch 173/200, Batch 12/45, Loss: 3.2093024253845215\n",
            "Epoch 173/200, Batch 13/45, Loss: 2.140712261199951\n",
            "Epoch 173/200, Batch 14/45, Loss: 2.9050521850585938\n",
            "Epoch 173/200, Batch 15/45, Loss: 1.9626646041870117\n",
            "Epoch 173/200, Batch 16/45, Loss: 2.0051674842834473\n",
            "Epoch 173/200, Batch 17/45, Loss: 2.753263473510742\n",
            "Epoch 173/200, Batch 18/45, Loss: 2.0934712886810303\n",
            "Epoch 173/200, Batch 19/45, Loss: 2.112884759902954\n",
            "Epoch 173/200, Batch 20/45, Loss: 2.1195180416107178\n",
            "Epoch 173/200, Batch 21/45, Loss: 1.6930251121520996\n",
            "Epoch 173/200, Batch 22/45, Loss: 1.7429052591323853\n",
            "Epoch 173/200, Batch 23/45, Loss: 2.0152206420898438\n",
            "Epoch 173/200, Batch 24/45, Loss: 2.064425468444824\n",
            "Epoch 173/200, Batch 25/45, Loss: 1.5531411170959473\n",
            "Epoch 173/200, Batch 26/45, Loss: 1.762486219406128\n",
            "Epoch 173/200, Batch 27/45, Loss: 2.279512882232666\n",
            "Epoch 173/200, Batch 28/45, Loss: 1.816901683807373\n",
            "Epoch 173/200, Batch 29/45, Loss: 1.5299756526947021\n",
            "Epoch 173/200, Batch 30/45, Loss: 2.500302791595459\n",
            "Epoch 173/200, Batch 31/45, Loss: 4.755700588226318\n",
            "Epoch 173/200, Batch 32/45, Loss: 2.3082921504974365\n",
            "Epoch 173/200, Batch 33/45, Loss: 2.2126195430755615\n",
            "Epoch 173/200, Batch 34/45, Loss: 1.8678090572357178\n",
            "Epoch 173/200, Batch 35/45, Loss: 2.7663707733154297\n",
            "Epoch 173/200, Batch 36/45, Loss: 1.8035223484039307\n",
            "Epoch 173/200, Batch 37/45, Loss: 2.02962589263916\n",
            "Epoch 173/200, Batch 38/45, Loss: 2.417895555496216\n",
            "Epoch 173/200, Batch 39/45, Loss: 1.9197123050689697\n",
            "Epoch 173/200, Batch 40/45, Loss: 1.6218830347061157\n",
            "Epoch 173/200, Batch 41/45, Loss: 2.2357850074768066\n",
            "Epoch 173/200, Batch 42/45, Loss: 2.279935121536255\n",
            "Epoch 173/200, Batch 43/45, Loss: 2.0611743927001953\n",
            "Epoch 173/200, Batch 44/45, Loss: 2.3001155853271484\n",
            "Epoch 173/200, Batch 45/45, Loss: 1.6554522514343262\n",
            "Validating and Checkpointing!\n",
            "Model is not good (might be overfitting)! Current val MSE:  25.76283609867096 Best Val MSE:  24.743369102478027\n",
            "Epoch:  174 , Time Elapsed:  57.027434865633644  mins\n",
            "Epoch 174/200, Batch 1/45, Loss: 2.2345263957977295\n",
            "Epoch 174/200, Batch 2/45, Loss: 2.312154769897461\n",
            "Epoch 174/200, Batch 3/45, Loss: 2.0738611221313477\n",
            "Epoch 174/200, Batch 4/45, Loss: 1.87599515914917\n",
            "Epoch 174/200, Batch 5/45, Loss: 1.9600210189819336\n",
            "Epoch 174/200, Batch 6/45, Loss: 2.264725685119629\n",
            "Epoch 174/200, Batch 7/45, Loss: 2.1698081493377686\n",
            "Epoch 174/200, Batch 8/45, Loss: 2.1166179180145264\n",
            "Epoch 174/200, Batch 9/45, Loss: 2.430846691131592\n",
            "Epoch 174/200, Batch 10/45, Loss: 2.0786194801330566\n",
            "Epoch 174/200, Batch 11/45, Loss: 1.86385178565979\n",
            "Epoch 174/200, Batch 12/45, Loss: 2.0797948837280273\n",
            "Epoch 174/200, Batch 13/45, Loss: 2.3258843421936035\n",
            "Epoch 174/200, Batch 14/45, Loss: 2.3840653896331787\n",
            "Epoch 174/200, Batch 15/45, Loss: 1.9232838153839111\n",
            "Epoch 174/200, Batch 16/45, Loss: 2.483130931854248\n",
            "Epoch 174/200, Batch 17/45, Loss: 2.3451409339904785\n",
            "Epoch 174/200, Batch 18/45, Loss: 2.018601179122925\n",
            "Epoch 174/200, Batch 19/45, Loss: 2.537123203277588\n",
            "Epoch 174/200, Batch 20/45, Loss: 1.7735190391540527\n",
            "Epoch 174/200, Batch 21/45, Loss: 1.974212646484375\n",
            "Epoch 174/200, Batch 22/45, Loss: 1.9395970106124878\n",
            "Epoch 174/200, Batch 23/45, Loss: 2.015432834625244\n",
            "Epoch 174/200, Batch 24/45, Loss: 1.9723498821258545\n",
            "Epoch 174/200, Batch 25/45, Loss: 1.9789032936096191\n",
            "Epoch 174/200, Batch 26/45, Loss: 2.38897442817688\n",
            "Epoch 174/200, Batch 27/45, Loss: 2.9962058067321777\n",
            "Epoch 174/200, Batch 28/45, Loss: 1.7811391353607178\n",
            "Epoch 174/200, Batch 29/45, Loss: 2.5089941024780273\n",
            "Epoch 174/200, Batch 30/45, Loss: 2.2983319759368896\n",
            "Epoch 174/200, Batch 31/45, Loss: 2.0780887603759766\n",
            "Epoch 174/200, Batch 32/45, Loss: 3.189873456954956\n",
            "Epoch 174/200, Batch 33/45, Loss: 2.071347951889038\n",
            "Epoch 174/200, Batch 34/45, Loss: 2.2448935508728027\n",
            "Epoch 174/200, Batch 35/45, Loss: 2.060994863510132\n",
            "Epoch 174/200, Batch 36/45, Loss: 2.7982096672058105\n",
            "Epoch 174/200, Batch 37/45, Loss: 1.9629762172698975\n",
            "Epoch 174/200, Batch 38/45, Loss: 2.0225391387939453\n",
            "Epoch 174/200, Batch 39/45, Loss: 2.1160318851470947\n",
            "Epoch 174/200, Batch 40/45, Loss: 2.595644474029541\n",
            "Epoch 174/200, Batch 41/45, Loss: 2.5425028800964355\n",
            "Epoch 174/200, Batch 42/45, Loss: 1.741955041885376\n",
            "Epoch 174/200, Batch 43/45, Loss: 3.1969099044799805\n",
            "Epoch 174/200, Batch 44/45, Loss: 1.925067663192749\n",
            "Epoch 174/200, Batch 45/45, Loss: 2.643012285232544\n",
            "Validating and Checkpointing!\n",
            "Model is not good (might be overfitting)! Current val MSE:  25.413208961486816 Best Val MSE:  24.743369102478027\n",
            "Epoch:  175 , Time Elapsed:  57.34730846881867  mins\n",
            "Epoch 175/200, Batch 1/45, Loss: 2.2388434410095215\n",
            "Epoch 175/200, Batch 2/45, Loss: 2.1444454193115234\n",
            "Epoch 175/200, Batch 3/45, Loss: 1.6523348093032837\n",
            "Epoch 175/200, Batch 4/45, Loss: 2.51773738861084\n",
            "Epoch 175/200, Batch 5/45, Loss: 2.309094190597534\n",
            "Epoch 175/200, Batch 6/45, Loss: 2.3455376625061035\n",
            "Epoch 175/200, Batch 7/45, Loss: 1.6310396194458008\n",
            "Epoch 175/200, Batch 8/45, Loss: 2.2647054195404053\n",
            "Epoch 175/200, Batch 9/45, Loss: 2.004685878753662\n",
            "Epoch 175/200, Batch 10/45, Loss: 2.0461978912353516\n",
            "Epoch 175/200, Batch 11/45, Loss: 2.113631248474121\n",
            "Epoch 175/200, Batch 12/45, Loss: 1.501682162284851\n",
            "Epoch 175/200, Batch 13/45, Loss: 2.7499899864196777\n",
            "Epoch 175/200, Batch 14/45, Loss: 1.9676601886749268\n",
            "Epoch 175/200, Batch 15/45, Loss: 1.8797645568847656\n",
            "Epoch 175/200, Batch 16/45, Loss: 3.0602593421936035\n",
            "Epoch 175/200, Batch 17/45, Loss: 2.119725465774536\n",
            "Epoch 175/200, Batch 18/45, Loss: 2.6201345920562744\n",
            "Epoch 175/200, Batch 19/45, Loss: 2.0346317291259766\n",
            "Epoch 175/200, Batch 20/45, Loss: 2.271362543106079\n",
            "Epoch 175/200, Batch 21/45, Loss: 2.0860612392425537\n",
            "Epoch 175/200, Batch 22/45, Loss: 2.557096242904663\n",
            "Epoch 175/200, Batch 23/45, Loss: 3.1721086502075195\n",
            "Epoch 175/200, Batch 24/45, Loss: 2.507547616958618\n",
            "Epoch 175/200, Batch 25/45, Loss: 2.2988178730010986\n",
            "Epoch 175/200, Batch 26/45, Loss: 2.2262516021728516\n",
            "Epoch 175/200, Batch 27/45, Loss: 2.602027416229248\n",
            "Epoch 175/200, Batch 28/45, Loss: 2.1411728858947754\n",
            "Epoch 175/200, Batch 29/45, Loss: 2.4196901321411133\n",
            "Epoch 175/200, Batch 30/45, Loss: 2.1244354248046875\n",
            "Epoch 175/200, Batch 31/45, Loss: 1.9646613597869873\n",
            "Epoch 175/200, Batch 32/45, Loss: 1.885915994644165\n",
            "Epoch 175/200, Batch 33/45, Loss: 2.3938770294189453\n",
            "Epoch 175/200, Batch 34/45, Loss: 2.5430264472961426\n",
            "Epoch 175/200, Batch 35/45, Loss: 1.7722710371017456\n",
            "Epoch 175/200, Batch 36/45, Loss: 2.3472654819488525\n",
            "Epoch 175/200, Batch 37/45, Loss: 2.3810415267944336\n",
            "Epoch 175/200, Batch 38/45, Loss: 2.0859761238098145\n",
            "Epoch 175/200, Batch 39/45, Loss: 2.347130298614502\n",
            "Epoch 175/200, Batch 40/45, Loss: 2.7627415657043457\n",
            "Epoch 175/200, Batch 41/45, Loss: 1.8107000589370728\n",
            "Epoch 175/200, Batch 42/45, Loss: 1.8617498874664307\n",
            "Epoch 175/200, Batch 43/45, Loss: 2.7564966678619385\n",
            "Epoch 175/200, Batch 44/45, Loss: 2.5944995880126953\n",
            "Epoch 175/200, Batch 45/45, Loss: 1.9728150367736816\n",
            "Validating and Checkpointing!\n",
            "Model is not good (might be overfitting)! Current val MSE:  47.590378761291504 Best Val MSE:  24.743369102478027\n",
            "Epoch:  176 , Time Elapsed:  57.69850333531698  mins\n",
            "Epoch 176/200, Batch 1/45, Loss: 1.8413395881652832\n",
            "Epoch 176/200, Batch 2/45, Loss: 1.632214069366455\n",
            "Epoch 176/200, Batch 3/45, Loss: 2.0247912406921387\n",
            "Epoch 176/200, Batch 4/45, Loss: 2.4064781665802\n",
            "Epoch 176/200, Batch 5/45, Loss: 1.8248494863510132\n",
            "Epoch 176/200, Batch 6/45, Loss: 2.2540769577026367\n",
            "Epoch 176/200, Batch 7/45, Loss: 2.6711416244506836\n",
            "Epoch 176/200, Batch 8/45, Loss: 2.2035248279571533\n",
            "Epoch 176/200, Batch 9/45, Loss: 2.2872610092163086\n",
            "Epoch 176/200, Batch 10/45, Loss: 3.330540418624878\n",
            "Epoch 176/200, Batch 11/45, Loss: 2.528388500213623\n",
            "Epoch 176/200, Batch 12/45, Loss: 2.0399250984191895\n",
            "Epoch 176/200, Batch 13/45, Loss: 1.6783638000488281\n",
            "Epoch 176/200, Batch 14/45, Loss: 1.52652108669281\n",
            "Epoch 176/200, Batch 15/45, Loss: 2.0927538871765137\n",
            "Epoch 176/200, Batch 16/45, Loss: 1.8802106380462646\n",
            "Epoch 176/200, Batch 17/45, Loss: 2.1392910480499268\n",
            "Epoch 176/200, Batch 18/45, Loss: 2.0183563232421875\n",
            "Epoch 176/200, Batch 19/45, Loss: 1.7099765539169312\n",
            "Epoch 176/200, Batch 20/45, Loss: 2.0379884243011475\n",
            "Epoch 176/200, Batch 21/45, Loss: 2.60206937789917\n",
            "Epoch 176/200, Batch 22/45, Loss: 2.550302028656006\n",
            "Epoch 176/200, Batch 23/45, Loss: 1.5625361204147339\n",
            "Epoch 176/200, Batch 24/45, Loss: 1.7434227466583252\n",
            "Epoch 176/200, Batch 25/45, Loss: 2.3539202213287354\n",
            "Epoch 176/200, Batch 26/45, Loss: 2.2391350269317627\n",
            "Epoch 176/200, Batch 27/45, Loss: 2.1434903144836426\n",
            "Epoch 176/200, Batch 28/45, Loss: 1.5587520599365234\n",
            "Epoch 176/200, Batch 29/45, Loss: 1.8528423309326172\n",
            "Epoch 176/200, Batch 30/45, Loss: 1.6390131711959839\n",
            "Epoch 176/200, Batch 31/45, Loss: 1.5900579690933228\n",
            "Epoch 176/200, Batch 32/45, Loss: 1.7605104446411133\n",
            "Epoch 176/200, Batch 33/45, Loss: 1.6531473398208618\n",
            "Epoch 176/200, Batch 34/45, Loss: 2.954920530319214\n",
            "Epoch 176/200, Batch 35/45, Loss: 1.9161746501922607\n",
            "Epoch 176/200, Batch 36/45, Loss: 2.1381754875183105\n",
            "Epoch 176/200, Batch 37/45, Loss: 1.791183352470398\n",
            "Epoch 176/200, Batch 38/45, Loss: 1.7020487785339355\n",
            "Epoch 176/200, Batch 39/45, Loss: 1.2085686922073364\n",
            "Epoch 176/200, Batch 40/45, Loss: 1.256332516670227\n",
            "Epoch 176/200, Batch 41/45, Loss: 1.7703914642333984\n",
            "Epoch 176/200, Batch 42/45, Loss: 1.705965518951416\n",
            "Epoch 176/200, Batch 43/45, Loss: 2.0935280323028564\n",
            "Epoch 176/200, Batch 44/45, Loss: 2.3787968158721924\n",
            "Epoch 176/200, Batch 45/45, Loss: 2.002497673034668\n",
            "Validating and Checkpointing!\n",
            "Model is not good (might be overfitting)! Current val MSE:  25.771679282188416 Best Val MSE:  24.743369102478027\n",
            "Epoch:  177 , Time Elapsed:  58.007067290941876  mins\n",
            "Epoch 177/200, Batch 1/45, Loss: 2.145907402038574\n",
            "Epoch 177/200, Batch 2/45, Loss: 1.8723523616790771\n",
            "Epoch 177/200, Batch 3/45, Loss: 3.0343503952026367\n",
            "Epoch 177/200, Batch 4/45, Loss: 1.7455321550369263\n",
            "Epoch 177/200, Batch 5/45, Loss: 1.9998464584350586\n",
            "Epoch 177/200, Batch 6/45, Loss: 2.5711617469787598\n",
            "Epoch 177/200, Batch 7/45, Loss: 1.844665765762329\n",
            "Epoch 177/200, Batch 8/45, Loss: 2.275280714035034\n",
            "Epoch 177/200, Batch 9/45, Loss: 2.071476936340332\n",
            "Epoch 177/200, Batch 10/45, Loss: 2.2013702392578125\n",
            "Epoch 177/200, Batch 11/45, Loss: 1.8331166505813599\n",
            "Epoch 177/200, Batch 12/45, Loss: 3.1160926818847656\n",
            "Epoch 177/200, Batch 13/45, Loss: 2.1899266242980957\n",
            "Epoch 177/200, Batch 14/45, Loss: 2.0474460124969482\n",
            "Epoch 177/200, Batch 15/45, Loss: 1.6735293865203857\n",
            "Epoch 177/200, Batch 16/45, Loss: 2.261697769165039\n",
            "Epoch 177/200, Batch 17/45, Loss: 1.8759078979492188\n",
            "Epoch 177/200, Batch 18/45, Loss: 2.2520334720611572\n",
            "Epoch 177/200, Batch 19/45, Loss: 2.5227530002593994\n",
            "Epoch 177/200, Batch 20/45, Loss: 1.5543491840362549\n",
            "Epoch 177/200, Batch 21/45, Loss: 2.6624584197998047\n",
            "Epoch 177/200, Batch 22/45, Loss: 2.143439292907715\n",
            "Epoch 177/200, Batch 23/45, Loss: 2.62964129447937\n",
            "Epoch 177/200, Batch 24/45, Loss: 2.381582260131836\n",
            "Epoch 177/200, Batch 25/45, Loss: 2.3887619972229004\n",
            "Epoch 177/200, Batch 26/45, Loss: 1.9401332139968872\n",
            "Epoch 177/200, Batch 27/45, Loss: 2.7028439044952393\n",
            "Epoch 177/200, Batch 28/45, Loss: 2.172560930252075\n",
            "Epoch 177/200, Batch 29/45, Loss: 1.7115135192871094\n",
            "Epoch 177/200, Batch 30/45, Loss: 2.3061022758483887\n",
            "Epoch 177/200, Batch 31/45, Loss: 2.1320571899414062\n",
            "Epoch 177/200, Batch 32/45, Loss: 2.85341739654541\n",
            "Epoch 177/200, Batch 33/45, Loss: 2.843106746673584\n",
            "Epoch 177/200, Batch 34/45, Loss: 3.291595697402954\n",
            "Epoch 177/200, Batch 35/45, Loss: 1.8218289613723755\n",
            "Epoch 177/200, Batch 36/45, Loss: 1.9338529109954834\n",
            "Epoch 177/200, Batch 37/45, Loss: 2.1717050075531006\n",
            "Epoch 177/200, Batch 38/45, Loss: 2.2908005714416504\n",
            "Epoch 177/200, Batch 39/45, Loss: 1.5837769508361816\n",
            "Epoch 177/200, Batch 40/45, Loss: 1.7329027652740479\n",
            "Epoch 177/200, Batch 41/45, Loss: 2.2852823734283447\n",
            "Epoch 177/200, Batch 42/45, Loss: 2.025920867919922\n",
            "Epoch 177/200, Batch 43/45, Loss: 2.6996169090270996\n",
            "Epoch 177/200, Batch 44/45, Loss: 2.0168793201446533\n",
            "Epoch 177/200, Batch 45/45, Loss: 2.0676157474517822\n",
            "Validating and Checkpointing!\n",
            "Model is not good (might be overfitting)! Current val MSE:  26.965948224067688 Best Val MSE:  24.743369102478027\n",
            "Epoch:  178 , Time Elapsed:  58.323517672220866  mins\n",
            "Epoch 178/200, Batch 1/45, Loss: 2.175652027130127\n",
            "Epoch 178/200, Batch 2/45, Loss: 2.902372360229492\n",
            "Epoch 178/200, Batch 3/45, Loss: 1.8822942972183228\n",
            "Epoch 178/200, Batch 4/45, Loss: 2.762648105621338\n",
            "Epoch 178/200, Batch 5/45, Loss: 1.6766166687011719\n",
            "Epoch 178/200, Batch 6/45, Loss: 2.2085840702056885\n",
            "Epoch 178/200, Batch 7/45, Loss: 1.6465568542480469\n",
            "Epoch 178/200, Batch 8/45, Loss: 1.803253173828125\n",
            "Epoch 178/200, Batch 9/45, Loss: 2.046847343444824\n",
            "Epoch 178/200, Batch 10/45, Loss: 2.446136713027954\n",
            "Epoch 178/200, Batch 11/45, Loss: 2.197463035583496\n",
            "Epoch 178/200, Batch 12/45, Loss: 2.439652442932129\n",
            "Epoch 178/200, Batch 13/45, Loss: 2.5039162635803223\n",
            "Epoch 178/200, Batch 14/45, Loss: 1.928924560546875\n",
            "Epoch 178/200, Batch 15/45, Loss: 2.8906407356262207\n",
            "Epoch 178/200, Batch 16/45, Loss: 2.098257303237915\n",
            "Epoch 178/200, Batch 17/45, Loss: 2.5009584426879883\n",
            "Epoch 178/200, Batch 18/45, Loss: 1.3974871635437012\n",
            "Epoch 178/200, Batch 19/45, Loss: 1.801145076751709\n",
            "Epoch 178/200, Batch 20/45, Loss: 2.6184935569763184\n",
            "Epoch 178/200, Batch 21/45, Loss: 1.521927833557129\n",
            "Epoch 178/200, Batch 22/45, Loss: 1.8003509044647217\n",
            "Epoch 178/200, Batch 23/45, Loss: 2.1075806617736816\n",
            "Epoch 178/200, Batch 24/45, Loss: 2.576648235321045\n",
            "Epoch 178/200, Batch 25/45, Loss: 2.8889200687408447\n",
            "Epoch 178/200, Batch 26/45, Loss: 2.3081257343292236\n",
            "Epoch 178/200, Batch 27/45, Loss: 2.4057633876800537\n",
            "Epoch 178/200, Batch 28/45, Loss: 1.965728998184204\n",
            "Epoch 178/200, Batch 29/45, Loss: 2.200709342956543\n",
            "Epoch 178/200, Batch 30/45, Loss: 2.3296945095062256\n",
            "Epoch 178/200, Batch 31/45, Loss: 1.9605872631072998\n",
            "Epoch 178/200, Batch 32/45, Loss: 2.273458480834961\n",
            "Epoch 178/200, Batch 33/45, Loss: 2.5505480766296387\n",
            "Epoch 178/200, Batch 34/45, Loss: 2.385767936706543\n",
            "Epoch 178/200, Batch 35/45, Loss: 1.992738962173462\n",
            "Epoch 178/200, Batch 36/45, Loss: 2.0974514484405518\n",
            "Epoch 178/200, Batch 37/45, Loss: 1.8509438037872314\n",
            "Epoch 178/200, Batch 38/45, Loss: 1.8813049793243408\n",
            "Epoch 178/200, Batch 39/45, Loss: 2.3627877235412598\n",
            "Epoch 178/200, Batch 40/45, Loss: 2.63271427154541\n",
            "Epoch 178/200, Batch 41/45, Loss: 1.7795028686523438\n",
            "Epoch 178/200, Batch 42/45, Loss: 1.9915943145751953\n",
            "Epoch 178/200, Batch 43/45, Loss: 2.0491106510162354\n",
            "Epoch 178/200, Batch 44/45, Loss: 1.9299614429473877\n",
            "Epoch 178/200, Batch 45/45, Loss: 1.2022762298583984\n",
            "Validating and Checkpointing!\n",
            "Model is not good (might be overfitting)! Current val MSE:  26.53348958492279 Best Val MSE:  24.743369102478027\n",
            "Epoch:  179 , Time Elapsed:  58.67458229064941  mins\n",
            "Epoch 179/200, Batch 1/45, Loss: 2.1282827854156494\n",
            "Epoch 179/200, Batch 2/45, Loss: 2.6521155834198\n",
            "Epoch 179/200, Batch 3/45, Loss: 1.7163739204406738\n",
            "Epoch 179/200, Batch 4/45, Loss: 2.4978249073028564\n",
            "Epoch 179/200, Batch 5/45, Loss: 3.129377603530884\n",
            "Epoch 179/200, Batch 6/45, Loss: 1.7479828596115112\n",
            "Epoch 179/200, Batch 7/45, Loss: 1.7756574153900146\n",
            "Epoch 179/200, Batch 8/45, Loss: 1.8288933038711548\n",
            "Epoch 179/200, Batch 9/45, Loss: 1.948237419128418\n",
            "Epoch 179/200, Batch 10/45, Loss: 1.483980417251587\n",
            "Epoch 179/200, Batch 11/45, Loss: 2.3625431060791016\n",
            "Epoch 179/200, Batch 12/45, Loss: 1.8260118961334229\n",
            "Epoch 179/200, Batch 13/45, Loss: 2.258044481277466\n",
            "Epoch 179/200, Batch 14/45, Loss: 1.627829670906067\n",
            "Epoch 179/200, Batch 15/45, Loss: 2.2265524864196777\n",
            "Epoch 179/200, Batch 16/45, Loss: 2.000133752822876\n",
            "Epoch 179/200, Batch 17/45, Loss: 1.2915524244308472\n",
            "Epoch 179/200, Batch 18/45, Loss: 2.0152945518493652\n",
            "Epoch 179/200, Batch 19/45, Loss: 2.7833499908447266\n",
            "Epoch 179/200, Batch 20/45, Loss: 1.855718970298767\n",
            "Epoch 179/200, Batch 21/45, Loss: 1.998732328414917\n",
            "Epoch 179/200, Batch 22/45, Loss: 1.5974199771881104\n",
            "Epoch 179/200, Batch 23/45, Loss: 2.6040866374969482\n",
            "Epoch 179/200, Batch 24/45, Loss: 2.483365297317505\n",
            "Epoch 179/200, Batch 25/45, Loss: 2.222649335861206\n",
            "Epoch 179/200, Batch 26/45, Loss: 1.899414300918579\n",
            "Epoch 179/200, Batch 27/45, Loss: 2.365914821624756\n",
            "Epoch 179/200, Batch 28/45, Loss: 1.773756742477417\n",
            "Epoch 179/200, Batch 29/45, Loss: 1.6604491472244263\n",
            "Epoch 179/200, Batch 30/45, Loss: 2.40008807182312\n",
            "Epoch 179/200, Batch 31/45, Loss: 1.779948115348816\n",
            "Epoch 179/200, Batch 32/45, Loss: 2.152831554412842\n",
            "Epoch 179/200, Batch 33/45, Loss: 2.227363109588623\n",
            "Epoch 179/200, Batch 34/45, Loss: 2.3315680027008057\n",
            "Epoch 179/200, Batch 35/45, Loss: 2.407900810241699\n",
            "Epoch 179/200, Batch 36/45, Loss: 2.470237970352173\n",
            "Epoch 179/200, Batch 37/45, Loss: 2.6135921478271484\n",
            "Epoch 179/200, Batch 38/45, Loss: 2.2913403511047363\n",
            "Epoch 179/200, Batch 39/45, Loss: 2.6365530490875244\n",
            "Epoch 179/200, Batch 40/45, Loss: 2.285916805267334\n",
            "Epoch 179/200, Batch 41/45, Loss: 2.4013495445251465\n",
            "Epoch 179/200, Batch 42/45, Loss: 1.521008014678955\n",
            "Epoch 179/200, Batch 43/45, Loss: 2.6716861724853516\n",
            "Epoch 179/200, Batch 44/45, Loss: 2.303471565246582\n",
            "Epoch 179/200, Batch 45/45, Loss: 2.8773298263549805\n",
            "Validating and Checkpointing!\n",
            "Model is not good (might be overfitting)! Current val MSE:  28.448307871818542 Best Val MSE:  24.743369102478027\n",
            "Epoch:  180 , Time Elapsed:  58.99684743086497  mins\n",
            "Epoch 180/200, Batch 1/45, Loss: 1.7707209587097168\n",
            "Epoch 180/200, Batch 2/45, Loss: 2.408763885498047\n",
            "Epoch 180/200, Batch 3/45, Loss: 2.338639736175537\n",
            "Epoch 180/200, Batch 4/45, Loss: 1.9113463163375854\n",
            "Epoch 180/200, Batch 5/45, Loss: 2.3509294986724854\n",
            "Epoch 180/200, Batch 6/45, Loss: 2.328727960586548\n",
            "Epoch 180/200, Batch 7/45, Loss: 6.811543941497803\n",
            "Epoch 180/200, Batch 8/45, Loss: 1.8523198366165161\n",
            "Epoch 180/200, Batch 9/45, Loss: 2.021651268005371\n",
            "Epoch 180/200, Batch 10/45, Loss: 2.1558141708374023\n",
            "Epoch 180/200, Batch 11/45, Loss: 1.5151255130767822\n",
            "Epoch 180/200, Batch 12/45, Loss: 2.394407272338867\n",
            "Epoch 180/200, Batch 13/45, Loss: 2.3696343898773193\n",
            "Epoch 180/200, Batch 14/45, Loss: 2.263564348220825\n",
            "Epoch 180/200, Batch 15/45, Loss: 2.158083915710449\n",
            "Epoch 180/200, Batch 16/45, Loss: 2.2136754989624023\n",
            "Epoch 180/200, Batch 17/45, Loss: 2.049919366836548\n",
            "Epoch 180/200, Batch 18/45, Loss: 2.1977691650390625\n",
            "Epoch 180/200, Batch 19/45, Loss: 2.105229616165161\n",
            "Epoch 180/200, Batch 20/45, Loss: 2.396451950073242\n",
            "Epoch 180/200, Batch 21/45, Loss: 1.8068418502807617\n",
            "Epoch 180/200, Batch 22/45, Loss: 2.518075942993164\n",
            "Epoch 180/200, Batch 23/45, Loss: 2.3540031909942627\n",
            "Epoch 180/200, Batch 24/45, Loss: 1.890920877456665\n",
            "Epoch 180/200, Batch 25/45, Loss: 2.3432157039642334\n",
            "Epoch 180/200, Batch 26/45, Loss: 2.0145106315612793\n",
            "Epoch 180/200, Batch 27/45, Loss: 1.7922343015670776\n",
            "Epoch 180/200, Batch 28/45, Loss: 1.9310986995697021\n",
            "Epoch 180/200, Batch 29/45, Loss: 2.4549105167388916\n",
            "Epoch 180/200, Batch 30/45, Loss: 1.5317896604537964\n",
            "Epoch 180/200, Batch 31/45, Loss: 2.6747798919677734\n",
            "Epoch 180/200, Batch 32/45, Loss: 2.042750835418701\n",
            "Epoch 180/200, Batch 33/45, Loss: 1.7150863409042358\n",
            "Epoch 180/200, Batch 34/45, Loss: 2.0672483444213867\n",
            "Epoch 180/200, Batch 35/45, Loss: 1.9920787811279297\n",
            "Epoch 180/200, Batch 36/45, Loss: 2.3658902645111084\n",
            "Epoch 180/200, Batch 37/45, Loss: 2.638852119445801\n",
            "Epoch 180/200, Batch 38/45, Loss: 2.088989019393921\n",
            "Epoch 180/200, Batch 39/45, Loss: 1.90999436378479\n",
            "Epoch 180/200, Batch 40/45, Loss: 2.7253401279449463\n",
            "Epoch 180/200, Batch 41/45, Loss: 2.235992193222046\n",
            "Epoch 180/200, Batch 42/45, Loss: 2.0377073287963867\n",
            "Epoch 180/200, Batch 43/45, Loss: 1.773705005645752\n",
            "Epoch 180/200, Batch 44/45, Loss: 2.6009480953216553\n",
            "Epoch 180/200, Batch 45/45, Loss: 2.4821460247039795\n",
            "Validating and Checkpointing!\n",
            "Model is not good (might be overfitting)! Current val MSE:  25.160518646240234 Best Val MSE:  24.743369102478027\n",
            "Epoch:  181 , Time Elapsed:  59.313861866792045  mins\n",
            "Epoch 181/200, Batch 1/45, Loss: 2.6360831260681152\n",
            "Epoch 181/200, Batch 2/45, Loss: 2.057492256164551\n",
            "Epoch 181/200, Batch 3/45, Loss: 2.365579605102539\n",
            "Epoch 181/200, Batch 4/45, Loss: 2.6654129028320312\n",
            "Epoch 181/200, Batch 5/45, Loss: 2.0509865283966064\n",
            "Epoch 181/200, Batch 6/45, Loss: 2.584160566329956\n",
            "Epoch 181/200, Batch 7/45, Loss: 2.0567197799682617\n",
            "Epoch 181/200, Batch 8/45, Loss: 2.106193780899048\n",
            "Epoch 181/200, Batch 9/45, Loss: 2.146880865097046\n",
            "Epoch 181/200, Batch 10/45, Loss: 2.6917929649353027\n",
            "Epoch 181/200, Batch 11/45, Loss: 1.8950763940811157\n",
            "Epoch 181/200, Batch 12/45, Loss: 2.078216552734375\n",
            "Epoch 181/200, Batch 13/45, Loss: 1.959256649017334\n",
            "Epoch 181/200, Batch 14/45, Loss: 1.8776133060455322\n",
            "Epoch 181/200, Batch 15/45, Loss: 2.414682626724243\n",
            "Epoch 181/200, Batch 16/45, Loss: 1.8797240257263184\n",
            "Epoch 181/200, Batch 17/45, Loss: 2.008347511291504\n",
            "Epoch 181/200, Batch 18/45, Loss: 2.4752776622772217\n",
            "Epoch 181/200, Batch 19/45, Loss: 2.059577703475952\n",
            "Epoch 181/200, Batch 20/45, Loss: 2.1825153827667236\n",
            "Epoch 181/200, Batch 21/45, Loss: 2.500880241394043\n",
            "Epoch 181/200, Batch 22/45, Loss: 2.416349411010742\n",
            "Epoch 181/200, Batch 23/45, Loss: 2.6349411010742188\n",
            "Epoch 181/200, Batch 24/45, Loss: 2.425067901611328\n",
            "Epoch 181/200, Batch 25/45, Loss: 2.3207015991210938\n",
            "Epoch 181/200, Batch 26/45, Loss: 1.7447068691253662\n",
            "Epoch 181/200, Batch 27/45, Loss: 2.069227457046509\n",
            "Epoch 181/200, Batch 28/45, Loss: 2.2148919105529785\n",
            "Epoch 181/200, Batch 29/45, Loss: 1.1606950759887695\n",
            "Epoch 181/200, Batch 30/45, Loss: 2.3921000957489014\n",
            "Epoch 181/200, Batch 31/45, Loss: 1.9792927503585815\n",
            "Epoch 181/200, Batch 32/45, Loss: 2.2306880950927734\n",
            "Epoch 181/200, Batch 33/45, Loss: 2.1176652908325195\n",
            "Epoch 181/200, Batch 34/45, Loss: 2.0672414302825928\n",
            "Epoch 181/200, Batch 35/45, Loss: 1.7958606481552124\n",
            "Epoch 181/200, Batch 36/45, Loss: 2.75907564163208\n",
            "Epoch 181/200, Batch 37/45, Loss: 2.751682758331299\n",
            "Epoch 181/200, Batch 38/45, Loss: 2.0008583068847656\n",
            "Epoch 181/200, Batch 39/45, Loss: 2.5337209701538086\n",
            "Epoch 181/200, Batch 40/45, Loss: 2.749729871749878\n",
            "Epoch 181/200, Batch 41/45, Loss: 2.6801774501800537\n",
            "Epoch 181/200, Batch 42/45, Loss: 1.8678582906723022\n",
            "Epoch 181/200, Batch 43/45, Loss: 2.5319297313690186\n",
            "Epoch 181/200, Batch 44/45, Loss: 1.580923080444336\n",
            "Epoch 181/200, Batch 45/45, Loss: 1.938749074935913\n",
            "Validating and Checkpointing!\n",
            "Model is not good (might be overfitting)! Current val MSE:  25.625104427337646 Best Val MSE:  24.743369102478027\n",
            "Epoch:  182 , Time Elapsed:  59.65164051453272  mins\n",
            "Epoch 182/200, Batch 1/45, Loss: 3.6475398540496826\n",
            "Epoch 182/200, Batch 2/45, Loss: 2.1104278564453125\n",
            "Epoch 182/200, Batch 3/45, Loss: 2.255955457687378\n",
            "Epoch 182/200, Batch 4/45, Loss: 2.1267833709716797\n",
            "Epoch 182/200, Batch 5/45, Loss: 1.8113216161727905\n",
            "Epoch 182/200, Batch 6/45, Loss: 2.287203550338745\n",
            "Epoch 182/200, Batch 7/45, Loss: 1.7880306243896484\n",
            "Epoch 182/200, Batch 8/45, Loss: 1.9003928899765015\n",
            "Epoch 182/200, Batch 9/45, Loss: 1.902637004852295\n",
            "Epoch 182/200, Batch 10/45, Loss: 2.196916103363037\n",
            "Epoch 182/200, Batch 11/45, Loss: 1.6407464742660522\n",
            "Epoch 182/200, Batch 12/45, Loss: 1.9564200639724731\n",
            "Epoch 182/200, Batch 13/45, Loss: 2.740719795227051\n",
            "Epoch 182/200, Batch 14/45, Loss: 2.4404983520507812\n",
            "Epoch 182/200, Batch 15/45, Loss: 2.3174147605895996\n",
            "Epoch 182/200, Batch 16/45, Loss: 2.384486436843872\n",
            "Epoch 182/200, Batch 17/45, Loss: 1.8194257020950317\n",
            "Epoch 182/200, Batch 18/45, Loss: 2.3218493461608887\n",
            "Epoch 182/200, Batch 19/45, Loss: 2.3721518516540527\n",
            "Epoch 182/200, Batch 20/45, Loss: 2.079440116882324\n",
            "Epoch 182/200, Batch 21/45, Loss: 1.9927929639816284\n",
            "Epoch 182/200, Batch 22/45, Loss: 1.6192182302474976\n",
            "Epoch 182/200, Batch 23/45, Loss: 2.9510531425476074\n",
            "Epoch 182/200, Batch 24/45, Loss: 2.3028676509857178\n",
            "Epoch 182/200, Batch 25/45, Loss: 2.079479217529297\n",
            "Epoch 182/200, Batch 26/45, Loss: 2.3828420639038086\n",
            "Epoch 182/200, Batch 27/45, Loss: 1.788872241973877\n",
            "Epoch 182/200, Batch 28/45, Loss: 1.9482444524765015\n",
            "Epoch 182/200, Batch 29/45, Loss: 2.4057185649871826\n",
            "Epoch 182/200, Batch 30/45, Loss: 2.014920949935913\n",
            "Epoch 182/200, Batch 31/45, Loss: 2.1914186477661133\n",
            "Epoch 182/200, Batch 32/45, Loss: 2.4306716918945312\n",
            "Epoch 182/200, Batch 33/45, Loss: 2.5401971340179443\n",
            "Epoch 182/200, Batch 34/45, Loss: 1.400315523147583\n",
            "Epoch 182/200, Batch 35/45, Loss: 1.34425950050354\n",
            "Epoch 182/200, Batch 36/45, Loss: 3.5248589515686035\n",
            "Epoch 182/200, Batch 37/45, Loss: 1.3764638900756836\n",
            "Epoch 182/200, Batch 38/45, Loss: 2.043243885040283\n",
            "Epoch 182/200, Batch 39/45, Loss: 2.1973304748535156\n",
            "Epoch 182/200, Batch 40/45, Loss: 2.2220401763916016\n",
            "Epoch 182/200, Batch 41/45, Loss: 1.5307716131210327\n",
            "Epoch 182/200, Batch 42/45, Loss: 1.9509789943695068\n",
            "Epoch 182/200, Batch 43/45, Loss: 2.164669990539551\n",
            "Epoch 182/200, Batch 44/45, Loss: 1.9255802631378174\n",
            "Epoch 182/200, Batch 45/45, Loss: 2.4722208976745605\n",
            "Validating and Checkpointing!\n",
            "Model is not good (might be overfitting)! Current val MSE:  25.504255294799805 Best Val MSE:  24.743369102478027\n",
            "Epoch:  183 , Time Elapsed:  59.969257307052615  mins\n",
            "Epoch 183/200, Batch 1/45, Loss: 3.7834606170654297\n",
            "Epoch 183/200, Batch 2/45, Loss: 1.0851643085479736\n",
            "Epoch 183/200, Batch 3/45, Loss: 1.7062461376190186\n",
            "Epoch 183/200, Batch 4/45, Loss: 2.1125495433807373\n",
            "Epoch 183/200, Batch 5/45, Loss: 1.9741474390029907\n",
            "Epoch 183/200, Batch 6/45, Loss: 2.1634175777435303\n",
            "Epoch 183/200, Batch 7/45, Loss: 2.2558963298797607\n",
            "Epoch 183/200, Batch 8/45, Loss: 1.9742381572723389\n",
            "Epoch 183/200, Batch 9/45, Loss: 1.169205665588379\n",
            "Epoch 183/200, Batch 10/45, Loss: 2.3907570838928223\n",
            "Epoch 183/200, Batch 11/45, Loss: 2.0201218128204346\n",
            "Epoch 183/200, Batch 12/45, Loss: 2.0308237075805664\n",
            "Epoch 183/200, Batch 13/45, Loss: 2.262202024459839\n",
            "Epoch 183/200, Batch 14/45, Loss: 2.109119176864624\n",
            "Epoch 183/200, Batch 15/45, Loss: 2.7787511348724365\n",
            "Epoch 183/200, Batch 16/45, Loss: 2.456817626953125\n",
            "Epoch 183/200, Batch 17/45, Loss: 2.0227270126342773\n",
            "Epoch 183/200, Batch 18/45, Loss: 1.4289917945861816\n",
            "Epoch 183/200, Batch 19/45, Loss: 2.197957992553711\n",
            "Epoch 183/200, Batch 20/45, Loss: 2.0462141036987305\n",
            "Epoch 183/200, Batch 21/45, Loss: 2.5384714603424072\n",
            "Epoch 183/200, Batch 22/45, Loss: 2.1852405071258545\n",
            "Epoch 183/200, Batch 23/45, Loss: 2.0470757484436035\n",
            "Epoch 183/200, Batch 24/45, Loss: 1.8427212238311768\n",
            "Epoch 183/200, Batch 25/45, Loss: 2.2245640754699707\n",
            "Epoch 183/200, Batch 26/45, Loss: 2.369344711303711\n",
            "Epoch 183/200, Batch 27/45, Loss: 2.657170295715332\n",
            "Epoch 183/200, Batch 28/45, Loss: 2.610288143157959\n",
            "Epoch 183/200, Batch 29/45, Loss: 2.4023194313049316\n",
            "Epoch 183/200, Batch 30/45, Loss: 1.9497056007385254\n",
            "Epoch 183/200, Batch 31/45, Loss: 3.3877041339874268\n",
            "Epoch 183/200, Batch 32/45, Loss: 2.2343335151672363\n",
            "Epoch 183/200, Batch 33/45, Loss: 2.0427441596984863\n",
            "Epoch 183/200, Batch 34/45, Loss: 1.93964421749115\n",
            "Epoch 183/200, Batch 35/45, Loss: 1.9586751461029053\n",
            "Epoch 183/200, Batch 36/45, Loss: 2.480802536010742\n",
            "Epoch 183/200, Batch 37/45, Loss: 2.433063507080078\n",
            "Epoch 183/200, Batch 38/45, Loss: 2.5030264854431152\n",
            "Epoch 183/200, Batch 39/45, Loss: 1.837456226348877\n",
            "Epoch 183/200, Batch 40/45, Loss: 1.884618878364563\n",
            "Epoch 183/200, Batch 41/45, Loss: 1.6156291961669922\n",
            "Epoch 183/200, Batch 42/45, Loss: 2.042788505554199\n",
            "Epoch 183/200, Batch 43/45, Loss: 2.035404682159424\n",
            "Epoch 183/200, Batch 44/45, Loss: 2.4266135692596436\n",
            "Epoch 183/200, Batch 45/45, Loss: 2.203770637512207\n",
            "Validating and Checkpointing!\n",
            "Best model Saved! Val MSE:  24.62479865550995\n",
            "Epoch:  184 , Time Elapsed:  60.34296463727951  mins\n",
            "Epoch 184/200, Batch 1/45, Loss: 2.3979644775390625\n",
            "Epoch 184/200, Batch 2/45, Loss: 1.3542683124542236\n",
            "Epoch 184/200, Batch 3/45, Loss: 2.2212510108947754\n",
            "Epoch 184/200, Batch 4/45, Loss: 1.6473050117492676\n",
            "Epoch 184/200, Batch 5/45, Loss: 2.3445937633514404\n",
            "Epoch 184/200, Batch 6/45, Loss: 2.088155508041382\n",
            "Epoch 184/200, Batch 7/45, Loss: 2.305867910385132\n",
            "Epoch 184/200, Batch 8/45, Loss: 2.4429726600646973\n",
            "Epoch 184/200, Batch 9/45, Loss: 2.7687580585479736\n",
            "Epoch 184/200, Batch 10/45, Loss: 2.201796054840088\n",
            "Epoch 184/200, Batch 11/45, Loss: 2.421290159225464\n",
            "Epoch 184/200, Batch 12/45, Loss: 2.122419595718384\n",
            "Epoch 184/200, Batch 13/45, Loss: 1.8962693214416504\n",
            "Epoch 184/200, Batch 14/45, Loss: 2.1433968544006348\n",
            "Epoch 184/200, Batch 15/45, Loss: 1.833854079246521\n",
            "Epoch 184/200, Batch 16/45, Loss: 2.2644338607788086\n",
            "Epoch 184/200, Batch 17/45, Loss: 2.5244972705841064\n",
            "Epoch 184/200, Batch 18/45, Loss: 2.148033618927002\n",
            "Epoch 184/200, Batch 19/45, Loss: 2.099001407623291\n",
            "Epoch 184/200, Batch 20/45, Loss: 2.279470443725586\n",
            "Epoch 184/200, Batch 21/45, Loss: 2.180473804473877\n",
            "Epoch 184/200, Batch 22/45, Loss: 2.839123010635376\n",
            "Epoch 184/200, Batch 23/45, Loss: 2.568416118621826\n",
            "Epoch 184/200, Batch 24/45, Loss: 2.7427711486816406\n",
            "Epoch 184/200, Batch 25/45, Loss: 1.9379496574401855\n",
            "Epoch 184/200, Batch 26/45, Loss: 1.6592611074447632\n",
            "Epoch 184/200, Batch 27/45, Loss: 1.5981426239013672\n",
            "Epoch 184/200, Batch 28/45, Loss: 1.7482378482818604\n",
            "Epoch 184/200, Batch 29/45, Loss: 2.078286647796631\n",
            "Epoch 184/200, Batch 30/45, Loss: 1.8807272911071777\n",
            "Epoch 184/200, Batch 31/45, Loss: 2.2099087238311768\n",
            "Epoch 184/200, Batch 32/45, Loss: 1.8238728046417236\n",
            "Epoch 184/200, Batch 33/45, Loss: 1.835705280303955\n",
            "Epoch 184/200, Batch 34/45, Loss: 1.6273446083068848\n",
            "Epoch 184/200, Batch 35/45, Loss: 1.6504218578338623\n",
            "Epoch 184/200, Batch 36/45, Loss: 2.512824535369873\n",
            "Epoch 184/200, Batch 37/45, Loss: 1.6554840803146362\n",
            "Epoch 184/200, Batch 38/45, Loss: 2.411623001098633\n",
            "Epoch 184/200, Batch 39/45, Loss: 1.6069039106369019\n",
            "Epoch 184/200, Batch 40/45, Loss: 2.355034828186035\n",
            "Epoch 184/200, Batch 41/45, Loss: 2.0908539295196533\n",
            "Epoch 184/200, Batch 42/45, Loss: 1.0906801223754883\n",
            "Epoch 184/200, Batch 43/45, Loss: 1.6711969375610352\n",
            "Epoch 184/200, Batch 44/45, Loss: 2.1020398139953613\n",
            "Epoch 184/200, Batch 45/45, Loss: 1.8166688680648804\n",
            "Validating and Checkpointing!\n",
            "Model is not good (might be overfitting)! Current val MSE:  25.485645651817322 Best Val MSE:  24.62479865550995\n",
            "Epoch:  185 , Time Elapsed:  60.656032820542656  mins\n",
            "Epoch 185/200, Batch 1/45, Loss: 1.3662075996398926\n",
            "Epoch 185/200, Batch 2/45, Loss: 2.2003586292266846\n",
            "Epoch 185/200, Batch 3/45, Loss: 2.2316946983337402\n",
            "Epoch 185/200, Batch 4/45, Loss: 2.0436835289001465\n",
            "Epoch 185/200, Batch 5/45, Loss: 2.2654030323028564\n",
            "Epoch 185/200, Batch 6/45, Loss: 2.5086541175842285\n",
            "Epoch 185/200, Batch 7/45, Loss: 1.9527380466461182\n",
            "Epoch 185/200, Batch 8/45, Loss: 1.7087228298187256\n",
            "Epoch 185/200, Batch 9/45, Loss: 2.019052505493164\n",
            "Epoch 185/200, Batch 10/45, Loss: 3.598361015319824\n",
            "Epoch 185/200, Batch 11/45, Loss: 1.7697861194610596\n",
            "Epoch 185/200, Batch 12/45, Loss: 2.398587942123413\n",
            "Epoch 185/200, Batch 13/45, Loss: 1.7767583131790161\n",
            "Epoch 185/200, Batch 14/45, Loss: 3.8404953479766846\n",
            "Epoch 185/200, Batch 15/45, Loss: 2.3055524826049805\n",
            "Epoch 185/200, Batch 16/45, Loss: 2.1285738945007324\n",
            "Epoch 185/200, Batch 17/45, Loss: 1.9261879920959473\n",
            "Epoch 185/200, Batch 18/45, Loss: 1.9295897483825684\n",
            "Epoch 185/200, Batch 19/45, Loss: 2.4851274490356445\n",
            "Epoch 185/200, Batch 20/45, Loss: 2.413625717163086\n",
            "Epoch 185/200, Batch 21/45, Loss: 1.7189011573791504\n",
            "Epoch 185/200, Batch 22/45, Loss: 2.099787712097168\n",
            "Epoch 185/200, Batch 23/45, Loss: 2.1993494033813477\n",
            "Epoch 185/200, Batch 24/45, Loss: 1.6924519538879395\n",
            "Epoch 185/200, Batch 25/45, Loss: 2.2139389514923096\n",
            "Epoch 185/200, Batch 26/45, Loss: 2.2039146423339844\n",
            "Epoch 185/200, Batch 27/45, Loss: 2.921387195587158\n",
            "Epoch 185/200, Batch 28/45, Loss: 2.356294631958008\n",
            "Epoch 185/200, Batch 29/45, Loss: 1.8518091440200806\n",
            "Epoch 185/200, Batch 30/45, Loss: 2.4874207973480225\n",
            "Epoch 185/200, Batch 31/45, Loss: 2.625824213027954\n",
            "Epoch 185/200, Batch 32/45, Loss: 2.2926807403564453\n",
            "Epoch 185/200, Batch 33/45, Loss: 2.6663551330566406\n",
            "Epoch 185/200, Batch 34/45, Loss: 1.6792898178100586\n",
            "Epoch 185/200, Batch 35/45, Loss: 2.837865114212036\n",
            "Epoch 185/200, Batch 36/45, Loss: 2.642509698867798\n",
            "Epoch 185/200, Batch 37/45, Loss: 2.3782858848571777\n",
            "Epoch 185/200, Batch 38/45, Loss: 2.35499906539917\n",
            "Epoch 185/200, Batch 39/45, Loss: 1.9481549263000488\n",
            "Epoch 185/200, Batch 40/45, Loss: 2.1897120475769043\n",
            "Epoch 185/200, Batch 41/45, Loss: 2.653409004211426\n",
            "Epoch 185/200, Batch 42/45, Loss: 2.1146910190582275\n",
            "Epoch 185/200, Batch 43/45, Loss: 2.1329803466796875\n",
            "Epoch 185/200, Batch 44/45, Loss: 2.311845541000366\n",
            "Epoch 185/200, Batch 45/45, Loss: 2.8854494094848633\n",
            "Validating and Checkpointing!\n",
            "Model is not good (might be overfitting)! Current val MSE:  25.937485933303833 Best Val MSE:  24.62479865550995\n",
            "Epoch:  186 , Time Elapsed:  60.97736845811208  mins\n",
            "Epoch 186/200, Batch 1/45, Loss: 2.480731725692749\n",
            "Epoch 186/200, Batch 2/45, Loss: 1.8582370281219482\n",
            "Epoch 186/200, Batch 3/45, Loss: 2.2206923961639404\n",
            "Epoch 186/200, Batch 4/45, Loss: 2.409512996673584\n",
            "Epoch 186/200, Batch 5/45, Loss: 1.6483527421951294\n",
            "Epoch 186/200, Batch 6/45, Loss: 2.0972702503204346\n",
            "Epoch 186/200, Batch 7/45, Loss: 2.097393274307251\n",
            "Epoch 186/200, Batch 8/45, Loss: 2.011361837387085\n",
            "Epoch 186/200, Batch 9/45, Loss: 2.2967379093170166\n",
            "Epoch 186/200, Batch 10/45, Loss: 2.429518699645996\n",
            "Epoch 186/200, Batch 11/45, Loss: 2.5844876766204834\n",
            "Epoch 186/200, Batch 12/45, Loss: 1.852280855178833\n",
            "Epoch 186/200, Batch 13/45, Loss: 1.4707889556884766\n",
            "Epoch 186/200, Batch 14/45, Loss: 1.6949979066848755\n",
            "Epoch 186/200, Batch 15/45, Loss: 1.5251107215881348\n",
            "Epoch 186/200, Batch 16/45, Loss: 1.6386680603027344\n",
            "Epoch 186/200, Batch 17/45, Loss: 2.0826001167297363\n",
            "Epoch 186/200, Batch 18/45, Loss: 2.1517980098724365\n",
            "Epoch 186/200, Batch 19/45, Loss: 2.168133497238159\n",
            "Epoch 186/200, Batch 20/45, Loss: 2.258568525314331\n",
            "Epoch 186/200, Batch 21/45, Loss: 1.754420280456543\n",
            "Epoch 186/200, Batch 22/45, Loss: 2.513845682144165\n",
            "Epoch 186/200, Batch 23/45, Loss: 1.7351418733596802\n",
            "Epoch 186/200, Batch 24/45, Loss: 1.4331622123718262\n",
            "Epoch 186/200, Batch 25/45, Loss: 2.3050782680511475\n",
            "Epoch 186/200, Batch 26/45, Loss: 2.047658920288086\n",
            "Epoch 186/200, Batch 27/45, Loss: 1.6749616861343384\n",
            "Epoch 186/200, Batch 28/45, Loss: 1.4544020891189575\n",
            "Epoch 186/200, Batch 29/45, Loss: 2.3259780406951904\n",
            "Epoch 186/200, Batch 30/45, Loss: 1.6933683156967163\n",
            "Epoch 186/200, Batch 31/45, Loss: 1.829702615737915\n",
            "Epoch 186/200, Batch 32/45, Loss: 4.341272830963135\n",
            "Epoch 186/200, Batch 33/45, Loss: 2.8112051486968994\n",
            "Epoch 186/200, Batch 34/45, Loss: 1.890360951423645\n",
            "Epoch 186/200, Batch 35/45, Loss: 1.6066179275512695\n",
            "Epoch 186/200, Batch 36/45, Loss: 1.3747363090515137\n",
            "Epoch 186/200, Batch 37/45, Loss: 1.818148136138916\n",
            "Epoch 186/200, Batch 38/45, Loss: 1.8620315790176392\n",
            "Epoch 186/200, Batch 39/45, Loss: 1.6059291362762451\n",
            "Epoch 186/200, Batch 40/45, Loss: 1.9964512586593628\n",
            "Epoch 186/200, Batch 41/45, Loss: 1.670600175857544\n",
            "Epoch 186/200, Batch 42/45, Loss: 1.9570746421813965\n",
            "Epoch 186/200, Batch 43/45, Loss: 2.0834803581237793\n",
            "Epoch 186/200, Batch 44/45, Loss: 1.3569415807724\n",
            "Epoch 186/200, Batch 45/45, Loss: 2.216763496398926\n",
            "Validating and Checkpointing!\n",
            "Model is not good (might be overfitting)! Current val MSE:  24.651370882987976 Best Val MSE:  24.62479865550995\n",
            "Epoch:  187 , Time Elapsed:  61.32977673610051  mins\n",
            "Epoch 187/200, Batch 1/45, Loss: 1.7943753004074097\n",
            "Epoch 187/200, Batch 2/45, Loss: 2.040482997894287\n",
            "Epoch 187/200, Batch 3/45, Loss: 2.367666482925415\n",
            "Epoch 187/200, Batch 4/45, Loss: 2.2511210441589355\n",
            "Epoch 187/200, Batch 5/45, Loss: 1.7264076471328735\n",
            "Epoch 187/200, Batch 6/45, Loss: 1.914063572883606\n",
            "Epoch 187/200, Batch 7/45, Loss: 2.209132194519043\n",
            "Epoch 187/200, Batch 8/45, Loss: 2.0635852813720703\n",
            "Epoch 187/200, Batch 9/45, Loss: 1.6482155323028564\n",
            "Epoch 187/200, Batch 10/45, Loss: 1.8211369514465332\n",
            "Epoch 187/200, Batch 11/45, Loss: 2.035404682159424\n",
            "Epoch 187/200, Batch 12/45, Loss: 2.8623099327087402\n",
            "Epoch 187/200, Batch 13/45, Loss: 2.077943801879883\n",
            "Epoch 187/200, Batch 14/45, Loss: 2.120704412460327\n",
            "Epoch 187/200, Batch 15/45, Loss: 2.9187493324279785\n",
            "Epoch 187/200, Batch 16/45, Loss: 2.08585524559021\n",
            "Epoch 187/200, Batch 17/45, Loss: 1.8823524713516235\n",
            "Epoch 187/200, Batch 18/45, Loss: 2.0616159439086914\n",
            "Epoch 187/200, Batch 19/45, Loss: 2.111495018005371\n",
            "Epoch 187/200, Batch 20/45, Loss: 2.3676724433898926\n",
            "Epoch 187/200, Batch 21/45, Loss: 2.0007293224334717\n",
            "Epoch 187/200, Batch 22/45, Loss: 1.997182846069336\n",
            "Epoch 187/200, Batch 23/45, Loss: 2.1737325191497803\n",
            "Epoch 187/200, Batch 24/45, Loss: 1.7282230854034424\n",
            "Epoch 187/200, Batch 25/45, Loss: 2.0843820571899414\n",
            "Epoch 187/200, Batch 26/45, Loss: 2.250354051589966\n",
            "Epoch 187/200, Batch 27/45, Loss: 2.265542507171631\n",
            "Epoch 187/200, Batch 28/45, Loss: 3.540205717086792\n",
            "Epoch 187/200, Batch 29/45, Loss: 1.7562459707260132\n",
            "Epoch 187/200, Batch 30/45, Loss: 1.9972589015960693\n",
            "Epoch 187/200, Batch 31/45, Loss: 2.348559617996216\n",
            "Epoch 187/200, Batch 32/45, Loss: 2.014658212661743\n",
            "Epoch 187/200, Batch 33/45, Loss: 1.532068133354187\n",
            "Epoch 187/200, Batch 34/45, Loss: 2.11021089553833\n",
            "Epoch 187/200, Batch 35/45, Loss: 2.546534299850464\n",
            "Epoch 187/200, Batch 36/45, Loss: 1.8925857543945312\n",
            "Epoch 187/200, Batch 37/45, Loss: 2.857513666152954\n",
            "Epoch 187/200, Batch 38/45, Loss: 2.6658623218536377\n",
            "Epoch 187/200, Batch 39/45, Loss: 2.293382406234741\n",
            "Epoch 187/200, Batch 40/45, Loss: 1.0101375579833984\n",
            "Epoch 187/200, Batch 41/45, Loss: 2.5977630615234375\n",
            "Epoch 187/200, Batch 42/45, Loss: 2.075160264968872\n",
            "Epoch 187/200, Batch 43/45, Loss: 1.7109642028808594\n",
            "Epoch 187/200, Batch 44/45, Loss: 2.1870694160461426\n",
            "Epoch 187/200, Batch 45/45, Loss: 2.745288848876953\n",
            "Validating and Checkpointing!\n",
            "Model is not good (might be overfitting)! Current val MSE:  24.732275009155273 Best Val MSE:  24.62479865550995\n",
            "Epoch:  188 , Time Elapsed:  61.63478713830312  mins\n",
            "Epoch 188/200, Batch 1/45, Loss: 2.680485486984253\n",
            "Epoch 188/200, Batch 2/45, Loss: 1.5038241147994995\n",
            "Epoch 188/200, Batch 3/45, Loss: 2.6580123901367188\n",
            "Epoch 188/200, Batch 4/45, Loss: 2.080718994140625\n",
            "Epoch 188/200, Batch 5/45, Loss: 2.53657603263855\n",
            "Epoch 188/200, Batch 6/45, Loss: 2.032884120941162\n",
            "Epoch 188/200, Batch 7/45, Loss: 2.0731558799743652\n",
            "Epoch 188/200, Batch 8/45, Loss: 1.5860456228256226\n",
            "Epoch 188/200, Batch 9/45, Loss: 2.4213004112243652\n",
            "Epoch 188/200, Batch 10/45, Loss: 2.443159580230713\n",
            "Epoch 188/200, Batch 11/45, Loss: 2.046741485595703\n",
            "Epoch 188/200, Batch 12/45, Loss: 1.9358769655227661\n",
            "Epoch 188/200, Batch 13/45, Loss: 2.1233747005462646\n",
            "Epoch 188/200, Batch 14/45, Loss: 2.3785929679870605\n",
            "Epoch 188/200, Batch 15/45, Loss: 2.0573930740356445\n",
            "Epoch 188/200, Batch 16/45, Loss: 1.873739242553711\n",
            "Epoch 188/200, Batch 17/45, Loss: 2.6600208282470703\n",
            "Epoch 188/200, Batch 18/45, Loss: 2.3522424697875977\n",
            "Epoch 188/200, Batch 19/45, Loss: 2.040070056915283\n",
            "Epoch 188/200, Batch 20/45, Loss: 2.1648976802825928\n",
            "Epoch 188/200, Batch 21/45, Loss: 2.0594024658203125\n",
            "Epoch 188/200, Batch 22/45, Loss: 1.9759401082992554\n",
            "Epoch 188/200, Batch 23/45, Loss: 2.8812475204467773\n",
            "Epoch 188/200, Batch 24/45, Loss: 1.5417460203170776\n",
            "Epoch 188/200, Batch 25/45, Loss: 2.3585543632507324\n",
            "Epoch 188/200, Batch 26/45, Loss: 2.0607452392578125\n",
            "Epoch 188/200, Batch 27/45, Loss: 1.6562645435333252\n",
            "Epoch 188/200, Batch 28/45, Loss: 1.9136592149734497\n",
            "Epoch 188/200, Batch 29/45, Loss: 1.7124091386795044\n",
            "Epoch 188/200, Batch 30/45, Loss: 2.4008641242980957\n",
            "Epoch 188/200, Batch 31/45, Loss: 1.6780043840408325\n",
            "Epoch 188/200, Batch 32/45, Loss: 2.0797762870788574\n",
            "Epoch 188/200, Batch 33/45, Loss: 1.190953254699707\n",
            "Epoch 188/200, Batch 34/45, Loss: 2.100107192993164\n",
            "Epoch 188/200, Batch 35/45, Loss: 1.6911383867263794\n",
            "Epoch 188/200, Batch 36/45, Loss: 2.987689971923828\n",
            "Epoch 188/200, Batch 37/45, Loss: 1.614372968673706\n",
            "Epoch 188/200, Batch 38/45, Loss: 2.248230218887329\n",
            "Epoch 188/200, Batch 39/45, Loss: 2.3880319595336914\n",
            "Epoch 188/200, Batch 40/45, Loss: 1.7127487659454346\n",
            "Epoch 188/200, Batch 41/45, Loss: 2.07330584526062\n",
            "Epoch 188/200, Batch 42/45, Loss: 2.4597678184509277\n",
            "Epoch 188/200, Batch 43/45, Loss: 1.837871789932251\n",
            "Epoch 188/200, Batch 44/45, Loss: 1.788881540298462\n",
            "Epoch 188/200, Batch 45/45, Loss: 2.7894773483276367\n",
            "Validating and Checkpointing!\n",
            "Model is not good (might be overfitting)! Current val MSE:  26.134822607040405 Best Val MSE:  24.62479865550995\n",
            "Epoch:  189 , Time Elapsed:  61.95105924208959  mins\n",
            "Epoch 189/200, Batch 1/45, Loss: 1.631154179573059\n",
            "Epoch 189/200, Batch 2/45, Loss: 2.9062843322753906\n",
            "Epoch 189/200, Batch 3/45, Loss: 1.6052403450012207\n",
            "Epoch 189/200, Batch 4/45, Loss: 2.1694178581237793\n",
            "Epoch 189/200, Batch 5/45, Loss: 2.071941375732422\n",
            "Epoch 189/200, Batch 6/45, Loss: 1.7433857917785645\n",
            "Epoch 189/200, Batch 7/45, Loss: 2.312844753265381\n",
            "Epoch 189/200, Batch 8/45, Loss: 2.3960442543029785\n",
            "Epoch 189/200, Batch 9/45, Loss: 1.5903427600860596\n",
            "Epoch 189/200, Batch 10/45, Loss: 1.85111665725708\n",
            "Epoch 189/200, Batch 11/45, Loss: 2.0652294158935547\n",
            "Epoch 189/200, Batch 12/45, Loss: 1.997412085533142\n",
            "Epoch 189/200, Batch 13/45, Loss: 1.3836863040924072\n",
            "Epoch 189/200, Batch 14/45, Loss: 1.8329554796218872\n",
            "Epoch 189/200, Batch 15/45, Loss: 2.2429347038269043\n",
            "Epoch 189/200, Batch 16/45, Loss: 1.735591173171997\n",
            "Epoch 189/200, Batch 17/45, Loss: 1.9807802438735962\n",
            "Epoch 189/200, Batch 18/45, Loss: 2.6848578453063965\n",
            "Epoch 189/200, Batch 19/45, Loss: 2.5195322036743164\n",
            "Epoch 189/200, Batch 20/45, Loss: 1.8594738245010376\n",
            "Epoch 189/200, Batch 21/45, Loss: 3.931979179382324\n",
            "Epoch 189/200, Batch 22/45, Loss: 2.472501516342163\n",
            "Epoch 189/200, Batch 23/45, Loss: 1.9357738494873047\n",
            "Epoch 189/200, Batch 24/45, Loss: 2.1479332447052\n",
            "Epoch 189/200, Batch 25/45, Loss: 2.266740560531616\n",
            "Epoch 189/200, Batch 26/45, Loss: 2.4130654335021973\n",
            "Epoch 189/200, Batch 27/45, Loss: 1.6221340894699097\n",
            "Epoch 189/200, Batch 28/45, Loss: 2.1603784561157227\n",
            "Epoch 189/200, Batch 29/45, Loss: 2.3482396602630615\n",
            "Epoch 189/200, Batch 30/45, Loss: 1.680596113204956\n",
            "Epoch 189/200, Batch 31/45, Loss: 2.014359951019287\n",
            "Epoch 189/200, Batch 32/45, Loss: 2.0223681926727295\n",
            "Epoch 189/200, Batch 33/45, Loss: 2.0618557929992676\n",
            "Epoch 189/200, Batch 34/45, Loss: 2.509336471557617\n",
            "Epoch 189/200, Batch 35/45, Loss: 1.925011396408081\n",
            "Epoch 189/200, Batch 36/45, Loss: 1.355939507484436\n",
            "Epoch 189/200, Batch 37/45, Loss: 2.604459762573242\n",
            "Epoch 189/200, Batch 38/45, Loss: 3.0691943168640137\n",
            "Epoch 189/200, Batch 39/45, Loss: 2.027338981628418\n",
            "Epoch 189/200, Batch 40/45, Loss: 2.525068521499634\n",
            "Epoch 189/200, Batch 41/45, Loss: 1.8775655031204224\n",
            "Epoch 189/200, Batch 42/45, Loss: 1.8484776020050049\n",
            "Epoch 189/200, Batch 43/45, Loss: 1.6853528022766113\n",
            "Epoch 189/200, Batch 44/45, Loss: 1.836669921875\n",
            "Epoch 189/200, Batch 45/45, Loss: 2.656885862350464\n",
            "Validating and Checkpointing!\n",
            "Model is not good (might be overfitting)! Current val MSE:  25.18750298023224 Best Val MSE:  24.62479865550995\n",
            "Epoch:  190 , Time Elapsed:  62.299457029501596  mins\n",
            "Epoch 190/200, Batch 1/45, Loss: 2.0443663597106934\n",
            "Epoch 190/200, Batch 2/45, Loss: 2.0869598388671875\n",
            "Epoch 190/200, Batch 3/45, Loss: 2.8239858150482178\n",
            "Epoch 190/200, Batch 4/45, Loss: 1.7706732749938965\n",
            "Epoch 190/200, Batch 5/45, Loss: 2.3385446071624756\n",
            "Epoch 190/200, Batch 6/45, Loss: 2.8786802291870117\n",
            "Epoch 190/200, Batch 7/45, Loss: 2.204646587371826\n",
            "Epoch 190/200, Batch 8/45, Loss: 2.6795315742492676\n",
            "Epoch 190/200, Batch 9/45, Loss: 1.7466621398925781\n",
            "Epoch 190/200, Batch 10/45, Loss: 2.458922863006592\n",
            "Epoch 190/200, Batch 11/45, Loss: 2.665865421295166\n",
            "Epoch 190/200, Batch 12/45, Loss: 2.171194076538086\n",
            "Epoch 190/200, Batch 13/45, Loss: 2.7136006355285645\n",
            "Epoch 190/200, Batch 14/45, Loss: 2.1849915981292725\n",
            "Epoch 190/200, Batch 15/45, Loss: 2.2718393802642822\n",
            "Epoch 190/200, Batch 16/45, Loss: 1.8327645063400269\n",
            "Epoch 190/200, Batch 17/45, Loss: 1.9376481771469116\n",
            "Epoch 190/200, Batch 18/45, Loss: 2.4562478065490723\n",
            "Epoch 190/200, Batch 19/45, Loss: 2.338491201400757\n",
            "Epoch 190/200, Batch 20/45, Loss: 1.9346742630004883\n",
            "Epoch 190/200, Batch 21/45, Loss: 2.2035226821899414\n",
            "Epoch 190/200, Batch 22/45, Loss: 2.117774248123169\n",
            "Epoch 190/200, Batch 23/45, Loss: 2.180224657058716\n",
            "Epoch 190/200, Batch 24/45, Loss: 1.976553201675415\n",
            "Epoch 190/200, Batch 25/45, Loss: 1.2946280241012573\n",
            "Epoch 190/200, Batch 26/45, Loss: 1.7322136163711548\n",
            "Epoch 190/200, Batch 27/45, Loss: 1.926937460899353\n",
            "Epoch 190/200, Batch 28/45, Loss: 2.2737085819244385\n",
            "Epoch 190/200, Batch 29/45, Loss: 2.2506368160247803\n",
            "Epoch 190/200, Batch 30/45, Loss: 3.15335750579834\n",
            "Epoch 190/200, Batch 31/45, Loss: 2.3996150493621826\n",
            "Epoch 190/200, Batch 32/45, Loss: 2.506161689758301\n",
            "Epoch 190/200, Batch 33/45, Loss: 1.6165411472320557\n",
            "Epoch 190/200, Batch 34/45, Loss: 3.030344247817993\n",
            "Epoch 190/200, Batch 35/45, Loss: 2.458068370819092\n",
            "Epoch 190/200, Batch 36/45, Loss: 2.5610103607177734\n",
            "Epoch 190/200, Batch 37/45, Loss: 2.1687276363372803\n",
            "Epoch 190/200, Batch 38/45, Loss: 2.3753252029418945\n",
            "Epoch 190/200, Batch 39/45, Loss: 2.5525009632110596\n",
            "Epoch 190/200, Batch 40/45, Loss: 2.310690402984619\n",
            "Epoch 190/200, Batch 41/45, Loss: 2.508707046508789\n",
            "Epoch 190/200, Batch 42/45, Loss: 2.4466099739074707\n",
            "Epoch 190/200, Batch 43/45, Loss: 2.113412618637085\n",
            "Epoch 190/200, Batch 44/45, Loss: 2.3421573638916016\n",
            "Epoch 190/200, Batch 45/45, Loss: 2.52386736869812\n",
            "Validating and Checkpointing!\n",
            "Model is not good (might be overfitting)! Current val MSE:  24.822856664657593 Best Val MSE:  24.62479865550995\n",
            "Epoch:  191 , Time Elapsed:  62.613464025656384  mins\n",
            "Epoch 191/200, Batch 1/45, Loss: 2.1578993797302246\n",
            "Epoch 191/200, Batch 2/45, Loss: 2.103487730026245\n",
            "Epoch 191/200, Batch 3/45, Loss: 2.966303586959839\n",
            "Epoch 191/200, Batch 4/45, Loss: 2.2963671684265137\n",
            "Epoch 191/200, Batch 5/45, Loss: 2.0947911739349365\n",
            "Epoch 191/200, Batch 6/45, Loss: 2.145909309387207\n",
            "Epoch 191/200, Batch 7/45, Loss: 1.8166213035583496\n",
            "Epoch 191/200, Batch 8/45, Loss: 1.8899158239364624\n",
            "Epoch 191/200, Batch 9/45, Loss: 1.6740331649780273\n",
            "Epoch 191/200, Batch 10/45, Loss: 2.380833387374878\n",
            "Epoch 191/200, Batch 11/45, Loss: 1.632550597190857\n",
            "Epoch 191/200, Batch 12/45, Loss: 1.9869849681854248\n",
            "Epoch 191/200, Batch 13/45, Loss: 2.1806488037109375\n",
            "Epoch 191/200, Batch 14/45, Loss: 2.014392852783203\n",
            "Epoch 191/200, Batch 15/45, Loss: 1.9454518556594849\n",
            "Epoch 191/200, Batch 16/45, Loss: 2.448791265487671\n",
            "Epoch 191/200, Batch 17/45, Loss: 1.3264741897583008\n",
            "Epoch 191/200, Batch 18/45, Loss: 1.979823350906372\n",
            "Epoch 191/200, Batch 19/45, Loss: 2.050811290740967\n",
            "Epoch 191/200, Batch 20/45, Loss: 2.473414421081543\n",
            "Epoch 191/200, Batch 21/45, Loss: 2.4226081371307373\n",
            "Epoch 191/200, Batch 22/45, Loss: 2.471112012863159\n",
            "Epoch 191/200, Batch 23/45, Loss: 2.321429967880249\n",
            "Epoch 191/200, Batch 24/45, Loss: 1.8525199890136719\n",
            "Epoch 191/200, Batch 25/45, Loss: 1.7465062141418457\n",
            "Epoch 191/200, Batch 26/45, Loss: 2.0395450592041016\n",
            "Epoch 191/200, Batch 27/45, Loss: 2.0669455528259277\n",
            "Epoch 191/200, Batch 28/45, Loss: 1.5730876922607422\n",
            "Epoch 191/200, Batch 29/45, Loss: 1.9153677225112915\n",
            "Epoch 191/200, Batch 30/45, Loss: 2.2876904010772705\n",
            "Epoch 191/200, Batch 31/45, Loss: 2.1777265071868896\n",
            "Epoch 191/200, Batch 32/45, Loss: 1.895000696182251\n",
            "Epoch 191/200, Batch 33/45, Loss: 2.2682671546936035\n",
            "Epoch 191/200, Batch 34/45, Loss: 1.8125646114349365\n",
            "Epoch 191/200, Batch 35/45, Loss: 2.1042017936706543\n",
            "Epoch 191/200, Batch 36/45, Loss: 2.8152284622192383\n",
            "Epoch 191/200, Batch 37/45, Loss: 2.567531108856201\n",
            "Epoch 191/200, Batch 38/45, Loss: 1.9466702938079834\n",
            "Epoch 191/200, Batch 39/45, Loss: 2.020421266555786\n",
            "Epoch 191/200, Batch 40/45, Loss: 3.0598304271698\n",
            "Epoch 191/200, Batch 41/45, Loss: 1.7823058366775513\n",
            "Epoch 191/200, Batch 42/45, Loss: 1.5529464483261108\n",
            "Epoch 191/200, Batch 43/45, Loss: 2.1827735900878906\n",
            "Epoch 191/200, Batch 44/45, Loss: 2.4973933696746826\n",
            "Epoch 191/200, Batch 45/45, Loss: 2.9569787979125977\n",
            "Validating and Checkpointing!\n",
            "Model is not good (might be overfitting)! Current val MSE:  28.280972599983215 Best Val MSE:  24.62479865550995\n",
            "Epoch:  192 , Time Elapsed:  62.930864775180815  mins\n",
            "Epoch 192/200, Batch 1/45, Loss: 2.6296956539154053\n",
            "Epoch 192/200, Batch 2/45, Loss: 2.4012322425842285\n",
            "Epoch 192/200, Batch 3/45, Loss: 2.2023251056671143\n",
            "Epoch 192/200, Batch 4/45, Loss: 2.2548165321350098\n",
            "Epoch 192/200, Batch 5/45, Loss: 1.9148468971252441\n",
            "Epoch 192/200, Batch 6/45, Loss: 1.8716013431549072\n",
            "Epoch 192/200, Batch 7/45, Loss: 2.2200284004211426\n",
            "Epoch 192/200, Batch 8/45, Loss: 2.25900936126709\n",
            "Epoch 192/200, Batch 9/45, Loss: 1.7968980073928833\n",
            "Epoch 192/200, Batch 10/45, Loss: 2.5514628887176514\n",
            "Epoch 192/200, Batch 11/45, Loss: 2.0312442779541016\n",
            "Epoch 192/200, Batch 12/45, Loss: 1.9934453964233398\n",
            "Epoch 192/200, Batch 13/45, Loss: 2.060650587081909\n",
            "Epoch 192/200, Batch 14/45, Loss: 1.5348174571990967\n",
            "Epoch 192/200, Batch 15/45, Loss: 2.353365898132324\n",
            "Epoch 192/200, Batch 16/45, Loss: 2.2309632301330566\n",
            "Epoch 192/200, Batch 17/45, Loss: 2.378746509552002\n",
            "Epoch 192/200, Batch 18/45, Loss: 2.5586915016174316\n",
            "Epoch 192/200, Batch 19/45, Loss: 2.6650936603546143\n",
            "Epoch 192/200, Batch 20/45, Loss: 1.591793179512024\n",
            "Epoch 192/200, Batch 21/45, Loss: 2.657266139984131\n",
            "Epoch 192/200, Batch 22/45, Loss: 2.5967659950256348\n",
            "Epoch 192/200, Batch 23/45, Loss: 3.044482707977295\n",
            "Epoch 192/200, Batch 24/45, Loss: 1.8535267114639282\n",
            "Epoch 192/200, Batch 25/45, Loss: 2.7146992683410645\n",
            "Epoch 192/200, Batch 26/45, Loss: 1.4889847040176392\n",
            "Epoch 192/200, Batch 27/45, Loss: 2.5058670043945312\n",
            "Epoch 192/200, Batch 28/45, Loss: 1.5304694175720215\n",
            "Epoch 192/200, Batch 29/45, Loss: 2.3684630393981934\n",
            "Epoch 192/200, Batch 30/45, Loss: 2.2889599800109863\n",
            "Epoch 192/200, Batch 31/45, Loss: 1.7134077548980713\n",
            "Epoch 192/200, Batch 32/45, Loss: 2.213989019393921\n",
            "Epoch 192/200, Batch 33/45, Loss: 2.58646821975708\n",
            "Epoch 192/200, Batch 34/45, Loss: 1.8836419582366943\n",
            "Epoch 192/200, Batch 35/45, Loss: 1.7500908374786377\n",
            "Epoch 192/200, Batch 36/45, Loss: 2.584066867828369\n",
            "Epoch 192/200, Batch 37/45, Loss: 1.821573257446289\n",
            "Epoch 192/200, Batch 38/45, Loss: 1.9984709024429321\n",
            "Epoch 192/200, Batch 39/45, Loss: 2.4273533821105957\n",
            "Epoch 192/200, Batch 40/45, Loss: 1.6488609313964844\n",
            "Epoch 192/200, Batch 41/45, Loss: 1.8492162227630615\n",
            "Epoch 192/200, Batch 42/45, Loss: 1.9824355840682983\n",
            "Epoch 192/200, Batch 43/45, Loss: 2.1107101440429688\n",
            "Epoch 192/200, Batch 44/45, Loss: 2.653174877166748\n",
            "Epoch 192/200, Batch 45/45, Loss: 2.286900758743286\n",
            "Validating and Checkpointing!\n",
            "Model is not good (might be overfitting)! Current val MSE:  27.781620979309082 Best Val MSE:  24.62479865550995\n",
            "Epoch:  193 , Time Elapsed:  63.27935113112132  mins\n",
            "Epoch 193/200, Batch 1/45, Loss: 1.5640933513641357\n",
            "Epoch 193/200, Batch 2/45, Loss: 2.9560890197753906\n",
            "Epoch 193/200, Batch 3/45, Loss: 2.527329921722412\n",
            "Epoch 193/200, Batch 4/45, Loss: 1.6079434156417847\n",
            "Epoch 193/200, Batch 5/45, Loss: 1.880418062210083\n",
            "Epoch 193/200, Batch 6/45, Loss: 1.5909435749053955\n",
            "Epoch 193/200, Batch 7/45, Loss: 1.9941070079803467\n",
            "Epoch 193/200, Batch 8/45, Loss: 2.273919105529785\n",
            "Epoch 193/200, Batch 9/45, Loss: 2.175119400024414\n",
            "Epoch 193/200, Batch 10/45, Loss: 3.2373838424682617\n",
            "Epoch 193/200, Batch 11/45, Loss: 2.322939872741699\n",
            "Epoch 193/200, Batch 12/45, Loss: 1.9082083702087402\n",
            "Epoch 193/200, Batch 13/45, Loss: 2.3868556022644043\n",
            "Epoch 193/200, Batch 14/45, Loss: 1.3319153785705566\n",
            "Epoch 193/200, Batch 15/45, Loss: 2.424097776412964\n",
            "Epoch 193/200, Batch 16/45, Loss: 1.4320337772369385\n",
            "Epoch 193/200, Batch 17/45, Loss: 1.2168354988098145\n",
            "Epoch 193/200, Batch 18/45, Loss: 2.3452279567718506\n",
            "Epoch 193/200, Batch 19/45, Loss: 1.7891446352005005\n",
            "Epoch 193/200, Batch 20/45, Loss: 1.8379082679748535\n",
            "Epoch 193/200, Batch 21/45, Loss: 2.4173314571380615\n",
            "Epoch 193/200, Batch 22/45, Loss: 1.7616584300994873\n",
            "Epoch 193/200, Batch 23/45, Loss: 3.5725526809692383\n",
            "Epoch 193/200, Batch 24/45, Loss: 2.2174181938171387\n",
            "Epoch 193/200, Batch 25/45, Loss: 1.9186949729919434\n",
            "Epoch 193/200, Batch 26/45, Loss: 2.661935329437256\n",
            "Epoch 193/200, Batch 27/45, Loss: 2.2213706970214844\n",
            "Epoch 193/200, Batch 28/45, Loss: 1.6136826276779175\n",
            "Epoch 193/200, Batch 29/45, Loss: 2.7886667251586914\n",
            "Epoch 193/200, Batch 30/45, Loss: 2.2833499908447266\n",
            "Epoch 193/200, Batch 31/45, Loss: 1.5541932582855225\n",
            "Epoch 193/200, Batch 32/45, Loss: 2.0738468170166016\n",
            "Epoch 193/200, Batch 33/45, Loss: 1.54397714138031\n",
            "Epoch 193/200, Batch 34/45, Loss: 2.1141293048858643\n",
            "Epoch 193/200, Batch 35/45, Loss: 2.2147903442382812\n",
            "Epoch 193/200, Batch 36/45, Loss: 2.4750092029571533\n",
            "Epoch 193/200, Batch 37/45, Loss: 2.0351028442382812\n",
            "Epoch 193/200, Batch 38/45, Loss: 1.7462931871414185\n",
            "Epoch 193/200, Batch 39/45, Loss: 1.9027142524719238\n",
            "Epoch 193/200, Batch 40/45, Loss: 2.526323080062866\n",
            "Epoch 193/200, Batch 41/45, Loss: 2.054356336593628\n",
            "Epoch 193/200, Batch 42/45, Loss: 2.7416908740997314\n",
            "Epoch 193/200, Batch 43/45, Loss: 2.2907071113586426\n",
            "Epoch 193/200, Batch 44/45, Loss: 2.394773006439209\n",
            "Epoch 193/200, Batch 45/45, Loss: 1.8972012996673584\n",
            "Validating and Checkpointing!\n",
            "Model is not good (might be overfitting)! Current val MSE:  24.8489111661911 Best Val MSE:  24.62479865550995\n",
            "Epoch:  194 , Time Elapsed:  63.58936022520065  mins\n",
            "Epoch 194/200, Batch 1/45, Loss: 2.0099399089813232\n",
            "Epoch 194/200, Batch 2/45, Loss: 2.163465738296509\n",
            "Epoch 194/200, Batch 3/45, Loss: 2.041320562362671\n",
            "Epoch 194/200, Batch 4/45, Loss: 1.8199185132980347\n",
            "Epoch 194/200, Batch 5/45, Loss: 2.2818384170532227\n",
            "Epoch 194/200, Batch 6/45, Loss: 1.781158447265625\n",
            "Epoch 194/200, Batch 7/45, Loss: 2.3688721656799316\n",
            "Epoch 194/200, Batch 8/45, Loss: 1.8397774696350098\n",
            "Epoch 194/200, Batch 9/45, Loss: 1.675389289855957\n",
            "Epoch 194/200, Batch 10/45, Loss: 2.522500514984131\n",
            "Epoch 194/200, Batch 11/45, Loss: 1.6027634143829346\n",
            "Epoch 194/200, Batch 12/45, Loss: 2.221541404724121\n",
            "Epoch 194/200, Batch 13/45, Loss: 2.0210962295532227\n",
            "Epoch 194/200, Batch 14/45, Loss: 2.629117965698242\n",
            "Epoch 194/200, Batch 15/45, Loss: 1.9408903121948242\n",
            "Epoch 194/200, Batch 16/45, Loss: 2.4620938301086426\n",
            "Epoch 194/200, Batch 17/45, Loss: 1.546135663986206\n",
            "Epoch 194/200, Batch 18/45, Loss: 2.2113685607910156\n",
            "Epoch 194/200, Batch 19/45, Loss: 1.210340976715088\n",
            "Epoch 194/200, Batch 20/45, Loss: 2.9266061782836914\n",
            "Epoch 194/200, Batch 21/45, Loss: 2.069171905517578\n",
            "Epoch 194/200, Batch 22/45, Loss: 1.485326886177063\n",
            "Epoch 194/200, Batch 23/45, Loss: 1.751816749572754\n",
            "Epoch 194/200, Batch 24/45, Loss: 2.600001811981201\n",
            "Epoch 194/200, Batch 25/45, Loss: 2.030562162399292\n",
            "Epoch 194/200, Batch 26/45, Loss: 2.371603488922119\n",
            "Epoch 194/200, Batch 27/45, Loss: 2.0538296699523926\n",
            "Epoch 194/200, Batch 28/45, Loss: 1.8275153636932373\n",
            "Epoch 194/200, Batch 29/45, Loss: 2.310668468475342\n",
            "Epoch 194/200, Batch 30/45, Loss: 1.9924004077911377\n",
            "Epoch 194/200, Batch 31/45, Loss: 2.562997817993164\n",
            "Epoch 194/200, Batch 32/45, Loss: 2.304144859313965\n",
            "Epoch 194/200, Batch 33/45, Loss: 2.83180570602417\n",
            "Epoch 194/200, Batch 34/45, Loss: 2.003283739089966\n",
            "Epoch 194/200, Batch 35/45, Loss: 2.28084659576416\n",
            "Epoch 194/200, Batch 36/45, Loss: 2.2494516372680664\n",
            "Epoch 194/200, Batch 37/45, Loss: 1.7167999744415283\n",
            "Epoch 194/200, Batch 38/45, Loss: 2.264634609222412\n",
            "Epoch 194/200, Batch 39/45, Loss: 1.8948050737380981\n",
            "Epoch 194/200, Batch 40/45, Loss: 2.1714882850646973\n",
            "Epoch 194/200, Batch 41/45, Loss: 2.5081605911254883\n",
            "Epoch 194/200, Batch 42/45, Loss: 2.728998899459839\n",
            "Epoch 194/200, Batch 43/45, Loss: 2.2381691932678223\n",
            "Epoch 194/200, Batch 44/45, Loss: 2.277761459350586\n",
            "Epoch 194/200, Batch 45/45, Loss: 1.9431729316711426\n",
            "Validating and Checkpointing!\n",
            "Model is not good (might be overfitting)! Current val MSE:  24.968119382858276 Best Val MSE:  24.62479865550995\n",
            "Epoch:  195 , Time Elapsed:  63.9093572974205  mins\n",
            "Epoch 195/200, Batch 1/45, Loss: 2.4186081886291504\n",
            "Epoch 195/200, Batch 2/45, Loss: 2.5243513584136963\n",
            "Epoch 195/200, Batch 3/45, Loss: 2.071514129638672\n",
            "Epoch 195/200, Batch 4/45, Loss: 2.1640443801879883\n",
            "Epoch 195/200, Batch 5/45, Loss: 2.297053337097168\n",
            "Epoch 195/200, Batch 6/45, Loss: 2.7184879779815674\n",
            "Epoch 195/200, Batch 7/45, Loss: 1.820075273513794\n",
            "Epoch 195/200, Batch 8/45, Loss: 2.297405481338501\n",
            "Epoch 195/200, Batch 9/45, Loss: 2.3515520095825195\n",
            "Epoch 195/200, Batch 10/45, Loss: 2.183786392211914\n",
            "Epoch 195/200, Batch 11/45, Loss: 3.0142393112182617\n",
            "Epoch 195/200, Batch 12/45, Loss: 2.5569043159484863\n",
            "Epoch 195/200, Batch 13/45, Loss: 2.6083362102508545\n",
            "Epoch 195/200, Batch 14/45, Loss: 2.1295104026794434\n",
            "Epoch 195/200, Batch 15/45, Loss: 2.2649879455566406\n",
            "Epoch 195/200, Batch 16/45, Loss: 1.4026254415512085\n",
            "Epoch 195/200, Batch 17/45, Loss: 2.453479290008545\n",
            "Epoch 195/200, Batch 18/45, Loss: 2.3433141708374023\n",
            "Epoch 195/200, Batch 19/45, Loss: 2.3440585136413574\n",
            "Epoch 195/200, Batch 20/45, Loss: 1.812402367591858\n",
            "Epoch 195/200, Batch 21/45, Loss: 1.5811116695404053\n",
            "Epoch 195/200, Batch 22/45, Loss: 3.1121621131896973\n",
            "Epoch 195/200, Batch 23/45, Loss: 1.8687961101531982\n",
            "Epoch 195/200, Batch 24/45, Loss: 2.141568660736084\n",
            "Epoch 195/200, Batch 25/45, Loss: 1.7811496257781982\n",
            "Epoch 195/200, Batch 26/45, Loss: 2.469533681869507\n",
            "Epoch 195/200, Batch 27/45, Loss: 2.460452079772949\n",
            "Epoch 195/200, Batch 28/45, Loss: 2.813300371170044\n",
            "Epoch 195/200, Batch 29/45, Loss: 2.2991788387298584\n",
            "Epoch 195/200, Batch 30/45, Loss: 2.5539519786834717\n",
            "Epoch 195/200, Batch 31/45, Loss: 2.069732904434204\n",
            "Epoch 195/200, Batch 32/45, Loss: 1.9716826677322388\n",
            "Epoch 195/200, Batch 33/45, Loss: 1.8491041660308838\n",
            "Epoch 195/200, Batch 34/45, Loss: 1.8101751804351807\n",
            "Epoch 195/200, Batch 35/45, Loss: 2.5990138053894043\n",
            "Epoch 195/200, Batch 36/45, Loss: 2.389735221862793\n",
            "Epoch 195/200, Batch 37/45, Loss: 2.204070806503296\n",
            "Epoch 195/200, Batch 38/45, Loss: 2.614598274230957\n",
            "Epoch 195/200, Batch 39/45, Loss: 1.9119443893432617\n",
            "Epoch 195/200, Batch 40/45, Loss: 2.216493844985962\n",
            "Epoch 195/200, Batch 41/45, Loss: 2.6450257301330566\n",
            "Epoch 195/200, Batch 42/45, Loss: 2.7067060470581055\n",
            "Epoch 195/200, Batch 43/45, Loss: 1.5775411128997803\n",
            "Epoch 195/200, Batch 44/45, Loss: 2.4537441730499268\n",
            "Epoch 195/200, Batch 45/45, Loss: 1.7556073665618896\n",
            "Validating and Checkpointing!\n",
            "Model is not good (might be overfitting)! Current val MSE:  25.900529623031616 Best Val MSE:  24.62479865550995\n",
            "Epoch:  196 , Time Elapsed:  64.25459558169047  mins\n",
            "Epoch 196/200, Batch 1/45, Loss: 2.359614849090576\n",
            "Epoch 196/200, Batch 2/45, Loss: 1.5785675048828125\n",
            "Epoch 196/200, Batch 3/45, Loss: 2.195611000061035\n",
            "Epoch 196/200, Batch 4/45, Loss: 2.042896270751953\n",
            "Epoch 196/200, Batch 5/45, Loss: 2.42105770111084\n",
            "Epoch 196/200, Batch 6/45, Loss: 1.6812827587127686\n",
            "Epoch 196/200, Batch 7/45, Loss: 2.495983600616455\n",
            "Epoch 196/200, Batch 8/45, Loss: 1.9894211292266846\n",
            "Epoch 196/200, Batch 9/45, Loss: 2.443228006362915\n",
            "Epoch 196/200, Batch 10/45, Loss: 1.5134363174438477\n",
            "Epoch 196/200, Batch 11/45, Loss: 2.3758509159088135\n",
            "Epoch 196/200, Batch 12/45, Loss: 2.2136406898498535\n",
            "Epoch 196/200, Batch 13/45, Loss: 1.8262016773223877\n",
            "Epoch 196/200, Batch 14/45, Loss: 1.9863348007202148\n",
            "Epoch 196/200, Batch 15/45, Loss: 1.6421709060668945\n",
            "Epoch 196/200, Batch 16/45, Loss: 1.8265082836151123\n",
            "Epoch 196/200, Batch 17/45, Loss: 1.6535378694534302\n",
            "Epoch 196/200, Batch 18/45, Loss: 2.182436227798462\n",
            "Epoch 196/200, Batch 19/45, Loss: 1.7820723056793213\n",
            "Epoch 196/200, Batch 20/45, Loss: 2.711479663848877\n",
            "Epoch 196/200, Batch 21/45, Loss: 2.433234691619873\n",
            "Epoch 196/200, Batch 22/45, Loss: 1.9132930040359497\n",
            "Epoch 196/200, Batch 23/45, Loss: 1.9763414859771729\n",
            "Epoch 196/200, Batch 24/45, Loss: 2.4255523681640625\n",
            "Epoch 196/200, Batch 25/45, Loss: 3.876943826675415\n",
            "Epoch 196/200, Batch 26/45, Loss: 2.26239275932312\n",
            "Epoch 196/200, Batch 27/45, Loss: 1.786485195159912\n",
            "Epoch 196/200, Batch 28/45, Loss: 2.2296862602233887\n",
            "Epoch 196/200, Batch 29/45, Loss: 2.106410503387451\n",
            "Epoch 196/200, Batch 30/45, Loss: 1.7210184335708618\n",
            "Epoch 196/200, Batch 31/45, Loss: 1.7588691711425781\n",
            "Epoch 196/200, Batch 32/45, Loss: 3.878410577774048\n",
            "Epoch 196/200, Batch 33/45, Loss: 2.189197063446045\n",
            "Epoch 196/200, Batch 34/45, Loss: 2.6588006019592285\n",
            "Epoch 196/200, Batch 35/45, Loss: 2.3154735565185547\n",
            "Epoch 196/200, Batch 36/45, Loss: 2.0935986042022705\n",
            "Epoch 196/200, Batch 37/45, Loss: 2.022491455078125\n",
            "Epoch 196/200, Batch 38/45, Loss: 1.7755531072616577\n",
            "Epoch 196/200, Batch 39/45, Loss: 2.2330172061920166\n",
            "Epoch 196/200, Batch 40/45, Loss: 1.9846763610839844\n",
            "Epoch 196/200, Batch 41/45, Loss: 2.635169744491577\n",
            "Epoch 196/200, Batch 42/45, Loss: 2.311915636062622\n",
            "Epoch 196/200, Batch 43/45, Loss: 1.63775634765625\n",
            "Epoch 196/200, Batch 44/45, Loss: 1.6430368423461914\n",
            "Epoch 196/200, Batch 45/45, Loss: 2.012009620666504\n",
            "Validating and Checkpointing!\n",
            "Model is not good (might be overfitting)! Current val MSE:  28.03400707244873 Best Val MSE:  24.62479865550995\n",
            "Epoch:  197 , Time Elapsed:  64.57562271753947  mins\n",
            "Epoch 197/200, Batch 1/45, Loss: 1.547654628753662\n",
            "Epoch 197/200, Batch 2/45, Loss: 2.1481151580810547\n",
            "Epoch 197/200, Batch 3/45, Loss: 2.4917795658111572\n",
            "Epoch 197/200, Batch 4/45, Loss: 2.2689805030822754\n",
            "Epoch 197/200, Batch 5/45, Loss: 2.595937728881836\n",
            "Epoch 197/200, Batch 6/45, Loss: 1.1854193210601807\n",
            "Epoch 197/200, Batch 7/45, Loss: 2.5739197731018066\n",
            "Epoch 197/200, Batch 8/45, Loss: 2.593292713165283\n",
            "Epoch 197/200, Batch 9/45, Loss: 2.348097085952759\n",
            "Epoch 197/200, Batch 10/45, Loss: 2.75331449508667\n",
            "Epoch 197/200, Batch 11/45, Loss: 1.3852100372314453\n",
            "Epoch 197/200, Batch 12/45, Loss: 2.3740007877349854\n",
            "Epoch 197/200, Batch 13/45, Loss: 2.6822264194488525\n",
            "Epoch 197/200, Batch 14/45, Loss: 2.075246572494507\n",
            "Epoch 197/200, Batch 15/45, Loss: 2.0802817344665527\n",
            "Epoch 197/200, Batch 16/45, Loss: 2.8469467163085938\n",
            "Epoch 197/200, Batch 17/45, Loss: 2.5571722984313965\n",
            "Epoch 197/200, Batch 18/45, Loss: 3.1344070434570312\n",
            "Epoch 197/200, Batch 19/45, Loss: 1.7740440368652344\n",
            "Epoch 197/200, Batch 20/45, Loss: 1.8271746635437012\n",
            "Epoch 197/200, Batch 21/45, Loss: 1.578613519668579\n",
            "Epoch 197/200, Batch 22/45, Loss: 2.2261133193969727\n",
            "Epoch 197/200, Batch 23/45, Loss: 2.398963451385498\n",
            "Epoch 197/200, Batch 24/45, Loss: 2.3723201751708984\n",
            "Epoch 197/200, Batch 25/45, Loss: 2.1066455841064453\n",
            "Epoch 197/200, Batch 26/45, Loss: 1.9457015991210938\n",
            "Epoch 197/200, Batch 27/45, Loss: 2.0937156677246094\n",
            "Epoch 197/200, Batch 28/45, Loss: 1.9679341316223145\n",
            "Epoch 197/200, Batch 29/45, Loss: 1.6510021686553955\n",
            "Epoch 197/200, Batch 30/45, Loss: 2.83477783203125\n",
            "Epoch 197/200, Batch 31/45, Loss: 1.6821379661560059\n",
            "Epoch 197/200, Batch 32/45, Loss: 1.9440494775772095\n",
            "Epoch 197/200, Batch 33/45, Loss: 2.2255992889404297\n",
            "Epoch 197/200, Batch 34/45, Loss: 2.159031391143799\n",
            "Epoch 197/200, Batch 35/45, Loss: 2.0582494735717773\n",
            "Epoch 197/200, Batch 36/45, Loss: 1.3267697095870972\n",
            "Epoch 197/200, Batch 37/45, Loss: 2.425334930419922\n",
            "Epoch 197/200, Batch 38/45, Loss: 2.1285505294799805\n",
            "Epoch 197/200, Batch 39/45, Loss: 2.1459951400756836\n",
            "Epoch 197/200, Batch 40/45, Loss: 2.1975317001342773\n",
            "Epoch 197/200, Batch 41/45, Loss: 1.885352373123169\n",
            "Epoch 197/200, Batch 42/45, Loss: 2.277935743331909\n",
            "Epoch 197/200, Batch 43/45, Loss: 1.9318783283233643\n",
            "Epoch 197/200, Batch 44/45, Loss: 1.9969135522842407\n",
            "Epoch 197/200, Batch 45/45, Loss: 1.9038176536560059\n",
            "Validating and Checkpointing!\n",
            "Model is not good (might be overfitting)! Current val MSE:  28.126105904579163 Best Val MSE:  24.62479865550995\n",
            "Epoch:  198 , Time Elapsed:  64.90236285130183  mins\n",
            "Epoch 198/200, Batch 1/45, Loss: 2.193126678466797\n",
            "Epoch 198/200, Batch 2/45, Loss: 2.642554998397827\n",
            "Epoch 198/200, Batch 3/45, Loss: 1.9145493507385254\n",
            "Epoch 198/200, Batch 4/45, Loss: 2.030294179916382\n",
            "Epoch 198/200, Batch 5/45, Loss: 2.1391868591308594\n",
            "Epoch 198/200, Batch 6/45, Loss: 2.7400927543640137\n",
            "Epoch 198/200, Batch 7/45, Loss: 2.2537832260131836\n",
            "Epoch 198/200, Batch 8/45, Loss: 2.3430838584899902\n",
            "Epoch 198/200, Batch 9/45, Loss: 2.24599027633667\n",
            "Epoch 198/200, Batch 10/45, Loss: 1.5504690408706665\n",
            "Epoch 198/200, Batch 11/45, Loss: 1.8552842140197754\n",
            "Epoch 198/200, Batch 12/45, Loss: 2.124640941619873\n",
            "Epoch 198/200, Batch 13/45, Loss: 2.803759813308716\n",
            "Epoch 198/200, Batch 14/45, Loss: 2.355459213256836\n",
            "Epoch 198/200, Batch 15/45, Loss: 2.5028834342956543\n",
            "Epoch 198/200, Batch 16/45, Loss: 2.4153192043304443\n",
            "Epoch 198/200, Batch 17/45, Loss: 2.213629961013794\n",
            "Epoch 198/200, Batch 18/45, Loss: 2.1889545917510986\n",
            "Epoch 198/200, Batch 19/45, Loss: 1.9875248670578003\n",
            "Epoch 198/200, Batch 20/45, Loss: 1.8142013549804688\n",
            "Epoch 198/200, Batch 21/45, Loss: 2.1787807941436768\n",
            "Epoch 198/200, Batch 22/45, Loss: 2.1198925971984863\n",
            "Epoch 198/200, Batch 23/45, Loss: 2.0958776473999023\n",
            "Epoch 198/200, Batch 24/45, Loss: 2.2806215286254883\n",
            "Epoch 198/200, Batch 25/45, Loss: 1.426783800125122\n",
            "Epoch 198/200, Batch 26/45, Loss: 2.208984375\n",
            "Epoch 198/200, Batch 27/45, Loss: 2.4453542232513428\n",
            "Epoch 198/200, Batch 28/45, Loss: 1.9725873470306396\n",
            "Epoch 198/200, Batch 29/45, Loss: 2.57999324798584\n",
            "Epoch 198/200, Batch 30/45, Loss: 2.869297742843628\n",
            "Epoch 198/200, Batch 31/45, Loss: 2.0584776401519775\n",
            "Epoch 198/200, Batch 32/45, Loss: 2.0913467407226562\n",
            "Epoch 198/200, Batch 33/45, Loss: 2.169602394104004\n",
            "Epoch 198/200, Batch 34/45, Loss: 2.283548355102539\n",
            "Epoch 198/200, Batch 35/45, Loss: 2.724581718444824\n",
            "Epoch 198/200, Batch 36/45, Loss: 2.464280843734741\n",
            "Epoch 198/200, Batch 37/45, Loss: 2.0960967540740967\n",
            "Epoch 198/200, Batch 38/45, Loss: 2.4160916805267334\n",
            "Epoch 198/200, Batch 39/45, Loss: 1.7092149257659912\n",
            "Epoch 198/200, Batch 40/45, Loss: 1.786767601966858\n",
            "Epoch 198/200, Batch 41/45, Loss: 2.6839418411254883\n",
            "Epoch 198/200, Batch 42/45, Loss: 1.6378138065338135\n",
            "Epoch 198/200, Batch 43/45, Loss: 2.913015365600586\n",
            "Epoch 198/200, Batch 44/45, Loss: 2.114719867706299\n",
            "Epoch 198/200, Batch 45/45, Loss: 2.5628859996795654\n",
            "Validating and Checkpointing!\n",
            "Model is not good (might be overfitting)! Current val MSE:  25.63526725769043 Best Val MSE:  24.62479865550995\n",
            "Epoch:  199 , Time Elapsed:  65.22749993006389  mins\n",
            "Epoch 199/200, Batch 1/45, Loss: 1.7009671926498413\n",
            "Epoch 199/200, Batch 2/45, Loss: 2.087629556655884\n",
            "Epoch 199/200, Batch 3/45, Loss: 1.4053852558135986\n",
            "Epoch 199/200, Batch 4/45, Loss: 1.833866834640503\n",
            "Epoch 199/200, Batch 5/45, Loss: 2.3387930393218994\n",
            "Epoch 199/200, Batch 6/45, Loss: 1.591352105140686\n",
            "Epoch 199/200, Batch 7/45, Loss: 2.6179423332214355\n",
            "Epoch 199/200, Batch 8/45, Loss: 2.281055212020874\n",
            "Epoch 199/200, Batch 9/45, Loss: 2.0404515266418457\n",
            "Epoch 199/200, Batch 10/45, Loss: 1.6230965852737427\n",
            "Epoch 199/200, Batch 11/45, Loss: 1.8630670309066772\n",
            "Epoch 199/200, Batch 12/45, Loss: 1.9629995822906494\n",
            "Epoch 199/200, Batch 13/45, Loss: 2.0296802520751953\n",
            "Epoch 199/200, Batch 14/45, Loss: 2.3669469356536865\n",
            "Epoch 199/200, Batch 15/45, Loss: 1.9605821371078491\n",
            "Epoch 199/200, Batch 16/45, Loss: 2.2847421169281006\n",
            "Epoch 199/200, Batch 17/45, Loss: 1.6504038572311401\n",
            "Epoch 199/200, Batch 18/45, Loss: 2.0047173500061035\n",
            "Epoch 199/200, Batch 19/45, Loss: 1.8728666305541992\n",
            "Epoch 199/200, Batch 20/45, Loss: 2.271143913269043\n",
            "Epoch 199/200, Batch 21/45, Loss: 2.8735976219177246\n",
            "Epoch 199/200, Batch 22/45, Loss: 1.8892910480499268\n",
            "Epoch 199/200, Batch 23/45, Loss: 1.9771642684936523\n",
            "Epoch 199/200, Batch 24/45, Loss: 2.861785888671875\n",
            "Epoch 199/200, Batch 25/45, Loss: 1.732049822807312\n",
            "Epoch 199/200, Batch 26/45, Loss: 2.7402291297912598\n",
            "Epoch 199/200, Batch 27/45, Loss: 1.5127687454223633\n",
            "Epoch 199/200, Batch 28/45, Loss: 2.3081023693084717\n",
            "Epoch 199/200, Batch 29/45, Loss: 2.3084490299224854\n",
            "Epoch 199/200, Batch 30/45, Loss: 2.0959811210632324\n",
            "Epoch 199/200, Batch 31/45, Loss: 2.03065824508667\n",
            "Epoch 199/200, Batch 32/45, Loss: 1.9526519775390625\n",
            "Epoch 199/200, Batch 33/45, Loss: 1.8525574207305908\n",
            "Epoch 199/200, Batch 34/45, Loss: 2.122396945953369\n",
            "Epoch 199/200, Batch 35/45, Loss: 1.998762845993042\n",
            "Epoch 199/200, Batch 36/45, Loss: 2.148033380508423\n",
            "Epoch 199/200, Batch 37/45, Loss: 1.6233123540878296\n",
            "Epoch 199/200, Batch 38/45, Loss: 2.3263540267944336\n",
            "Epoch 199/200, Batch 39/45, Loss: 2.1012701988220215\n",
            "Epoch 199/200, Batch 40/45, Loss: 2.2167906761169434\n",
            "Epoch 199/200, Batch 41/45, Loss: 2.20198917388916\n",
            "Epoch 199/200, Batch 42/45, Loss: 2.0996642112731934\n",
            "Epoch 199/200, Batch 43/45, Loss: 2.2487430572509766\n",
            "Epoch 199/200, Batch 44/45, Loss: 1.9232348203659058\n",
            "Epoch 199/200, Batch 45/45, Loss: 2.457155227661133\n",
            "Validating and Checkpointing!\n",
            "Model is not good (might be overfitting)! Current val MSE:  47.44078290462494 Best Val MSE:  24.62479865550995\n",
            "Epoch:  200 , Time Elapsed:  65.5490002711614  mins\n",
            "Epoch 200/200, Batch 1/45, Loss: 1.4886054992675781\n",
            "Epoch 200/200, Batch 2/45, Loss: 2.6819581985473633\n",
            "Epoch 200/200, Batch 3/45, Loss: 2.909426689147949\n",
            "Epoch 200/200, Batch 4/45, Loss: 1.720449686050415\n",
            "Epoch 200/200, Batch 5/45, Loss: 1.8977863788604736\n",
            "Epoch 200/200, Batch 6/45, Loss: 2.8131661415100098\n",
            "Epoch 200/200, Batch 7/45, Loss: 2.6883978843688965\n",
            "Epoch 200/200, Batch 8/45, Loss: 2.61745023727417\n",
            "Epoch 200/200, Batch 9/45, Loss: 2.310272216796875\n",
            "Epoch 200/200, Batch 10/45, Loss: 2.6740832328796387\n",
            "Epoch 200/200, Batch 11/45, Loss: 1.7798032760620117\n",
            "Epoch 200/200, Batch 12/45, Loss: 2.5995140075683594\n",
            "Epoch 200/200, Batch 13/45, Loss: 1.8955384492874146\n",
            "Epoch 200/200, Batch 14/45, Loss: 2.168128728866577\n",
            "Epoch 200/200, Batch 15/45, Loss: 2.1376514434814453\n",
            "Epoch 200/200, Batch 16/45, Loss: 1.8054544925689697\n",
            "Epoch 200/200, Batch 17/45, Loss: 2.3051841259002686\n",
            "Epoch 200/200, Batch 18/45, Loss: 1.613457441329956\n",
            "Epoch 200/200, Batch 19/45, Loss: 1.8248655796051025\n",
            "Epoch 200/200, Batch 20/45, Loss: 1.8782979249954224\n",
            "Epoch 200/200, Batch 21/45, Loss: 2.141923189163208\n",
            "Epoch 200/200, Batch 22/45, Loss: 1.978611946105957\n",
            "Epoch 200/200, Batch 23/45, Loss: 2.87315034866333\n",
            "Epoch 200/200, Batch 24/45, Loss: 2.2528672218322754\n",
            "Epoch 200/200, Batch 25/45, Loss: 2.8638060092926025\n",
            "Epoch 200/200, Batch 26/45, Loss: 1.7209733724594116\n",
            "Epoch 200/200, Batch 27/45, Loss: 2.2095131874084473\n",
            "Epoch 200/200, Batch 28/45, Loss: 2.825894594192505\n",
            "Epoch 200/200, Batch 29/45, Loss: 1.702901005744934\n",
            "Epoch 200/200, Batch 30/45, Loss: 1.5737618207931519\n",
            "Epoch 200/200, Batch 31/45, Loss: 1.8985564708709717\n",
            "Epoch 200/200, Batch 32/45, Loss: 1.9453366994857788\n",
            "Epoch 200/200, Batch 33/45, Loss: 2.6227331161499023\n",
            "Epoch 200/200, Batch 34/45, Loss: 1.5583361387252808\n",
            "Epoch 200/200, Batch 35/45, Loss: 1.6348719596862793\n",
            "Epoch 200/200, Batch 36/45, Loss: 2.2264111042022705\n",
            "Epoch 200/200, Batch 37/45, Loss: 1.8883110284805298\n",
            "Epoch 200/200, Batch 38/45, Loss: 1.5874977111816406\n",
            "Epoch 200/200, Batch 39/45, Loss: 1.7166054248809814\n",
            "Epoch 200/200, Batch 40/45, Loss: 1.664881944656372\n",
            "Epoch 200/200, Batch 41/45, Loss: 1.0732659101486206\n",
            "Epoch 200/200, Batch 42/45, Loss: 2.8005635738372803\n",
            "Epoch 200/200, Batch 43/45, Loss: 2.6069419384002686\n",
            "Epoch 200/200, Batch 44/45, Loss: 1.9518351554870605\n",
            "Epoch 200/200, Batch 45/45, Loss: 2.0248167514801025\n",
            "Validating and Checkpointing!\n",
            "Model is not good (might be overfitting)! Current val MSE:  25.089511513710022 Best Val MSE:  24.62479865550995\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LNiS2urzSs2j"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}