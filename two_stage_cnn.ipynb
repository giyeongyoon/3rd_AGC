{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPnFH3iykOAv4tEFkWr1fX0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/giyeongyoon/3rd_AGC/blob/master/two_stage_cnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "참고문헌\n",
        "\n",
        "*   Estimation of Greenhouse Lettuce Growth Indices Based on a\n",
        "Two-Stage CNN Using RGB-D Images\n",
        "[Data Availability](https://data.4tu.nl/articles/dataset/3rd_Autonomous_Greenhouse_Challenge_Online_Challenge_Lettuce_Images/15023088/1)"
      ],
      "metadata": {
        "id": "3bkKclafAbdy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install albumentations==1.1.0\n",
        "!pip install agml"
      ],
      "metadata": {
        "id": "XLaGOhdVUlfi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import libraries"
      ],
      "metadata": {
        "id": "ALohVFC0auKG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torchvision import models, transforms\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "import albumentations as A\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "P_2XWYP4Zv5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download 2021 Autonomous Greenhouse Challenge dataset"
      ],
      "metadata": {
        "id": "abOZYP-oaxYR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import agml\n",
        "loader = agml.data.AgMLDataLoader('autonomous_greenhouse_regression', dataset_path = './')"
      ],
      "metadata": {
        "id": "CVAC52rkZqpt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define data and output directories"
      ],
      "metadata": {
        "id": "lQsG8Ypqa50C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sav_dir='model_weights/'\n",
        "if not os.path.exists(sav_dir):\n",
        "    os.mkdir(sav_dir)\n",
        "# Comment these two lines and uncomment the next two if you've already croppped the images to another directory\n",
        "RGB_Data_Dir   = './autonomous_greenhouse_regression/images/'\n",
        "Depth_Data_Dir = './autonomous_greenhouse_regression/depth_images/'\n",
        "\n",
        "\n",
        "# RGB_Data_Dir='./autonomous_greenhouse_regression/cropped_images/'\n",
        "# Depth_Data_Dir='./autonomous_greenhouse_regression/cropped_depth_images/'\n",
        "\n",
        "\n",
        "JSON_Files_Dir = './autonomous_greenhouse_regression/annotations.json'"
      ],
      "metadata": {
        "id": "wTyEOaaMaRVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Crop"
      ],
      "metadata": {
        "id": "kgPu9nJXbaBc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "min_x=650\n",
        "max_x=1450\n",
        "min_y=200\n",
        "max_y=900\n",
        "cropped_img_dir='./autonomous_greenhouse_regression/cropped_images/'\n",
        "\n",
        "cropped_depth_img_dir='./autonomous_greenhouse_regression/cropped_depth_images/'\n",
        "\n",
        "if not os.path.exists(cropped_img_dir):\n",
        "    os.mkdir(cropped_img_dir)\n",
        "\n",
        "if not os.path.exists(cropped_depth_img_dir):\n",
        "    os.mkdir(cropped_depth_img_dir)\n",
        "\n",
        "for im in os.listdir(RGB_Data_Dir):\n",
        "    img = cv2.imread(RGB_Data_Dir+im)\n",
        "    crop_img = img[min_y:max_y,min_x:max_x]\n",
        "    cv2.imwrite(cropped_img_dir+im, crop_img)\n",
        "\n",
        "for depth_im in os.listdir(Depth_Data_Dir):\n",
        "    depth_img = cv2.imread(Depth_Data_Dir+depth_im, 0)\n",
        "    crop_depth_img = depth_img[min_y:max_y,min_x:max_x]\n",
        "    cv2.imwrite(cropped_depth_img_dir+depth_im, crop_depth_img)\n",
        "\n",
        "RGB_Data_Dir   = cropped_img_dir\n",
        "Depth_Data_Dir = cropped_depth_img_dir"
      ],
      "metadata": {
        "id": "eAraU7jEaWdI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the targets"
      ],
      "metadata": {
        "id": "DYiZ1l5Gbn38"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# df= pd.read_json(JSON_Files_Dir)\n",
        "# row = df.iloc[0]\n",
        "# print(list(row['outputs']['regression']))\n",
        "# print(list(row['outputs']['regression'].values()))"
      ],
      "metadata": {
        "id": "D3jGVvR3nHr6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create PyTorch dataset, create PyTorch dataloader, and split train/val/test"
      ],
      "metadata": {
        "id": "xLkRE7Dz9cNF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "split_seed = 12\n",
        "num_epochs = 400"
      ],
      "metadata": {
        "id": "kNmIXaMzqr4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GreenhouseDataset(Dataset):\n",
        "    def __init__(self, rgb_dir, d_dir, jsonfile_dir, rgb_transforms=None, d_transforms=None):\n",
        "\n",
        "        self.df= pd.read_json(jsonfile_dir)\n",
        "        # flatten_json is a custom function to flat the nested json files!\n",
        "\n",
        "        self.rgb_transforms = rgb_transforms\n",
        "        self.d_transforms = d_transforms\n",
        "        self.rgb_dir = rgb_dir\n",
        "        self.d_dir = d_dir\n",
        "        self.num_outputs = len(self.df.iloc[0]['outputs']['regression'])\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # load images\n",
        "        row=self.df.iloc[idx]\n",
        "\n",
        "        rgb = plt.imread(self.rgb_dir+row['image'])\n",
        "        depth = plt.imread(self.d_dir+row['depth_image'])\n",
        "        depth = np.expand_dims(depth, 2)\n",
        "\n",
        "        target = list(row['outputs']['regression'].values())\n",
        "\n",
        "        #make sure your img and mask array are in this format before passing into albumentations transforms, img.shape=[H, W, C]\n",
        "        if self.rgb_transforms is not None:\n",
        "            aug_rgb = self.rgb_transforms(image=rgb)\n",
        "            rgb = aug_rgb['image']\n",
        "        elif self.d_transforms is not None:\n",
        "            aug_depth = self.d_transforms(image=depth)\n",
        "            depth = aug_depth['image']\n",
        "\n",
        "        rgb = np.transpose(rgb, (2,0,1))\n",
        "        depth = np.transpose(depth, (2,0,1))\n",
        "\n",
        "        #pytorch wants a different format for the image ([C, H, W])\n",
        "        rgb = torch.as_tensor(rgb, dtype=torch.float32)\n",
        "        depth = torch.as_tensor(depth, dtype=torch.float32)\n",
        "        target=torch.as_tensor(target, dtype=torch.float32)\n",
        "\n",
        "        return rgb, depth, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)"
      ],
      "metadata": {
        "id": "oSuPJTaesS2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## FIGURE OUT HOW TO CROP ALL THE IMAGES TO GET RID OF EXTRANIOUS PIXELS\n",
        "def get_transforms(train, means, stds):\n",
        "    if train:\n",
        "        transforms = A.Compose([\n",
        "        # A.Crop(x_min=650, y_min=200, x_max=1450, y_max=900, always_apply=False, p=1.0),\n",
        "        A.Flip(p=0.5),\n",
        "        A.ShiftScaleRotate(always_apply=False, p=0.5, shift_limit=(-0.06, 0.06), scale_limit=(-0.1, 0.1), rotate_limit=(-5, 5), interpolation=0, border_mode=0, value=means, mask_value=None),\n",
        "        A.Normalize(mean=means, std=stds, max_pixel_value=1.0, always_apply=False, p=1.0)\n",
        "        ])\n",
        "    else:\n",
        "        transforms =  A.Compose([\n",
        "        # A.Crop(x_min=650, y_min=200, x_max=1450, y_max=900, always_apply=False, p=1.0),\n",
        "        A.Normalize(mean=means, std=stds, max_pixel_value=1.0, always_apply=False, p=1.0)\n",
        "        ])\n",
        "    return transforms"
      ],
      "metadata": {
        "id": "8lbDBskoAeTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the PyTorch datalaoder the autonomous greenhouse dataset.\n",
        "dataset = GreenhouseDataset(rgb_dir = RGB_Data_Dir,\n",
        "                            d_dir = Depth_Data_Dir,\n",
        "                            jsonfile_dir = JSON_Files_Dir,\n",
        "                            rgb_transforms = get_transforms(train=False, means=[0,0,0],stds=[1,1,1]),\n",
        "                            d_transforms = get_transforms(train=False, means=[0,0,0],stds=[1,1,1]))\n",
        "\n",
        "# Remove last 50 images from training/validation set. These are the test set.\n",
        "dataset.df= dataset.df.iloc[:-50]\n",
        "\n",
        "# Split train and validation set. Stratify based on variety.\n",
        "train_split, val_split = train_test_split(dataset.df,\n",
        "                                          test_size = 0.2,\n",
        "                                          random_state = split_seed,\n",
        "                                          stratify = dataset.df['outputs'].str['classification']) #change to None if you don't have class info\n",
        "train = torch.utils.data.Subset(dataset, train_split.index.tolist())\n",
        "val   = torch.utils.data.Subset(dataset, val_split.index.tolist())\n",
        "\n",
        "# Create train and validation dataloaders\n",
        "train_loader = torch.utils.data.DataLoader(train, batch_size=6, num_workers=6, shuffle=True)\n",
        "val_loader   = torch.utils.data.DataLoader(val,   batch_size=6, shuffle=False, num_workers=6)\n"
      ],
      "metadata": {
        "id": "8EiGEF_aB97C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Determine the mean and standard deviation of images for normalization (Only need to do once for a new dataset)"
      ],
      "metadata": {
        "id": "LTIEHfezNIP2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# this part is just to check the MEAN and STD of the dataset (dont run unless you need mu and sigma)\n",
        "\n",
        "n_rgb = 0\n",
        "n_depth = 0\n",
        "mean_rgb = 0.\n",
        "std_rgb = 0.\n",
        "mean_depth = 0.\n",
        "std_depth = 0.\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=5, shuffle=False, num_workers=12)\n",
        "for rgb, depth, _ in dataloader:\n",
        "\n",
        "    # Rearrange batch to be the shape of [B, C, W * H]\n",
        "    rgb = rgb.view(rgb.size(0), rgb.size(1), -1)\n",
        "    depth = depth.view(depth.size(0), depth.size(1), -1)\n",
        "    # Update total number of images\n",
        "    n_rgb += rgb.size(0)\n",
        "    n_depth += depth.size(0)\n",
        "    # Compute mean and std here\n",
        "    mean_rgb += rgb.mean(2).sum(0)\n",
        "    std_rgb += rgb.std(2).sum(0)\n",
        "    mean_depth += depth.mean(2).sum(0)\n",
        "    std_depth += depth.std(2).sum(0)\n",
        "\n",
        "# Final step\n",
        "mean_rgb /= n_rgb\n",
        "std_rgb /= n_rgb\n",
        "mean_depth /= n_depth\n",
        "std_depth /= n_depth\n",
        "\n",
        "print('Mean of RGB: '+ str(mean_rgb))\n",
        "print('Standard Deviation of RGB', str(std_rgb))\n",
        "print('Mean of Depth: '+ str(mean_depth))\n",
        "print('Standard Deviation of Depth', str(std_depth))"
      ],
      "metadata": {
        "id": "VaPuJGfrNHyL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Copy the output of the previous cells into here to avoid needing to redetermine mean and std every time"
      ],
      "metadata": {
        "id": "QXTiFQFyNN8J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.means = [0.5482, 0.4620, 0.3602, 0.0127]  #these values were copied from the previous cell\n",
        "dataset.stds = [0.1639, 0.1761, 0.2659, 0.0035]   #copy and paste the values to avoid having\n",
        "                                                  # to rerun the previous cell for every iteration"
      ],
      "metadata": {
        "id": "yIJPvxNGNPQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set device"
      ],
      "metadata": {
        "id": "KQdSPPdfxgZZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available else 'cpu')"
      ],
      "metadata": {
        "id": "_lwuZHp7lhDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model"
      ],
      "metadata": {
        "id": "fSuTkH-lxl4b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install torchsummary\n",
        "# import torchsummary\n",
        "\n",
        "# model = models.resnet50(pretrained=True)\n",
        "# model = model.cuda()\n",
        "# torchsummary.summary(model, (3, 224, 224))"
      ],
      "metadata": {
        "id": "QRIBA-cl6K82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FirstStageModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FirstStageModel, self).__init__()\n",
        "        # RGB Model\n",
        "        self.rgb_processing_block = nn.Sequential(nn.Conv2D(3, 32, kernel_size=3, stride=2, padding=1),\n",
        "                                       nn.Conv2D(32, 3, kernel_size=1),\n",
        "                                       nn.AdaptiveAvgPool2d((224, 224)))\n",
        "        self.rgb_encoder = models.resnet50(pretrained=True)\n",
        "        self.rgb_regressor = nn.Sequential(nn.ReLU(replace=True),\n",
        "                                           nn.Dropout(0.5),\n",
        "                                           nn.Linear(1000, 256),\n",
        "                                           nn.ReLU(replace=True),\n",
        "                                           nn.Dropout(0.5),\n",
        "                                           nn.Linear(256, 4))\n",
        "\n",
        "\n",
        "        # Depth Model\n",
        "        self.depth_processing_block = nn.Sequential(nn.Conv2D(1, 32, kernel_size=3, stride=2, padding=1),\n",
        "                                         nn.Conv2D(32, 1, kernel_size=1),\n",
        "                                         nn.AdaptiveAvgPool2d((224, 224)))\n",
        "        self.depth_encoder = models.resnet50(pretrained=False)\n",
        "        self.depth_regressor = nn.Sequential(nn.ReLU(replace=True),\n",
        "                                             nn.Dropout(0.5),\n",
        "                                             nn.Linear(1000, 256),\n",
        "                                             nn.ReLU(replace=True),\n",
        "                                             nn.Dropout(0.5),\n",
        "                                             nn.Linear(256, 1))\n",
        "\n",
        "        self.final = nn.Sequential(nn.Dropout(0.5),\n",
        "                                   nn.Linear(5, 2048),\n",
        "                                   nn.ReLU(replace=True)\n",
        "                                   nn.Dropout(0.5),\n",
        "                                   nn.Linear(2048, 3),\n",
        "                                   nn.Dropout(0.5))\n",
        "\n",
        "    def forward(self, rgb, depth):\n",
        "        rgb_out = self.rgb_processing_block(rgb)\n",
        "        rgb_out = self.rgb_encoder(rgb_out)\n",
        "        rgb_out = self.rgb_regressor(rgb_out)\n",
        "\n",
        "        depth_out = self.depth_processing_block(depth)\n",
        "        depth_out = self.depth_encoder(depth_out)\n",
        "        output2 = self.depth_regressor(depth_out)  # height\n",
        "\n",
        "        output1 = torch.cat([rgb_out, depth_out], dim=1)\n",
        "        output1 = self.final(output1)  # fresh weight, dry weight, diameter\n",
        "\n",
        "        return output1, output2"
      ],
      "metadata": {
        "id": "YCQReoyZNukl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SecondStageModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SecondStageModel, self).__init__()\n",
        "        self.regressor1 = nn.Sequential(nn.Linear(4, 2048),\n",
        "                                        nn.ReLU(replace=True),\n",
        "                                        nn.Dropout(0.5),\n",
        "                                        nn.Linear(2048, 2048),\n",
        "                                        nn.ReLU(replace=True),\n",
        "                                        nn.Dropout(0.5),\n",
        "                                        nn.Linear(2048, 1))  # dry weight\n",
        "        self.regressor2 = nn.Sequential(nn.Linear(4, 2048),\n",
        "                                        nn.ReLU(replace=True),\n",
        "                                        nn.Dropout(0.5),\n",
        "                                        nn.Linear(2048, 2048),\n",
        "                                        nn.ReLU(replace=True),\n",
        "                                        nn.Dropout(0.5),\n",
        "                                        nn.Linear(2048, 1))  # leaf area\n",
        "\n",
        "    def forward(self, output1, output2):\n",
        "        x = torch.cat([output1, output2], dim=1)\n",
        "        output3 = self.regressor1(x)\n",
        "        output4 = self.regressor2(x)\n",
        "        return output3, output4"
      ],
      "metadata": {
        "id": "v8hx07F3N1np"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "first_stage_model = FirstStageModel()\n",
        "second_stage_model = SecondStageModel()"
      ],
      "metadata": {
        "id": "v85tGtd9EVb6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameter"
      ],
      "metadata": {
        "id": "sC7ZVHCguAxD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 0.001\n",
        "epochs = 200\n",
        "batch_size = 32"
      ],
      "metadata": {
        "id": "OF_4klISuEiZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loss and optimizer"
      ],
      "metadata": {
        "id": "OC2hzRDJxwXf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion_stage1 = nn.MSELoss()\n",
        "criterion_stage2 = nn.MSELoss()\n",
        "\n",
        "optimizer_stage1 = optim.Adam(first_stage_model.parameters(), lr=lr)\n",
        "optimizer_stage2 = optim.Adam(second_stage_model.parameters(), lr=lr)"
      ],
      "metadata": {
        "id": "zKHuh3uFNirl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train"
      ],
      "metadata": {
        "id": "zRU-TELFxy6S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(first_stage_model, second_stage_model, dataset, device, sav_dir,\n",
        "             criterion_stage1, criterion_stage2, writer, epoch, val_loader, best_val_loss):\n",
        "    current_val_loss = 0\n",
        "    # training_val_loss=0s\n",
        "\n",
        "    first_stage_model.eval()\n",
        "    second_stage_model.eval()\n",
        "    print('Validating and Checkpointing!')\n",
        "\n",
        "    dataset.rgb_transforms = get_transforms(train=True, means=dataset.means[:3], stds=dataset.stds[:3])\n",
        "    dataset.d_transforms = get_transforms(train=True, means=dataset.means[3:], stds=dataset.stds[3:])\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (rgb, depth, label) in enumerate(val_loader):\n",
        "            rgb = rgb.to(device)\n",
        "            depth = depth.to(device)\n",
        "            label = label.to(device)\n",
        "\n",
        "            pred1, pred2 = first_stage_model(rgb, depth)\n",
        "            combined_output = torch.cat([pred1, pred2], dim=1)\n",
        "            pred3, pred4 = second_stage_model(combined_output)\n",
        "\n",
        "            loss_stage1 = criterion_stage1(pred1, label[:, [0,2]]) + criterion_stage1(pred2, label[:, 3])\n",
        "            loss_stage2 = criterion_stage2(pred3, label[:, 1]) + criterion_stage2(pred4, label[:, 4])\n",
        "            total_loss = loss_stage1 + loss_stage2\n",
        "            # acc=nmse(preds.detach(), targets)\n",
        "            current_val_loss = current_val_loss + total_loss.item()\n",
        "            # training_val_loss=training_val_loss+loss.detach().cpu().numpy()\n",
        "\n",
        "        # writer.add_scalar(\"MSE Loss/val\", training_val_loss, epoch)\n",
        "        writer.add_scalar(\"NMSE Loss/val\", current_val_loss, epoch)\n",
        "\n",
        "    if current_val_loss < best_val_loss or epoch == 0:\n",
        "        best_val_loss = current_val_loss\n",
        "        torch.save(model.state_dict(), sav_dir+'bestmodel' + '.pth') # should be fixed!\n",
        "        print('Best model Saved! Val NMSE: ', str(best_val_loss))\n",
        "        with open('run.txt', 'a') as f:\n",
        "            f.write('\\n')\n",
        "            f.write('Best model Saved! Val NMSE: '+ str(best_val_loss))\n",
        "\n",
        "    else:\n",
        "        print('Model is not good (might be overfitting)! Current val NMSE: ', str(current_val_loss), 'Best Val NMSE: ', str(best_val_loss))\n",
        "        with open('run.txt', 'a') as f:\n",
        "            f.write('\\n')\n",
        "            f.write('Model is not good (might be overfitting)! Current val NMSE: '+ str(current_val_loss)+ 'Best Val NMSE: '+ str(best_val_loss))\n",
        "    return best_val_loss"
      ],
      "metadata": {
        "id": "KlbFf8hQg3vM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "    first_stage_model.train()\n",
        "    second_stage_model.train()\n",
        "\n",
        "    first_stage_model.to(device)\n",
        "    second_stage_model.to(device)\n",
        "\n",
        "    dataset.rgb_transforms = get_transforms(train=True, means=dataset.means[:3], stds=dataset.stds[:3])\n",
        "    dataset.d_transforms = get_transforms(train=True, means=dataset.means[3:], stds=dataset.stds[3:])\n",
        "\n",
        "    best_val_loss = 9999999 # initial dummy value\n",
        "    current_val_loss = 0\n",
        "\n",
        "    writer = SummaryWriter()\n",
        "    start = time.time()\n",
        "\n",
        "    for i, (rgb, depth, label) in enumerate(train_loader):\n",
        "        print('Epoch: ', str(epoch + 1), ', Time Elapsed: ', str((time.time()-start)/60), ' mins')\n",
        "\n",
        "        rgb = rgb.to(device)\n",
        "        depth = depth.to(device)\n",
        "        label = label.to(device)  # ['FreshWeightShoot', 'DryWeightShoot', 'Height', 'Diameter', 'LeafArea']\n",
        "\n",
        "        # Forward pass - First stage\n",
        "        pred1, pred2 = first_stage_model(rgb, depth)  # pred1: fresh weight, dry weight, diameter\n",
        "                                                          # pred2: height\n",
        "\n",
        "        # Combine outputs for second stage\n",
        "        combined_output = torch.cat([pred1, pred2], dim=1)\n",
        "\n",
        "        # Forward pass - Second stage\n",
        "        pred3, pred4 = second_stage_model(combined_output)  # pred3: dry weight\n",
        "                                                                # pred4: leaf area\n",
        "\n",
        "        # Calculate loss\n",
        "        loss_stage1 = criterion_stage1(pred1, label[:, [0,2]]) + criterion_stage1(pred2, label[:, 3])\n",
        "        loss_stage2 = criterion_stage2(pred3, label[:, 1]) + criterion_stage2(pred4, label[:, 4])\n",
        "        total_loss = loss_stage1 + loss_stage2\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer_stage1.zero_grad()\n",
        "        optimizer_stage2.zero_grad()\n",
        "        total_loss.backward()\n",
        "        optimizer_stage1.step()\n",
        "        optimizer_stage2.step()\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{epochs}, Batch {i+1}/{len(train_loader)}, Loss1: {loss_stage1.item()}, Loss2: {loss_stage2.item()}, Total_Loss: {total_loss.item()}')\n",
        "\n",
        "        best_val_loss = validate(first_stage_model, second_stage_model, dataset, device, sav_dir,\n",
        "                                 criterion_stage1, criterion_stage2, writer, epoch, val_loader, best_val_loss)"
      ],
      "metadata": {
        "id": "960MdEI7r2-1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}