{"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOWxE4QE/55NVUPcA//ZlAe","include_colab_link":true},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30627,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a href=\"https://colab.research.google.com/github/giyeongyoon/3rd_AGC/blob/master/1_stage_cnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>","metadata":{"id":"view-in-github","colab_type":"text"}},{"cell_type":"code","source":"%%capture\n!pip install albumentations==1.1.0\n!pip install agml","metadata":{"id":"P7YSvf_oKlPr","execution":{"iopub.status.busy":"2023-12-18T06:31:44.926292Z","iopub.execute_input":"2023-12-18T06:31:44.926662Z","iopub.status.idle":"2023-12-18T06:32:10.533333Z","shell.execute_reply.started":"2023-12-18T06:31:44.926635Z","shell.execute_reply":"2023-12-18T06:32:10.532104Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"Import libraries","metadata":{"id":"XeVhzZvgLCt7"}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torchvision import models, transforms\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport os\nimport cv2\nimport albumentations as A\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport time","metadata":{"id":"6BZ4j2lzK6wQ","execution":{"iopub.status.busy":"2023-12-18T06:33:54.782802Z","iopub.execute_input":"2023-12-18T06:33:54.783659Z","iopub.status.idle":"2023-12-18T06:34:09.131801Z","shell.execute_reply.started":"2023-12-18T06:33:54.783618Z","shell.execute_reply":"2023-12-18T06:34:09.130906Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Download 2021 Autonomous Greenhouse Challenge dataset","metadata":{"id":"JhSMln48K_iJ"}},{"cell_type":"code","source":"import agml\nloader = agml.data.AgMLDataLoader('autonomous_greenhouse_regression', dataset_path = './')","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"T3-oDY-ZLBdJ","outputId":"c70a52e1-8544-41ba-d56b-aeb64dca1d07","execution":{"iopub.status.busy":"2023-12-18T06:34:09.133680Z","iopub.execute_input":"2023-12-18T06:34:09.134252Z","iopub.status.idle":"2023-12-18T06:34:58.928141Z","shell.execute_reply.started":"2023-12-18T06:34:09.134223Z","shell.execute_reply":"2023-12-18T06:34:58.926537Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"Downloading autonomous_greenhouse_regression (size = 887.2 MB): 887226368it [00:40, 21882569.67it/s]                               \n","output_type":"stream"},{"name":"stdout","text":"[AgML Download]: Extracting files for autonomous_greenhouse_regression... Done!\n\n================================================================================\nYou have just downloaded \u001b[1mautonomous_greenhouse_regression\u001b[0m.\n\nThis dataset is licensed under the \u001b[1mCC BY-SA 4.0\u001b[0m license.\nTo learn more, visit: https://creativecommons.org/licenses/by-sa/4.0/\n\nWhen using this dataset, please cite the following:\n\n@misc{https://doi.org/10.4121/15023088.v1,\n  doi = {10.4121/15023088.V1},\n  url = {https://data.4tu.nl/articles/_/15023088/1},\n  author = {Hemming,  S. (Silke) and de Zwart,  H.F. (Feije) and Elings,  A. (Anne) and bijlaard,  monique and Marrewijk,  van,  Bart and Petropoulou,  Anna},\n  keywords = {Horticultural Crops,  Mechanical Engineering,  FOS: Mechanical engineering,  Artificial Intelligence and Image Processing,  FOS: Computer and information sciences,  Horticultural Production,  FOS: Agriculture,  forestry and fisheries,  Autonomous Greenhouse Challenge,  autonomous greenhouse,  Artificial Intelligence,  image processing,  computer vision,  Horticulture,  Lettuce,  sensors,  non-destructive sensing},\n  title = {3rd Autonomous Greenhouse Challenge: Online Challenge Lettuce Images},\n  publisher = {4TU.ResearchData},\n  year = {2021},\n  copyright = {Creative Commons Attribution 4.0 International}\n}\n\nYou can find additional information about this dataset at:\nhttps://data.4tu.nl/articles/dataset/3rd_Autonomous_Greenhouse_Challenge_Online_Challenge_Lettuce_Images/15023088/1\n\nThis message will \u001b[1mnot\u001b[0m be automatically shown\nagain. To view this message again, in an AgMLDataLoader\nrun `loader.info.citation_summary()`. Otherwise, you\ncan use `agml.data.source(<name>).citation_summary().`\n\nYou can find your dataset at /kaggle/working/autonomous_greenhouse_regression.\n================================================================================\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/agml/data/metadata.py:153\u001b[0m, in \u001b[0;36mDatasetMetadata.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;66;03m# Some weird behavior with lookups can happen.\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/agml/data/metadata.py:214\u001b[0m, in \u001b[0;36mDatasetMetadata.num_to_class\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapping[class_type], \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m--> 214\u001b[0m     nums \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mfloat\u001b[39m(i)) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m mapping[\u001b[43mclass_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m()]]\n\u001b[1;32m    215\u001b[0m     out[class_type] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(nums, mapping[class_type]\u001b[38;5;241m.\u001b[39mvalues()))\n","\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'keys'","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01magml\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m loader \u001b[38;5;241m=\u001b[39m \u001b[43magml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAgMLDataLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mautonomous_greenhouse_regression\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/agml/data/loader.py:183\u001b[0m, in \u001b[0;36mAgMLDataLoader.__init__\u001b[0;34m(self, dataset, **kwargs)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_split \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;66;03m# Set the direct access metadata properties like `num_images` and\u001b[39;00m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;66;03m# `classes`, since these can be modified depending on the state of\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;66;03m# the loader, whilst the `info` parameter attributes cannot.\u001b[39;00m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_meta_properties \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_images\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mnum_images,\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclasses\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mclasses,\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_classes\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mnum_classes,\n\u001b[0;32m--> 183\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_to_class\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_info\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_to_class\u001b[49m,\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass_to_num\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mclass_to_num,\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata_distributions\u001b[39m\u001b[38;5;124m'\u001b[39m: {\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mnum_images}}\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/agml/data/metadata.py:157\u001b[0m, in \u001b[0;36mDatasetMetadata.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata[key]\n\u001b[0;32m--> 157\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m    158\u001b[0m     maybe_you_meant(\n\u001b[1;32m    159\u001b[0m         key, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived invalid info parameter: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    160\u001b[0m         source \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata\u001b[38;5;241m.\u001b[39mkeys()))\n","\u001b[0;31mAttributeError\u001b[0m: Received invalid info parameter: 'num_to_class'."],"ename":"AttributeError","evalue":"Received invalid info parameter: 'num_to_class'.","output_type":"error"}]},{"cell_type":"markdown","source":"Define data and output directories","metadata":{"id":"D5kHbdg3LMBF"}},{"cell_type":"code","source":"sav_dir='model_weights/'\nif not os.path.exists(sav_dir):\n    os.mkdir(sav_dir)\n# Comment these two lines and uncomment the next two if you've already croppped the images to another directory\nRGB_Data_Dir   = './autonomous_greenhouse_regression/images/'\nDepth_Data_Dir = './autonomous_greenhouse_regression/depth_images/'\n\n\n# RGB_Data_Dir='./autonomous_greenhouse_regression/cropped_images/'\n# Depth_Data_Dir='./autonomous_greenhouse_regression/cropped_depth_images/'\n\n\nJSON_Files_Dir = './autonomous_greenhouse_regression/annotations.json'","metadata":{"id":"8BTdADxwLLH0","execution":{"iopub.status.busy":"2023-12-18T06:57:28.688438Z","iopub.execute_input":"2023-12-18T06:57:28.689264Z","iopub.status.idle":"2023-12-18T06:57:28.694559Z","shell.execute_reply.started":"2023-12-18T06:57:28.689222Z","shell.execute_reply":"2023-12-18T06:57:28.693637Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"Crop","metadata":{"id":"Xwp2Qb1KLPNm"}},{"cell_type":"code","source":"# import matplotlib.pyplot as plt\nmin_x=650\nmax_x=1450\nmin_y=200\nmax_y=900\ncropped_img_dir='./autonomous_greenhouse_regression/cropped_images/'\n\ncropped_depth_img_dir='./autonomous_greenhouse_regression/cropped_depth_images/'\n\nif not os.path.exists(cropped_img_dir):\n    os.mkdir(cropped_img_dir)\n\nif not os.path.exists(cropped_depth_img_dir):\n    os.mkdir(cropped_depth_img_dir)\n\nfor im in os.listdir(RGB_Data_Dir):\n    img = cv2.imread(RGB_Data_Dir+im)\n    crop_img = img[min_y:max_y,min_x:max_x]\n    cv2.imwrite(cropped_img_dir+im, crop_img)\n\nfor depth_im in os.listdir(Depth_Data_Dir):\n    depth_img = cv2.imread(Depth_Data_Dir+depth_im, 0)\n    crop_depth_img = depth_img[min_y:max_y,min_x:max_x]\n    cv2.imwrite(cropped_depth_img_dir+depth_im, crop_depth_img)\n\nRGB_Data_Dir   = cropped_img_dir\nDepth_Data_Dir = cropped_depth_img_dir","metadata":{"id":"5P0jSdmTLPyW","execution":{"iopub.status.busy":"2023-12-18T06:57:30.265688Z","iopub.execute_input":"2023-12-18T06:57:30.266551Z","iopub.status.idle":"2023-12-18T06:58:07.053571Z","shell.execute_reply.started":"2023-12-18T06:57:30.266515Z","shell.execute_reply":"2023-12-18T06:58:07.052528Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"Create PyTorch dataset, create PyTorch dataloader, and split train/val/test","metadata":{"id":"ZIGY6TuxLSs-"}},{"cell_type":"code","source":"split_seed = 12\nnum_epochs = 400","metadata":{"id":"Lfl_4s8ILTMC","execution":{"iopub.status.busy":"2023-12-18T06:58:07.055214Z","iopub.execute_input":"2023-12-18T06:58:07.055521Z","iopub.status.idle":"2023-12-18T06:58:07.059515Z","shell.execute_reply.started":"2023-12-18T06:58:07.055495Z","shell.execute_reply":"2023-12-18T06:58:07.058583Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class GreenhouseDataset(Dataset):\n    def __init__(self, rgb_dir, d_dir, jsonfile_dir, rgb_transforms=None, d_transforms=None):\n\n        self.df= pd.read_json(jsonfile_dir)\n        # flatten_json is a custom function to flat the nested json files!\n\n        self.rgb_transforms = rgb_transforms\n        self.d_transforms = d_transforms\n        self.rgb_dir = rgb_dir\n        self.d_dir = d_dir\n        self.num_outputs = len(self.df.iloc[0]['outputs']['regression'])\n\n\n    def __getitem__(self, idx):\n        # load images\n        row=self.df.iloc[idx]\n\n        rgb = plt.imread(self.rgb_dir+row['image'])\n        depth = plt.imread(self.d_dir+row['depth_image'])\n        depth = np.expand_dims(depth, 2)\n\n        target = list(row['outputs']['regression'].values())\n\n        #make sure your img and mask array are in this format before passing into albumentations transforms, img.shape=[H, W, C]\n        if self.rgb_transforms is not None:\n            aug_rgb = self.rgb_transforms(image=rgb)\n            rgb = aug_rgb['image']\n        elif self.d_transforms is not None:\n            aug_depth = self.d_transforms(image=depth)\n            depth = aug_depth['image']\n\n        rgb = np.transpose(rgb, (2,0,1))\n        depth = np.transpose(depth, (2,0,1))\n\n        #pytorch wants a different format for the image ([C, H, W])\n        rgb = torch.as_tensor(rgb, dtype=torch.float32)\n        depth = torch.as_tensor(depth, dtype=torch.float32)\n        target=torch.as_tensor(target, dtype=torch.float32)\n\n        return rgb, depth, target\n\n    def __len__(self):\n        return len(self.df)","metadata":{"id":"ZXdeZfsiLZIJ","execution":{"iopub.status.busy":"2023-12-18T06:58:07.060951Z","iopub.execute_input":"2023-12-18T06:58:07.061374Z","iopub.status.idle":"2023-12-18T06:58:07.073398Z","shell.execute_reply.started":"2023-12-18T06:58:07.061341Z","shell.execute_reply":"2023-12-18T06:58:07.072556Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"## FIGURE OUT HOW TO CROP ALL THE IMAGES TO GET RID OF EXTRANIOUS PIXELS\ndef get_transforms(train, means, stds):\n    if train:\n        transforms = A.Compose([\n        # A.Crop(x_min=650, y_min=200, x_max=1450, y_max=900, always_apply=False, p=1.0),\n        A.Flip(p=0.5),\n        A.ShiftScaleRotate(always_apply=False, p=0.5, shift_limit=(-0.06, 0.06), scale_limit=(-0.1, 0.1), rotate_limit=(-5, 5), interpolation=0, border_mode=0, value=means, mask_value=None),\n        A.Normalize(mean=means, std=stds, max_pixel_value=1.0, always_apply=False, p=1.0)\n        ])\n    else:\n        transforms =  A.Compose([\n        # A.Crop(x_min=650, y_min=200, x_max=1450, y_max=900, always_apply=False, p=1.0),\n        A.Normalize(mean=means, std=stds, max_pixel_value=1.0, always_apply=False, p=1.0)\n        ])\n    return transforms","metadata":{"id":"__m1vXj4LcDy","execution":{"iopub.status.busy":"2023-12-18T06:58:07.076054Z","iopub.execute_input":"2023-12-18T06:58:07.076376Z","iopub.status.idle":"2023-12-18T06:58:07.086696Z","shell.execute_reply.started":"2023-12-18T06:58:07.076342Z","shell.execute_reply":"2023-12-18T06:58:07.085721Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Instantiate the PyTorch datalaoder the autonomous greenhouse dataset.\ndataset = GreenhouseDataset(rgb_dir = RGB_Data_Dir,\n                            d_dir = Depth_Data_Dir,\n                            jsonfile_dir = JSON_Files_Dir,\n                            rgb_transforms = get_transforms(train=False, means=[0,0,0],stds=[1,1,1]),\n                            d_transforms = get_transforms(train=False, means=[0,0,0],stds=[1,1,1]))\n\n# Remove last 50 images from training/validation set. These are the test set.\ndataset.df= dataset.df.iloc[:-50]\n\n# Split train and validation set. Stratify based on variety.\ntrain_split, val_split = train_test_split(dataset.df,\n                                          test_size = 0.2,\n                                          random_state = split_seed,\n                                          stratify = dataset.df['outputs'].str['classification']) #change to None if you don't have class info\ntrain = torch.utils.data.Subset(dataset, train_split.index.tolist())\nval   = torch.utils.data.Subset(dataset, val_split.index.tolist())\n\n# Create train and validation dataloaders\ntrain_loader = torch.utils.data.DataLoader(train, batch_size=6, num_workers=6, shuffle=True)\nval_loader   = torch.utils.data.DataLoader(val,   batch_size=6, shuffle=False, num_workers=6)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zjw72weiLdB-","outputId":"5e6497cd-6a28-49fa-c7df-3909877db513","execution":{"iopub.status.busy":"2023-12-18T06:58:07.087749Z","iopub.execute_input":"2023-12-18T06:58:07.088100Z","iopub.status.idle":"2023-12-18T06:58:07.124658Z","shell.execute_reply.started":"2023-12-18T06:58:07.088075Z","shell.execute_reply":"2023-12-18T06:58:07.123853Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Determine the mean and standard deviation of images for normalization (Only need to do once for a new dataset)","metadata":{"id":"vGXtUu6LLiHV"}},{"cell_type":"code","source":"# this part is just to check the MEAN and STD of the dataset (dont run unless you need mu and sigma)\n\nn_rgb = 0\nn_depth = 0\nmean_rgb = 0.\nstd_rgb = 0.\nmean_depth = 0.\nstd_depth = 0.\ndataloader = torch.utils.data.DataLoader(dataset, batch_size=5, shuffle=False, num_workers=12)\nfor rgb, depth, _ in dataloader:\n\n    # Rearrange batch to be the shape of [B, C, W * H]\n    rgb = rgb.view(rgb.size(0), rgb.size(1), -1)\n    depth = depth.view(depth.size(0), depth.size(1), -1)\n    # Update total number of images\n    n_rgb += rgb.size(0)\n    n_depth += depth.size(0)\n    # Compute mean and std here\n    mean_rgb += rgb.mean(2).sum(0)\n    std_rgb += rgb.std(2).sum(0)\n    mean_depth += depth.mean(2).sum(0)\n    std_depth += depth.std(2).sum(0)\n\n# Final step\nmean_rgb /= n_rgb\nstd_rgb /= n_rgb\nmean_depth /= n_depth\nstd_depth /= n_depth\n\nprint('Mean of RGB: '+ str(mean_rgb))\nprint('Standard Deviation of RGB', str(std_rgb))\nprint('Mean of Depth: '+ str(mean_depth))\nprint('Standard Deviation of Depth', str(std_depth))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OHD3FI6ULioT","outputId":"b8becdb2-f5d2-4947-fae5-2a48d46ef9d0","execution":{"iopub.status.busy":"2023-12-18T06:58:07.125749Z","iopub.execute_input":"2023-12-18T06:58:07.126087Z","iopub.status.idle":"2023-12-18T06:58:16.946956Z","shell.execute_reply.started":"2023-12-18T06:58:07.126058Z","shell.execute_reply":"2023-12-18T06:58:16.945869Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 12 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Mean of RGB: tensor([0.5482, 0.4620, 0.3602])\nStandard Deviation of RGB tensor([0.1639, 0.1761, 0.2659])\nMean of Depth: tensor([0.0127])\nStandard Deviation of Depth tensor([0.0035])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Copy the output of the previous cells into here to avoid needing to redetermine mean and std every time","metadata":{"id":"6eveLzxiLlUm"}},{"cell_type":"code","source":"dataset.means = [0.5482, 0.4620, 0.3602, 0.0127]  #these values were copied from the previous cell\ndataset.stds = [0.1639, 0.1761, 0.2659, 0.0035]   #copy and paste the values to avoid having\n                                                  # to rerun the previous cell for every iteration","metadata":{"id":"qAp_KbHvLl0P","execution":{"iopub.status.busy":"2023-12-18T06:58:16.948423Z","iopub.execute_input":"2023-12-18T06:58:16.948785Z","iopub.status.idle":"2023-12-18T06:58:16.953763Z","shell.execute_reply.started":"2023-12-18T06:58:16.948750Z","shell.execute_reply":"2023-12-18T06:58:16.952906Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"Set device","metadata":{"id":"B9is_me-LndB"}},{"cell_type":"code","source":"# Set device\ndevice = torch.device('cuda' if torch.cuda.is_available else 'cpu')","metadata":{"id":"AhORwK3MLo4y","execution":{"iopub.status.busy":"2023-12-18T06:58:16.954946Z","iopub.execute_input":"2023-12-18T06:58:16.955300Z","iopub.status.idle":"2023-12-18T06:58:16.962715Z","shell.execute_reply.started":"2023-12-18T06:58:16.955270Z","shell.execute_reply":"2023-12-18T06:58:16.962025Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"Model","metadata":{"id":"7dMdA76JLuEK"}},{"cell_type":"code","source":"class FirstStageModel(nn.Module):\n    def __init__(self):\n        super(FirstStageModel, self).__init__()\n        # RGB Model\n        self.rgb_processing_block = nn.Sequential(nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1),\n                                                  nn.ReLU(),\n                                                  nn.Conv2d(32, 3, kernel_size=1),\n                                                  nn.ReLU(),\n                                                  nn.AdaptiveAvgPool2d((224, 224)))\n        self.rgb_encoder = models.resnet18(pretrained=True)\n        self.rgb_regressor = nn.Sequential(nn.ReLU(),\n                                           nn.Dropout(0.05),\n                                           nn.Linear(1000, 256),\n                                           nn.ReLU(),\n                                           nn.Dropout(0.05),\n                                           nn.Linear(256, 3),\n                                           nn.ReLU())\n\n\n        # Depth Model\n        self.depth_processing_block = nn.Sequential(nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1),\n                                                    nn.ReLU(),\n                                                    nn.Conv2d(32, 1, kernel_size=1),\n                                                    nn.ReLU(),\n                                                    nn.AdaptiveAvgPool2d((224, 224)))\n        self.depth_encoder = models.resnet50(pretrained=True)\n        self.depth_encoder.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        self.depth_regressor = nn.Sequential(nn.ReLU(),\n                                             nn.Dropout(0.05),\n                                             nn.Linear(1000, 256),\n                                             nn.ReLU(),\n                                             nn.Dropout(0.05),\n                                             nn.Linear(256, 1),\n                                             nn.ReLU())\n\n        self.final = nn.Sequential(nn.Dropout(0.05),\n                                   nn.Linear(4, 2048),\n                                   nn.ReLU(),\n                                   nn.Dropout(0.05),\n                                   nn.Linear(2048, 2048),\n                                   nn.ReLU(),\n                                   nn.Dropout(0.05),\n                                   nn.Linear(2048, 3),\n                                   nn.ReLU())\n\n    def forward(self, rgb, depth):\n        rgb_out = self.rgb_processing_block(rgb)\n        rgb_out = self.rgb_encoder(rgb_out)\n        rgb_out = self.rgb_regressor(rgb_out)\n\n        depth_out = self.depth_processing_block(depth)\n        depth_out = self.depth_encoder(depth_out)\n        output2 = self.depth_regressor(depth_out)  # height\n\n        output1 = torch.cat([rgb_out, output2], dim=1)\n        output1 = self.final(output1)  # fresh weight, dry weight, diameter\n\n        return output1, output2","metadata":{"id":"aeLgAf9vLvPc","execution":{"iopub.status.busy":"2023-12-18T08:14:16.651112Z","iopub.execute_input":"2023-12-18T08:14:16.651493Z","iopub.status.idle":"2023-12-18T08:14:16.666615Z","shell.execute_reply.started":"2023-12-18T08:14:16.651455Z","shell.execute_reply":"2023-12-18T08:14:16.665634Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"model = FirstStageModel()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EryoPktnMl9w","outputId":"2eeffd75-8abf-459b-d708-e0e390ab7226","execution":{"iopub.status.busy":"2023-12-18T08:14:20.085716Z","iopub.execute_input":"2023-12-18T08:14:20.086384Z","iopub.status.idle":"2023-12-18T08:14:23.066113Z","shell.execute_reply.started":"2023-12-18T08:14:20.086348Z","shell.execute_reply":"2023-12-18T08:14:23.065058Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n100%|██████████| 97.8M/97.8M [00:02<00:00, 46.9MB/s]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Hyperparameter","metadata":{"id":"jgW2kfIZMs_1"}},{"cell_type":"code","source":"lr = 0.0005\nepochs = 200\nbatch_size = 32","metadata":{"id":"RftkG91CMu32","execution":{"iopub.status.busy":"2023-12-18T08:14:33.253788Z","iopub.execute_input":"2023-12-18T08:14:33.254180Z","iopub.status.idle":"2023-12-18T08:14:33.258686Z","shell.execute_reply.started":"2023-12-18T08:14:33.254147Z","shell.execute_reply":"2023-12-18T08:14:33.257772Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"NMSE loss","metadata":{"id":"XPEmPNzLNG6k"}},{"cell_type":"code","source":"class NMSELoss(nn.Module):\n    def __init__(self):\n          # super(diceloss, self).init()\n        super(NMSELoss, self).__init__()\n          # print('HI')\n    def forward(self, pred, target):\n        if target.size() != pred.size():\n              raise ValueError(\"Target size ({}) must be the same as input size ({})\".format(target.size(), pred.size()))\n\n        num=torch.sum((target-pred)**2,0)\n        den=torch.sum(target**2,0)\n\n        return torch.sum(num/den)","metadata":{"id":"W7B_B5Y9NKbH","execution":{"iopub.status.busy":"2023-12-18T08:14:35.907035Z","iopub.execute_input":"2023-12-18T08:14:35.907378Z","iopub.status.idle":"2023-12-18T08:14:35.913930Z","shell.execute_reply.started":"2023-12-18T08:14:35.907354Z","shell.execute_reply":"2023-12-18T08:14:35.912871Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"Loss and optimizer","metadata":{"id":"dbPA1SYpNAqi"}},{"cell_type":"code","source":"criterion = NMSELoss()\n# optimizer = optim.Adam(model.parameters(), lr=lr)\noptimizer = torch.optim.Adam(model.parameters(),\n                            lr=lr,\n                            betas=(0.9, 0.999),\n                            eps=1e-08,\n                            weight_decay = 0,\n                            amsgrad = False)","metadata":{"id":"QgiJS3F_NCXm","execution":{"iopub.status.busy":"2023-12-18T08:14:37.715799Z","iopub.execute_input":"2023-12-18T08:14:37.716185Z","iopub.status.idle":"2023-12-18T08:14:37.723086Z","shell.execute_reply.started":"2023-12-18T08:14:37.716151Z","shell.execute_reply":"2023-12-18T08:14:37.722024Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"Train","metadata":{"id":"W3mXlgx1M3th"}},{"cell_type":"code","source":"def train_single_epoch(model, dataset, device,\n                       criterion, optimizer,\n                       writer, epoch, train_loader):\n    model.train()\n\n    dataset.rgb_transforms = get_transforms(train=True, means=dataset.means[:3], stds=dataset.stds[:3])\n    dataset.d_transforms = get_transforms(train=True, means=dataset.means[3:], stds=dataset.stds[3:])\n\n    for i, (rgb, depth, label) in enumerate(train_loader):\n        rgb = rgb.to(device)\n        depth = depth.to(device)\n        label = label.to(device)  # ['FreshWeightShoot', 'DryWeightShoot', 'Height', 'Diameter', 'LeafArea']\n\n        # Forward pass - First stage\n        pred1, pred2 = model(rgb, depth)  # pred1: fresh weight, dry weight, diameter\n                                                      # pred2: height\n\n        # Calculate loss\n        pred = torch.cat([pred1[:, :2], pred2], dim=1)\n        pred = torch.cat([pred, pred1[:, 2:]], dim=1)  # fresh weight, dry weight, height, diameter\n        loss = criterion(pred, label[:, :4])\n\n        # Backward pass and optimization\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        print(f'Epoch {epoch+1}/{epochs}, Batch {i+1}/{len(train_loader)}, Loss: {loss.item()}')\n        with open('run.txt', 'a') as f:\n            f.write('\\n')\n            f.write('Train MSE: '+ str(loss.tolist()))\n","metadata":{"id":"atHQ5B1XM5NP","execution":{"iopub.status.busy":"2023-12-18T08:14:39.664702Z","iopub.execute_input":"2023-12-18T08:14:39.665418Z","iopub.status.idle":"2023-12-18T08:14:39.674811Z","shell.execute_reply.started":"2023-12-18T08:14:39.665380Z","shell.execute_reply":"2023-12-18T08:14:39.673806Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"def validate(model, dataset, device, sav_dir, criterion, writer, epoch, val_loader, best_val_loss):\n    current_val_loss = 0\n    # training_val_loss=0s\n\n    model.eval()\n    print('Validating and Checkpointing!')\n\n    dataset.rgb_transforms = get_transforms(train=True, means=dataset.means[:3], stds=dataset.stds[:3])\n    dataset.d_transforms = get_transforms(train=True, means=dataset.means[3:], stds=dataset.stds[3:])\n\n    with torch.no_grad():\n        for i, (rgb, depth, label) in enumerate(val_loader):\n            rgb = rgb.to(device)\n            depth = depth.to(device)\n            label = label.to(device)\n\n            pred1, pred2 = model(rgb, depth)\n\n            pred = torch.cat([pred1[:, :2], pred2], dim=1)\n            pred = torch.cat([pred, pred1[:, 2:]], dim=1)\n            loss = criterion(pred, label[:, :4])\n            # acc=nmse(preds.detach(), targets)\n            current_val_loss = current_val_loss + loss.item()\n            # training_val_loss=training_val_loss+loss.detach().cpu().numpy()\n\n        # writer.add_scalar(\"MSE Loss/val\", training_val_loss, epoch)\n        writer.add_scalar(\"MSE Loss/val\", current_val_loss, epoch)\n\n    if current_val_loss < best_val_loss or epoch == 0:\n        best_val_loss = current_val_loss\n        torch.save(model.state_dict(), sav_dir+'bestmodel' + '.pth')\n        print('Best model Saved! Val MSE: ', str(best_val_loss))\n        with open('run.txt', 'a') as f:\n            f.write('\\n')\n            f.write('Best model Saved! Val MSE: '+ str(best_val_loss))\n\n    else:\n        print('Model is not good (might be overfitting)! Current val MSE: ', str(current_val_loss), 'Best Val MSE: ', str(best_val_loss))\n        with open('run.txt', 'a') as f:\n            f.write('\\n')\n            f.write('Model is not good (might be overfitting)! Current val MSE: '+ str(current_val_loss)+ 'Best Val MSE: '+ str(best_val_loss))\n    return best_val_loss","metadata":{"id":"ieE0dxwgP3F0","execution":{"iopub.status.busy":"2023-12-18T08:14:40.210955Z","iopub.execute_input":"2023-12-18T08:14:40.211334Z","iopub.status.idle":"2023-12-18T08:14:40.223403Z","shell.execute_reply.started":"2023-12-18T08:14:40.211302Z","shell.execute_reply":"2023-12-18T08:14:40.222428Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"model.to(device)\n\nbest_val_loss = 9999999 # initial dummy value\ncurrent_val_loss = 0\n\nwriter = SummaryWriter()\nstart = time.time()\n\nfor epoch in range(epochs):\n    with open('run.txt', 'a') as f:\n                f.write('\\n')\n                f.write('Epoch: '+ str(epoch + 1) + ', Time Elapsed: '+ str((time.time()-start)/60) + ' mins')\n    print('Epoch: ', str(epoch + 1), ', Time Elapsed: ', str((time.time()-start)/60), ' mins')\n    train_single_epoch(model, dataset, device,\n                        criterion, optimizer,\n                        writer, epoch, train_loader)\n    best_val_loss = validate(model, dataset, device, sav_dir,\n                                criterion, writer, epoch, val_loader, best_val_loss)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B38GTKhJP6rJ","outputId":"bffef5e6-8a87-4cef-8539-0225c7be0655","execution":{"iopub.status.busy":"2023-12-18T08:14:43.115392Z","iopub.execute_input":"2023-12-18T08:14:43.116068Z","iopub.status.idle":"2023-12-18T08:46:44.793672Z","shell.execute_reply.started":"2023-12-18T08:14:43.116032Z","shell.execute_reply":"2023-12-18T08:46:44.792474Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"Epoch:  1 , Time Elapsed:  4.676977793375651e-06  mins\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/200, Batch 1/45, Loss: 3.9917185306549072\nEpoch 1/200, Batch 2/45, Loss: 3.5947341918945312\nEpoch 1/200, Batch 3/45, Loss: 2.9587182998657227\nEpoch 1/200, Batch 4/45, Loss: 2.3351645469665527\nEpoch 1/200, Batch 5/45, Loss: 2.313126564025879\nEpoch 1/200, Batch 6/45, Loss: 2.6145248413085938\nEpoch 1/200, Batch 7/45, Loss: 2.124751567840576\nEpoch 1/200, Batch 8/45, Loss: 1.45382821559906\nEpoch 1/200, Batch 9/45, Loss: 1.9717528820037842\nEpoch 1/200, Batch 10/45, Loss: 2.059291362762451\nEpoch 1/200, Batch 11/45, Loss: 1.7117973566055298\nEpoch 1/200, Batch 12/45, Loss: 1.4393820762634277\nEpoch 1/200, Batch 13/45, Loss: 1.6519888639450073\nEpoch 1/200, Batch 14/45, Loss: 1.5347002744674683\nEpoch 1/200, Batch 15/45, Loss: 1.6708985567092896\nEpoch 1/200, Batch 16/45, Loss: 1.6592895984649658\nEpoch 1/200, Batch 17/45, Loss: 1.5419538021087646\nEpoch 1/200, Batch 18/45, Loss: 1.5327928066253662\nEpoch 1/200, Batch 19/45, Loss: 1.598617434501648\nEpoch 1/200, Batch 20/45, Loss: 1.4923272132873535\nEpoch 1/200, Batch 21/45, Loss: 1.3746740818023682\nEpoch 1/200, Batch 22/45, Loss: 1.6199642419815063\nEpoch 1/200, Batch 23/45, Loss: 1.3337033987045288\nEpoch 1/200, Batch 24/45, Loss: 3.114158868789673\nEpoch 1/200, Batch 25/45, Loss: 1.6066908836364746\nEpoch 1/200, Batch 26/45, Loss: 1.6007771492004395\nEpoch 1/200, Batch 27/45, Loss: 1.7452850341796875\nEpoch 1/200, Batch 28/45, Loss: 1.3146047592163086\nEpoch 1/200, Batch 29/45, Loss: 1.3058719635009766\nEpoch 1/200, Batch 30/45, Loss: 1.43441903591156\nEpoch 1/200, Batch 31/45, Loss: 1.4899308681488037\nEpoch 1/200, Batch 32/45, Loss: 1.611887812614441\nEpoch 1/200, Batch 33/45, Loss: 1.375658392906189\nEpoch 1/200, Batch 34/45, Loss: 1.4428691864013672\nEpoch 1/200, Batch 35/45, Loss: 1.4996533393859863\nEpoch 1/200, Batch 36/45, Loss: 1.4036706686019897\nEpoch 1/200, Batch 37/45, Loss: 1.3321720361709595\nEpoch 1/200, Batch 38/45, Loss: 1.3473610877990723\nEpoch 1/200, Batch 39/45, Loss: 1.1772394180297852\nEpoch 1/200, Batch 40/45, Loss: 1.6040687561035156\nEpoch 1/200, Batch 41/45, Loss: 1.4544885158538818\nEpoch 1/200, Batch 42/45, Loss: 1.7205398082733154\nEpoch 1/200, Batch 43/45, Loss: 1.549601435661316\nEpoch 1/200, Batch 44/45, Loss: 1.4736392498016357\nEpoch 1/200, Batch 45/45, Loss: 2.1476030349731445\nValidating and Checkpointing!\nBest model Saved! Val MSE:  51.89651644229889\nEpoch:  2 , Time Elapsed:  0.16554857889811198  mins\nEpoch 2/200, Batch 1/45, Loss: 1.383277177810669\nEpoch 2/200, Batch 2/45, Loss: 1.3480631113052368\nEpoch 2/200, Batch 3/45, Loss: 2.7510435581207275\nEpoch 2/200, Batch 4/45, Loss: 1.267152190208435\nEpoch 2/200, Batch 5/45, Loss: 1.9420007467269897\nEpoch 2/200, Batch 6/45, Loss: 1.9583253860473633\nEpoch 2/200, Batch 7/45, Loss: 1.9243608713150024\nEpoch 2/200, Batch 8/45, Loss: 1.778597354888916\nEpoch 2/200, Batch 9/45, Loss: 1.6671980619430542\nEpoch 2/200, Batch 10/45, Loss: 1.6969091892242432\nEpoch 2/200, Batch 11/45, Loss: 1.6338363885879517\nEpoch 2/200, Batch 12/45, Loss: 1.805446982383728\nEpoch 2/200, Batch 13/45, Loss: 1.7425448894500732\nEpoch 2/200, Batch 14/45, Loss: 1.7300758361816406\nEpoch 2/200, Batch 15/45, Loss: 1.7685075998306274\nEpoch 2/200, Batch 16/45, Loss: 1.6973345279693604\nEpoch 2/200, Batch 17/45, Loss: 1.540727138519287\nEpoch 2/200, Batch 18/45, Loss: 1.5618075132369995\nEpoch 2/200, Batch 19/45, Loss: 1.6020997762680054\nEpoch 2/200, Batch 20/45, Loss: 1.7542282342910767\nEpoch 2/200, Batch 21/45, Loss: 1.3147387504577637\nEpoch 2/200, Batch 22/45, Loss: 1.336591124534607\nEpoch 2/200, Batch 23/45, Loss: 1.343003511428833\nEpoch 2/200, Batch 24/45, Loss: 1.655348300933838\nEpoch 2/200, Batch 25/45, Loss: 1.3900879621505737\nEpoch 2/200, Batch 26/45, Loss: 1.3163957595825195\nEpoch 2/200, Batch 27/45, Loss: 1.158462405204773\nEpoch 2/200, Batch 28/45, Loss: 1.299538493156433\nEpoch 2/200, Batch 29/45, Loss: 1.4475622177124023\nEpoch 2/200, Batch 30/45, Loss: 1.6507917642593384\nEpoch 2/200, Batch 31/45, Loss: 1.2502251863479614\nEpoch 2/200, Batch 32/45, Loss: 1.5512192249298096\nEpoch 2/200, Batch 33/45, Loss: 1.5691540241241455\nEpoch 2/200, Batch 34/45, Loss: 1.4633097648620605\nEpoch 2/200, Batch 35/45, Loss: 1.4108456373214722\nEpoch 2/200, Batch 36/45, Loss: 1.976335883140564\nEpoch 2/200, Batch 37/45, Loss: 1.4215983152389526\nEpoch 2/200, Batch 38/45, Loss: 1.3168013095855713\nEpoch 2/200, Batch 39/45, Loss: 1.375\nEpoch 2/200, Batch 40/45, Loss: 1.4764645099639893\nEpoch 2/200, Batch 41/45, Loss: 1.06459379196167\nEpoch 2/200, Batch 42/45, Loss: 25.26154136657715\nEpoch 2/200, Batch 43/45, Loss: 1.838026762008667\nEpoch 2/200, Batch 44/45, Loss: 4.147940158843994\nEpoch 2/200, Batch 45/45, Loss: 2.7375330924987793\nValidating and Checkpointing!\nBest model Saved! Val MSE:  36.84473776817322\nEpoch:  3 , Time Elapsed:  0.3368023912111918  mins\nEpoch 3/200, Batch 1/45, Loss: 2.908051013946533\nEpoch 3/200, Batch 2/45, Loss: 2.7628073692321777\nEpoch 3/200, Batch 3/45, Loss: 2.903808116912842\nEpoch 3/200, Batch 4/45, Loss: 2.986755847930908\nEpoch 3/200, Batch 5/45, Loss: 2.906538486480713\nEpoch 3/200, Batch 6/45, Loss: 2.9111225605010986\nEpoch 3/200, Batch 7/45, Loss: 2.777440071105957\nEpoch 3/200, Batch 8/45, Loss: 2.8237297534942627\nEpoch 3/200, Batch 9/45, Loss: 2.7988038063049316\nEpoch 3/200, Batch 10/45, Loss: 2.5732412338256836\nEpoch 3/200, Batch 11/45, Loss: 2.5709147453308105\nEpoch 3/200, Batch 12/45, Loss: 2.561931610107422\nEpoch 3/200, Batch 13/45, Loss: 2.5915145874023438\nEpoch 3/200, Batch 14/45, Loss: 2.418948173522949\nEpoch 3/200, Batch 15/45, Loss: 2.8236942291259766\nEpoch 3/200, Batch 16/45, Loss: 2.0230114459991455\nEpoch 3/200, Batch 17/45, Loss: 4.168011665344238\nEpoch 3/200, Batch 18/45, Loss: 2.1116788387298584\nEpoch 3/200, Batch 19/45, Loss: 2.260746479034424\nEpoch 3/200, Batch 20/45, Loss: 2.280165195465088\nEpoch 3/200, Batch 21/45, Loss: 2.207648277282715\nEpoch 3/200, Batch 22/45, Loss: 2.5039660930633545\nEpoch 3/200, Batch 23/45, Loss: 2.0707104206085205\nEpoch 3/200, Batch 24/45, Loss: 2.5446977615356445\nEpoch 3/200, Batch 25/45, Loss: 2.5530591011047363\nEpoch 3/200, Batch 26/45, Loss: 2.1894898414611816\nEpoch 3/200, Batch 27/45, Loss: 2.0386698246002197\nEpoch 3/200, Batch 28/45, Loss: 1.7312780618667603\nEpoch 3/200, Batch 29/45, Loss: 2.0393784046173096\nEpoch 3/200, Batch 30/45, Loss: 1.8189204931259155\nEpoch 3/200, Batch 31/45, Loss: 3.6704981327056885\nEpoch 3/200, Batch 32/45, Loss: 1.9812840223312378\nEpoch 3/200, Batch 33/45, Loss: 1.4675248861312866\nEpoch 3/200, Batch 34/45, Loss: 1.752672791481018\nEpoch 3/200, Batch 35/45, Loss: 1.728860855102539\nEpoch 3/200, Batch 36/45, Loss: 1.4567863941192627\nEpoch 3/200, Batch 37/45, Loss: 1.711714267730713\nEpoch 3/200, Batch 38/45, Loss: 1.8089544773101807\nEpoch 3/200, Batch 39/45, Loss: 1.3596972227096558\nEpoch 3/200, Batch 40/45, Loss: 1.504729986190796\nEpoch 3/200, Batch 41/45, Loss: 2.0011417865753174\nEpoch 3/200, Batch 42/45, Loss: 1.867018699645996\nEpoch 3/200, Batch 43/45, Loss: 1.2136191129684448\nEpoch 3/200, Batch 44/45, Loss: 1.7558658123016357\nEpoch 3/200, Batch 45/45, Loss: 1.3945634365081787\nValidating and Checkpointing!\nBest model Saved! Val MSE:  27.573997139930725\nEpoch:  4 , Time Elapsed:  0.508993136882782  mins\nEpoch 4/200, Batch 1/45, Loss: 1.792733073234558\nEpoch 4/200, Batch 2/45, Loss: 1.5151143074035645\nEpoch 4/200, Batch 3/45, Loss: 1.2586843967437744\nEpoch 4/200, Batch 4/45, Loss: 1.5084303617477417\nEpoch 4/200, Batch 5/45, Loss: 1.1662101745605469\nEpoch 4/200, Batch 6/45, Loss: 1.2283499240875244\nEpoch 4/200, Batch 7/45, Loss: 2.385375499725342\nEpoch 4/200, Batch 8/45, Loss: 1.007939100265503\nEpoch 4/200, Batch 9/45, Loss: 1.3569331169128418\nEpoch 4/200, Batch 10/45, Loss: 1.2196226119995117\nEpoch 4/200, Batch 11/45, Loss: 1.1526398658752441\nEpoch 4/200, Batch 12/45, Loss: 1.8007769584655762\nEpoch 4/200, Batch 13/45, Loss: 1.5440433025360107\nEpoch 4/200, Batch 14/45, Loss: 1.560288429260254\nEpoch 4/200, Batch 15/45, Loss: 1.4967215061187744\nEpoch 4/200, Batch 16/45, Loss: 1.1230751276016235\nEpoch 4/200, Batch 17/45, Loss: 1.0008164644241333\nEpoch 4/200, Batch 18/45, Loss: 0.7400630712509155\nEpoch 4/200, Batch 19/45, Loss: 1.2312721014022827\nEpoch 4/200, Batch 20/45, Loss: 0.9333304762840271\nEpoch 4/200, Batch 21/45, Loss: 0.9719829559326172\nEpoch 4/200, Batch 22/45, Loss: 1.044695496559143\nEpoch 4/200, Batch 23/45, Loss: 1.043158769607544\nEpoch 4/200, Batch 24/45, Loss: 0.9763918519020081\nEpoch 4/200, Batch 25/45, Loss: 0.6307361721992493\nEpoch 4/200, Batch 26/45, Loss: 0.9588178992271423\nEpoch 4/200, Batch 27/45, Loss: 0.5804096460342407\nEpoch 4/200, Batch 28/45, Loss: 1.0471165180206299\nEpoch 4/200, Batch 29/45, Loss: 0.7858271598815918\nEpoch 4/200, Batch 30/45, Loss: 1.1368262767791748\nEpoch 4/200, Batch 31/45, Loss: 1.115355372428894\nEpoch 4/200, Batch 32/45, Loss: 0.5629689693450928\nEpoch 4/200, Batch 33/45, Loss: 0.4424867033958435\nEpoch 4/200, Batch 34/45, Loss: 1.1035510301589966\nEpoch 4/200, Batch 35/45, Loss: 0.9455479383468628\nEpoch 4/200, Batch 36/45, Loss: 0.9973546266555786\nEpoch 4/200, Batch 37/45, Loss: 0.5763165950775146\nEpoch 4/200, Batch 38/45, Loss: 0.5641818046569824\nEpoch 4/200, Batch 39/45, Loss: 0.47375059127807617\nEpoch 4/200, Batch 40/45, Loss: 0.9155972003936768\nEpoch 4/200, Batch 41/45, Loss: 0.700560450553894\nEpoch 4/200, Batch 42/45, Loss: 0.7159745693206787\nEpoch 4/200, Batch 43/45, Loss: 1.2375212907791138\nEpoch 4/200, Batch 44/45, Loss: 1.085919737815857\nEpoch 4/200, Batch 45/45, Loss: 0.5364153385162354\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  38.67077988386154 Best Val MSE:  27.573997139930725\nEpoch:  5 , Time Elapsed:  0.6685013175010681  mins\nEpoch 5/200, Batch 1/45, Loss: 0.4233439564704895\nEpoch 5/200, Batch 2/45, Loss: 0.4855136573314667\nEpoch 5/200, Batch 3/45, Loss: 0.4632314145565033\nEpoch 5/200, Batch 4/45, Loss: 0.7342599034309387\nEpoch 5/200, Batch 5/45, Loss: 0.875339925289154\nEpoch 5/200, Batch 6/45, Loss: 0.48418349027633667\nEpoch 5/200, Batch 7/45, Loss: 0.4275866746902466\nEpoch 5/200, Batch 8/45, Loss: 0.4290156960487366\nEpoch 5/200, Batch 9/45, Loss: 0.35243967175483704\nEpoch 5/200, Batch 10/45, Loss: 0.5937148332595825\nEpoch 5/200, Batch 11/45, Loss: 0.27545616030693054\nEpoch 5/200, Batch 12/45, Loss: 0.8155336380004883\nEpoch 5/200, Batch 13/45, Loss: 3.3549957275390625\nEpoch 5/200, Batch 14/45, Loss: 1.143959403038025\nEpoch 5/200, Batch 15/45, Loss: 0.9908801317214966\nEpoch 5/200, Batch 16/45, Loss: 1.092435598373413\nEpoch 5/200, Batch 17/45, Loss: 1.3541146516799927\nEpoch 5/200, Batch 18/45, Loss: 1.093377947807312\nEpoch 5/200, Batch 19/45, Loss: 1.3626465797424316\nEpoch 5/200, Batch 20/45, Loss: 0.960715651512146\nEpoch 5/200, Batch 21/45, Loss: 0.7740442752838135\nEpoch 5/200, Batch 22/45, Loss: 0.9765834808349609\nEpoch 5/200, Batch 23/45, Loss: 1.5974763631820679\nEpoch 5/200, Batch 24/45, Loss: 0.6528485417366028\nEpoch 5/200, Batch 25/45, Loss: 1.6580274105072021\nEpoch 5/200, Batch 26/45, Loss: 1.0577892065048218\nEpoch 5/200, Batch 27/45, Loss: 0.7697917222976685\nEpoch 5/200, Batch 28/45, Loss: 2.5588676929473877\nEpoch 5/200, Batch 29/45, Loss: 1.0148248672485352\nEpoch 5/200, Batch 30/45, Loss: 0.4930863678455353\nEpoch 5/200, Batch 31/45, Loss: 1.0210907459259033\nEpoch 5/200, Batch 32/45, Loss: 1.2801424264907837\nEpoch 5/200, Batch 33/45, Loss: 1.2210042476654053\nEpoch 5/200, Batch 34/45, Loss: 0.8060505390167236\nEpoch 5/200, Batch 35/45, Loss: 1.297645092010498\nEpoch 5/200, Batch 36/45, Loss: 1.1930724382400513\nEpoch 5/200, Batch 37/45, Loss: 1.3423991203308105\nEpoch 5/200, Batch 38/45, Loss: 0.9805136919021606\nEpoch 5/200, Batch 39/45, Loss: 0.5146441459655762\nEpoch 5/200, Batch 40/45, Loss: 1.0662879943847656\nEpoch 5/200, Batch 41/45, Loss: 0.8068434000015259\nEpoch 5/200, Batch 42/45, Loss: 0.551821231842041\nEpoch 5/200, Batch 43/45, Loss: 0.8704121112823486\nEpoch 5/200, Batch 44/45, Loss: 1.0737050771713257\nEpoch 5/200, Batch 45/45, Loss: 0.7358521819114685\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  35.39389955997467 Best Val MSE:  27.573997139930725\nEpoch:  6 , Time Elapsed:  0.835646657148997  mins\nEpoch 6/200, Batch 1/45, Loss: 4.291367053985596\nEpoch 6/200, Batch 2/45, Loss: 0.9306505918502808\nEpoch 6/200, Batch 3/45, Loss: 0.4119401276111603\nEpoch 6/200, Batch 4/45, Loss: 1.0765249729156494\nEpoch 6/200, Batch 5/45, Loss: 0.33284127712249756\nEpoch 6/200, Batch 6/45, Loss: 0.896540641784668\nEpoch 6/200, Batch 7/45, Loss: 1.1089470386505127\nEpoch 6/200, Batch 8/45, Loss: 1.3505682945251465\nEpoch 6/200, Batch 9/45, Loss: 0.7941921949386597\nEpoch 6/200, Batch 10/45, Loss: 1.7421231269836426\nEpoch 6/200, Batch 11/45, Loss: 0.9490569829940796\nEpoch 6/200, Batch 12/45, Loss: 0.9076755046844482\nEpoch 6/200, Batch 13/45, Loss: 0.7007390856742859\nEpoch 6/200, Batch 14/45, Loss: 1.298340082168579\nEpoch 6/200, Batch 15/45, Loss: 0.9942941665649414\nEpoch 6/200, Batch 16/45, Loss: 1.6844482421875\nEpoch 6/200, Batch 17/45, Loss: 0.6383457183837891\nEpoch 6/200, Batch 18/45, Loss: 0.9021115899085999\nEpoch 6/200, Batch 19/45, Loss: 0.9767020344734192\nEpoch 6/200, Batch 20/45, Loss: 1.123737096786499\nEpoch 6/200, Batch 21/45, Loss: 0.760286808013916\nEpoch 6/200, Batch 22/45, Loss: 0.4609656035900116\nEpoch 6/200, Batch 23/45, Loss: 0.9283795356750488\nEpoch 6/200, Batch 24/45, Loss: 1.0836811065673828\nEpoch 6/200, Batch 25/45, Loss: 0.6661747694015503\nEpoch 6/200, Batch 26/45, Loss: 0.27873316407203674\nEpoch 6/200, Batch 27/45, Loss: 0.6073055267333984\nEpoch 6/200, Batch 28/45, Loss: 0.6531749963760376\nEpoch 6/200, Batch 29/45, Loss: 0.5192040801048279\nEpoch 6/200, Batch 30/45, Loss: 0.5938998460769653\nEpoch 6/200, Batch 31/45, Loss: 0.6811386942863464\nEpoch 6/200, Batch 32/45, Loss: 0.7603132724761963\nEpoch 6/200, Batch 33/45, Loss: 1.0507140159606934\nEpoch 6/200, Batch 34/45, Loss: 0.3365967273712158\nEpoch 6/200, Batch 35/45, Loss: 0.7041008472442627\nEpoch 6/200, Batch 36/45, Loss: 0.42896008491516113\nEpoch 6/200, Batch 37/45, Loss: 0.2663789391517639\nEpoch 6/200, Batch 38/45, Loss: 0.6585522294044495\nEpoch 6/200, Batch 39/45, Loss: 0.6992701888084412\nEpoch 6/200, Batch 40/45, Loss: 0.8961838483810425\nEpoch 6/200, Batch 41/45, Loss: 0.41415220499038696\nEpoch 6/200, Batch 42/45, Loss: 0.3295954763889313\nEpoch 6/200, Batch 43/45, Loss: 0.29404863715171814\nEpoch 6/200, Batch 44/45, Loss: 0.2755112051963806\nEpoch 6/200, Batch 45/45, Loss: 0.6112157106399536\nValidating and Checkpointing!\nBest model Saved! Val MSE:  19.417753219604492\nEpoch:  7 , Time Elapsed:  0.9981304446856181  mins\nEpoch 7/200, Batch 1/45, Loss: 0.30170682072639465\nEpoch 7/200, Batch 2/45, Loss: 0.44281548261642456\nEpoch 7/200, Batch 3/45, Loss: 0.21852026879787445\nEpoch 7/200, Batch 4/45, Loss: 0.4528370797634125\nEpoch 7/200, Batch 5/45, Loss: 0.26381635665893555\nEpoch 7/200, Batch 6/45, Loss: 0.252734899520874\nEpoch 7/200, Batch 7/45, Loss: 0.426443487405777\nEpoch 7/200, Batch 8/45, Loss: 0.2763883173465729\nEpoch 7/200, Batch 9/45, Loss: 2.4193005561828613\nEpoch 7/200, Batch 10/45, Loss: 0.34032905101776123\nEpoch 7/200, Batch 11/45, Loss: 0.32537126541137695\nEpoch 7/200, Batch 12/45, Loss: 0.6928254961967468\nEpoch 7/200, Batch 13/45, Loss: 0.667172372341156\nEpoch 7/200, Batch 14/45, Loss: 0.7612431049346924\nEpoch 7/200, Batch 15/45, Loss: 0.5850353240966797\nEpoch 7/200, Batch 16/45, Loss: 0.4125865697860718\nEpoch 7/200, Batch 17/45, Loss: 0.32535475492477417\nEpoch 7/200, Batch 18/45, Loss: 0.4754156470298767\nEpoch 7/200, Batch 19/45, Loss: 0.5439357757568359\nEpoch 7/200, Batch 20/45, Loss: 0.4912928342819214\nEpoch 7/200, Batch 21/45, Loss: 0.9186958074569702\nEpoch 7/200, Batch 22/45, Loss: 0.6229826211929321\nEpoch 7/200, Batch 23/45, Loss: 0.7515767216682434\nEpoch 7/200, Batch 24/45, Loss: 0.7090556621551514\nEpoch 7/200, Batch 25/45, Loss: 0.36922937631607056\nEpoch 7/200, Batch 26/45, Loss: 0.27159857749938965\nEpoch 7/200, Batch 27/45, Loss: 0.3675079345703125\nEpoch 7/200, Batch 28/45, Loss: 0.3402390480041504\nEpoch 7/200, Batch 29/45, Loss: 0.46983760595321655\nEpoch 7/200, Batch 30/45, Loss: 0.4899517297744751\nEpoch 7/200, Batch 31/45, Loss: 0.44966623187065125\nEpoch 7/200, Batch 32/45, Loss: 0.43713831901550293\nEpoch 7/200, Batch 33/45, Loss: 0.6905274987220764\nEpoch 7/200, Batch 34/45, Loss: 0.4029909372329712\nEpoch 7/200, Batch 35/45, Loss: 0.2365594208240509\nEpoch 7/200, Batch 36/45, Loss: 4.494409084320068\nEpoch 7/200, Batch 37/45, Loss: 0.6500586271286011\nEpoch 7/200, Batch 38/45, Loss: 0.39620885252952576\nEpoch 7/200, Batch 39/45, Loss: 0.431313157081604\nEpoch 7/200, Batch 40/45, Loss: 0.7933858036994934\nEpoch 7/200, Batch 41/45, Loss: 1.2375798225402832\nEpoch 7/200, Batch 42/45, Loss: 1.1031380891799927\nEpoch 7/200, Batch 43/45, Loss: 1.1694152355194092\nEpoch 7/200, Batch 44/45, Loss: 0.8841766715049744\nEpoch 7/200, Batch 45/45, Loss: 0.7322707772254944\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  1291.941873550415 Best Val MSE:  19.417753219604492\nEpoch:  8 , Time Elapsed:  1.1591545939445496  mins\nEpoch 8/200, Batch 1/45, Loss: 0.4966127872467041\nEpoch 8/200, Batch 2/45, Loss: 0.6295815706253052\nEpoch 8/200, Batch 3/45, Loss: 0.9161152243614197\nEpoch 8/200, Batch 4/45, Loss: 0.33427321910858154\nEpoch 8/200, Batch 5/45, Loss: 0.3995808959007263\nEpoch 8/200, Batch 6/45, Loss: 0.19759154319763184\nEpoch 8/200, Batch 7/45, Loss: 1.150012493133545\nEpoch 8/200, Batch 8/45, Loss: 0.7967197895050049\nEpoch 8/200, Batch 9/45, Loss: 0.5676252841949463\nEpoch 8/200, Batch 10/45, Loss: 0.7901203036308289\nEpoch 8/200, Batch 11/45, Loss: 1.2316635847091675\nEpoch 8/200, Batch 12/45, Loss: 0.835065484046936\nEpoch 8/200, Batch 13/45, Loss: 0.4798652231693268\nEpoch 8/200, Batch 14/45, Loss: 1.670436143875122\nEpoch 8/200, Batch 15/45, Loss: 1.199360966682434\nEpoch 8/200, Batch 16/45, Loss: 0.6535906195640564\nEpoch 8/200, Batch 17/45, Loss: 0.7385858297348022\nEpoch 8/200, Batch 18/45, Loss: 1.0101184844970703\nEpoch 8/200, Batch 19/45, Loss: 1.3718966245651245\nEpoch 8/200, Batch 20/45, Loss: 1.268320083618164\nEpoch 8/200, Batch 21/45, Loss: 1.0718271732330322\nEpoch 8/200, Batch 22/45, Loss: 1.5310683250427246\nEpoch 8/200, Batch 23/45, Loss: 1.506961464881897\nEpoch 8/200, Batch 24/45, Loss: 1.0188889503479004\nEpoch 8/200, Batch 25/45, Loss: 0.8388597369194031\nEpoch 8/200, Batch 26/45, Loss: 0.4286273419857025\nEpoch 8/200, Batch 27/45, Loss: 1.2492952346801758\nEpoch 8/200, Batch 28/45, Loss: 0.516533374786377\nEpoch 8/200, Batch 29/45, Loss: 0.6533029079437256\nEpoch 8/200, Batch 30/45, Loss: 0.5003640651702881\nEpoch 8/200, Batch 31/45, Loss: 0.3449898362159729\nEpoch 8/200, Batch 32/45, Loss: 0.425714910030365\nEpoch 8/200, Batch 33/45, Loss: 1.041490077972412\nEpoch 8/200, Batch 34/45, Loss: 0.6110612154006958\nEpoch 8/200, Batch 35/45, Loss: 1.0297729969024658\nEpoch 8/200, Batch 36/45, Loss: 0.8069040775299072\nEpoch 8/200, Batch 37/45, Loss: 0.3458997905254364\nEpoch 8/200, Batch 38/45, Loss: 0.6699232459068298\nEpoch 8/200, Batch 39/45, Loss: 0.7268570065498352\nEpoch 8/200, Batch 40/45, Loss: 0.31926944851875305\nEpoch 8/200, Batch 41/45, Loss: 2.947960138320923\nEpoch 8/200, Batch 42/45, Loss: 0.5929864645004272\nEpoch 8/200, Batch 43/45, Loss: 0.7996516823768616\nEpoch 8/200, Batch 44/45, Loss: 0.9770759344100952\nEpoch 8/200, Batch 45/45, Loss: 1.0231701135635376\nValidating and Checkpointing!\nBest model Saved! Val MSE:  14.735699832439423\nEpoch:  9 , Time Elapsed:  1.3245392600695292  mins\nEpoch 9/200, Batch 1/45, Loss: 1.1974443197250366\nEpoch 9/200, Batch 2/45, Loss: 1.2079318761825562\nEpoch 9/200, Batch 3/45, Loss: 0.8605200052261353\nEpoch 9/200, Batch 4/45, Loss: 0.45472365617752075\nEpoch 9/200, Batch 5/45, Loss: 0.6087749004364014\nEpoch 9/200, Batch 6/45, Loss: 0.728776752948761\nEpoch 9/200, Batch 7/45, Loss: 0.4099319279193878\nEpoch 9/200, Batch 8/45, Loss: 0.7459686994552612\nEpoch 9/200, Batch 9/45, Loss: 0.7562446594238281\nEpoch 9/200, Batch 10/45, Loss: 0.5575532913208008\nEpoch 9/200, Batch 11/45, Loss: 0.5672675967216492\nEpoch 9/200, Batch 12/45, Loss: 0.5747763514518738\nEpoch 9/200, Batch 13/45, Loss: 0.7971919775009155\nEpoch 9/200, Batch 14/45, Loss: 0.760607898235321\nEpoch 9/200, Batch 15/45, Loss: 0.6514096260070801\nEpoch 9/200, Batch 16/45, Loss: 0.5194219946861267\nEpoch 9/200, Batch 17/45, Loss: 0.6822136044502258\nEpoch 9/200, Batch 18/45, Loss: 0.9573919773101807\nEpoch 9/200, Batch 19/45, Loss: 0.3740788400173187\nEpoch 9/200, Batch 20/45, Loss: 0.6404726505279541\nEpoch 9/200, Batch 21/45, Loss: 0.33174633979797363\nEpoch 9/200, Batch 22/45, Loss: 0.26442140340805054\nEpoch 9/200, Batch 23/45, Loss: 0.332352876663208\nEpoch 9/200, Batch 24/45, Loss: 1.4057778120040894\nEpoch 9/200, Batch 25/45, Loss: 0.3121528625488281\nEpoch 9/200, Batch 26/45, Loss: 0.7105815410614014\nEpoch 9/200, Batch 27/45, Loss: 0.8083363175392151\nEpoch 9/200, Batch 28/45, Loss: 0.3580473065376282\nEpoch 9/200, Batch 29/45, Loss: 0.4793526530265808\nEpoch 9/200, Batch 30/45, Loss: 0.3017135560512543\nEpoch 9/200, Batch 31/45, Loss: 0.2812964618206024\nEpoch 9/200, Batch 32/45, Loss: 0.41300034523010254\nEpoch 9/200, Batch 33/45, Loss: 0.4475913345813751\nEpoch 9/200, Batch 34/45, Loss: 0.3256908059120178\nEpoch 9/200, Batch 35/45, Loss: 0.4099527597427368\nEpoch 9/200, Batch 36/45, Loss: 0.39908891916275024\nEpoch 9/200, Batch 37/45, Loss: 1.1182608604431152\nEpoch 9/200, Batch 38/45, Loss: 0.42941170930862427\nEpoch 9/200, Batch 39/45, Loss: 0.48005062341690063\nEpoch 9/200, Batch 40/45, Loss: 0.8165547251701355\nEpoch 9/200, Batch 41/45, Loss: 0.9689538478851318\nEpoch 9/200, Batch 42/45, Loss: 0.627240777015686\nEpoch 9/200, Batch 43/45, Loss: 0.4483609199523926\nEpoch 9/200, Batch 44/45, Loss: 0.5780019760131836\nEpoch 9/200, Batch 45/45, Loss: 0.48347777128219604\nValidating and Checkpointing!\nBest model Saved! Val MSE:  14.13761967420578\nEpoch:  10 , Time Elapsed:  1.4880669554074606  mins\nEpoch 10/200, Batch 1/45, Loss: 2.3518869876861572\nEpoch 10/200, Batch 2/45, Loss: 0.799146294593811\nEpoch 10/200, Batch 3/45, Loss: 0.708903431892395\nEpoch 10/200, Batch 4/45, Loss: 0.6362760066986084\nEpoch 10/200, Batch 5/45, Loss: 0.937795102596283\nEpoch 10/200, Batch 6/45, Loss: 0.6564173102378845\nEpoch 10/200, Batch 7/45, Loss: 0.4155370891094208\nEpoch 10/200, Batch 8/45, Loss: 0.5534360408782959\nEpoch 10/200, Batch 9/45, Loss: 0.20595097541809082\nEpoch 10/200, Batch 10/45, Loss: 0.35925188660621643\nEpoch 10/200, Batch 11/45, Loss: 0.6057356595993042\nEpoch 10/200, Batch 12/45, Loss: 0.9497045278549194\nEpoch 10/200, Batch 13/45, Loss: 0.17281301319599152\nEpoch 10/200, Batch 14/45, Loss: 0.839879035949707\nEpoch 10/200, Batch 15/45, Loss: 1.0413906574249268\nEpoch 10/200, Batch 16/45, Loss: 0.4165721535682678\nEpoch 10/200, Batch 17/45, Loss: 0.9258885979652405\nEpoch 10/200, Batch 18/45, Loss: 3.179051399230957\nEpoch 10/200, Batch 19/45, Loss: 1.0595570802688599\nEpoch 10/200, Batch 20/45, Loss: 0.2583615183830261\nEpoch 10/200, Batch 21/45, Loss: 0.5640454888343811\nEpoch 10/200, Batch 22/45, Loss: 0.8579807281494141\nEpoch 10/200, Batch 23/45, Loss: 1.4953118562698364\nEpoch 10/200, Batch 24/45, Loss: 1.4923434257507324\nEpoch 10/200, Batch 25/45, Loss: 0.9035068154335022\nEpoch 10/200, Batch 26/45, Loss: 1.4362456798553467\nEpoch 10/200, Batch 27/45, Loss: 1.5135681629180908\nEpoch 10/200, Batch 28/45, Loss: 1.4248743057250977\nEpoch 10/200, Batch 29/45, Loss: 1.2040600776672363\nEpoch 10/200, Batch 30/45, Loss: 0.7718505263328552\nEpoch 10/200, Batch 31/45, Loss: 0.5286340713500977\nEpoch 10/200, Batch 32/45, Loss: 0.6558340787887573\nEpoch 10/200, Batch 33/45, Loss: 0.26255983114242554\nEpoch 10/200, Batch 34/45, Loss: 1.240077018737793\nEpoch 10/200, Batch 35/45, Loss: 0.5525344610214233\nEpoch 10/200, Batch 36/45, Loss: 0.5317651629447937\nEpoch 10/200, Batch 37/45, Loss: 0.4671786427497864\nEpoch 10/200, Batch 38/45, Loss: 1.574944019317627\nEpoch 10/200, Batch 39/45, Loss: 0.7751688361167908\nEpoch 10/200, Batch 40/45, Loss: 0.6534636616706848\nEpoch 10/200, Batch 41/45, Loss: 0.5890597105026245\nEpoch 10/200, Batch 42/45, Loss: 0.3646114766597748\nEpoch 10/200, Batch 43/45, Loss: 0.5922471284866333\nEpoch 10/200, Batch 44/45, Loss: 0.7826530933380127\nEpoch 10/200, Batch 45/45, Loss: 0.20431143045425415\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  14.352412521839142 Best Val MSE:  14.13761967420578\nEpoch:  11 , Time Elapsed:  1.6457046786944072  mins\nEpoch 11/200, Batch 1/45, Loss: 0.36577898263931274\nEpoch 11/200, Batch 2/45, Loss: 0.7720106840133667\nEpoch 11/200, Batch 3/45, Loss: 0.7719629406929016\nEpoch 11/200, Batch 4/45, Loss: 0.35146504640579224\nEpoch 11/200, Batch 5/45, Loss: 1.1334857940673828\nEpoch 11/200, Batch 6/45, Loss: 0.26778262853622437\nEpoch 11/200, Batch 7/45, Loss: 0.4993293881416321\nEpoch 11/200, Batch 8/45, Loss: 0.6659781336784363\nEpoch 11/200, Batch 9/45, Loss: 0.3992413878440857\nEpoch 11/200, Batch 10/45, Loss: 0.3385680317878723\nEpoch 11/200, Batch 11/45, Loss: 0.32905250787734985\nEpoch 11/200, Batch 12/45, Loss: 0.4410543441772461\nEpoch 11/200, Batch 13/45, Loss: 0.46821102499961853\nEpoch 11/200, Batch 14/45, Loss: 1.1805238723754883\nEpoch 11/200, Batch 15/45, Loss: 0.2472900003194809\nEpoch 11/200, Batch 16/45, Loss: 0.4527479410171509\nEpoch 11/200, Batch 17/45, Loss: 0.5056354999542236\nEpoch 11/200, Batch 18/45, Loss: 0.4854773283004761\nEpoch 11/200, Batch 19/45, Loss: 0.30430111289024353\nEpoch 11/200, Batch 20/45, Loss: 0.7825211882591248\nEpoch 11/200, Batch 21/45, Loss: 0.4007663130760193\nEpoch 11/200, Batch 22/45, Loss: 0.7315342426300049\nEpoch 11/200, Batch 23/45, Loss: 0.6820328831672668\nEpoch 11/200, Batch 24/45, Loss: 0.6377750635147095\nEpoch 11/200, Batch 25/45, Loss: 0.8715211153030396\nEpoch 11/200, Batch 26/45, Loss: 0.42760932445526123\nEpoch 11/200, Batch 27/45, Loss: 0.4694300889968872\nEpoch 11/200, Batch 28/45, Loss: 0.1877787560224533\nEpoch 11/200, Batch 29/45, Loss: 0.3261885344982147\nEpoch 11/200, Batch 30/45, Loss: 0.530402660369873\nEpoch 11/200, Batch 31/45, Loss: 0.2589196264743805\nEpoch 11/200, Batch 32/45, Loss: 0.3849654793739319\nEpoch 11/200, Batch 33/45, Loss: 0.311823308467865\nEpoch 11/200, Batch 34/45, Loss: 0.5379370450973511\nEpoch 11/200, Batch 35/45, Loss: 1.8049145936965942\nEpoch 11/200, Batch 36/45, Loss: 0.4652195870876312\nEpoch 11/200, Batch 37/45, Loss: 0.37141886353492737\nEpoch 11/200, Batch 38/45, Loss: 0.46796661615371704\nEpoch 11/200, Batch 39/45, Loss: 1.1051936149597168\nEpoch 11/200, Batch 40/45, Loss: 0.7549412250518799\nEpoch 11/200, Batch 41/45, Loss: 0.8810034990310669\nEpoch 11/200, Batch 42/45, Loss: 0.40299326181411743\nEpoch 11/200, Batch 43/45, Loss: 0.5181968212127686\nEpoch 11/200, Batch 44/45, Loss: 0.9846034646034241\nEpoch 11/200, Batch 45/45, Loss: 0.5288258194923401\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  16.953273117542267 Best Val MSE:  14.13761967420578\nEpoch:  12 , Time Elapsed:  1.808026127020518  mins\nEpoch 12/200, Batch 1/45, Loss: 0.42138779163360596\nEpoch 12/200, Batch 2/45, Loss: 0.7345002293586731\nEpoch 12/200, Batch 3/45, Loss: 0.719558835029602\nEpoch 12/200, Batch 4/45, Loss: 0.4471551775932312\nEpoch 12/200, Batch 5/45, Loss: 0.9683635234832764\nEpoch 12/200, Batch 6/45, Loss: 0.2831820547580719\nEpoch 12/200, Batch 7/45, Loss: 0.22289592027664185\nEpoch 12/200, Batch 8/45, Loss: 0.5162122845649719\nEpoch 12/200, Batch 9/45, Loss: 0.257302463054657\nEpoch 12/200, Batch 10/45, Loss: 0.45073267817497253\nEpoch 12/200, Batch 11/45, Loss: 0.45368653535842896\nEpoch 12/200, Batch 12/45, Loss: 0.47521284222602844\nEpoch 12/200, Batch 13/45, Loss: 0.6092009544372559\nEpoch 12/200, Batch 14/45, Loss: 0.23318739235401154\nEpoch 12/200, Batch 15/45, Loss: 0.6474825739860535\nEpoch 12/200, Batch 16/45, Loss: 0.2976931035518646\nEpoch 12/200, Batch 17/45, Loss: 0.3159291744232178\nEpoch 12/200, Batch 18/45, Loss: 0.29934877157211304\nEpoch 12/200, Batch 19/45, Loss: 0.29213404655456543\nEpoch 12/200, Batch 20/45, Loss: 0.32127833366394043\nEpoch 12/200, Batch 21/45, Loss: 0.41501617431640625\nEpoch 12/200, Batch 22/45, Loss: 0.3573441207408905\nEpoch 12/200, Batch 23/45, Loss: 0.21936100721359253\nEpoch 12/200, Batch 24/45, Loss: 0.2919415235519409\nEpoch 12/200, Batch 25/45, Loss: 0.3952762484550476\nEpoch 12/200, Batch 26/45, Loss: 0.21608974039554596\nEpoch 12/200, Batch 27/45, Loss: 0.3713859021663666\nEpoch 12/200, Batch 28/45, Loss: 0.8250600099563599\nEpoch 12/200, Batch 29/45, Loss: 0.43178680539131165\nEpoch 12/200, Batch 30/45, Loss: 0.43381965160369873\nEpoch 12/200, Batch 31/45, Loss: 0.5904405117034912\nEpoch 12/200, Batch 32/45, Loss: 0.31508350372314453\nEpoch 12/200, Batch 33/45, Loss: 0.7224778532981873\nEpoch 12/200, Batch 34/45, Loss: 0.9959647059440613\nEpoch 12/200, Batch 35/45, Loss: 0.5224405527114868\nEpoch 12/200, Batch 36/45, Loss: 0.2641914486885071\nEpoch 12/200, Batch 37/45, Loss: 0.5070769786834717\nEpoch 12/200, Batch 38/45, Loss: 0.7168300151824951\nEpoch 12/200, Batch 39/45, Loss: 0.3512435555458069\nEpoch 12/200, Batch 40/45, Loss: 0.847974419593811\nEpoch 12/200, Batch 41/45, Loss: 0.2870313227176666\nEpoch 12/200, Batch 42/45, Loss: 0.3679533302783966\nEpoch 12/200, Batch 43/45, Loss: 0.9250210523605347\nEpoch 12/200, Batch 44/45, Loss: 0.37249335646629333\nEpoch 12/200, Batch 45/45, Loss: 0.6282904148101807\nValidating and Checkpointing!\nBest model Saved! Val MSE:  12.868683457374573\nEpoch:  13 , Time Elapsed:  1.9749112367630004  mins\nEpoch 13/200, Batch 1/45, Loss: 2.329219102859497\nEpoch 13/200, Batch 2/45, Loss: 0.9233813285827637\nEpoch 13/200, Batch 3/45, Loss: 0.7286010384559631\nEpoch 13/200, Batch 4/45, Loss: 0.931890606880188\nEpoch 13/200, Batch 5/45, Loss: 1.3950048685073853\nEpoch 13/200, Batch 6/45, Loss: 0.3623981177806854\nEpoch 13/200, Batch 7/45, Loss: 0.18828696012496948\nEpoch 13/200, Batch 8/45, Loss: 0.48031455278396606\nEpoch 13/200, Batch 9/45, Loss: 0.49176931381225586\nEpoch 13/200, Batch 10/45, Loss: 0.4018481969833374\nEpoch 13/200, Batch 11/45, Loss: 0.3450871706008911\nEpoch 13/200, Batch 12/45, Loss: 0.4800507128238678\nEpoch 13/200, Batch 13/45, Loss: 0.7031672596931458\nEpoch 13/200, Batch 14/45, Loss: 0.38846319913864136\nEpoch 13/200, Batch 15/45, Loss: 0.6086206436157227\nEpoch 13/200, Batch 16/45, Loss: 0.8313331604003906\nEpoch 13/200, Batch 17/45, Loss: 0.638550341129303\nEpoch 13/200, Batch 18/45, Loss: 0.710472822189331\nEpoch 13/200, Batch 19/45, Loss: 0.6386316418647766\nEpoch 13/200, Batch 20/45, Loss: 0.4835784137248993\nEpoch 13/200, Batch 21/45, Loss: 0.4871896207332611\nEpoch 13/200, Batch 22/45, Loss: 0.33145609498023987\nEpoch 13/200, Batch 23/45, Loss: 0.6223764419555664\nEpoch 13/200, Batch 24/45, Loss: 0.23620811104774475\nEpoch 13/200, Batch 25/45, Loss: 0.5500948429107666\nEpoch 13/200, Batch 26/45, Loss: 0.37042033672332764\nEpoch 13/200, Batch 27/45, Loss: 0.2782130241394043\nEpoch 13/200, Batch 28/45, Loss: 0.3621559143066406\nEpoch 13/200, Batch 29/45, Loss: 0.7733680009841919\nEpoch 13/200, Batch 30/45, Loss: 0.2886989116668701\nEpoch 13/200, Batch 31/45, Loss: 0.38669222593307495\nEpoch 13/200, Batch 32/45, Loss: 0.734204888343811\nEpoch 13/200, Batch 33/45, Loss: 0.49725019931793213\nEpoch 13/200, Batch 34/45, Loss: 0.19888333976268768\nEpoch 13/200, Batch 35/45, Loss: 0.2718985080718994\nEpoch 13/200, Batch 36/45, Loss: 0.2002892792224884\nEpoch 13/200, Batch 37/45, Loss: 0.4861193895339966\nEpoch 13/200, Batch 38/45, Loss: 0.3767251968383789\nEpoch 13/200, Batch 39/45, Loss: 0.5315526723861694\nEpoch 13/200, Batch 40/45, Loss: 0.3730970025062561\nEpoch 13/200, Batch 41/45, Loss: 0.5340597033500671\nEpoch 13/200, Batch 42/45, Loss: 1.0323338508605957\nEpoch 13/200, Batch 43/45, Loss: 0.8772094249725342\nEpoch 13/200, Batch 44/45, Loss: 0.5987535119056702\nEpoch 13/200, Batch 45/45, Loss: 0.3373067378997803\nValidating and Checkpointing!\nBest model Saved! Val MSE:  11.521474361419678\nEpoch:  14 , Time Elapsed:  2.139945097764333  mins\nEpoch 14/200, Batch 1/45, Loss: 0.2911735773086548\nEpoch 14/200, Batch 2/45, Loss: 0.2893427908420563\nEpoch 14/200, Batch 3/45, Loss: 0.500296950340271\nEpoch 14/200, Batch 4/45, Loss: 0.16685721278190613\nEpoch 14/200, Batch 5/45, Loss: 0.297784686088562\nEpoch 14/200, Batch 6/45, Loss: 0.4907352924346924\nEpoch 14/200, Batch 7/45, Loss: 0.7651878595352173\nEpoch 14/200, Batch 8/45, Loss: 0.3497511148452759\nEpoch 14/200, Batch 9/45, Loss: 0.9851791858673096\nEpoch 14/200, Batch 10/45, Loss: 0.43268752098083496\nEpoch 14/200, Batch 11/45, Loss: 0.5101929306983948\nEpoch 14/200, Batch 12/45, Loss: 0.29760798811912537\nEpoch 14/200, Batch 13/45, Loss: 0.3249678313732147\nEpoch 14/200, Batch 14/45, Loss: 0.27849146723747253\nEpoch 14/200, Batch 15/45, Loss: 0.4536181092262268\nEpoch 14/200, Batch 16/45, Loss: 0.670225203037262\nEpoch 14/200, Batch 17/45, Loss: 0.48643434047698975\nEpoch 14/200, Batch 18/45, Loss: 0.5356606245040894\nEpoch 14/200, Batch 19/45, Loss: 0.6072648763656616\nEpoch 14/200, Batch 20/45, Loss: 0.9465895295143127\nEpoch 14/200, Batch 21/45, Loss: 0.3878428339958191\nEpoch 14/200, Batch 22/45, Loss: 0.3719539940357208\nEpoch 14/200, Batch 23/45, Loss: 0.7276284098625183\nEpoch 14/200, Batch 24/45, Loss: 0.4132957458496094\nEpoch 14/200, Batch 25/45, Loss: 0.19859716296195984\nEpoch 14/200, Batch 26/45, Loss: 2.117814779281616\nEpoch 14/200, Batch 27/45, Loss: 0.3208402991294861\nEpoch 14/200, Batch 28/45, Loss: 0.4574991464614868\nEpoch 14/200, Batch 29/45, Loss: 0.5760388374328613\nEpoch 14/200, Batch 30/45, Loss: 0.4598916172981262\nEpoch 14/200, Batch 31/45, Loss: 0.7160634994506836\nEpoch 14/200, Batch 32/45, Loss: 0.5815863013267517\nEpoch 14/200, Batch 33/45, Loss: 0.8551379442214966\nEpoch 14/200, Batch 34/45, Loss: 0.6146281957626343\nEpoch 14/200, Batch 35/45, Loss: 0.502665102481842\nEpoch 14/200, Batch 36/45, Loss: 0.24272014200687408\nEpoch 14/200, Batch 37/45, Loss: 0.270862340927124\nEpoch 14/200, Batch 38/45, Loss: 0.35186126828193665\nEpoch 14/200, Batch 39/45, Loss: 0.42294904589653015\nEpoch 14/200, Batch 40/45, Loss: 0.2194426953792572\nEpoch 14/200, Batch 41/45, Loss: 0.30603325366973877\nEpoch 14/200, Batch 42/45, Loss: 0.3742918372154236\nEpoch 14/200, Batch 43/45, Loss: 0.3652249574661255\nEpoch 14/200, Batch 44/45, Loss: 0.18698514997959137\nEpoch 14/200, Batch 45/45, Loss: 0.4193688631057739\nValidating and Checkpointing!\nBest model Saved! Val MSE:  10.529143214225769\nEpoch:  15 , Time Elapsed:  2.306020780404409  mins\nEpoch 15/200, Batch 1/45, Loss: 0.39255809783935547\nEpoch 15/200, Batch 2/45, Loss: 0.34983816742897034\nEpoch 15/200, Batch 3/45, Loss: 0.366333544254303\nEpoch 15/200, Batch 4/45, Loss: 0.27367547154426575\nEpoch 15/200, Batch 5/45, Loss: 0.2863551080226898\nEpoch 15/200, Batch 6/45, Loss: 0.2733597159385681\nEpoch 15/200, Batch 7/45, Loss: 0.41609710454940796\nEpoch 15/200, Batch 8/45, Loss: 0.32600200176239014\nEpoch 15/200, Batch 9/45, Loss: 0.28595253825187683\nEpoch 15/200, Batch 10/45, Loss: 0.3183760643005371\nEpoch 15/200, Batch 11/45, Loss: 0.30820852518081665\nEpoch 15/200, Batch 12/45, Loss: 0.4009952247142792\nEpoch 15/200, Batch 13/45, Loss: 0.6811635494232178\nEpoch 15/200, Batch 14/45, Loss: 0.3184340000152588\nEpoch 15/200, Batch 15/45, Loss: 0.5940824747085571\nEpoch 15/200, Batch 16/45, Loss: 0.22733157873153687\nEpoch 15/200, Batch 17/45, Loss: 0.5504980087280273\nEpoch 15/200, Batch 18/45, Loss: 0.6000331044197083\nEpoch 15/200, Batch 19/45, Loss: 0.4737176299095154\nEpoch 15/200, Batch 20/45, Loss: 0.3309444189071655\nEpoch 15/200, Batch 21/45, Loss: 0.23237724602222443\nEpoch 15/200, Batch 22/45, Loss: 0.3817020058631897\nEpoch 15/200, Batch 23/45, Loss: 0.3329967260360718\nEpoch 15/200, Batch 24/45, Loss: 0.18420834839344025\nEpoch 15/200, Batch 25/45, Loss: 0.31328970193862915\nEpoch 15/200, Batch 26/45, Loss: 0.7398790121078491\nEpoch 15/200, Batch 27/45, Loss: 0.28986018896102905\nEpoch 15/200, Batch 28/45, Loss: 0.48140549659729004\nEpoch 15/200, Batch 29/45, Loss: 0.22760558128356934\nEpoch 15/200, Batch 30/45, Loss: 0.2572776675224304\nEpoch 15/200, Batch 31/45, Loss: 0.5479182600975037\nEpoch 15/200, Batch 32/45, Loss: 0.2331731915473938\nEpoch 15/200, Batch 33/45, Loss: 0.20885604619979858\nEpoch 15/200, Batch 34/45, Loss: 0.5120038390159607\nEpoch 15/200, Batch 35/45, Loss: 0.47380948066711426\nEpoch 15/200, Batch 36/45, Loss: 0.3605666160583496\nEpoch 15/200, Batch 37/45, Loss: 0.38114845752716064\nEpoch 15/200, Batch 38/45, Loss: 0.21291324496269226\nEpoch 15/200, Batch 39/45, Loss: 0.41609251499176025\nEpoch 15/200, Batch 40/45, Loss: 0.4677477478981018\nEpoch 15/200, Batch 41/45, Loss: 0.6017919182777405\nEpoch 15/200, Batch 42/45, Loss: 0.2027548849582672\nEpoch 15/200, Batch 43/45, Loss: 0.5200836658477783\nEpoch 15/200, Batch 44/45, Loss: 1.1182714700698853\nEpoch 15/200, Batch 45/45, Loss: 0.4250025749206543\nValidating and Checkpointing!\nBest model Saved! Val MSE:  9.099566549062729\nEpoch:  16 , Time Elapsed:  2.4781891544659933  mins\nEpoch 16/200, Batch 1/45, Loss: 0.21600306034088135\nEpoch 16/200, Batch 2/45, Loss: 0.294245183467865\nEpoch 16/200, Batch 3/45, Loss: 0.31207388639450073\nEpoch 16/200, Batch 4/45, Loss: 0.3322572708129883\nEpoch 16/200, Batch 5/45, Loss: 0.22475525736808777\nEpoch 16/200, Batch 6/45, Loss: 0.3557626008987427\nEpoch 16/200, Batch 7/45, Loss: 0.24178792536258698\nEpoch 16/200, Batch 8/45, Loss: 0.4224572777748108\nEpoch 16/200, Batch 9/45, Loss: 0.31631672382354736\nEpoch 16/200, Batch 10/45, Loss: 0.2554083466529846\nEpoch 16/200, Batch 11/45, Loss: 0.4984554648399353\nEpoch 16/200, Batch 12/45, Loss: 0.26502054929733276\nEpoch 16/200, Batch 13/45, Loss: 0.3989345133304596\nEpoch 16/200, Batch 14/45, Loss: 0.25411754846572876\nEpoch 16/200, Batch 15/45, Loss: 0.47032785415649414\nEpoch 16/200, Batch 16/45, Loss: 0.1869356632232666\nEpoch 16/200, Batch 17/45, Loss: 0.5102893114089966\nEpoch 16/200, Batch 18/45, Loss: 0.2674412131309509\nEpoch 16/200, Batch 19/45, Loss: 0.6621633172035217\nEpoch 16/200, Batch 20/45, Loss: 0.3883487582206726\nEpoch 16/200, Batch 21/45, Loss: 0.3042888045310974\nEpoch 16/200, Batch 22/45, Loss: 0.4149559438228607\nEpoch 16/200, Batch 23/45, Loss: 0.27067622542381287\nEpoch 16/200, Batch 24/45, Loss: 0.35711669921875\nEpoch 16/200, Batch 25/45, Loss: 0.3653692305088043\nEpoch 16/200, Batch 26/45, Loss: 0.41177260875701904\nEpoch 16/200, Batch 27/45, Loss: 0.35074806213378906\nEpoch 16/200, Batch 28/45, Loss: 0.39490482211112976\nEpoch 16/200, Batch 29/45, Loss: 0.3051808178424835\nEpoch 16/200, Batch 30/45, Loss: 0.3219483494758606\nEpoch 16/200, Batch 31/45, Loss: 0.5248981714248657\nEpoch 16/200, Batch 32/45, Loss: 0.5008386373519897\nEpoch 16/200, Batch 33/45, Loss: 0.515534520149231\nEpoch 16/200, Batch 34/45, Loss: 0.2988026738166809\nEpoch 16/200, Batch 35/45, Loss: 0.31868448853492737\nEpoch 16/200, Batch 36/45, Loss: 0.22938676178455353\nEpoch 16/200, Batch 37/45, Loss: 0.25160127878189087\nEpoch 16/200, Batch 38/45, Loss: 0.19418248534202576\nEpoch 16/200, Batch 39/45, Loss: 0.23413990437984467\nEpoch 16/200, Batch 40/45, Loss: 0.15736214816570282\nEpoch 16/200, Batch 41/45, Loss: 0.1938030868768692\nEpoch 16/200, Batch 42/45, Loss: 0.28840798139572144\nEpoch 16/200, Batch 43/45, Loss: 0.4182029962539673\nEpoch 16/200, Batch 44/45, Loss: 0.24180364608764648\nEpoch 16/200, Batch 45/45, Loss: 0.26853740215301514\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  10.844250798225403 Best Val MSE:  9.099566549062729\nEpoch:  17 , Time Elapsed:  2.6368294954299927  mins\nEpoch 17/200, Batch 1/45, Loss: 0.41662412881851196\nEpoch 17/200, Batch 2/45, Loss: 0.18592318892478943\nEpoch 17/200, Batch 3/45, Loss: 0.41040948033332825\nEpoch 17/200, Batch 4/45, Loss: 0.1755266934633255\nEpoch 17/200, Batch 5/45, Loss: 0.8718016743659973\nEpoch 17/200, Batch 6/45, Loss: 0.29284361004829407\nEpoch 17/200, Batch 7/45, Loss: 0.28821659088134766\nEpoch 17/200, Batch 8/45, Loss: 0.2179776132106781\nEpoch 17/200, Batch 9/45, Loss: 0.3536524176597595\nEpoch 17/200, Batch 10/45, Loss: 0.3382536768913269\nEpoch 17/200, Batch 11/45, Loss: 0.4368330240249634\nEpoch 17/200, Batch 12/45, Loss: 0.2871553897857666\nEpoch 17/200, Batch 13/45, Loss: 0.3307013213634491\nEpoch 17/200, Batch 14/45, Loss: 0.46686023473739624\nEpoch 17/200, Batch 15/45, Loss: 0.2187519669532776\nEpoch 17/200, Batch 16/45, Loss: 0.729417622089386\nEpoch 17/200, Batch 17/45, Loss: 1.1472963094711304\nEpoch 17/200, Batch 18/45, Loss: 0.4144846796989441\nEpoch 17/200, Batch 19/45, Loss: 0.2613111138343811\nEpoch 17/200, Batch 20/45, Loss: 0.7163861989974976\nEpoch 17/200, Batch 21/45, Loss: 0.5757403373718262\nEpoch 17/200, Batch 22/45, Loss: 0.4770095944404602\nEpoch 17/200, Batch 23/45, Loss: 0.9137977361679077\nEpoch 17/200, Batch 24/45, Loss: 0.5884417295455933\nEpoch 17/200, Batch 25/45, Loss: 0.6388798952102661\nEpoch 17/200, Batch 26/45, Loss: 0.17087116837501526\nEpoch 17/200, Batch 27/45, Loss: 0.3661275804042816\nEpoch 17/200, Batch 28/45, Loss: 0.3101752698421478\nEpoch 17/200, Batch 29/45, Loss: 0.3818897008895874\nEpoch 17/200, Batch 30/45, Loss: 0.37602949142456055\nEpoch 17/200, Batch 31/45, Loss: 0.2253674864768982\nEpoch 17/200, Batch 32/45, Loss: 0.22579097747802734\nEpoch 17/200, Batch 33/45, Loss: 1.377619981765747\nEpoch 17/200, Batch 34/45, Loss: 0.2942798137664795\nEpoch 17/200, Batch 35/45, Loss: 0.7261753082275391\nEpoch 17/200, Batch 36/45, Loss: 0.44701892137527466\nEpoch 17/200, Batch 37/45, Loss: 0.5148307681083679\nEpoch 17/200, Batch 38/45, Loss: 0.2637123167514801\nEpoch 17/200, Batch 39/45, Loss: 0.312112033367157\nEpoch 17/200, Batch 40/45, Loss: 0.47945788502693176\nEpoch 17/200, Batch 41/45, Loss: 0.5221049785614014\nEpoch 17/200, Batch 42/45, Loss: 0.7618675827980042\nEpoch 17/200, Batch 43/45, Loss: 0.29909956455230713\nEpoch 17/200, Batch 44/45, Loss: 0.31613612174987793\nEpoch 17/200, Batch 45/45, Loss: 0.39018362760543823\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  12.458955645561218 Best Val MSE:  9.099566549062729\nEpoch:  18 , Time Elapsed:  2.796749226252238  mins\nEpoch 18/200, Batch 1/45, Loss: 0.459126353263855\nEpoch 18/200, Batch 2/45, Loss: 0.2443985641002655\nEpoch 18/200, Batch 3/45, Loss: 0.2078787386417389\nEpoch 18/200, Batch 4/45, Loss: 0.31744909286499023\nEpoch 18/200, Batch 5/45, Loss: 0.6958153247833252\nEpoch 18/200, Batch 6/45, Loss: 1.1139333248138428\nEpoch 18/200, Batch 7/45, Loss: 0.24106769263744354\nEpoch 18/200, Batch 8/45, Loss: 0.5350667238235474\nEpoch 18/200, Batch 9/45, Loss: 0.31528228521347046\nEpoch 18/200, Batch 10/45, Loss: 0.34347933530807495\nEpoch 18/200, Batch 11/45, Loss: 0.574216902256012\nEpoch 18/200, Batch 12/45, Loss: 0.5773473978042603\nEpoch 18/200, Batch 13/45, Loss: 0.5932485461235046\nEpoch 18/200, Batch 14/45, Loss: 0.4287785291671753\nEpoch 18/200, Batch 15/45, Loss: 0.24540115892887115\nEpoch 18/200, Batch 16/45, Loss: 0.21661925315856934\nEpoch 18/200, Batch 17/45, Loss: 0.49678903818130493\nEpoch 18/200, Batch 18/45, Loss: 0.2571262717247009\nEpoch 18/200, Batch 19/45, Loss: 0.39267414808273315\nEpoch 18/200, Batch 20/45, Loss: 0.3257504105567932\nEpoch 18/200, Batch 21/45, Loss: 0.3162592351436615\nEpoch 18/200, Batch 22/45, Loss: 0.2797040641307831\nEpoch 18/200, Batch 23/45, Loss: 0.24759507179260254\nEpoch 18/200, Batch 24/45, Loss: 0.3127468228340149\nEpoch 18/200, Batch 25/45, Loss: 0.46339383721351624\nEpoch 18/200, Batch 26/45, Loss: 0.4131048321723938\nEpoch 18/200, Batch 27/45, Loss: 0.34257906675338745\nEpoch 18/200, Batch 28/45, Loss: 0.22079400718212128\nEpoch 18/200, Batch 29/45, Loss: 0.3106880784034729\nEpoch 18/200, Batch 30/45, Loss: 0.19211536645889282\nEpoch 18/200, Batch 31/45, Loss: 0.28666597604751587\nEpoch 18/200, Batch 32/45, Loss: 0.6162959337234497\nEpoch 18/200, Batch 33/45, Loss: 0.2716295123100281\nEpoch 18/200, Batch 34/45, Loss: 0.5757237672805786\nEpoch 18/200, Batch 35/45, Loss: 0.23384635150432587\nEpoch 18/200, Batch 36/45, Loss: 0.41141945123672485\nEpoch 18/200, Batch 37/45, Loss: 0.41209861636161804\nEpoch 18/200, Batch 38/45, Loss: 0.2389364093542099\nEpoch 18/200, Batch 39/45, Loss: 0.7835991382598877\nEpoch 18/200, Batch 40/45, Loss: 0.4004581868648529\nEpoch 18/200, Batch 41/45, Loss: 0.28670191764831543\nEpoch 18/200, Batch 42/45, Loss: 0.383352667093277\nEpoch 18/200, Batch 43/45, Loss: 0.35901331901550293\nEpoch 18/200, Batch 44/45, Loss: 0.3834441006183624\nEpoch 18/200, Batch 45/45, Loss: 0.21268144249916077\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  12.742029666900635 Best Val MSE:  9.099566549062729\nEpoch:  19 , Time Elapsed:  2.9609338005383807  mins\nEpoch 19/200, Batch 1/45, Loss: 0.14868013560771942\nEpoch 19/200, Batch 2/45, Loss: 0.11354680359363556\nEpoch 19/200, Batch 3/45, Loss: 0.3072713315486908\nEpoch 19/200, Batch 4/45, Loss: 0.24150964617729187\nEpoch 19/200, Batch 5/45, Loss: 0.22477620840072632\nEpoch 19/200, Batch 6/45, Loss: 0.3105776309967041\nEpoch 19/200, Batch 7/45, Loss: 0.2374645471572876\nEpoch 19/200, Batch 8/45, Loss: 0.2553926110267639\nEpoch 19/200, Batch 9/45, Loss: 0.17658880352973938\nEpoch 19/200, Batch 10/45, Loss: 0.3595896065235138\nEpoch 19/200, Batch 11/45, Loss: 0.3668094277381897\nEpoch 19/200, Batch 12/45, Loss: 0.34264013171195984\nEpoch 19/200, Batch 13/45, Loss: 0.26639875769615173\nEpoch 19/200, Batch 14/45, Loss: 0.34623581171035767\nEpoch 19/200, Batch 15/45, Loss: 0.40119606256484985\nEpoch 19/200, Batch 16/45, Loss: 0.3067488670349121\nEpoch 19/200, Batch 17/45, Loss: 0.38073456287384033\nEpoch 19/200, Batch 18/45, Loss: 0.2537267208099365\nEpoch 19/200, Batch 19/45, Loss: 0.38523560762405396\nEpoch 19/200, Batch 20/45, Loss: 0.4301283359527588\nEpoch 19/200, Batch 21/45, Loss: 0.3517190217971802\nEpoch 19/200, Batch 22/45, Loss: 0.3271629214286804\nEpoch 19/200, Batch 23/45, Loss: 0.4114190936088562\nEpoch 19/200, Batch 24/45, Loss: 0.2081979513168335\nEpoch 19/200, Batch 25/45, Loss: 0.339569091796875\nEpoch 19/200, Batch 26/45, Loss: 0.22393353283405304\nEpoch 19/200, Batch 27/45, Loss: 0.3208509087562561\nEpoch 19/200, Batch 28/45, Loss: 0.20115602016448975\nEpoch 19/200, Batch 29/45, Loss: 0.337302565574646\nEpoch 19/200, Batch 30/45, Loss: 1.193912386894226\nEpoch 19/200, Batch 31/45, Loss: 0.21493344008922577\nEpoch 19/200, Batch 32/45, Loss: 0.32880789041519165\nEpoch 19/200, Batch 33/45, Loss: 0.39048922061920166\nEpoch 19/200, Batch 34/45, Loss: 0.37578046321868896\nEpoch 19/200, Batch 35/45, Loss: 0.21839196979999542\nEpoch 19/200, Batch 36/45, Loss: 0.19839966297149658\nEpoch 19/200, Batch 37/45, Loss: 0.33043354749679565\nEpoch 19/200, Batch 38/45, Loss: 0.3370908796787262\nEpoch 19/200, Batch 39/45, Loss: 0.4158574342727661\nEpoch 19/200, Batch 40/45, Loss: 0.4446830153465271\nEpoch 19/200, Batch 41/45, Loss: 0.20298567414283752\nEpoch 19/200, Batch 42/45, Loss: 0.2734087109565735\nEpoch 19/200, Batch 43/45, Loss: 0.399689257144928\nEpoch 19/200, Batch 44/45, Loss: 0.261925607919693\nEpoch 19/200, Batch 45/45, Loss: 0.35363900661468506\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  9.979255437850952 Best Val MSE:  9.099566549062729\nEpoch:  20 , Time Elapsed:  3.118570105234782  mins\nEpoch 20/200, Batch 1/45, Loss: 0.24198293685913086\nEpoch 20/200, Batch 2/45, Loss: 11.569402694702148\nEpoch 20/200, Batch 3/45, Loss: 0.5566307902336121\nEpoch 20/200, Batch 4/45, Loss: 0.7706769704818726\nEpoch 20/200, Batch 5/45, Loss: 1.174942970275879\nEpoch 20/200, Batch 6/45, Loss: 1.6548525094985962\nEpoch 20/200, Batch 7/45, Loss: 1.8872802257537842\nEpoch 20/200, Batch 8/45, Loss: 2.0429773330688477\nEpoch 20/200, Batch 9/45, Loss: 2.153822898864746\nEpoch 20/200, Batch 10/45, Loss: 2.2639970779418945\nEpoch 20/200, Batch 11/45, Loss: 1.913522720336914\nEpoch 20/200, Batch 12/45, Loss: 2.1879303455352783\nEpoch 20/200, Batch 13/45, Loss: 2.1099002361297607\nEpoch 20/200, Batch 14/45, Loss: 2.233715534210205\nEpoch 20/200, Batch 15/45, Loss: 1.4595463275909424\nEpoch 20/200, Batch 16/45, Loss: 2.0546317100524902\nEpoch 20/200, Batch 17/45, Loss: 2.0462687015533447\nEpoch 20/200, Batch 18/45, Loss: 1.4915825128555298\nEpoch 20/200, Batch 19/45, Loss: 1.6364400386810303\nEpoch 20/200, Batch 20/45, Loss: 1.3581409454345703\nEpoch 20/200, Batch 21/45, Loss: 1.4135758876800537\nEpoch 20/200, Batch 22/45, Loss: 1.7484902143478394\nEpoch 20/200, Batch 23/45, Loss: 1.5970515012741089\nEpoch 20/200, Batch 24/45, Loss: 1.4684773683547974\nEpoch 20/200, Batch 25/45, Loss: 2.0029773712158203\nEpoch 20/200, Batch 26/45, Loss: 1.4701253175735474\nEpoch 20/200, Batch 27/45, Loss: 1.4871466159820557\nEpoch 20/200, Batch 28/45, Loss: 1.8438973426818848\nEpoch 20/200, Batch 29/45, Loss: 1.4794533252716064\nEpoch 20/200, Batch 30/45, Loss: 1.3461045026779175\nEpoch 20/200, Batch 31/45, Loss: 1.33915376663208\nEpoch 20/200, Batch 32/45, Loss: 1.4600269794464111\nEpoch 20/200, Batch 33/45, Loss: 1.287701964378357\nEpoch 20/200, Batch 34/45, Loss: 1.3605659008026123\nEpoch 20/200, Batch 35/45, Loss: 1.4321911334991455\nEpoch 20/200, Batch 36/45, Loss: 1.2458076477050781\nEpoch 20/200, Batch 37/45, Loss: 1.687016487121582\nEpoch 20/200, Batch 38/45, Loss: 1.4388903379440308\nEpoch 20/200, Batch 39/45, Loss: 1.4975595474243164\nEpoch 20/200, Batch 40/45, Loss: 1.2794328927993774\nEpoch 20/200, Batch 41/45, Loss: 1.7560166120529175\nEpoch 20/200, Batch 42/45, Loss: 1.735644817352295\nEpoch 20/200, Batch 43/45, Loss: 1.8459168672561646\nEpoch 20/200, Batch 44/45, Loss: 1.4720371961593628\nEpoch 20/200, Batch 45/45, Loss: 0.8887637853622437\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  27.494057178497314 Best Val MSE:  9.099566549062729\nEpoch:  21 , Time Elapsed:  3.279332427183787  mins\nEpoch 21/200, Batch 1/45, Loss: 1.606736183166504\nEpoch 21/200, Batch 2/45, Loss: 1.3314658403396606\nEpoch 21/200, Batch 3/45, Loss: 1.3219757080078125\nEpoch 21/200, Batch 4/45, Loss: 0.7697381973266602\nEpoch 21/200, Batch 5/45, Loss: 0.7529807090759277\nEpoch 21/200, Batch 6/45, Loss: 0.25468623638153076\nEpoch 21/200, Batch 7/45, Loss: 0.5913344621658325\nEpoch 21/200, Batch 8/45, Loss: 1.5622227191925049\nEpoch 21/200, Batch 9/45, Loss: 2.012331008911133\nEpoch 21/200, Batch 10/45, Loss: 0.6189199686050415\nEpoch 21/200, Batch 11/45, Loss: 1.293935775756836\nEpoch 21/200, Batch 12/45, Loss: 0.5222728252410889\nEpoch 21/200, Batch 13/45, Loss: 0.49401918053627014\nEpoch 21/200, Batch 14/45, Loss: 0.6616142392158508\nEpoch 21/200, Batch 15/45, Loss: 1.1011537313461304\nEpoch 21/200, Batch 16/45, Loss: 1.101552963256836\nEpoch 21/200, Batch 17/45, Loss: 1.1148625612258911\nEpoch 21/200, Batch 18/45, Loss: 0.9116544723510742\nEpoch 21/200, Batch 19/45, Loss: 1.055207371711731\nEpoch 21/200, Batch 20/45, Loss: 0.9229639172554016\nEpoch 21/200, Batch 21/45, Loss: 0.948368489742279\nEpoch 21/200, Batch 22/45, Loss: 1.0567381381988525\nEpoch 21/200, Batch 23/45, Loss: 1.00642728805542\nEpoch 21/200, Batch 24/45, Loss: 1.0183438062667847\nEpoch 21/200, Batch 25/45, Loss: 0.6338595747947693\nEpoch 21/200, Batch 26/45, Loss: 1.3099570274353027\nEpoch 21/200, Batch 27/45, Loss: 1.477996826171875\nEpoch 21/200, Batch 28/45, Loss: 0.8367655873298645\nEpoch 21/200, Batch 29/45, Loss: 0.9086048603057861\nEpoch 21/200, Batch 30/45, Loss: 1.0255826711654663\nEpoch 21/200, Batch 31/45, Loss: 0.6485280394554138\nEpoch 21/200, Batch 32/45, Loss: 0.49024805426597595\nEpoch 21/200, Batch 33/45, Loss: 0.5249756574630737\nEpoch 21/200, Batch 34/45, Loss: 0.9163980484008789\nEpoch 21/200, Batch 35/45, Loss: 0.8119287490844727\nEpoch 21/200, Batch 36/45, Loss: 1.1377125978469849\nEpoch 21/200, Batch 37/45, Loss: 0.5861897468566895\nEpoch 21/200, Batch 38/45, Loss: 0.32597649097442627\nEpoch 21/200, Batch 39/45, Loss: 0.7752417325973511\nEpoch 21/200, Batch 40/45, Loss: 0.682538628578186\nEpoch 21/200, Batch 41/45, Loss: 0.267902672290802\nEpoch 21/200, Batch 42/45, Loss: 0.5531232357025146\nEpoch 21/200, Batch 43/45, Loss: 0.5929172039031982\nEpoch 21/200, Batch 44/45, Loss: 0.5652316808700562\nEpoch 21/200, Batch 45/45, Loss: 0.32220199704170227\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  12.15419751405716 Best Val MSE:  9.099566549062729\nEpoch:  22 , Time Elapsed:  3.4395479440689085  mins\nEpoch 22/200, Batch 1/45, Loss: 0.3272428512573242\nEpoch 22/200, Batch 2/45, Loss: 0.6547962427139282\nEpoch 22/200, Batch 3/45, Loss: 0.8349823951721191\nEpoch 22/200, Batch 4/45, Loss: 0.41103795170783997\nEpoch 22/200, Batch 5/45, Loss: 0.5893111824989319\nEpoch 22/200, Batch 6/45, Loss: 0.2638150453567505\nEpoch 22/200, Batch 7/45, Loss: 0.24716220796108246\nEpoch 22/200, Batch 8/45, Loss: 0.44346916675567627\nEpoch 22/200, Batch 9/45, Loss: 0.3815953731536865\nEpoch 22/200, Batch 10/45, Loss: 0.7777445316314697\nEpoch 22/200, Batch 11/45, Loss: 0.4828319549560547\nEpoch 22/200, Batch 12/45, Loss: 0.3751078248023987\nEpoch 22/200, Batch 13/45, Loss: 0.4200981557369232\nEpoch 22/200, Batch 14/45, Loss: 0.4394451379776001\nEpoch 22/200, Batch 15/45, Loss: 3.20908522605896\nEpoch 22/200, Batch 16/45, Loss: 0.35705995559692383\nEpoch 22/200, Batch 17/45, Loss: 0.5464418530464172\nEpoch 22/200, Batch 18/45, Loss: 0.5818098783493042\nEpoch 22/200, Batch 19/45, Loss: 0.7179199457168579\nEpoch 22/200, Batch 20/45, Loss: 0.4934960603713989\nEpoch 22/200, Batch 21/45, Loss: 0.3260033130645752\nEpoch 22/200, Batch 22/45, Loss: 0.5728049278259277\nEpoch 22/200, Batch 23/45, Loss: 0.6307876706123352\nEpoch 22/200, Batch 24/45, Loss: 0.5660059452056885\nEpoch 22/200, Batch 25/45, Loss: 0.7833178043365479\nEpoch 22/200, Batch 26/45, Loss: 0.8875036835670471\nEpoch 22/200, Batch 27/45, Loss: 0.38247543573379517\nEpoch 22/200, Batch 28/45, Loss: 0.23092496395111084\nEpoch 22/200, Batch 29/45, Loss: 0.20336630940437317\nEpoch 22/200, Batch 30/45, Loss: 0.5771834254264832\nEpoch 22/200, Batch 31/45, Loss: 0.6477188467979431\nEpoch 22/200, Batch 32/45, Loss: 0.7003631591796875\nEpoch 22/200, Batch 33/45, Loss: 0.8640421032905579\nEpoch 22/200, Batch 34/45, Loss: 1.1089013814926147\nEpoch 22/200, Batch 35/45, Loss: 0.14621588587760925\nEpoch 22/200, Batch 36/45, Loss: 0.24178466200828552\nEpoch 22/200, Batch 37/45, Loss: 0.326064795255661\nEpoch 22/200, Batch 38/45, Loss: 0.6160643100738525\nEpoch 22/200, Batch 39/45, Loss: 0.8383077383041382\nEpoch 22/200, Batch 40/45, Loss: 0.3136454224586487\nEpoch 22/200, Batch 41/45, Loss: 0.5838828086853027\nEpoch 22/200, Batch 42/45, Loss: 0.6204738616943359\nEpoch 22/200, Batch 43/45, Loss: 0.3830350935459137\nEpoch 22/200, Batch 44/45, Loss: 0.3804056644439697\nEpoch 22/200, Batch 45/45, Loss: 0.38311147689819336\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  10.49622094631195 Best Val MSE:  9.099566549062729\nEpoch:  23 , Time Elapsed:  3.5994649569193524  mins\nEpoch 23/200, Batch 1/45, Loss: 0.7745392322540283\nEpoch 23/200, Batch 2/45, Loss: 0.2475389838218689\nEpoch 23/200, Batch 3/45, Loss: 0.5725336074829102\nEpoch 23/200, Batch 4/45, Loss: 0.5027977824211121\nEpoch 23/200, Batch 5/45, Loss: 0.3787873387336731\nEpoch 23/200, Batch 6/45, Loss: 0.542277455329895\nEpoch 23/200, Batch 7/45, Loss: 0.3102228045463562\nEpoch 23/200, Batch 8/45, Loss: 0.6822941303253174\nEpoch 23/200, Batch 9/45, Loss: 0.4284302592277527\nEpoch 23/200, Batch 10/45, Loss: 1.1128886938095093\nEpoch 23/200, Batch 11/45, Loss: 0.4879409670829773\nEpoch 23/200, Batch 12/45, Loss: 0.3959200978279114\nEpoch 23/200, Batch 13/45, Loss: 0.6967554092407227\nEpoch 23/200, Batch 14/45, Loss: 0.4236235022544861\nEpoch 23/200, Batch 15/45, Loss: 0.5259672403335571\nEpoch 23/200, Batch 16/45, Loss: 0.2897355556488037\nEpoch 23/200, Batch 17/45, Loss: 0.4888867139816284\nEpoch 23/200, Batch 18/45, Loss: 0.4296373128890991\nEpoch 23/200, Batch 19/45, Loss: 0.3146989345550537\nEpoch 23/200, Batch 20/45, Loss: 0.6126370429992676\nEpoch 23/200, Batch 21/45, Loss: 0.22075554728507996\nEpoch 23/200, Batch 22/45, Loss: 0.3788408935070038\nEpoch 23/200, Batch 23/45, Loss: 0.5947610139846802\nEpoch 23/200, Batch 24/45, Loss: 0.27732133865356445\nEpoch 23/200, Batch 25/45, Loss: 0.3260079026222229\nEpoch 23/200, Batch 26/45, Loss: 0.250113308429718\nEpoch 23/200, Batch 27/45, Loss: 0.266940176486969\nEpoch 23/200, Batch 28/45, Loss: 0.2789596617221832\nEpoch 23/200, Batch 29/45, Loss: 0.268565833568573\nEpoch 23/200, Batch 30/45, Loss: 0.701901912689209\nEpoch 23/200, Batch 31/45, Loss: 0.2865445911884308\nEpoch 23/200, Batch 32/45, Loss: 0.40664786100387573\nEpoch 23/200, Batch 33/45, Loss: 0.4197676181793213\nEpoch 23/200, Batch 34/45, Loss: 0.2208353579044342\nEpoch 23/200, Batch 35/45, Loss: 0.2542659342288971\nEpoch 23/200, Batch 36/45, Loss: 0.2878701686859131\nEpoch 23/200, Batch 37/45, Loss: 0.22180607914924622\nEpoch 23/200, Batch 38/45, Loss: 0.46557968854904175\nEpoch 23/200, Batch 39/45, Loss: 0.11142683029174805\nEpoch 23/200, Batch 40/45, Loss: 0.875838577747345\nEpoch 23/200, Batch 41/45, Loss: 0.5645086169242859\nEpoch 23/200, Batch 42/45, Loss: 0.4142165184020996\nEpoch 23/200, Batch 43/45, Loss: 0.35416531562805176\nEpoch 23/200, Batch 44/45, Loss: 0.402704119682312\nEpoch 23/200, Batch 45/45, Loss: 0.3459005057811737\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  9.948277980089188 Best Val MSE:  9.099566549062729\nEpoch:  24 , Time Elapsed:  3.757446527481079  mins\nEpoch 24/200, Batch 1/45, Loss: 0.22518649697303772\nEpoch 24/200, Batch 2/45, Loss: 0.43866586685180664\nEpoch 24/200, Batch 3/45, Loss: 0.3650834858417511\nEpoch 24/200, Batch 4/45, Loss: 0.27521052956581116\nEpoch 24/200, Batch 5/45, Loss: 0.3554866909980774\nEpoch 24/200, Batch 6/45, Loss: 0.2893832325935364\nEpoch 24/200, Batch 7/45, Loss: 0.6564590930938721\nEpoch 24/200, Batch 8/45, Loss: 0.3889144957065582\nEpoch 24/200, Batch 9/45, Loss: 0.4078987240791321\nEpoch 24/200, Batch 10/45, Loss: 0.3096674382686615\nEpoch 24/200, Batch 11/45, Loss: 0.21112871170043945\nEpoch 24/200, Batch 12/45, Loss: 0.35241734981536865\nEpoch 24/200, Batch 13/45, Loss: 0.3155086040496826\nEpoch 24/200, Batch 14/45, Loss: 0.6197929382324219\nEpoch 24/200, Batch 15/45, Loss: 0.634742259979248\nEpoch 24/200, Batch 16/45, Loss: 0.5769418478012085\nEpoch 24/200, Batch 17/45, Loss: 0.7505941390991211\nEpoch 24/200, Batch 18/45, Loss: 0.553166389465332\nEpoch 24/200, Batch 19/45, Loss: 0.37558406591415405\nEpoch 24/200, Batch 20/45, Loss: 0.6588044166564941\nEpoch 24/200, Batch 21/45, Loss: 0.603863537311554\nEpoch 24/200, Batch 22/45, Loss: 0.4986379146575928\nEpoch 24/200, Batch 23/45, Loss: 0.28894758224487305\nEpoch 24/200, Batch 24/45, Loss: 0.6982257962226868\nEpoch 24/200, Batch 25/45, Loss: 0.25684019923210144\nEpoch 24/200, Batch 26/45, Loss: 0.5531668663024902\nEpoch 24/200, Batch 27/45, Loss: 0.4902104437351227\nEpoch 24/200, Batch 28/45, Loss: 0.3926805257797241\nEpoch 24/200, Batch 29/45, Loss: 0.23963701725006104\nEpoch 24/200, Batch 30/45, Loss: 0.2065359652042389\nEpoch 24/200, Batch 31/45, Loss: 1.5227140188217163\nEpoch 24/200, Batch 32/45, Loss: 0.5180819034576416\nEpoch 24/200, Batch 33/45, Loss: 0.49052056670188904\nEpoch 24/200, Batch 34/45, Loss: 0.6327744126319885\nEpoch 24/200, Batch 35/45, Loss: 1.33530855178833\nEpoch 24/200, Batch 36/45, Loss: 1.5831513404846191\nEpoch 24/200, Batch 37/45, Loss: 0.9903719425201416\nEpoch 24/200, Batch 38/45, Loss: 0.8585802316665649\nEpoch 24/200, Batch 39/45, Loss: 0.980359673500061\nEpoch 24/200, Batch 40/45, Loss: 1.0545451641082764\nEpoch 24/200, Batch 41/45, Loss: 1.0942356586456299\nEpoch 24/200, Batch 42/45, Loss: 0.7844909429550171\nEpoch 24/200, Batch 43/45, Loss: 0.16807231307029724\nEpoch 24/200, Batch 44/45, Loss: 0.46624764800071716\nEpoch 24/200, Batch 45/45, Loss: 0.47134682536125183\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  14.92229962348938 Best Val MSE:  9.099566549062729\nEpoch:  25 , Time Elapsed:  3.9149816910425823  mins\nEpoch 25/200, Batch 1/45, Loss: 0.36907824873924255\nEpoch 25/200, Batch 2/45, Loss: 1.622912049293518\nEpoch 25/200, Batch 3/45, Loss: 0.40218013525009155\nEpoch 25/200, Batch 4/45, Loss: 0.5816972851753235\nEpoch 25/200, Batch 5/45, Loss: 0.6973059177398682\nEpoch 25/200, Batch 6/45, Loss: 0.30170804262161255\nEpoch 25/200, Batch 7/45, Loss: 0.6701139211654663\nEpoch 25/200, Batch 8/45, Loss: 0.4945570230484009\nEpoch 25/200, Batch 9/45, Loss: 0.5766891241073608\nEpoch 25/200, Batch 10/45, Loss: 0.6273285150527954\nEpoch 25/200, Batch 11/45, Loss: 0.3955949544906616\nEpoch 25/200, Batch 12/45, Loss: 0.5416378378868103\nEpoch 25/200, Batch 13/45, Loss: 0.6061387062072754\nEpoch 25/200, Batch 14/45, Loss: 0.27386292815208435\nEpoch 25/200, Batch 15/45, Loss: 0.4343286156654358\nEpoch 25/200, Batch 16/45, Loss: 0.3707130551338196\nEpoch 25/200, Batch 17/45, Loss: 0.392628014087677\nEpoch 25/200, Batch 18/45, Loss: 0.4357503652572632\nEpoch 25/200, Batch 19/45, Loss: 0.40084564685821533\nEpoch 25/200, Batch 20/45, Loss: 0.5905116200447083\nEpoch 25/200, Batch 21/45, Loss: 0.2849583923816681\nEpoch 25/200, Batch 22/45, Loss: 1.1866395473480225\nEpoch 25/200, Batch 23/45, Loss: 0.2807769477367401\nEpoch 25/200, Batch 24/45, Loss: 0.28090721368789673\nEpoch 25/200, Batch 25/45, Loss: 0.4711514711380005\nEpoch 25/200, Batch 26/45, Loss: 0.4708781838417053\nEpoch 25/200, Batch 27/45, Loss: 0.1610216498374939\nEpoch 25/200, Batch 28/45, Loss: 0.28760388493537903\nEpoch 25/200, Batch 29/45, Loss: 0.154394268989563\nEpoch 25/200, Batch 30/45, Loss: 0.35588881373405457\nEpoch 25/200, Batch 31/45, Loss: 0.6314328908920288\nEpoch 25/200, Batch 32/45, Loss: 0.5478405952453613\nEpoch 25/200, Batch 33/45, Loss: 0.22422665357589722\nEpoch 25/200, Batch 34/45, Loss: 0.3736206293106079\nEpoch 25/200, Batch 35/45, Loss: 0.24438589811325073\nEpoch 25/200, Batch 36/45, Loss: 0.2410440891981125\nEpoch 25/200, Batch 37/45, Loss: 1.4959635734558105\nEpoch 25/200, Batch 38/45, Loss: 1.7125911712646484\nEpoch 25/200, Batch 39/45, Loss: 0.3830416202545166\nEpoch 25/200, Batch 40/45, Loss: 0.3592974841594696\nEpoch 25/200, Batch 41/45, Loss: 0.6263097524642944\nEpoch 25/200, Batch 42/45, Loss: 0.29869675636291504\nEpoch 25/200, Batch 43/45, Loss: 1.0920504331588745\nEpoch 25/200, Batch 44/45, Loss: 1.3901934623718262\nEpoch 25/200, Batch 45/45, Loss: 1.2091407775878906\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  15.399110078811646 Best Val MSE:  9.099566549062729\nEpoch:  26 , Time Elapsed:  4.077469118436178  mins\nEpoch 26/200, Batch 1/45, Loss: 0.7919008731842041\nEpoch 26/200, Batch 2/45, Loss: 0.743360161781311\nEpoch 26/200, Batch 3/45, Loss: 0.9826595783233643\nEpoch 26/200, Batch 4/45, Loss: 0.46109744906425476\nEpoch 26/200, Batch 5/45, Loss: 0.37223541736602783\nEpoch 26/200, Batch 6/45, Loss: 1.7362650632858276\nEpoch 26/200, Batch 7/45, Loss: 0.2980188727378845\nEpoch 26/200, Batch 8/45, Loss: 0.4424492418766022\nEpoch 26/200, Batch 9/45, Loss: 0.48223644495010376\nEpoch 26/200, Batch 10/45, Loss: 0.5658426880836487\nEpoch 26/200, Batch 11/45, Loss: 0.4535641670227051\nEpoch 26/200, Batch 12/45, Loss: 0.8508833646774292\nEpoch 26/200, Batch 13/45, Loss: 0.4311389923095703\nEpoch 26/200, Batch 14/45, Loss: 0.28647732734680176\nEpoch 26/200, Batch 15/45, Loss: 2.3653948307037354\nEpoch 26/200, Batch 16/45, Loss: 0.4459030032157898\nEpoch 26/200, Batch 17/45, Loss: 0.638386607170105\nEpoch 26/200, Batch 18/45, Loss: 0.37360668182373047\nEpoch 26/200, Batch 19/45, Loss: 0.5117405652999878\nEpoch 26/200, Batch 20/45, Loss: 0.8691333532333374\nEpoch 26/200, Batch 21/45, Loss: 0.47171229124069214\nEpoch 26/200, Batch 22/45, Loss: 0.9890058040618896\nEpoch 26/200, Batch 23/45, Loss: 0.7649353742599487\nEpoch 26/200, Batch 24/45, Loss: 1.072645664215088\nEpoch 26/200, Batch 25/45, Loss: 0.46322840452194214\nEpoch 26/200, Batch 26/45, Loss: 0.9416401386260986\nEpoch 26/200, Batch 27/45, Loss: 1.2636330127716064\nEpoch 26/200, Batch 28/45, Loss: 0.4827647805213928\nEpoch 26/200, Batch 29/45, Loss: 0.3677668571472168\nEpoch 26/200, Batch 30/45, Loss: 0.739874541759491\nEpoch 26/200, Batch 31/45, Loss: 0.4124554693698883\nEpoch 26/200, Batch 32/45, Loss: 0.3934900760650635\nEpoch 26/200, Batch 33/45, Loss: 0.5709507465362549\nEpoch 26/200, Batch 34/45, Loss: 0.4106621742248535\nEpoch 26/200, Batch 35/45, Loss: 0.3964792490005493\nEpoch 26/200, Batch 36/45, Loss: 0.4971819221973419\nEpoch 26/200, Batch 37/45, Loss: 0.6044011116027832\nEpoch 26/200, Batch 38/45, Loss: 0.5850446820259094\nEpoch 26/200, Batch 39/45, Loss: 0.6426335573196411\nEpoch 26/200, Batch 40/45, Loss: 0.5079851150512695\nEpoch 26/200, Batch 41/45, Loss: 0.4308045506477356\nEpoch 26/200, Batch 42/45, Loss: 0.36338186264038086\nEpoch 26/200, Batch 43/45, Loss: 0.4565831422805786\nEpoch 26/200, Batch 44/45, Loss: 0.40822768211364746\nEpoch 26/200, Batch 45/45, Loss: 0.5136997699737549\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  11.645787715911865 Best Val MSE:  9.099566549062729\nEpoch:  27 , Time Elapsed:  4.236189063390096  mins\nEpoch 27/200, Batch 1/45, Loss: 0.30319637060165405\nEpoch 27/200, Batch 2/45, Loss: 0.4473514258861542\nEpoch 27/200, Batch 3/45, Loss: 0.6201975345611572\nEpoch 27/200, Batch 4/45, Loss: 0.2056475281715393\nEpoch 27/200, Batch 5/45, Loss: 0.5747206807136536\nEpoch 27/200, Batch 6/45, Loss: 0.43335095047950745\nEpoch 27/200, Batch 7/45, Loss: 0.30206823348999023\nEpoch 27/200, Batch 8/45, Loss: 0.18990269303321838\nEpoch 27/200, Batch 9/45, Loss: 0.31691697239875793\nEpoch 27/200, Batch 10/45, Loss: 0.35576915740966797\nEpoch 27/200, Batch 11/45, Loss: 0.5647067427635193\nEpoch 27/200, Batch 12/45, Loss: 0.43900465965270996\nEpoch 27/200, Batch 13/45, Loss: 0.8831185102462769\nEpoch 27/200, Batch 14/45, Loss: 0.21406924724578857\nEpoch 27/200, Batch 15/45, Loss: 0.44280412793159485\nEpoch 27/200, Batch 16/45, Loss: 0.5434117317199707\nEpoch 27/200, Batch 17/45, Loss: 0.19160887598991394\nEpoch 27/200, Batch 18/45, Loss: 0.3714708685874939\nEpoch 27/200, Batch 19/45, Loss: 0.29901638627052307\nEpoch 27/200, Batch 20/45, Loss: 0.32948076725006104\nEpoch 27/200, Batch 21/45, Loss: 0.18573515117168427\nEpoch 27/200, Batch 22/45, Loss: 0.3171481490135193\nEpoch 27/200, Batch 23/45, Loss: 0.2881833612918854\nEpoch 27/200, Batch 24/45, Loss: 0.19206199049949646\nEpoch 27/200, Batch 25/45, Loss: 12.540243148803711\nEpoch 27/200, Batch 26/45, Loss: 0.5483965873718262\nEpoch 27/200, Batch 27/45, Loss: 0.43143248558044434\nEpoch 27/200, Batch 28/45, Loss: 0.8404423594474792\nEpoch 27/200, Batch 29/45, Loss: 1.5776033401489258\nEpoch 27/200, Batch 30/45, Loss: 1.6675164699554443\nEpoch 27/200, Batch 31/45, Loss: 2.152078628540039\nEpoch 27/200, Batch 32/45, Loss: 2.0553853511810303\nEpoch 27/200, Batch 33/45, Loss: 1.8188493251800537\nEpoch 27/200, Batch 34/45, Loss: 2.285255193710327\nEpoch 27/200, Batch 35/45, Loss: 2.0683350563049316\nEpoch 27/200, Batch 36/45, Loss: 2.0393643379211426\nEpoch 27/200, Batch 37/45, Loss: 2.1759257316589355\nEpoch 27/200, Batch 38/45, Loss: 1.7085700035095215\nEpoch 27/200, Batch 39/45, Loss: 1.674120306968689\nEpoch 27/200, Batch 40/45, Loss: 1.4843281507492065\nEpoch 27/200, Batch 41/45, Loss: 1.6698963642120361\nEpoch 27/200, Batch 42/45, Loss: 1.6088850498199463\nEpoch 27/200, Batch 43/45, Loss: 1.3885939121246338\nEpoch 27/200, Batch 44/45, Loss: 1.5140944719314575\nEpoch 27/200, Batch 45/45, Loss: 1.584965705871582\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  19.62457311153412 Best Val MSE:  9.099566549062729\nEpoch:  28 , Time Elapsed:  4.396595040957133  mins\nEpoch 28/200, Batch 1/45, Loss: 1.4652800559997559\nEpoch 28/200, Batch 2/45, Loss: 1.4457201957702637\nEpoch 28/200, Batch 3/45, Loss: 1.4443937540054321\nEpoch 28/200, Batch 4/45, Loss: 1.7601299285888672\nEpoch 28/200, Batch 5/45, Loss: 1.4861476421356201\nEpoch 28/200, Batch 6/45, Loss: 1.3559263944625854\nEpoch 28/200, Batch 7/45, Loss: 1.508999228477478\nEpoch 28/200, Batch 8/45, Loss: 1.5791895389556885\nEpoch 28/200, Batch 9/45, Loss: 1.8180729150772095\nEpoch 28/200, Batch 10/45, Loss: 1.2451571226119995\nEpoch 28/200, Batch 11/45, Loss: 1.5403863191604614\nEpoch 28/200, Batch 12/45, Loss: 1.381421685218811\nEpoch 28/200, Batch 13/45, Loss: 1.5208054780960083\nEpoch 28/200, Batch 14/45, Loss: 1.8602190017700195\nEpoch 28/200, Batch 15/45, Loss: 1.4875494241714478\nEpoch 28/200, Batch 16/45, Loss: 1.1605781316757202\nEpoch 28/200, Batch 17/45, Loss: 1.460604190826416\nEpoch 28/200, Batch 18/45, Loss: 1.2391445636749268\nEpoch 28/200, Batch 19/45, Loss: 1.5126992464065552\nEpoch 28/200, Batch 20/45, Loss: 1.2037078142166138\nEpoch 28/200, Batch 21/45, Loss: 1.3627378940582275\nEpoch 28/200, Batch 22/45, Loss: 1.3233281373977661\nEpoch 28/200, Batch 23/45, Loss: 1.3589736223220825\nEpoch 28/200, Batch 24/45, Loss: 1.387241244316101\nEpoch 28/200, Batch 25/45, Loss: 1.2436270713806152\nEpoch 28/200, Batch 26/45, Loss: 2.4752037525177\nEpoch 28/200, Batch 27/45, Loss: 1.6479500532150269\nEpoch 28/200, Batch 28/45, Loss: 1.1829018592834473\nEpoch 28/200, Batch 29/45, Loss: 1.3867106437683105\nEpoch 28/200, Batch 30/45, Loss: 1.2810100317001343\nEpoch 28/200, Batch 31/45, Loss: 1.3191554546356201\nEpoch 28/200, Batch 32/45, Loss: 1.2682322263717651\nEpoch 28/200, Batch 33/45, Loss: 1.1953941583633423\nEpoch 28/200, Batch 34/45, Loss: 1.5268096923828125\nEpoch 28/200, Batch 35/45, Loss: 1.255410075187683\nEpoch 28/200, Batch 36/45, Loss: 1.6222429275512695\nEpoch 28/200, Batch 37/45, Loss: 1.293697476387024\nEpoch 28/200, Batch 38/45, Loss: 1.850295066833496\nEpoch 28/200, Batch 39/45, Loss: 1.2782455682754517\nEpoch 28/200, Batch 40/45, Loss: 1.3129523992538452\nEpoch 28/200, Batch 41/45, Loss: 1.8780796527862549\nEpoch 28/200, Batch 42/45, Loss: 1.2371770143508911\nEpoch 28/200, Batch 43/45, Loss: 1.3316278457641602\nEpoch 28/200, Batch 44/45, Loss: 1.2600674629211426\nEpoch 28/200, Batch 45/45, Loss: 1.4747893810272217\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  19.972315907478333 Best Val MSE:  9.099566549062729\nEpoch:  29 , Time Elapsed:  4.562451620896657  mins\nEpoch 29/200, Batch 1/45, Loss: 1.6261096000671387\nEpoch 29/200, Batch 2/45, Loss: 1.309819221496582\nEpoch 29/200, Batch 3/45, Loss: 1.1801661252975464\nEpoch 29/200, Batch 4/45, Loss: 1.263373851776123\nEpoch 29/200, Batch 5/45, Loss: 1.5576136112213135\nEpoch 29/200, Batch 6/45, Loss: 1.3793363571166992\nEpoch 29/200, Batch 7/45, Loss: 1.64302396774292\nEpoch 29/200, Batch 8/45, Loss: 1.2832571268081665\nEpoch 29/200, Batch 9/45, Loss: 1.2443406581878662\nEpoch 29/200, Batch 10/45, Loss: 1.207502007484436\nEpoch 29/200, Batch 11/45, Loss: 1.200655460357666\nEpoch 29/200, Batch 12/45, Loss: 1.563240885734558\nEpoch 29/200, Batch 13/45, Loss: 1.2820701599121094\nEpoch 29/200, Batch 14/45, Loss: 1.2783282995224\nEpoch 29/200, Batch 15/45, Loss: 1.464216709136963\nEpoch 29/200, Batch 16/45, Loss: 1.242314100265503\nEpoch 29/200, Batch 17/45, Loss: 1.1266576051712036\nEpoch 29/200, Batch 18/45, Loss: 1.2184779644012451\nEpoch 29/200, Batch 19/45, Loss: 1.3322595357894897\nEpoch 29/200, Batch 20/45, Loss: 1.2715771198272705\nEpoch 29/200, Batch 21/45, Loss: 1.825476884841919\nEpoch 29/200, Batch 22/45, Loss: 1.2276520729064941\nEpoch 29/200, Batch 23/45, Loss: 1.3670623302459717\nEpoch 29/200, Batch 24/45, Loss: 1.3474406003952026\nEpoch 29/200, Batch 25/45, Loss: 1.1571855545043945\nEpoch 29/200, Batch 26/45, Loss: 1.270827054977417\nEpoch 29/200, Batch 27/45, Loss: 1.1596187353134155\nEpoch 29/200, Batch 28/45, Loss: 1.4833967685699463\nEpoch 29/200, Batch 29/45, Loss: 1.2872300148010254\nEpoch 29/200, Batch 30/45, Loss: 1.188364028930664\nEpoch 29/200, Batch 31/45, Loss: 1.3428936004638672\nEpoch 29/200, Batch 32/45, Loss: 1.7844281196594238\nEpoch 29/200, Batch 33/45, Loss: 1.2195149660110474\nEpoch 29/200, Batch 34/45, Loss: 1.4156954288482666\nEpoch 29/200, Batch 35/45, Loss: 1.2900153398513794\nEpoch 29/200, Batch 36/45, Loss: 1.3142187595367432\nEpoch 29/200, Batch 37/45, Loss: 1.6112614870071411\nEpoch 29/200, Batch 38/45, Loss: 1.444530963897705\nEpoch 29/200, Batch 39/45, Loss: 1.4874465465545654\nEpoch 29/200, Batch 40/45, Loss: 1.3647116422653198\nEpoch 29/200, Batch 41/45, Loss: 1.2417023181915283\nEpoch 29/200, Batch 42/45, Loss: 1.7178760766983032\nEpoch 29/200, Batch 43/45, Loss: 1.200921654701233\nEpoch 29/200, Batch 44/45, Loss: 1.5648585557937622\nEpoch 29/200, Batch 45/45, Loss: 1.215044617652893\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  19.898011088371277 Best Val MSE:  9.099566549062729\nEpoch:  30 , Time Elapsed:  4.721893747647603  mins\nEpoch 30/200, Batch 1/45, Loss: 1.2145307064056396\nEpoch 30/200, Batch 2/45, Loss: 1.2333606481552124\nEpoch 30/200, Batch 3/45, Loss: 1.328648328781128\nEpoch 30/200, Batch 4/45, Loss: 1.4060473442077637\nEpoch 30/200, Batch 5/45, Loss: 1.2753511667251587\nEpoch 30/200, Batch 6/45, Loss: 1.4113181829452515\nEpoch 30/200, Batch 7/45, Loss: 1.2109014987945557\nEpoch 30/200, Batch 8/45, Loss: 1.373589038848877\nEpoch 30/200, Batch 9/45, Loss: 1.279687523841858\nEpoch 30/200, Batch 10/45, Loss: 1.3069348335266113\nEpoch 30/200, Batch 11/45, Loss: 1.2576870918273926\nEpoch 30/200, Batch 12/45, Loss: 1.120038390159607\nEpoch 30/200, Batch 13/45, Loss: 3.4846251010894775\nEpoch 30/200, Batch 14/45, Loss: 1.2523229122161865\nEpoch 30/200, Batch 15/45, Loss: 1.1895174980163574\nEpoch 30/200, Batch 16/45, Loss: 1.5090570449829102\nEpoch 30/200, Batch 17/45, Loss: 1.1741783618927002\nEpoch 30/200, Batch 18/45, Loss: 1.4404854774475098\nEpoch 30/200, Batch 19/45, Loss: 1.31871497631073\nEpoch 30/200, Batch 20/45, Loss: 1.4653915166854858\nEpoch 30/200, Batch 21/45, Loss: 1.3263874053955078\nEpoch 30/200, Batch 22/45, Loss: 1.4047787189483643\nEpoch 30/200, Batch 23/45, Loss: 1.2687995433807373\nEpoch 30/200, Batch 24/45, Loss: 1.3724857568740845\nEpoch 30/200, Batch 25/45, Loss: 1.2250763177871704\nEpoch 30/200, Batch 26/45, Loss: 1.2077428102493286\nEpoch 30/200, Batch 27/45, Loss: 1.4384291172027588\nEpoch 30/200, Batch 28/45, Loss: 1.157922387123108\nEpoch 30/200, Batch 29/45, Loss: 1.4144673347473145\nEpoch 30/200, Batch 30/45, Loss: 1.184682846069336\nEpoch 30/200, Batch 31/45, Loss: 1.3960323333740234\nEpoch 30/200, Batch 32/45, Loss: 1.2864251136779785\nEpoch 30/200, Batch 33/45, Loss: 1.3580269813537598\nEpoch 30/200, Batch 34/45, Loss: 1.5973585844039917\nEpoch 30/200, Batch 35/45, Loss: 1.2877576351165771\nEpoch 30/200, Batch 36/45, Loss: 1.5157923698425293\nEpoch 30/200, Batch 37/45, Loss: 1.3376325368881226\nEpoch 30/200, Batch 38/45, Loss: 1.5412571430206299\nEpoch 30/200, Batch 39/45, Loss: 1.2347620725631714\nEpoch 30/200, Batch 40/45, Loss: 1.871193289756775\nEpoch 30/200, Batch 41/45, Loss: 2.044832229614258\nEpoch 30/200, Batch 42/45, Loss: 1.2003010511398315\nEpoch 30/200, Batch 43/45, Loss: 1.3949075937271118\nEpoch 30/200, Batch 44/45, Loss: 1.3019540309906006\nEpoch 30/200, Batch 45/45, Loss: 1.5234543085098267\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  22.946503162384033 Best Val MSE:  9.099566549062729\nEpoch:  31 , Time Elapsed:  4.882860664526621  mins\nEpoch 31/200, Batch 1/45, Loss: 1.436569333076477\nEpoch 31/200, Batch 2/45, Loss: 1.2093069553375244\nEpoch 31/200, Batch 3/45, Loss: 1.3992170095443726\nEpoch 31/200, Batch 4/45, Loss: 1.3313392400741577\nEpoch 31/200, Batch 5/45, Loss: 1.2187221050262451\nEpoch 31/200, Batch 6/45, Loss: 1.4207897186279297\nEpoch 31/200, Batch 7/45, Loss: 1.4271764755249023\nEpoch 31/200, Batch 8/45, Loss: 1.67118239402771\nEpoch 31/200, Batch 9/45, Loss: 1.7526600360870361\nEpoch 31/200, Batch 10/45, Loss: 1.3198691606521606\nEpoch 31/200, Batch 11/45, Loss: 1.412055492401123\nEpoch 31/200, Batch 12/45, Loss: 1.251436710357666\nEpoch 31/200, Batch 13/45, Loss: 1.2837494611740112\nEpoch 31/200, Batch 14/45, Loss: 3.3124160766601562\nEpoch 31/200, Batch 15/45, Loss: 1.7694307565689087\nEpoch 31/200, Batch 16/45, Loss: 1.4928520917892456\nEpoch 31/200, Batch 17/45, Loss: 1.4665426015853882\nEpoch 31/200, Batch 18/45, Loss: 1.5100994110107422\nEpoch 31/200, Batch 19/45, Loss: 1.334272027015686\nEpoch 31/200, Batch 20/45, Loss: 1.2721691131591797\nEpoch 31/200, Batch 21/45, Loss: 1.3010371923446655\nEpoch 31/200, Batch 22/45, Loss: 1.659895658493042\nEpoch 31/200, Batch 23/45, Loss: 1.3624550104141235\nEpoch 31/200, Batch 24/45, Loss: 1.2217202186584473\nEpoch 31/200, Batch 25/45, Loss: 1.359863519668579\nEpoch 31/200, Batch 26/45, Loss: 1.2455772161483765\nEpoch 31/200, Batch 27/45, Loss: 1.3912547826766968\nEpoch 31/200, Batch 28/45, Loss: 1.18080735206604\nEpoch 31/200, Batch 29/45, Loss: 1.284698247909546\nEpoch 31/200, Batch 30/45, Loss: 1.3965585231781006\nEpoch 31/200, Batch 31/45, Loss: 1.286469578742981\nEpoch 31/200, Batch 32/45, Loss: 1.3753591775894165\nEpoch 31/200, Batch 33/45, Loss: 1.218261480331421\nEpoch 31/200, Batch 34/45, Loss: 1.2279566526412964\nEpoch 31/200, Batch 35/45, Loss: 1.2874971628189087\nEpoch 31/200, Batch 36/45, Loss: 1.1632148027420044\nEpoch 31/200, Batch 37/45, Loss: 1.111699104309082\nEpoch 31/200, Batch 38/45, Loss: 1.5415890216827393\nEpoch 31/200, Batch 39/45, Loss: 1.2677582502365112\nEpoch 31/200, Batch 40/45, Loss: 0.8152643442153931\nEpoch 31/200, Batch 41/45, Loss: 1.5383918285369873\nEpoch 31/200, Batch 42/45, Loss: 1.248272180557251\nEpoch 31/200, Batch 43/45, Loss: 0.5265507102012634\nEpoch 31/200, Batch 44/45, Loss: 0.5258277654647827\nEpoch 31/200, Batch 45/45, Loss: 0.4129149317741394\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  14.36979615688324 Best Val MSE:  9.099566549062729\nEpoch:  32 , Time Elapsed:  5.041722357273102  mins\nEpoch 32/200, Batch 1/45, Loss: 0.31632164120674133\nEpoch 32/200, Batch 2/45, Loss: 0.6112227439880371\nEpoch 32/200, Batch 3/45, Loss: 0.4540156424045563\nEpoch 32/200, Batch 4/45, Loss: 0.7272155284881592\nEpoch 32/200, Batch 5/45, Loss: 1.6464594602584839\nEpoch 32/200, Batch 6/45, Loss: 0.9746532440185547\nEpoch 32/200, Batch 7/45, Loss: 0.27545204758644104\nEpoch 32/200, Batch 8/45, Loss: 0.5347313284873962\nEpoch 32/200, Batch 9/45, Loss: 0.41895371675491333\nEpoch 32/200, Batch 10/45, Loss: 0.3857220709323883\nEpoch 32/200, Batch 11/45, Loss: 0.34123915433883667\nEpoch 32/200, Batch 12/45, Loss: 1.1828866004943848\nEpoch 32/200, Batch 13/45, Loss: 0.5196358561515808\nEpoch 32/200, Batch 14/45, Loss: 0.20144572854042053\nEpoch 32/200, Batch 15/45, Loss: 0.38029614090919495\nEpoch 32/200, Batch 16/45, Loss: 0.4336804151535034\nEpoch 32/200, Batch 17/45, Loss: 0.735491931438446\nEpoch 32/200, Batch 18/45, Loss: 0.2982102632522583\nEpoch 32/200, Batch 19/45, Loss: 0.2683781683444977\nEpoch 32/200, Batch 20/45, Loss: 0.38041338324546814\nEpoch 32/200, Batch 21/45, Loss: 0.31238114833831787\nEpoch 32/200, Batch 22/45, Loss: 0.6023764610290527\nEpoch 32/200, Batch 23/45, Loss: 0.3005848526954651\nEpoch 32/200, Batch 24/45, Loss: 0.36427929997444153\nEpoch 32/200, Batch 25/45, Loss: 3.727652072906494\nEpoch 32/200, Batch 26/45, Loss: 0.9270926713943481\nEpoch 32/200, Batch 27/45, Loss: 1.045812964439392\nEpoch 32/200, Batch 28/45, Loss: 1.9256927967071533\nEpoch 32/200, Batch 29/45, Loss: 1.0193660259246826\nEpoch 32/200, Batch 30/45, Loss: 1.1888431310653687\nEpoch 32/200, Batch 31/45, Loss: 1.1511473655700684\nEpoch 32/200, Batch 32/45, Loss: 1.2842552661895752\nEpoch 32/200, Batch 33/45, Loss: 0.7589939832687378\nEpoch 32/200, Batch 34/45, Loss: 1.1376808881759644\nEpoch 32/200, Batch 35/45, Loss: 0.3870498538017273\nEpoch 32/200, Batch 36/45, Loss: 0.3114793002605438\nEpoch 32/200, Batch 37/45, Loss: 0.5451909303665161\nEpoch 32/200, Batch 38/45, Loss: 0.3114517629146576\nEpoch 32/200, Batch 39/45, Loss: 0.6630949974060059\nEpoch 32/200, Batch 40/45, Loss: 0.636623740196228\nEpoch 32/200, Batch 41/45, Loss: 0.6061020493507385\nEpoch 32/200, Batch 42/45, Loss: 0.47629088163375854\nEpoch 32/200, Batch 43/45, Loss: 0.4596523344516754\nEpoch 32/200, Batch 44/45, Loss: 0.543370246887207\nEpoch 32/200, Batch 45/45, Loss: 0.2623979449272156\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  11.911825001239777 Best Val MSE:  9.099566549062729\nEpoch:  33 , Time Elapsed:  5.200199731191  mins\nEpoch 33/200, Batch 1/45, Loss: 0.5936875939369202\nEpoch 33/200, Batch 2/45, Loss: 0.4216683506965637\nEpoch 33/200, Batch 3/45, Loss: 0.15031187236309052\nEpoch 33/200, Batch 4/45, Loss: 0.4545007646083832\nEpoch 33/200, Batch 5/45, Loss: 0.25653088092803955\nEpoch 33/200, Batch 6/45, Loss: 0.5821097493171692\nEpoch 33/200, Batch 7/45, Loss: 0.7505098581314087\nEpoch 33/200, Batch 8/45, Loss: 0.36231833696365356\nEpoch 33/200, Batch 9/45, Loss: 0.5339443683624268\nEpoch 33/200, Batch 10/45, Loss: 0.15715432167053223\nEpoch 33/200, Batch 11/45, Loss: 0.26842671632766724\nEpoch 33/200, Batch 12/45, Loss: 0.3383602499961853\nEpoch 33/200, Batch 13/45, Loss: 0.24184849858283997\nEpoch 33/200, Batch 14/45, Loss: 0.4397243559360504\nEpoch 33/200, Batch 15/45, Loss: 0.6785944104194641\nEpoch 33/200, Batch 16/45, Loss: 0.3138343095779419\nEpoch 33/200, Batch 17/45, Loss: 0.8546653985977173\nEpoch 33/200, Batch 18/45, Loss: 0.33767104148864746\nEpoch 33/200, Batch 19/45, Loss: 0.45008385181427\nEpoch 33/200, Batch 20/45, Loss: 0.36092454195022583\nEpoch 33/200, Batch 21/45, Loss: 0.6796765923500061\nEpoch 33/200, Batch 22/45, Loss: 0.9544875025749207\nEpoch 33/200, Batch 23/45, Loss: 0.43394285440444946\nEpoch 33/200, Batch 24/45, Loss: 0.16816602647304535\nEpoch 33/200, Batch 25/45, Loss: 0.19064390659332275\nEpoch 33/200, Batch 26/45, Loss: 0.28005823493003845\nEpoch 33/200, Batch 27/45, Loss: 0.14708822965621948\nEpoch 33/200, Batch 28/45, Loss: 0.19612395763397217\nEpoch 33/200, Batch 29/45, Loss: 0.30673879384994507\nEpoch 33/200, Batch 30/45, Loss: 0.4784376919269562\nEpoch 33/200, Batch 31/45, Loss: 0.2564706802368164\nEpoch 33/200, Batch 32/45, Loss: 0.28296342492103577\nEpoch 33/200, Batch 33/45, Loss: 0.3755497634410858\nEpoch 33/200, Batch 34/45, Loss: 0.5808073878288269\nEpoch 33/200, Batch 35/45, Loss: 0.4407453238964081\nEpoch 33/200, Batch 36/45, Loss: 0.4040362238883972\nEpoch 33/200, Batch 37/45, Loss: 0.6892534494400024\nEpoch 33/200, Batch 38/45, Loss: 0.4969756007194519\nEpoch 33/200, Batch 39/45, Loss: 0.187157541513443\nEpoch 33/200, Batch 40/45, Loss: 0.510028064250946\nEpoch 33/200, Batch 41/45, Loss: 0.38121944665908813\nEpoch 33/200, Batch 42/45, Loss: 0.9698893427848816\nEpoch 33/200, Batch 43/45, Loss: 0.24905017018318176\nEpoch 33/200, Batch 44/45, Loss: 0.4692513346672058\nEpoch 33/200, Batch 45/45, Loss: 0.5080940127372742\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  10.731551110744476 Best Val MSE:  9.099566549062729\nEpoch:  34 , Time Elapsed:  5.35814608335495  mins\nEpoch 34/200, Batch 1/45, Loss: 0.25288891792297363\nEpoch 34/200, Batch 2/45, Loss: 0.3995433449745178\nEpoch 34/200, Batch 3/45, Loss: 0.2842189073562622\nEpoch 34/200, Batch 4/45, Loss: 0.43495869636535645\nEpoch 34/200, Batch 5/45, Loss: 0.32124245166778564\nEpoch 34/200, Batch 6/45, Loss: 0.2956200838088989\nEpoch 34/200, Batch 7/45, Loss: 0.5158767700195312\nEpoch 34/200, Batch 8/45, Loss: 0.24818339943885803\nEpoch 34/200, Batch 9/45, Loss: 0.41124486923217773\nEpoch 34/200, Batch 10/45, Loss: 0.3168618977069855\nEpoch 34/200, Batch 11/45, Loss: 0.2904495894908905\nEpoch 34/200, Batch 12/45, Loss: 0.2633630633354187\nEpoch 34/200, Batch 13/45, Loss: 0.2965995669364929\nEpoch 34/200, Batch 14/45, Loss: 0.2707288861274719\nEpoch 34/200, Batch 15/45, Loss: 0.4709351062774658\nEpoch 34/200, Batch 16/45, Loss: 0.5422149896621704\nEpoch 34/200, Batch 17/45, Loss: 0.4853777289390564\nEpoch 34/200, Batch 18/45, Loss: 0.59723961353302\nEpoch 34/200, Batch 19/45, Loss: 0.5879225134849548\nEpoch 34/200, Batch 20/45, Loss: 0.17776557803153992\nEpoch 34/200, Batch 21/45, Loss: 0.2521872818470001\nEpoch 34/200, Batch 22/45, Loss: 0.6313982605934143\nEpoch 34/200, Batch 23/45, Loss: 0.6659444570541382\nEpoch 34/200, Batch 24/45, Loss: 0.4439566433429718\nEpoch 34/200, Batch 25/45, Loss: 0.5602203607559204\nEpoch 34/200, Batch 26/45, Loss: 0.814724862575531\nEpoch 34/200, Batch 27/45, Loss: 0.33448415994644165\nEpoch 34/200, Batch 28/45, Loss: 0.5375605821609497\nEpoch 34/200, Batch 29/45, Loss: 0.45908451080322266\nEpoch 34/200, Batch 30/45, Loss: 0.6022148132324219\nEpoch 34/200, Batch 31/45, Loss: 0.48664385080337524\nEpoch 34/200, Batch 32/45, Loss: 0.31603819131851196\nEpoch 34/200, Batch 33/45, Loss: 0.27052563428878784\nEpoch 34/200, Batch 34/45, Loss: 0.24713920056819916\nEpoch 34/200, Batch 35/45, Loss: 0.4010043144226074\nEpoch 34/200, Batch 36/45, Loss: 0.49888625741004944\nEpoch 34/200, Batch 37/45, Loss: 0.309512734413147\nEpoch 34/200, Batch 38/45, Loss: 0.2579348087310791\nEpoch 34/200, Batch 39/45, Loss: 0.22549745440483093\nEpoch 34/200, Batch 40/45, Loss: 0.35017895698547363\nEpoch 34/200, Batch 41/45, Loss: 0.33071190118789673\nEpoch 34/200, Batch 42/45, Loss: 0.5822461843490601\nEpoch 34/200, Batch 43/45, Loss: 0.8629037737846375\nEpoch 34/200, Batch 44/45, Loss: 0.3946855664253235\nEpoch 34/200, Batch 45/45, Loss: 0.5335763096809387\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  10.817265450954437 Best Val MSE:  9.099566549062729\nEpoch:  35 , Time Elapsed:  5.517589787642161  mins\nEpoch 35/200, Batch 1/45, Loss: 0.24078905582427979\nEpoch 35/200, Batch 2/45, Loss: 0.18652160465717316\nEpoch 35/200, Batch 3/45, Loss: 0.30280476808547974\nEpoch 35/200, Batch 4/45, Loss: 0.5618677139282227\nEpoch 35/200, Batch 5/45, Loss: 0.5358798503875732\nEpoch 35/200, Batch 6/45, Loss: 0.4704378843307495\nEpoch 35/200, Batch 7/45, Loss: 0.63728928565979\nEpoch 35/200, Batch 8/45, Loss: 0.22043129801750183\nEpoch 35/200, Batch 9/45, Loss: 0.1740334928035736\nEpoch 35/200, Batch 10/45, Loss: 1.3666554689407349\nEpoch 35/200, Batch 11/45, Loss: 0.35630786418914795\nEpoch 35/200, Batch 12/45, Loss: 0.3419267535209656\nEpoch 35/200, Batch 13/45, Loss: 0.26826438307762146\nEpoch 35/200, Batch 14/45, Loss: 0.20146960020065308\nEpoch 35/200, Batch 15/45, Loss: 0.43234309554100037\nEpoch 35/200, Batch 16/45, Loss: 0.21021680533885956\nEpoch 35/200, Batch 17/45, Loss: 0.6674374938011169\nEpoch 35/200, Batch 18/45, Loss: 0.26395806670188904\nEpoch 35/200, Batch 19/45, Loss: 0.26140668988227844\nEpoch 35/200, Batch 20/45, Loss: 0.25816863775253296\nEpoch 35/200, Batch 21/45, Loss: 0.23261281847953796\nEpoch 35/200, Batch 22/45, Loss: 0.6839006543159485\nEpoch 35/200, Batch 23/45, Loss: 0.3418133556842804\nEpoch 35/200, Batch 24/45, Loss: 0.3524821996688843\nEpoch 35/200, Batch 25/45, Loss: 0.8614357113838196\nEpoch 35/200, Batch 26/45, Loss: 0.5478007793426514\nEpoch 35/200, Batch 27/45, Loss: 0.9615672826766968\nEpoch 35/200, Batch 28/45, Loss: 0.4633168876171112\nEpoch 35/200, Batch 29/45, Loss: 0.3014221489429474\nEpoch 35/200, Batch 30/45, Loss: 0.38351184129714966\nEpoch 35/200, Batch 31/45, Loss: 0.2540241479873657\nEpoch 35/200, Batch 32/45, Loss: 0.34809714555740356\nEpoch 35/200, Batch 33/45, Loss: 0.3575144410133362\nEpoch 35/200, Batch 34/45, Loss: 0.4647347331047058\nEpoch 35/200, Batch 35/45, Loss: 0.1976242959499359\nEpoch 35/200, Batch 36/45, Loss: 0.15798960626125336\nEpoch 35/200, Batch 37/45, Loss: 0.24919331073760986\nEpoch 35/200, Batch 38/45, Loss: 0.35622355341911316\nEpoch 35/200, Batch 39/45, Loss: 0.22592967748641968\nEpoch 35/200, Batch 40/45, Loss: 0.29556524753570557\nEpoch 35/200, Batch 41/45, Loss: 0.24652530252933502\nEpoch 35/200, Batch 42/45, Loss: 0.3361433148384094\nEpoch 35/200, Batch 43/45, Loss: 0.4030922055244446\nEpoch 35/200, Batch 44/45, Loss: 0.4203501343727112\nEpoch 35/200, Batch 45/45, Loss: 0.6692728996276855\nValidating and Checkpointing!\nBest model Saved! Val MSE:  8.94810563325882\nEpoch:  36 , Time Elapsed:  5.688294696807861  mins\nEpoch 36/200, Batch 1/45, Loss: 0.4006752371788025\nEpoch 36/200, Batch 2/45, Loss: 0.20729565620422363\nEpoch 36/200, Batch 3/45, Loss: 0.6389191746711731\nEpoch 36/200, Batch 4/45, Loss: 0.32345423102378845\nEpoch 36/200, Batch 5/45, Loss: 0.2773149013519287\nEpoch 36/200, Batch 6/45, Loss: 0.24150824546813965\nEpoch 36/200, Batch 7/45, Loss: 0.6871621608734131\nEpoch 36/200, Batch 8/45, Loss: 0.25185316801071167\nEpoch 36/200, Batch 9/45, Loss: 0.32717838883399963\nEpoch 36/200, Batch 10/45, Loss: 0.31104323267936707\nEpoch 36/200, Batch 11/45, Loss: 0.13312119245529175\nEpoch 36/200, Batch 12/45, Loss: 0.21910107135772705\nEpoch 36/200, Batch 13/45, Loss: 0.2659282386302948\nEpoch 36/200, Batch 14/45, Loss: 0.38720762729644775\nEpoch 36/200, Batch 15/45, Loss: 0.332470178604126\nEpoch 36/200, Batch 16/45, Loss: 0.3373461961746216\nEpoch 36/200, Batch 17/45, Loss: 0.22040776908397675\nEpoch 36/200, Batch 18/45, Loss: 0.45703837275505066\nEpoch 36/200, Batch 19/45, Loss: 0.3322051465511322\nEpoch 36/200, Batch 20/45, Loss: 0.4045407176017761\nEpoch 36/200, Batch 21/45, Loss: 0.4181603193283081\nEpoch 36/200, Batch 22/45, Loss: 0.4346902370452881\nEpoch 36/200, Batch 23/45, Loss: 0.3131254017353058\nEpoch 36/200, Batch 24/45, Loss: 0.31250229477882385\nEpoch 36/200, Batch 25/45, Loss: 0.7124594449996948\nEpoch 36/200, Batch 26/45, Loss: 0.1828964799642563\nEpoch 36/200, Batch 27/45, Loss: 0.5229054689407349\nEpoch 36/200, Batch 28/45, Loss: 0.4367879629135132\nEpoch 36/200, Batch 29/45, Loss: 0.21947598457336426\nEpoch 36/200, Batch 30/45, Loss: 0.27749669551849365\nEpoch 36/200, Batch 31/45, Loss: 0.7599090337753296\nEpoch 36/200, Batch 32/45, Loss: 0.5406302213668823\nEpoch 36/200, Batch 33/45, Loss: 0.5520686507225037\nEpoch 36/200, Batch 34/45, Loss: 0.7921085357666016\nEpoch 36/200, Batch 35/45, Loss: 0.5241689682006836\nEpoch 36/200, Batch 36/45, Loss: 0.09339535981416702\nEpoch 36/200, Batch 37/45, Loss: 0.37419936060905457\nEpoch 36/200, Batch 38/45, Loss: 0.39421170949935913\nEpoch 36/200, Batch 39/45, Loss: 0.3824101686477661\nEpoch 36/200, Batch 40/45, Loss: 0.213191956281662\nEpoch 36/200, Batch 41/45, Loss: 0.2073058933019638\nEpoch 36/200, Batch 42/45, Loss: 0.22736161947250366\nEpoch 36/200, Batch 43/45, Loss: 0.6955968737602234\nEpoch 36/200, Batch 44/45, Loss: 0.2988207936286926\nEpoch 36/200, Batch 45/45, Loss: 0.3619648516178131\nValidating and Checkpointing!\nBest model Saved! Val MSE:  6.859336465597153\nEpoch:  37 , Time Elapsed:  5.856156432628632  mins\nEpoch 37/200, Batch 1/45, Loss: 0.5290778875350952\nEpoch 37/200, Batch 2/45, Loss: 0.2257741242647171\nEpoch 37/200, Batch 3/45, Loss: 0.19908931851387024\nEpoch 37/200, Batch 4/45, Loss: 0.1965392827987671\nEpoch 37/200, Batch 5/45, Loss: 0.35623687505722046\nEpoch 37/200, Batch 6/45, Loss: 0.30787378549575806\nEpoch 37/200, Batch 7/45, Loss: 0.4210326075553894\nEpoch 37/200, Batch 8/45, Loss: 0.34635019302368164\nEpoch 37/200, Batch 9/45, Loss: 0.46599632501602173\nEpoch 37/200, Batch 10/45, Loss: 0.9900569915771484\nEpoch 37/200, Batch 11/45, Loss: 0.2535541355609894\nEpoch 37/200, Batch 12/45, Loss: 0.4905915856361389\nEpoch 37/200, Batch 13/45, Loss: 0.21159183979034424\nEpoch 37/200, Batch 14/45, Loss: 0.17995744943618774\nEpoch 37/200, Batch 15/45, Loss: 0.511231005191803\nEpoch 37/200, Batch 16/45, Loss: 0.31066274642944336\nEpoch 37/200, Batch 17/45, Loss: 0.3250272274017334\nEpoch 37/200, Batch 18/45, Loss: 0.5716655254364014\nEpoch 37/200, Batch 19/45, Loss: 0.4391781985759735\nEpoch 37/200, Batch 20/45, Loss: 0.4113014340400696\nEpoch 37/200, Batch 21/45, Loss: 0.2743368148803711\nEpoch 37/200, Batch 22/45, Loss: 0.5535117983818054\nEpoch 37/200, Batch 23/45, Loss: 0.35188135504722595\nEpoch 37/200, Batch 24/45, Loss: 0.21293461322784424\nEpoch 37/200, Batch 25/45, Loss: 0.2752799391746521\nEpoch 37/200, Batch 26/45, Loss: 0.23375578224658966\nEpoch 37/200, Batch 27/45, Loss: 0.2128797471523285\nEpoch 37/200, Batch 28/45, Loss: 0.2817801833152771\nEpoch 37/200, Batch 29/45, Loss: 0.29371345043182373\nEpoch 37/200, Batch 30/45, Loss: 0.4388968050479889\nEpoch 37/200, Batch 31/45, Loss: 0.7125453948974609\nEpoch 37/200, Batch 32/45, Loss: 0.28672048449516296\nEpoch 37/200, Batch 33/45, Loss: 0.3038429617881775\nEpoch 37/200, Batch 34/45, Loss: 0.1742970049381256\nEpoch 37/200, Batch 35/45, Loss: 0.19194574654102325\nEpoch 37/200, Batch 36/45, Loss: 0.513270378112793\nEpoch 37/200, Batch 37/45, Loss: 1.8260740041732788\nEpoch 37/200, Batch 38/45, Loss: 0.2684769034385681\nEpoch 37/200, Batch 39/45, Loss: 0.6611886620521545\nEpoch 37/200, Batch 40/45, Loss: 0.33942681550979614\nEpoch 37/200, Batch 41/45, Loss: 0.5152188539505005\nEpoch 37/200, Batch 42/45, Loss: 1.0132484436035156\nEpoch 37/200, Batch 43/45, Loss: 0.6635450720787048\nEpoch 37/200, Batch 44/45, Loss: 0.6674007773399353\nEpoch 37/200, Batch 45/45, Loss: 0.5894922018051147\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  11.40408980846405 Best Val MSE:  6.859336465597153\nEpoch:  38 , Time Elapsed:  6.014867262045542  mins\nEpoch 38/200, Batch 1/45, Loss: 0.7097439765930176\nEpoch 38/200, Batch 2/45, Loss: 0.19190439581871033\nEpoch 38/200, Batch 3/45, Loss: 1.4600017070770264\nEpoch 38/200, Batch 4/45, Loss: 0.3009524345397949\nEpoch 38/200, Batch 5/45, Loss: 0.34849950671195984\nEpoch 38/200, Batch 6/45, Loss: 0.31796714663505554\nEpoch 38/200, Batch 7/45, Loss: 0.5831900835037231\nEpoch 38/200, Batch 8/45, Loss: 0.30100592970848083\nEpoch 38/200, Batch 9/45, Loss: 0.2956206500530243\nEpoch 38/200, Batch 10/45, Loss: 0.2213902473449707\nEpoch 38/200, Batch 11/45, Loss: 0.3140992522239685\nEpoch 38/200, Batch 12/45, Loss: 0.525120735168457\nEpoch 38/200, Batch 13/45, Loss: 0.29156380891799927\nEpoch 38/200, Batch 14/45, Loss: 0.562148928642273\nEpoch 38/200, Batch 15/45, Loss: 0.7574827671051025\nEpoch 38/200, Batch 16/45, Loss: 0.259305477142334\nEpoch 38/200, Batch 17/45, Loss: 0.23107817769050598\nEpoch 38/200, Batch 18/45, Loss: 0.3855791687965393\nEpoch 38/200, Batch 19/45, Loss: 0.26863330602645874\nEpoch 38/200, Batch 20/45, Loss: 0.49637991189956665\nEpoch 38/200, Batch 21/45, Loss: 0.2537296414375305\nEpoch 38/200, Batch 22/45, Loss: 0.20215368270874023\nEpoch 38/200, Batch 23/45, Loss: 0.2964009642601013\nEpoch 38/200, Batch 24/45, Loss: 0.3014994263648987\nEpoch 38/200, Batch 25/45, Loss: 0.3083738684654236\nEpoch 38/200, Batch 26/45, Loss: 0.15280571579933167\nEpoch 38/200, Batch 27/45, Loss: 0.6974754333496094\nEpoch 38/200, Batch 28/45, Loss: 0.3287341296672821\nEpoch 38/200, Batch 29/45, Loss: 0.45210468769073486\nEpoch 38/200, Batch 30/45, Loss: 0.3567309081554413\nEpoch 38/200, Batch 31/45, Loss: 0.27784639596939087\nEpoch 38/200, Batch 32/45, Loss: 0.4390032887458801\nEpoch 38/200, Batch 33/45, Loss: 0.6525174975395203\nEpoch 38/200, Batch 34/45, Loss: 0.39722827076911926\nEpoch 38/200, Batch 35/45, Loss: 0.29104018211364746\nEpoch 38/200, Batch 36/45, Loss: 1.5431057214736938\nEpoch 38/200, Batch 37/45, Loss: 0.4590810537338257\nEpoch 38/200, Batch 38/45, Loss: 0.5151172876358032\nEpoch 38/200, Batch 39/45, Loss: 0.19023559987545013\nEpoch 38/200, Batch 40/45, Loss: 0.8461371064186096\nEpoch 38/200, Batch 41/45, Loss: 0.46504899859428406\nEpoch 38/200, Batch 42/45, Loss: 0.26875340938568115\nEpoch 38/200, Batch 43/45, Loss: 0.31175416707992554\nEpoch 38/200, Batch 44/45, Loss: 0.2693644165992737\nEpoch 38/200, Batch 45/45, Loss: 0.3947358727455139\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  10.306227445602417 Best Val MSE:  6.859336465597153\nEpoch:  39 , Time Elapsed:  6.181527980168661  mins\nEpoch 39/200, Batch 1/45, Loss: 0.14767208695411682\nEpoch 39/200, Batch 2/45, Loss: 0.27602919936180115\nEpoch 39/200, Batch 3/45, Loss: 1.5296201705932617\nEpoch 39/200, Batch 4/45, Loss: 0.3147415816783905\nEpoch 39/200, Batch 5/45, Loss: 0.30673447251319885\nEpoch 39/200, Batch 6/45, Loss: 0.38959965109825134\nEpoch 39/200, Batch 7/45, Loss: 0.5418819785118103\nEpoch 39/200, Batch 8/45, Loss: 0.22483092546463013\nEpoch 39/200, Batch 9/45, Loss: 0.5130120515823364\nEpoch 39/200, Batch 10/45, Loss: 0.4033837914466858\nEpoch 39/200, Batch 11/45, Loss: 0.36802566051483154\nEpoch 39/200, Batch 12/45, Loss: 1.2480939626693726\nEpoch 39/200, Batch 13/45, Loss: 0.651991605758667\nEpoch 39/200, Batch 14/45, Loss: 0.18199430406093597\nEpoch 39/200, Batch 15/45, Loss: 0.4077993333339691\nEpoch 39/200, Batch 16/45, Loss: 0.29917511343955994\nEpoch 39/200, Batch 17/45, Loss: 0.23465396463871002\nEpoch 39/200, Batch 18/45, Loss: 0.1883082538843155\nEpoch 39/200, Batch 19/45, Loss: 0.4665461778640747\nEpoch 39/200, Batch 20/45, Loss: 0.27082589268684387\nEpoch 39/200, Batch 21/45, Loss: 0.30193084478378296\nEpoch 39/200, Batch 22/45, Loss: 0.33842453360557556\nEpoch 39/200, Batch 23/45, Loss: 0.30597513914108276\nEpoch 39/200, Batch 24/45, Loss: 0.19573336839675903\nEpoch 39/200, Batch 25/45, Loss: 0.36109405755996704\nEpoch 39/200, Batch 26/45, Loss: 1.250915288925171\nEpoch 39/200, Batch 27/45, Loss: 0.2592666745185852\nEpoch 39/200, Batch 28/45, Loss: 0.25406450033187866\nEpoch 39/200, Batch 29/45, Loss: 0.4208526611328125\nEpoch 39/200, Batch 30/45, Loss: 0.42894789576530457\nEpoch 39/200, Batch 31/45, Loss: 0.6572110652923584\nEpoch 39/200, Batch 32/45, Loss: 0.41212016344070435\nEpoch 39/200, Batch 33/45, Loss: 0.49878448247909546\nEpoch 39/200, Batch 34/45, Loss: 0.4212314784526825\nEpoch 39/200, Batch 35/45, Loss: 0.35839441418647766\nEpoch 39/200, Batch 36/45, Loss: 0.15181663632392883\nEpoch 39/200, Batch 37/45, Loss: 0.19016033411026\nEpoch 39/200, Batch 38/45, Loss: 0.43564915657043457\nEpoch 39/200, Batch 39/45, Loss: 0.2938842177391052\nEpoch 39/200, Batch 40/45, Loss: 0.2613590359687805\nEpoch 39/200, Batch 41/45, Loss: 0.5301689505577087\nEpoch 39/200, Batch 42/45, Loss: 0.29738396406173706\nEpoch 39/200, Batch 43/45, Loss: 0.333660364151001\nEpoch 39/200, Batch 44/45, Loss: 0.2382826954126358\nEpoch 39/200, Batch 45/45, Loss: 0.20945164561271667\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  8.046209752559662 Best Val MSE:  6.859336465597153\nEpoch:  40 , Time Elapsed:  6.34011713663737  mins\nEpoch 40/200, Batch 1/45, Loss: 0.3996504843235016\nEpoch 40/200, Batch 2/45, Loss: 0.27137839794158936\nEpoch 40/200, Batch 3/45, Loss: 0.2648080587387085\nEpoch 40/200, Batch 4/45, Loss: 0.29934215545654297\nEpoch 40/200, Batch 5/45, Loss: 0.22861754894256592\nEpoch 40/200, Batch 6/45, Loss: 0.262676477432251\nEpoch 40/200, Batch 7/45, Loss: 0.46997517347335815\nEpoch 40/200, Batch 8/45, Loss: 0.5269376039505005\nEpoch 40/200, Batch 9/45, Loss: 0.5841420292854309\nEpoch 40/200, Batch 10/45, Loss: 0.36409273743629456\nEpoch 40/200, Batch 11/45, Loss: 0.2787436544895172\nEpoch 40/200, Batch 12/45, Loss: 0.8345319628715515\nEpoch 40/200, Batch 13/45, Loss: 0.2526059150695801\nEpoch 40/200, Batch 14/45, Loss: 0.722387969493866\nEpoch 40/200, Batch 15/45, Loss: 0.38081514835357666\nEpoch 40/200, Batch 16/45, Loss: 0.24021124839782715\nEpoch 40/200, Batch 17/45, Loss: 0.18675729632377625\nEpoch 40/200, Batch 18/45, Loss: 0.4309377074241638\nEpoch 40/200, Batch 19/45, Loss: 0.2468438297510147\nEpoch 40/200, Batch 20/45, Loss: 0.28080061078071594\nEpoch 40/200, Batch 21/45, Loss: 0.9940015077590942\nEpoch 40/200, Batch 22/45, Loss: 0.3864811658859253\nEpoch 40/200, Batch 23/45, Loss: 0.3644700050354004\nEpoch 40/200, Batch 24/45, Loss: 0.34742307662963867\nEpoch 40/200, Batch 25/45, Loss: 0.27161553502082825\nEpoch 40/200, Batch 26/45, Loss: 0.43564462661743164\nEpoch 40/200, Batch 27/45, Loss: 0.3131108283996582\nEpoch 40/200, Batch 28/45, Loss: 0.4381544291973114\nEpoch 40/200, Batch 29/45, Loss: 0.45194077491760254\nEpoch 40/200, Batch 30/45, Loss: 0.30331841111183167\nEpoch 40/200, Batch 31/45, Loss: 0.908258318901062\nEpoch 40/200, Batch 32/45, Loss: 0.29751867055892944\nEpoch 40/200, Batch 33/45, Loss: 0.5600430369377136\nEpoch 40/200, Batch 34/45, Loss: 0.41550320386886597\nEpoch 40/200, Batch 35/45, Loss: 0.6011621952056885\nEpoch 40/200, Batch 36/45, Loss: 0.4604794979095459\nEpoch 40/200, Batch 37/45, Loss: 0.34727445244789124\nEpoch 40/200, Batch 38/45, Loss: 0.3927741050720215\nEpoch 40/200, Batch 39/45, Loss: 0.29687297344207764\nEpoch 40/200, Batch 40/45, Loss: 0.4701538383960724\nEpoch 40/200, Batch 41/45, Loss: 0.3895321786403656\nEpoch 40/200, Batch 42/45, Loss: 0.7534286379814148\nEpoch 40/200, Batch 43/45, Loss: 0.26628971099853516\nEpoch 40/200, Batch 44/45, Loss: 0.16913127899169922\nEpoch 40/200, Batch 45/45, Loss: 0.1945170909166336\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  10.015086472034454 Best Val MSE:  6.859336465597153\nEpoch:  41 , Time Elapsed:  6.496062517166138  mins\nEpoch 41/200, Batch 1/45, Loss: 0.3126376271247864\nEpoch 41/200, Batch 2/45, Loss: 0.47904402017593384\nEpoch 41/200, Batch 3/45, Loss: 0.4473283886909485\nEpoch 41/200, Batch 4/45, Loss: 0.252482533454895\nEpoch 41/200, Batch 5/45, Loss: 0.282276451587677\nEpoch 41/200, Batch 6/45, Loss: 0.6026877760887146\nEpoch 41/200, Batch 7/45, Loss: 0.201256662607193\nEpoch 41/200, Batch 8/45, Loss: 0.4273218512535095\nEpoch 41/200, Batch 9/45, Loss: 0.3614228367805481\nEpoch 41/200, Batch 10/45, Loss: 0.1572578251361847\nEpoch 41/200, Batch 11/45, Loss: 0.6111575365066528\nEpoch 41/200, Batch 12/45, Loss: 0.9402603507041931\nEpoch 41/200, Batch 13/45, Loss: 0.3584829866886139\nEpoch 41/200, Batch 14/45, Loss: 0.3256474733352661\nEpoch 41/200, Batch 15/45, Loss: 0.14237692952156067\nEpoch 41/200, Batch 16/45, Loss: 0.31453371047973633\nEpoch 41/200, Batch 17/45, Loss: 0.36072617769241333\nEpoch 41/200, Batch 18/45, Loss: 0.25165244936943054\nEpoch 41/200, Batch 19/45, Loss: 1.1165823936462402\nEpoch 41/200, Batch 20/45, Loss: 0.28175705671310425\nEpoch 41/200, Batch 21/45, Loss: 0.3702191710472107\nEpoch 41/200, Batch 22/45, Loss: 0.24728968739509583\nEpoch 41/200, Batch 23/45, Loss: 0.3744406998157501\nEpoch 41/200, Batch 24/45, Loss: 0.40728533267974854\nEpoch 41/200, Batch 25/45, Loss: 0.1630590856075287\nEpoch 41/200, Batch 26/45, Loss: 0.28252264857292175\nEpoch 41/200, Batch 27/45, Loss: 0.7535039186477661\nEpoch 41/200, Batch 28/45, Loss: 0.7618251442909241\nEpoch 41/200, Batch 29/45, Loss: 0.24722039699554443\nEpoch 41/200, Batch 30/45, Loss: 0.3067227005958557\nEpoch 41/200, Batch 31/45, Loss: 0.47553157806396484\nEpoch 41/200, Batch 32/45, Loss: 0.27816343307495117\nEpoch 41/200, Batch 33/45, Loss: 0.26683491468429565\nEpoch 41/200, Batch 34/45, Loss: 0.4578356444835663\nEpoch 41/200, Batch 35/45, Loss: 0.21523675322532654\nEpoch 41/200, Batch 36/45, Loss: 0.3231797516345978\nEpoch 41/200, Batch 37/45, Loss: 0.4852495491504669\nEpoch 41/200, Batch 38/45, Loss: 0.44144341349601746\nEpoch 41/200, Batch 39/45, Loss: 0.3188078999519348\nEpoch 41/200, Batch 40/45, Loss: 0.31752100586891174\nEpoch 41/200, Batch 41/45, Loss: 0.5418899059295654\nEpoch 41/200, Batch 42/45, Loss: 0.33938807249069214\nEpoch 41/200, Batch 43/45, Loss: 0.49918195605278015\nEpoch 41/200, Batch 44/45, Loss: 0.3271738290786743\nEpoch 41/200, Batch 45/45, Loss: 0.6637924909591675\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  8.579962432384491 Best Val MSE:  6.859336465597153\nEpoch:  42 , Time Elapsed:  6.653599186738332  mins\nEpoch 42/200, Batch 1/45, Loss: 0.1968006193637848\nEpoch 42/200, Batch 2/45, Loss: 0.3803887367248535\nEpoch 42/200, Batch 3/45, Loss: 0.3523733615875244\nEpoch 42/200, Batch 4/45, Loss: 0.3341243267059326\nEpoch 42/200, Batch 5/45, Loss: 0.24009349942207336\nEpoch 42/200, Batch 6/45, Loss: 0.1834104210138321\nEpoch 42/200, Batch 7/45, Loss: 0.9138029217720032\nEpoch 42/200, Batch 8/45, Loss: 0.1631239950656891\nEpoch 42/200, Batch 9/45, Loss: 0.4614686667919159\nEpoch 42/200, Batch 10/45, Loss: 0.8914879560470581\nEpoch 42/200, Batch 11/45, Loss: 0.20216213166713715\nEpoch 42/200, Batch 12/45, Loss: 0.4004210829734802\nEpoch 42/200, Batch 13/45, Loss: 0.724077582359314\nEpoch 42/200, Batch 14/45, Loss: 0.4402232766151428\nEpoch 42/200, Batch 15/45, Loss: 0.4671010971069336\nEpoch 42/200, Batch 16/45, Loss: 0.534746527671814\nEpoch 42/200, Batch 17/45, Loss: 0.46383988857269287\nEpoch 42/200, Batch 18/45, Loss: 0.6355453729629517\nEpoch 42/200, Batch 19/45, Loss: 0.38056010007858276\nEpoch 42/200, Batch 20/45, Loss: 0.4078117907047272\nEpoch 42/200, Batch 21/45, Loss: 0.66434645652771\nEpoch 42/200, Batch 22/45, Loss: 0.3467412292957306\nEpoch 42/200, Batch 23/45, Loss: 0.32153892517089844\nEpoch 42/200, Batch 24/45, Loss: 0.999581515789032\nEpoch 42/200, Batch 25/45, Loss: 0.7457730770111084\nEpoch 42/200, Batch 26/45, Loss: 0.7493352890014648\nEpoch 42/200, Batch 27/45, Loss: 0.22397089004516602\nEpoch 42/200, Batch 28/45, Loss: 0.2052748203277588\nEpoch 42/200, Batch 29/45, Loss: 0.44948264956474304\nEpoch 42/200, Batch 30/45, Loss: 0.28295865654945374\nEpoch 42/200, Batch 31/45, Loss: 0.4836861193180084\nEpoch 42/200, Batch 32/45, Loss: 0.297235369682312\nEpoch 42/200, Batch 33/45, Loss: 0.2797161638736725\nEpoch 42/200, Batch 34/45, Loss: 0.3520226776599884\nEpoch 42/200, Batch 35/45, Loss: 0.19867216050624847\nEpoch 42/200, Batch 36/45, Loss: 0.5786289572715759\nEpoch 42/200, Batch 37/45, Loss: 0.9342687129974365\nEpoch 42/200, Batch 38/45, Loss: 0.5399978756904602\nEpoch 42/200, Batch 39/45, Loss: 0.6490247249603271\nEpoch 42/200, Batch 40/45, Loss: 0.4381585121154785\nEpoch 42/200, Batch 41/45, Loss: 0.3248627781867981\nEpoch 42/200, Batch 42/45, Loss: 0.2696074843406677\nEpoch 42/200, Batch 43/45, Loss: 0.6113894581794739\nEpoch 42/200, Batch 44/45, Loss: 0.22711895406246185\nEpoch 42/200, Batch 45/45, Loss: 0.32381653785705566\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  8.338722884654999 Best Val MSE:  6.859336465597153\nEpoch:  43 , Time Elapsed:  6.811416840553283  mins\nEpoch 43/200, Batch 1/45, Loss: 0.28175848722457886\nEpoch 43/200, Batch 2/45, Loss: 0.3963049352169037\nEpoch 43/200, Batch 3/45, Loss: 0.38932204246520996\nEpoch 43/200, Batch 4/45, Loss: 0.3541980981826782\nEpoch 43/200, Batch 5/45, Loss: 0.6186987161636353\nEpoch 43/200, Batch 6/45, Loss: 0.1632707417011261\nEpoch 43/200, Batch 7/45, Loss: 0.43704283237457275\nEpoch 43/200, Batch 8/45, Loss: 0.204740971326828\nEpoch 43/200, Batch 9/45, Loss: 0.31998056173324585\nEpoch 43/200, Batch 10/45, Loss: 0.4499911069869995\nEpoch 43/200, Batch 11/45, Loss: 0.2043323814868927\nEpoch 43/200, Batch 12/45, Loss: 0.2943395972251892\nEpoch 43/200, Batch 13/45, Loss: 0.2679833173751831\nEpoch 43/200, Batch 14/45, Loss: 0.7008664608001709\nEpoch 43/200, Batch 15/45, Loss: 0.2592000365257263\nEpoch 43/200, Batch 16/45, Loss: 0.22159525752067566\nEpoch 43/200, Batch 17/45, Loss: 0.39557695388793945\nEpoch 43/200, Batch 18/45, Loss: 0.69361412525177\nEpoch 43/200, Batch 19/45, Loss: 0.29103103280067444\nEpoch 43/200, Batch 20/45, Loss: 0.376414030790329\nEpoch 43/200, Batch 21/45, Loss: 0.3450116217136383\nEpoch 43/200, Batch 22/45, Loss: 0.2257823944091797\nEpoch 43/200, Batch 23/45, Loss: 0.3779921233654022\nEpoch 43/200, Batch 24/45, Loss: 0.24343489110469818\nEpoch 43/200, Batch 25/45, Loss: 0.17723271250724792\nEpoch 43/200, Batch 26/45, Loss: 0.5922185182571411\nEpoch 43/200, Batch 27/45, Loss: 0.19748979806900024\nEpoch 43/200, Batch 28/45, Loss: 0.44094786047935486\nEpoch 43/200, Batch 29/45, Loss: 0.1725645512342453\nEpoch 43/200, Batch 30/45, Loss: 0.37007373571395874\nEpoch 43/200, Batch 31/45, Loss: 0.33568906784057617\nEpoch 43/200, Batch 32/45, Loss: 0.36773648858070374\nEpoch 43/200, Batch 33/45, Loss: 0.19652171432971954\nEpoch 43/200, Batch 34/45, Loss: 0.4958491325378418\nEpoch 43/200, Batch 35/45, Loss: 0.1527934968471527\nEpoch 43/200, Batch 36/45, Loss: 0.49144673347473145\nEpoch 43/200, Batch 37/45, Loss: 0.44741830229759216\nEpoch 43/200, Batch 38/45, Loss: 0.5014889240264893\nEpoch 43/200, Batch 39/45, Loss: 0.9608150720596313\nEpoch 43/200, Batch 40/45, Loss: 0.3817814290523529\nEpoch 43/200, Batch 41/45, Loss: 0.3105309307575226\nEpoch 43/200, Batch 42/45, Loss: 0.24999046325683594\nEpoch 43/200, Batch 43/45, Loss: 0.4446812868118286\nEpoch 43/200, Batch 44/45, Loss: 0.5244917273521423\nEpoch 43/200, Batch 45/45, Loss: 0.8439509272575378\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  14.894158720970154 Best Val MSE:  6.859336465597153\nEpoch:  44 , Time Elapsed:  6.9678769906361895  mins\nEpoch 44/200, Batch 1/45, Loss: 0.6438666582107544\nEpoch 44/200, Batch 2/45, Loss: 0.4120712876319885\nEpoch 44/200, Batch 3/45, Loss: 0.40122562646865845\nEpoch 44/200, Batch 4/45, Loss: 0.31324756145477295\nEpoch 44/200, Batch 5/45, Loss: 0.47485682368278503\nEpoch 44/200, Batch 6/45, Loss: 0.35594600439071655\nEpoch 44/200, Batch 7/45, Loss: 0.2520146369934082\nEpoch 44/200, Batch 8/45, Loss: 0.24485936760902405\nEpoch 44/200, Batch 9/45, Loss: 1.030914545059204\nEpoch 44/200, Batch 10/45, Loss: 0.4494451880455017\nEpoch 44/200, Batch 11/45, Loss: 0.25722602009773254\nEpoch 44/200, Batch 12/45, Loss: 0.44334256649017334\nEpoch 44/200, Batch 13/45, Loss: 0.3680458068847656\nEpoch 44/200, Batch 14/45, Loss: 0.20656171441078186\nEpoch 44/200, Batch 15/45, Loss: 0.5481177568435669\nEpoch 44/200, Batch 16/45, Loss: 0.2747505307197571\nEpoch 44/200, Batch 17/45, Loss: 0.377663791179657\nEpoch 44/200, Batch 18/45, Loss: 0.2789691388607025\nEpoch 44/200, Batch 19/45, Loss: 0.44041693210601807\nEpoch 44/200, Batch 20/45, Loss: 0.49893975257873535\nEpoch 44/200, Batch 21/45, Loss: 0.4316340684890747\nEpoch 44/200, Batch 22/45, Loss: 0.37157630920410156\nEpoch 44/200, Batch 23/45, Loss: 0.2629516124725342\nEpoch 44/200, Batch 24/45, Loss: 0.5156438946723938\nEpoch 44/200, Batch 25/45, Loss: 0.31309324502944946\nEpoch 44/200, Batch 26/45, Loss: 0.5008604526519775\nEpoch 44/200, Batch 27/45, Loss: 0.24807579815387726\nEpoch 44/200, Batch 28/45, Loss: 0.2824857831001282\nEpoch 44/200, Batch 29/45, Loss: 0.2229158580303192\nEpoch 44/200, Batch 30/45, Loss: 0.4244428277015686\nEpoch 44/200, Batch 31/45, Loss: 0.4788424074649811\nEpoch 44/200, Batch 32/45, Loss: 0.3294524550437927\nEpoch 44/200, Batch 33/45, Loss: 0.5871157050132751\nEpoch 44/200, Batch 34/45, Loss: 0.2547910511493683\nEpoch 44/200, Batch 35/45, Loss: 0.3811108469963074\nEpoch 44/200, Batch 36/45, Loss: 0.5447770953178406\nEpoch 44/200, Batch 37/45, Loss: 0.8139352798461914\nEpoch 44/200, Batch 38/45, Loss: 0.2515500783920288\nEpoch 44/200, Batch 39/45, Loss: 0.2487824261188507\nEpoch 44/200, Batch 40/45, Loss: 0.44224876165390015\nEpoch 44/200, Batch 41/45, Loss: 0.25343120098114014\nEpoch 44/200, Batch 42/45, Loss: 0.29583466053009033\nEpoch 44/200, Batch 43/45, Loss: 0.3821314871311188\nEpoch 44/200, Batch 44/45, Loss: 0.38865572214126587\nEpoch 44/200, Batch 45/45, Loss: 0.4464207887649536\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  8.542262315750122 Best Val MSE:  6.859336465597153\nEpoch:  45 , Time Elapsed:  7.128671828905741  mins\nEpoch 45/200, Batch 1/45, Loss: 0.4020378589630127\nEpoch 45/200, Batch 2/45, Loss: 0.21238896250724792\nEpoch 45/200, Batch 3/45, Loss: 0.2913619875907898\nEpoch 45/200, Batch 4/45, Loss: 0.232244610786438\nEpoch 45/200, Batch 5/45, Loss: 0.5135122537612915\nEpoch 45/200, Batch 6/45, Loss: 0.3335488438606262\nEpoch 45/200, Batch 7/45, Loss: 0.2928263545036316\nEpoch 45/200, Batch 8/45, Loss: 0.31875884532928467\nEpoch 45/200, Batch 9/45, Loss: 0.3096827268600464\nEpoch 45/200, Batch 10/45, Loss: 0.17821690440177917\nEpoch 45/200, Batch 11/45, Loss: 0.29970723390579224\nEpoch 45/200, Batch 12/45, Loss: 0.3591548204421997\nEpoch 45/200, Batch 13/45, Loss: 0.19666440784931183\nEpoch 45/200, Batch 14/45, Loss: 0.3058915138244629\nEpoch 45/200, Batch 15/45, Loss: 0.5140923261642456\nEpoch 45/200, Batch 16/45, Loss: 0.1812586933374405\nEpoch 45/200, Batch 17/45, Loss: 0.4939880669116974\nEpoch 45/200, Batch 18/45, Loss: 0.4178164601325989\nEpoch 45/200, Batch 19/45, Loss: 0.310712605714798\nEpoch 45/200, Batch 20/45, Loss: 0.2303035706281662\nEpoch 45/200, Batch 21/45, Loss: 0.2671249806880951\nEpoch 45/200, Batch 22/45, Loss: 0.1887783706188202\nEpoch 45/200, Batch 23/45, Loss: 0.2917807698249817\nEpoch 45/200, Batch 24/45, Loss: 0.48297494649887085\nEpoch 45/200, Batch 25/45, Loss: 0.201430082321167\nEpoch 45/200, Batch 26/45, Loss: 0.19831039011478424\nEpoch 45/200, Batch 27/45, Loss: 0.3096829354763031\nEpoch 45/200, Batch 28/45, Loss: 0.3114127516746521\nEpoch 45/200, Batch 29/45, Loss: 0.2270216941833496\nEpoch 45/200, Batch 30/45, Loss: 0.2713406980037689\nEpoch 45/200, Batch 31/45, Loss: 0.33789873123168945\nEpoch 45/200, Batch 32/45, Loss: 0.1624135822057724\nEpoch 45/200, Batch 33/45, Loss: 0.18308480083942413\nEpoch 45/200, Batch 34/45, Loss: 0.37816229462623596\nEpoch 45/200, Batch 35/45, Loss: 0.43627750873565674\nEpoch 45/200, Batch 36/45, Loss: 0.19516560435295105\nEpoch 45/200, Batch 37/45, Loss: 1.119391679763794\nEpoch 45/200, Batch 38/45, Loss: 0.21203844249248505\nEpoch 45/200, Batch 39/45, Loss: 0.35284584760665894\nEpoch 45/200, Batch 40/45, Loss: 0.393365740776062\nEpoch 45/200, Batch 41/45, Loss: 0.38292086124420166\nEpoch 45/200, Batch 42/45, Loss: 0.3653416037559509\nEpoch 45/200, Batch 43/45, Loss: 0.2696850895881653\nEpoch 45/200, Batch 44/45, Loss: 0.34682607650756836\nEpoch 45/200, Batch 45/45, Loss: 0.38203883171081543\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  8.910738468170166 Best Val MSE:  6.859336465597153\nEpoch:  46 , Time Elapsed:  7.290956155459086  mins\nEpoch 46/200, Batch 1/45, Loss: 0.38580241799354553\nEpoch 46/200, Batch 2/45, Loss: 0.24413757026195526\nEpoch 46/200, Batch 3/45, Loss: 0.2544199526309967\nEpoch 46/200, Batch 4/45, Loss: 0.292400598526001\nEpoch 46/200, Batch 5/45, Loss: 0.3177796006202698\nEpoch 46/200, Batch 6/45, Loss: 0.40029019117355347\nEpoch 46/200, Batch 7/45, Loss: 0.25914427638053894\nEpoch 46/200, Batch 8/45, Loss: 0.3270913362503052\nEpoch 46/200, Batch 9/45, Loss: 0.32042786478996277\nEpoch 46/200, Batch 10/45, Loss: 0.5060232281684875\nEpoch 46/200, Batch 11/45, Loss: 0.34327083826065063\nEpoch 46/200, Batch 12/45, Loss: 0.31264248490333557\nEpoch 46/200, Batch 13/45, Loss: 0.41979092359542847\nEpoch 46/200, Batch 14/45, Loss: 0.24213597178459167\nEpoch 46/200, Batch 15/45, Loss: 0.43522220849990845\nEpoch 46/200, Batch 16/45, Loss: 0.29641926288604736\nEpoch 46/200, Batch 17/45, Loss: 0.34260261058807373\nEpoch 46/200, Batch 18/45, Loss: 0.4358496367931366\nEpoch 46/200, Batch 19/45, Loss: 0.2013026475906372\nEpoch 46/200, Batch 20/45, Loss: 0.28784477710723877\nEpoch 46/200, Batch 21/45, Loss: 0.42448729276657104\nEpoch 46/200, Batch 22/45, Loss: 0.30054765939712524\nEpoch 46/200, Batch 23/45, Loss: 0.3621142506599426\nEpoch 46/200, Batch 24/45, Loss: 0.35558921098709106\nEpoch 46/200, Batch 25/45, Loss: 0.179168239235878\nEpoch 46/200, Batch 26/45, Loss: 0.2050761729478836\nEpoch 46/200, Batch 27/45, Loss: 0.22570335865020752\nEpoch 46/200, Batch 28/45, Loss: 0.47815489768981934\nEpoch 46/200, Batch 29/45, Loss: 0.25201383233070374\nEpoch 46/200, Batch 30/45, Loss: 0.30354955792427063\nEpoch 46/200, Batch 31/45, Loss: 0.2043437957763672\nEpoch 46/200, Batch 32/45, Loss: 0.11948741972446442\nEpoch 46/200, Batch 33/45, Loss: 0.34175312519073486\nEpoch 46/200, Batch 34/45, Loss: 0.2381627857685089\nEpoch 46/200, Batch 35/45, Loss: 0.21268513798713684\nEpoch 46/200, Batch 36/45, Loss: 0.5101812481880188\nEpoch 46/200, Batch 37/45, Loss: 0.3514532446861267\nEpoch 46/200, Batch 38/45, Loss: 0.26787424087524414\nEpoch 46/200, Batch 39/45, Loss: 0.47683265805244446\nEpoch 46/200, Batch 40/45, Loss: 0.19656045734882355\nEpoch 46/200, Batch 41/45, Loss: 0.435094952583313\nEpoch 46/200, Batch 42/45, Loss: 0.5289520025253296\nEpoch 46/200, Batch 43/45, Loss: 0.2770102322101593\nEpoch 46/200, Batch 44/45, Loss: 0.26962536573410034\nEpoch 46/200, Batch 45/45, Loss: 0.5295040011405945\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  9.556007325649261 Best Val MSE:  6.859336465597153\nEpoch:  47 , Time Elapsed:  7.450399279594421  mins\nEpoch 47/200, Batch 1/45, Loss: 0.31571802496910095\nEpoch 47/200, Batch 2/45, Loss: 0.27411019802093506\nEpoch 47/200, Batch 3/45, Loss: 0.29011183977127075\nEpoch 47/200, Batch 4/45, Loss: 0.23989924788475037\nEpoch 47/200, Batch 5/45, Loss: 0.17468509078025818\nEpoch 47/200, Batch 6/45, Loss: 0.14766690135002136\nEpoch 47/200, Batch 7/45, Loss: 0.2602778971195221\nEpoch 47/200, Batch 8/45, Loss: 0.3581220507621765\nEpoch 47/200, Batch 9/45, Loss: 0.24764889478683472\nEpoch 47/200, Batch 10/45, Loss: 0.3328729569911957\nEpoch 47/200, Batch 11/45, Loss: 0.24039173126220703\nEpoch 47/200, Batch 12/45, Loss: 0.40774112939834595\nEpoch 47/200, Batch 13/45, Loss: 0.3160582184791565\nEpoch 47/200, Batch 14/45, Loss: 0.7368873953819275\nEpoch 47/200, Batch 15/45, Loss: 0.24711522459983826\nEpoch 47/200, Batch 16/45, Loss: 0.17677786946296692\nEpoch 47/200, Batch 17/45, Loss: 0.2868098318576813\nEpoch 47/200, Batch 18/45, Loss: 0.3346206545829773\nEpoch 47/200, Batch 19/45, Loss: 0.33174625039100647\nEpoch 47/200, Batch 20/45, Loss: 0.19745022058486938\nEpoch 47/200, Batch 21/45, Loss: 0.20749761164188385\nEpoch 47/200, Batch 22/45, Loss: 0.4681973159313202\nEpoch 47/200, Batch 23/45, Loss: 0.16717781126499176\nEpoch 47/200, Batch 24/45, Loss: 0.8547599911689758\nEpoch 47/200, Batch 25/45, Loss: 0.34066587686538696\nEpoch 47/200, Batch 26/45, Loss: 0.597496509552002\nEpoch 47/200, Batch 27/45, Loss: 0.3052298426628113\nEpoch 47/200, Batch 28/45, Loss: 0.4008651673793793\nEpoch 47/200, Batch 29/45, Loss: 0.5185835361480713\nEpoch 47/200, Batch 30/45, Loss: 0.2851055860519409\nEpoch 47/200, Batch 31/45, Loss: 0.5124742388725281\nEpoch 47/200, Batch 32/45, Loss: 0.423357754945755\nEpoch 47/200, Batch 33/45, Loss: 0.5373840928077698\nEpoch 47/200, Batch 34/45, Loss: 0.2387057989835739\nEpoch 47/200, Batch 35/45, Loss: 0.39059871435165405\nEpoch 47/200, Batch 36/45, Loss: 0.21800680458545685\nEpoch 47/200, Batch 37/45, Loss: 0.2385186105966568\nEpoch 47/200, Batch 38/45, Loss: 0.14974844455718994\nEpoch 47/200, Batch 39/45, Loss: 0.41360974311828613\nEpoch 47/200, Batch 40/45, Loss: 0.3580322861671448\nEpoch 47/200, Batch 41/45, Loss: 0.3040596842765808\nEpoch 47/200, Batch 42/45, Loss: 0.24341610074043274\nEpoch 47/200, Batch 43/45, Loss: 0.1718459129333496\nEpoch 47/200, Batch 44/45, Loss: 0.22886693477630615\nEpoch 47/200, Batch 45/45, Loss: 0.3130539655685425\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  9.376636922359467 Best Val MSE:  6.859336465597153\nEpoch:  48 , Time Elapsed:  7.612551422913869  mins\nEpoch 48/200, Batch 1/45, Loss: 0.5648771524429321\nEpoch 48/200, Batch 2/45, Loss: 0.17387673258781433\nEpoch 48/200, Batch 3/45, Loss: 0.3707937002182007\nEpoch 48/200, Batch 4/45, Loss: 0.2627232074737549\nEpoch 48/200, Batch 5/45, Loss: 0.6583026647567749\nEpoch 48/200, Batch 6/45, Loss: 0.4512985944747925\nEpoch 48/200, Batch 7/45, Loss: 0.23379608988761902\nEpoch 48/200, Batch 8/45, Loss: 0.35599184036254883\nEpoch 48/200, Batch 9/45, Loss: 0.320589154958725\nEpoch 48/200, Batch 10/45, Loss: 0.23340722918510437\nEpoch 48/200, Batch 11/45, Loss: 0.46381664276123047\nEpoch 48/200, Batch 12/45, Loss: 0.39822083711624146\nEpoch 48/200, Batch 13/45, Loss: 0.3284500241279602\nEpoch 48/200, Batch 14/45, Loss: 0.184657022356987\nEpoch 48/200, Batch 15/45, Loss: 0.3823610246181488\nEpoch 48/200, Batch 16/45, Loss: 0.18793705105781555\nEpoch 48/200, Batch 17/45, Loss: 0.19642463326454163\nEpoch 48/200, Batch 18/45, Loss: 0.23027460277080536\nEpoch 48/200, Batch 19/45, Loss: 0.27041035890579224\nEpoch 48/200, Batch 20/45, Loss: 0.2148536741733551\nEpoch 48/200, Batch 21/45, Loss: 1.3909910917282104\nEpoch 48/200, Batch 22/45, Loss: 0.24185115098953247\nEpoch 48/200, Batch 23/45, Loss: 0.35742390155792236\nEpoch 48/200, Batch 24/45, Loss: 0.35838842391967773\nEpoch 48/200, Batch 25/45, Loss: 0.6033217310905457\nEpoch 48/200, Batch 26/45, Loss: 0.6333549618721008\nEpoch 48/200, Batch 27/45, Loss: 0.4645804166793823\nEpoch 48/200, Batch 28/45, Loss: 0.5783864259719849\nEpoch 48/200, Batch 29/45, Loss: 0.49465522170066833\nEpoch 48/200, Batch 30/45, Loss: 0.35213395953178406\nEpoch 48/200, Batch 31/45, Loss: 0.46995458006858826\nEpoch 48/200, Batch 32/45, Loss: 0.3897634744644165\nEpoch 48/200, Batch 33/45, Loss: 0.1941516399383545\nEpoch 48/200, Batch 34/45, Loss: 0.274474561214447\nEpoch 48/200, Batch 35/45, Loss: 0.2481662482023239\nEpoch 48/200, Batch 36/45, Loss: 0.5698513984680176\nEpoch 48/200, Batch 37/45, Loss: 0.68027663230896\nEpoch 48/200, Batch 38/45, Loss: 0.24688899517059326\nEpoch 48/200, Batch 39/45, Loss: 0.22912895679473877\nEpoch 48/200, Batch 40/45, Loss: 0.22725346684455872\nEpoch 48/200, Batch 41/45, Loss: 0.42648014426231384\nEpoch 48/200, Batch 42/45, Loss: 0.3634090721607208\nEpoch 48/200, Batch 43/45, Loss: 0.3077075779438019\nEpoch 48/200, Batch 44/45, Loss: 0.3149859607219696\nEpoch 48/200, Batch 45/45, Loss: 0.37750184535980225\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  8.17314624786377 Best Val MSE:  6.859336465597153\nEpoch:  49 , Time Elapsed:  7.778164732456207  mins\nEpoch 49/200, Batch 1/45, Loss: 0.3160339891910553\nEpoch 49/200, Batch 2/45, Loss: 0.45860379934310913\nEpoch 49/200, Batch 3/45, Loss: 0.3537072241306305\nEpoch 49/200, Batch 4/45, Loss: 0.3971940875053406\nEpoch 49/200, Batch 5/45, Loss: 0.3466149568557739\nEpoch 49/200, Batch 6/45, Loss: 0.29354190826416016\nEpoch 49/200, Batch 7/45, Loss: 0.4236590564250946\nEpoch 49/200, Batch 8/45, Loss: 0.3740414083003998\nEpoch 49/200, Batch 9/45, Loss: 0.4421684145927429\nEpoch 49/200, Batch 10/45, Loss: 0.15606123208999634\nEpoch 49/200, Batch 11/45, Loss: 0.28195926547050476\nEpoch 49/200, Batch 12/45, Loss: 0.6488333344459534\nEpoch 49/200, Batch 13/45, Loss: 0.26486489176750183\nEpoch 49/200, Batch 14/45, Loss: 0.29490232467651367\nEpoch 49/200, Batch 15/45, Loss: 0.5669439435005188\nEpoch 49/200, Batch 16/45, Loss: 0.3522484302520752\nEpoch 49/200, Batch 17/45, Loss: 0.32294076681137085\nEpoch 49/200, Batch 18/45, Loss: 0.35649457573890686\nEpoch 49/200, Batch 19/45, Loss: 0.8524172306060791\nEpoch 49/200, Batch 20/45, Loss: 0.31599536538124084\nEpoch 49/200, Batch 21/45, Loss: 0.261086642742157\nEpoch 49/200, Batch 22/45, Loss: 0.3384794592857361\nEpoch 49/200, Batch 23/45, Loss: 0.23054291307926178\nEpoch 49/200, Batch 24/45, Loss: 0.28751298785209656\nEpoch 49/200, Batch 25/45, Loss: 0.2833651304244995\nEpoch 49/200, Batch 26/45, Loss: 0.5023704767227173\nEpoch 49/200, Batch 27/45, Loss: 0.8254652619361877\nEpoch 49/200, Batch 28/45, Loss: 0.3224407434463501\nEpoch 49/200, Batch 29/45, Loss: 0.1786782145500183\nEpoch 49/200, Batch 30/45, Loss: 0.10479910671710968\nEpoch 49/200, Batch 31/45, Loss: 0.3939819931983948\nEpoch 49/200, Batch 32/45, Loss: 0.8429572582244873\nEpoch 49/200, Batch 33/45, Loss: 0.42398393154144287\nEpoch 49/200, Batch 34/45, Loss: 0.16582538187503815\nEpoch 49/200, Batch 35/45, Loss: 0.37134063243865967\nEpoch 49/200, Batch 36/45, Loss: 0.1486276090145111\nEpoch 49/200, Batch 37/45, Loss: 0.2443968951702118\nEpoch 49/200, Batch 38/45, Loss: 0.1743737757205963\nEpoch 49/200, Batch 39/45, Loss: 0.29485809803009033\nEpoch 49/200, Batch 40/45, Loss: 0.47601673007011414\nEpoch 49/200, Batch 41/45, Loss: 0.2465674877166748\nEpoch 49/200, Batch 42/45, Loss: 0.2731740474700928\nEpoch 49/200, Batch 43/45, Loss: 0.3089975118637085\nEpoch 49/200, Batch 44/45, Loss: 0.30107754468917847\nEpoch 49/200, Batch 45/45, Loss: 0.2299099564552307\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  8.691136181354523 Best Val MSE:  6.859336465597153\nEpoch:  50 , Time Elapsed:  7.9389184474945065  mins\nEpoch 50/200, Batch 1/45, Loss: 0.277635395526886\nEpoch 50/200, Batch 2/45, Loss: 0.13598433136940002\nEpoch 50/200, Batch 3/45, Loss: 0.2628154456615448\nEpoch 50/200, Batch 4/45, Loss: 0.5092806816101074\nEpoch 50/200, Batch 5/45, Loss: 0.25802791118621826\nEpoch 50/200, Batch 6/45, Loss: 0.21615728735923767\nEpoch 50/200, Batch 7/45, Loss: 0.355682373046875\nEpoch 50/200, Batch 8/45, Loss: 0.4348834455013275\nEpoch 50/200, Batch 9/45, Loss: 0.5088809132575989\nEpoch 50/200, Batch 10/45, Loss: 0.2569171190261841\nEpoch 50/200, Batch 11/45, Loss: 0.3549172282218933\nEpoch 50/200, Batch 12/45, Loss: 0.34550508856773376\nEpoch 50/200, Batch 13/45, Loss: 0.7182977199554443\nEpoch 50/200, Batch 14/45, Loss: 0.29285722970962524\nEpoch 50/200, Batch 15/45, Loss: 0.3275628983974457\nEpoch 50/200, Batch 16/45, Loss: 0.3140602111816406\nEpoch 50/200, Batch 17/45, Loss: 0.40567323565483093\nEpoch 50/200, Batch 18/45, Loss: 0.7758827209472656\nEpoch 50/200, Batch 19/45, Loss: 0.4126417338848114\nEpoch 50/200, Batch 20/45, Loss: 0.40285438299179077\nEpoch 50/200, Batch 21/45, Loss: 0.35288968682289124\nEpoch 50/200, Batch 22/45, Loss: 0.514053463935852\nEpoch 50/200, Batch 23/45, Loss: 0.3038623034954071\nEpoch 50/200, Batch 24/45, Loss: 0.3013298511505127\nEpoch 50/200, Batch 25/45, Loss: 0.24454423785209656\nEpoch 50/200, Batch 26/45, Loss: 0.42166122794151306\nEpoch 50/200, Batch 27/45, Loss: 0.41617992520332336\nEpoch 50/200, Batch 28/45, Loss: 0.30277037620544434\nEpoch 50/200, Batch 29/45, Loss: 0.17919990420341492\nEpoch 50/200, Batch 30/45, Loss: 0.7982854843139648\nEpoch 50/200, Batch 31/45, Loss: 0.4007803797721863\nEpoch 50/200, Batch 32/45, Loss: 0.2269582599401474\nEpoch 50/200, Batch 33/45, Loss: 0.2868720293045044\nEpoch 50/200, Batch 34/45, Loss: 0.24195383489131927\nEpoch 50/200, Batch 35/45, Loss: 0.3009457588195801\nEpoch 50/200, Batch 36/45, Loss: 0.14015889167785645\nEpoch 50/200, Batch 37/45, Loss: 0.2599799633026123\nEpoch 50/200, Batch 38/45, Loss: 0.49413323402404785\nEpoch 50/200, Batch 39/45, Loss: 0.2712855935096741\nEpoch 50/200, Batch 40/45, Loss: 0.3388917148113251\nEpoch 50/200, Batch 41/45, Loss: 0.3535793125629425\nEpoch 50/200, Batch 42/45, Loss: 0.08760225772857666\nEpoch 50/200, Batch 43/45, Loss: 0.3422873020172119\nEpoch 50/200, Batch 44/45, Loss: 0.1762818992137909\nEpoch 50/200, Batch 45/45, Loss: 0.41886961460113525\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  8.840506792068481 Best Val MSE:  6.859336465597153\nEpoch:  51 , Time Elapsed:  8.097466508547464  mins\nEpoch 51/200, Batch 1/45, Loss: 0.4485875964164734\nEpoch 51/200, Batch 2/45, Loss: 0.30133533477783203\nEpoch 51/200, Batch 3/45, Loss: 0.3368041515350342\nEpoch 51/200, Batch 4/45, Loss: 0.41714048385620117\nEpoch 51/200, Batch 5/45, Loss: 0.24916256964206696\nEpoch 51/200, Batch 6/45, Loss: 0.5506927967071533\nEpoch 51/200, Batch 7/45, Loss: 0.22699487209320068\nEpoch 51/200, Batch 8/45, Loss: 0.3594440519809723\nEpoch 51/200, Batch 9/45, Loss: 0.21325869858264923\nEpoch 51/200, Batch 10/45, Loss: 0.25238099694252014\nEpoch 51/200, Batch 11/45, Loss: 0.3336966633796692\nEpoch 51/200, Batch 12/45, Loss: 0.29287010431289673\nEpoch 51/200, Batch 13/45, Loss: 0.20971176028251648\nEpoch 51/200, Batch 14/45, Loss: 0.7192492485046387\nEpoch 51/200, Batch 15/45, Loss: 0.23775920271873474\nEpoch 51/200, Batch 16/45, Loss: 0.29550862312316895\nEpoch 51/200, Batch 17/45, Loss: 0.4286429286003113\nEpoch 51/200, Batch 18/45, Loss: 0.43993470072746277\nEpoch 51/200, Batch 19/45, Loss: 0.7829551100730896\nEpoch 51/200, Batch 20/45, Loss: 0.1897011697292328\nEpoch 51/200, Batch 21/45, Loss: 0.31957000494003296\nEpoch 51/200, Batch 22/45, Loss: 0.5001468062400818\nEpoch 51/200, Batch 23/45, Loss: 0.5371390581130981\nEpoch 51/200, Batch 24/45, Loss: 0.48720836639404297\nEpoch 51/200, Batch 25/45, Loss: 0.20499172806739807\nEpoch 51/200, Batch 26/45, Loss: 0.4659155607223511\nEpoch 51/200, Batch 27/45, Loss: 0.1941295713186264\nEpoch 51/200, Batch 28/45, Loss: 0.3622548580169678\nEpoch 51/200, Batch 29/45, Loss: 0.23000632226467133\nEpoch 51/200, Batch 30/45, Loss: 0.16925907135009766\nEpoch 51/200, Batch 31/45, Loss: 0.3099013566970825\nEpoch 51/200, Batch 32/45, Loss: 0.5830308794975281\nEpoch 51/200, Batch 33/45, Loss: 0.28986862301826477\nEpoch 51/200, Batch 34/45, Loss: 0.347084641456604\nEpoch 51/200, Batch 35/45, Loss: 0.31257063150405884\nEpoch 51/200, Batch 36/45, Loss: 0.27955901622772217\nEpoch 51/200, Batch 37/45, Loss: 0.10442792624235153\nEpoch 51/200, Batch 38/45, Loss: 0.7757993936538696\nEpoch 51/200, Batch 39/45, Loss: 0.1970299780368805\nEpoch 51/200, Batch 40/45, Loss: 0.4420316219329834\nEpoch 51/200, Batch 41/45, Loss: 0.42345061898231506\nEpoch 51/200, Batch 42/45, Loss: 0.35464316606521606\nEpoch 51/200, Batch 43/45, Loss: 0.3076043725013733\nEpoch 51/200, Batch 44/45, Loss: 0.31636276841163635\nEpoch 51/200, Batch 45/45, Loss: 0.1563657522201538\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  8.386092722415924 Best Val MSE:  6.859336465597153\nEpoch:  52 , Time Elapsed:  8.257886695861817  mins\nEpoch 52/200, Batch 1/45, Loss: 0.42304080724716187\nEpoch 52/200, Batch 2/45, Loss: 0.36223989725112915\nEpoch 52/200, Batch 3/45, Loss: 0.2375386506319046\nEpoch 52/200, Batch 4/45, Loss: 0.2175360918045044\nEpoch 52/200, Batch 5/45, Loss: 0.12349706888198853\nEpoch 52/200, Batch 6/45, Loss: 0.22971370816230774\nEpoch 52/200, Batch 7/45, Loss: 0.24885383248329163\nEpoch 52/200, Batch 8/45, Loss: 0.22430554032325745\nEpoch 52/200, Batch 9/45, Loss: 0.5037358403205872\nEpoch 52/200, Batch 10/45, Loss: 0.3417406678199768\nEpoch 52/200, Batch 11/45, Loss: 0.3724502623081207\nEpoch 52/200, Batch 12/45, Loss: 0.3629811406135559\nEpoch 52/200, Batch 13/45, Loss: 0.25763508677482605\nEpoch 52/200, Batch 14/45, Loss: 0.3264442980289459\nEpoch 52/200, Batch 15/45, Loss: 2.7146987915039062\nEpoch 52/200, Batch 16/45, Loss: 0.27374154329299927\nEpoch 52/200, Batch 17/45, Loss: 0.49368494749069214\nEpoch 52/200, Batch 18/45, Loss: 0.8228728175163269\nEpoch 52/200, Batch 19/45, Loss: 0.4294591546058655\nEpoch 52/200, Batch 20/45, Loss: 0.535823404788971\nEpoch 52/200, Batch 21/45, Loss: 0.34778106212615967\nEpoch 52/200, Batch 22/45, Loss: 0.2692020833492279\nEpoch 52/200, Batch 23/45, Loss: 0.5584992170333862\nEpoch 52/200, Batch 24/45, Loss: 0.41545164585113525\nEpoch 52/200, Batch 25/45, Loss: 0.45363694429397583\nEpoch 52/200, Batch 26/45, Loss: 0.7528547048568726\nEpoch 52/200, Batch 27/45, Loss: 0.6126111149787903\nEpoch 52/200, Batch 28/45, Loss: 0.28055980801582336\nEpoch 52/200, Batch 29/45, Loss: 0.4853246212005615\nEpoch 52/200, Batch 30/45, Loss: 0.27042853832244873\nEpoch 52/200, Batch 31/45, Loss: 0.4321899116039276\nEpoch 52/200, Batch 32/45, Loss: 0.2541477680206299\nEpoch 52/200, Batch 33/45, Loss: 0.392426073551178\nEpoch 52/200, Batch 34/45, Loss: 0.1697627604007721\nEpoch 52/200, Batch 35/45, Loss: 0.28002697229385376\nEpoch 52/200, Batch 36/45, Loss: 0.28726252913475037\nEpoch 52/200, Batch 37/45, Loss: 0.4258134961128235\nEpoch 52/200, Batch 38/45, Loss: 0.28025758266448975\nEpoch 52/200, Batch 39/45, Loss: 0.3147204518318176\nEpoch 52/200, Batch 40/45, Loss: 0.4735885262489319\nEpoch 52/200, Batch 41/45, Loss: 0.28672367334365845\nEpoch 52/200, Batch 42/45, Loss: 0.32364100217819214\nEpoch 52/200, Batch 43/45, Loss: 0.38913702964782715\nEpoch 52/200, Batch 44/45, Loss: 0.22498324513435364\nEpoch 52/200, Batch 45/45, Loss: 0.3764920234680176\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  9.743396818637848 Best Val MSE:  6.859336465597153\nEpoch:  53 , Time Elapsed:  8.417994578679403  mins\nEpoch 53/200, Batch 1/45, Loss: 0.34896206855773926\nEpoch 53/200, Batch 2/45, Loss: 0.4144224524497986\nEpoch 53/200, Batch 3/45, Loss: 0.15381816029548645\nEpoch 53/200, Batch 4/45, Loss: 0.2838812470436096\nEpoch 53/200, Batch 5/45, Loss: 0.386799156665802\nEpoch 53/200, Batch 6/45, Loss: 0.1694236844778061\nEpoch 53/200, Batch 7/45, Loss: 0.4895394444465637\nEpoch 53/200, Batch 8/45, Loss: 0.35297468304634094\nEpoch 53/200, Batch 9/45, Loss: 0.3774203658103943\nEpoch 53/200, Batch 10/45, Loss: 0.657360851764679\nEpoch 53/200, Batch 11/45, Loss: 1.3337924480438232\nEpoch 53/200, Batch 12/45, Loss: 0.31885212659835815\nEpoch 53/200, Batch 13/45, Loss: 1.3523496389389038\nEpoch 53/200, Batch 14/45, Loss: 0.4728504419326782\nEpoch 53/200, Batch 15/45, Loss: 0.492644727230072\nEpoch 53/200, Batch 16/45, Loss: 0.48019376397132874\nEpoch 53/200, Batch 17/45, Loss: 0.27944180369377136\nEpoch 53/200, Batch 18/45, Loss: 0.7833720445632935\nEpoch 53/200, Batch 19/45, Loss: 1.201971173286438\nEpoch 53/200, Batch 20/45, Loss: 0.6629091501235962\nEpoch 53/200, Batch 21/45, Loss: 0.450489342212677\nEpoch 53/200, Batch 22/45, Loss: 0.7743130326271057\nEpoch 53/200, Batch 23/45, Loss: 0.617504358291626\nEpoch 53/200, Batch 24/45, Loss: 1.7378547191619873\nEpoch 53/200, Batch 25/45, Loss: 0.409677654504776\nEpoch 53/200, Batch 26/45, Loss: 0.8699108362197876\nEpoch 53/200, Batch 27/45, Loss: 0.5031664371490479\nEpoch 53/200, Batch 28/45, Loss: 0.20047321915626526\nEpoch 53/200, Batch 29/45, Loss: 0.4874088764190674\nEpoch 53/200, Batch 30/45, Loss: 0.6865571737289429\nEpoch 53/200, Batch 31/45, Loss: 0.21047531068325043\nEpoch 53/200, Batch 32/45, Loss: 0.4634382724761963\nEpoch 53/200, Batch 33/45, Loss: 1.0270417928695679\nEpoch 53/200, Batch 34/45, Loss: 0.4378366470336914\nEpoch 53/200, Batch 35/45, Loss: 0.5186315774917603\nEpoch 53/200, Batch 36/45, Loss: 0.43188971281051636\nEpoch 53/200, Batch 37/45, Loss: 0.6356130838394165\nEpoch 53/200, Batch 38/45, Loss: 0.422710120677948\nEpoch 53/200, Batch 39/45, Loss: 1.007926344871521\nEpoch 53/200, Batch 40/45, Loss: 0.5297122001647949\nEpoch 53/200, Batch 41/45, Loss: 0.6409205794334412\nEpoch 53/200, Batch 42/45, Loss: 0.43830105662345886\nEpoch 53/200, Batch 43/45, Loss: 0.5435570478439331\nEpoch 53/200, Batch 44/45, Loss: 0.404430091381073\nEpoch 53/200, Batch 45/45, Loss: 0.6717237830162048\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  8.54410570859909 Best Val MSE:  6.859336465597153\nEpoch:  54 , Time Elapsed:  8.575036418437957  mins\nEpoch 54/200, Batch 1/45, Loss: 0.26836615800857544\nEpoch 54/200, Batch 2/45, Loss: 0.49266868829727173\nEpoch 54/200, Batch 3/45, Loss: 0.702616810798645\nEpoch 54/200, Batch 4/45, Loss: 0.2313622236251831\nEpoch 54/200, Batch 5/45, Loss: 0.3243718445301056\nEpoch 54/200, Batch 6/45, Loss: 0.6800627112388611\nEpoch 54/200, Batch 7/45, Loss: 0.34381985664367676\nEpoch 54/200, Batch 8/45, Loss: 0.3069803714752197\nEpoch 54/200, Batch 9/45, Loss: 0.20992301404476166\nEpoch 54/200, Batch 10/45, Loss: 0.3231661319732666\nEpoch 54/200, Batch 11/45, Loss: 0.5189114809036255\nEpoch 54/200, Batch 12/45, Loss: 0.3169049024581909\nEpoch 54/200, Batch 13/45, Loss: 0.4479990601539612\nEpoch 54/200, Batch 14/45, Loss: 0.24660594761371613\nEpoch 54/200, Batch 15/45, Loss: 0.8502682447433472\nEpoch 54/200, Batch 16/45, Loss: 0.4863356649875641\nEpoch 54/200, Batch 17/45, Loss: 0.25308743119239807\nEpoch 54/200, Batch 18/45, Loss: 0.3788597881793976\nEpoch 54/200, Batch 19/45, Loss: 0.3133338689804077\nEpoch 54/200, Batch 20/45, Loss: 0.3096577823162079\nEpoch 54/200, Batch 21/45, Loss: 0.31618934869766235\nEpoch 54/200, Batch 22/45, Loss: 0.2772536873817444\nEpoch 54/200, Batch 23/45, Loss: 2.3706531524658203\nEpoch 54/200, Batch 24/45, Loss: 0.39071500301361084\nEpoch 54/200, Batch 25/45, Loss: 0.48175230622291565\nEpoch 54/200, Batch 26/45, Loss: 0.3160540461540222\nEpoch 54/200, Batch 27/45, Loss: 0.5228064656257629\nEpoch 54/200, Batch 28/45, Loss: 0.6091916561126709\nEpoch 54/200, Batch 29/45, Loss: 0.7476288080215454\nEpoch 54/200, Batch 30/45, Loss: 0.6499456167221069\nEpoch 54/200, Batch 31/45, Loss: 0.7914018630981445\nEpoch 54/200, Batch 32/45, Loss: 0.5131173133850098\nEpoch 54/200, Batch 33/45, Loss: 0.5902488231658936\nEpoch 54/200, Batch 34/45, Loss: 0.5173001289367676\nEpoch 54/200, Batch 35/45, Loss: 0.3943774402141571\nEpoch 54/200, Batch 36/45, Loss: 0.31526294350624084\nEpoch 54/200, Batch 37/45, Loss: 0.33291172981262207\nEpoch 54/200, Batch 38/45, Loss: 0.4615115225315094\nEpoch 54/200, Batch 39/45, Loss: 0.40829911828041077\nEpoch 54/200, Batch 40/45, Loss: 0.319837749004364\nEpoch 54/200, Batch 41/45, Loss: 0.5887975692749023\nEpoch 54/200, Batch 42/45, Loss: 0.3559456467628479\nEpoch 54/200, Batch 43/45, Loss: 0.824209451675415\nEpoch 54/200, Batch 44/45, Loss: 1.6278529167175293\nEpoch 54/200, Batch 45/45, Loss: 0.7285676002502441\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  11.17505407333374 Best Val MSE:  6.859336465597153\nEpoch:  55 , Time Elapsed:  8.738132770856222  mins\nEpoch 55/200, Batch 1/45, Loss: 0.2791226804256439\nEpoch 55/200, Batch 2/45, Loss: 0.5598212480545044\nEpoch 55/200, Batch 3/45, Loss: 1.5333362817764282\nEpoch 55/200, Batch 4/45, Loss: 0.29565954208374023\nEpoch 55/200, Batch 5/45, Loss: 0.33522409200668335\nEpoch 55/200, Batch 6/45, Loss: 0.44560885429382324\nEpoch 55/200, Batch 7/45, Loss: 0.34650707244873047\nEpoch 55/200, Batch 8/45, Loss: 0.5354072451591492\nEpoch 55/200, Batch 9/45, Loss: 0.8722636103630066\nEpoch 55/200, Batch 10/45, Loss: 0.296306848526001\nEpoch 55/200, Batch 11/45, Loss: 0.5641064047813416\nEpoch 55/200, Batch 12/45, Loss: 0.3503328859806061\nEpoch 55/200, Batch 13/45, Loss: 0.15252387523651123\nEpoch 55/200, Batch 14/45, Loss: 0.288176953792572\nEpoch 55/200, Batch 15/45, Loss: 0.2671390175819397\nEpoch 55/200, Batch 16/45, Loss: 0.2516661584377289\nEpoch 55/200, Batch 17/45, Loss: 0.3871682584285736\nEpoch 55/200, Batch 18/45, Loss: 0.34097492694854736\nEpoch 55/200, Batch 19/45, Loss: 0.3856021463871002\nEpoch 55/200, Batch 20/45, Loss: 0.6343220472335815\nEpoch 55/200, Batch 21/45, Loss: 0.2585187256336212\nEpoch 55/200, Batch 22/45, Loss: 0.202884241938591\nEpoch 55/200, Batch 23/45, Loss: 0.335685670375824\nEpoch 55/200, Batch 24/45, Loss: 0.29122018814086914\nEpoch 55/200, Batch 25/45, Loss: 0.20367366075515747\nEpoch 55/200, Batch 26/45, Loss: 0.33890774846076965\nEpoch 55/200, Batch 27/45, Loss: 0.3104132413864136\nEpoch 55/200, Batch 28/45, Loss: 0.4521114230155945\nEpoch 55/200, Batch 29/45, Loss: 0.4035365581512451\nEpoch 55/200, Batch 30/45, Loss: 1.5524756908416748\nEpoch 55/200, Batch 31/45, Loss: 0.5577024817466736\nEpoch 55/200, Batch 32/45, Loss: 0.3429797887802124\nEpoch 55/200, Batch 33/45, Loss: 0.49391061067581177\nEpoch 55/200, Batch 34/45, Loss: 0.5616658329963684\nEpoch 55/200, Batch 35/45, Loss: 0.33442872762680054\nEpoch 55/200, Batch 36/45, Loss: 0.324247807264328\nEpoch 55/200, Batch 37/45, Loss: 0.5076998472213745\nEpoch 55/200, Batch 38/45, Loss: 0.1816098690032959\nEpoch 55/200, Batch 39/45, Loss: 0.32934191823005676\nEpoch 55/200, Batch 40/45, Loss: 0.11644421517848969\nEpoch 55/200, Batch 41/45, Loss: 0.4103257656097412\nEpoch 55/200, Batch 42/45, Loss: 0.24501082301139832\nEpoch 55/200, Batch 43/45, Loss: 0.3304302990436554\nEpoch 55/200, Batch 44/45, Loss: 0.3924376964569092\nEpoch 55/200, Batch 45/45, Loss: 0.3152503967285156\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  7.211075037717819 Best Val MSE:  6.859336465597153\nEpoch:  56 , Time Elapsed:  8.899751174449921  mins\nEpoch 56/200, Batch 1/45, Loss: 0.41426968574523926\nEpoch 56/200, Batch 2/45, Loss: 0.3434920310974121\nEpoch 56/200, Batch 3/45, Loss: 0.3011072278022766\nEpoch 56/200, Batch 4/45, Loss: 0.238783061504364\nEpoch 56/200, Batch 5/45, Loss: 0.27387475967407227\nEpoch 56/200, Batch 6/45, Loss: 0.2395676076412201\nEpoch 56/200, Batch 7/45, Loss: 0.19986210763454437\nEpoch 56/200, Batch 8/45, Loss: 0.20010250806808472\nEpoch 56/200, Batch 9/45, Loss: 0.30594947934150696\nEpoch 56/200, Batch 10/45, Loss: 0.317216694355011\nEpoch 56/200, Batch 11/45, Loss: 0.40854424238204956\nEpoch 56/200, Batch 12/45, Loss: 0.38216912746429443\nEpoch 56/200, Batch 13/45, Loss: 0.21868056058883667\nEpoch 56/200, Batch 14/45, Loss: 0.23751965165138245\nEpoch 56/200, Batch 15/45, Loss: 0.215826615691185\nEpoch 56/200, Batch 16/45, Loss: 0.5967833995819092\nEpoch 56/200, Batch 17/45, Loss: 0.18850192427635193\nEpoch 56/200, Batch 18/45, Loss: 0.3269880712032318\nEpoch 56/200, Batch 19/45, Loss: 0.3420705199241638\nEpoch 56/200, Batch 20/45, Loss: 0.317533940076828\nEpoch 56/200, Batch 21/45, Loss: 0.26463085412979126\nEpoch 56/200, Batch 22/45, Loss: 0.4149996042251587\nEpoch 56/200, Batch 23/45, Loss: 0.28871047496795654\nEpoch 56/200, Batch 24/45, Loss: 0.3466305136680603\nEpoch 56/200, Batch 25/45, Loss: 0.28831061720848083\nEpoch 56/200, Batch 26/45, Loss: 0.19702577590942383\nEpoch 56/200, Batch 27/45, Loss: 0.16937673091888428\nEpoch 56/200, Batch 28/45, Loss: 0.3203737139701843\nEpoch 56/200, Batch 29/45, Loss: 0.164582759141922\nEpoch 56/200, Batch 30/45, Loss: 0.23586143553256989\nEpoch 56/200, Batch 31/45, Loss: 0.23338696360588074\nEpoch 56/200, Batch 32/45, Loss: 0.3786516785621643\nEpoch 56/200, Batch 33/45, Loss: 0.47654539346694946\nEpoch 56/200, Batch 34/45, Loss: 0.3751119375228882\nEpoch 56/200, Batch 35/45, Loss: 0.4765862226486206\nEpoch 56/200, Batch 36/45, Loss: 0.19845758378505707\nEpoch 56/200, Batch 37/45, Loss: 0.14664657413959503\nEpoch 56/200, Batch 38/45, Loss: 0.32234543561935425\nEpoch 56/200, Batch 39/45, Loss: 0.21163944900035858\nEpoch 56/200, Batch 40/45, Loss: 0.3615001440048218\nEpoch 56/200, Batch 41/45, Loss: 0.45175039768218994\nEpoch 56/200, Batch 42/45, Loss: 0.38155826926231384\nEpoch 56/200, Batch 43/45, Loss: 0.43609634041786194\nEpoch 56/200, Batch 44/45, Loss: 0.38821905851364136\nEpoch 56/200, Batch 45/45, Loss: 0.3457784056663513\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  11.323814332485199 Best Val MSE:  6.859336465597153\nEpoch:  57 , Time Elapsed:  9.057503100236257  mins\nEpoch 57/200, Batch 1/45, Loss: 0.30952316522598267\nEpoch 57/200, Batch 2/45, Loss: 0.5147391557693481\nEpoch 57/200, Batch 3/45, Loss: 0.3855080008506775\nEpoch 57/200, Batch 4/45, Loss: 0.4453439712524414\nEpoch 57/200, Batch 5/45, Loss: 0.3571074306964874\nEpoch 57/200, Batch 6/45, Loss: 0.39730551838874817\nEpoch 57/200, Batch 7/45, Loss: 0.38613277673721313\nEpoch 57/200, Batch 8/45, Loss: 0.3157803416252136\nEpoch 57/200, Batch 9/45, Loss: 0.3583959937095642\nEpoch 57/200, Batch 10/45, Loss: 0.336859792470932\nEpoch 57/200, Batch 11/45, Loss: 0.29850995540618896\nEpoch 57/200, Batch 12/45, Loss: 0.2078515738248825\nEpoch 57/200, Batch 13/45, Loss: 0.3152572214603424\nEpoch 57/200, Batch 14/45, Loss: 0.3260321617126465\nEpoch 57/200, Batch 15/45, Loss: 0.5162472724914551\nEpoch 57/200, Batch 16/45, Loss: 0.253811240196228\nEpoch 57/200, Batch 17/45, Loss: 0.26816117763519287\nEpoch 57/200, Batch 18/45, Loss: 0.2610822319984436\nEpoch 57/200, Batch 19/45, Loss: 0.42155513167381287\nEpoch 57/200, Batch 20/45, Loss: 0.3092503547668457\nEpoch 57/200, Batch 21/45, Loss: 0.2348126620054245\nEpoch 57/200, Batch 22/45, Loss: 0.24260954558849335\nEpoch 57/200, Batch 23/45, Loss: 0.20687469840049744\nEpoch 57/200, Batch 24/45, Loss: 0.22003290057182312\nEpoch 57/200, Batch 25/45, Loss: 0.20336011052131653\nEpoch 57/200, Batch 26/45, Loss: 0.5685968399047852\nEpoch 57/200, Batch 27/45, Loss: 0.4891074299812317\nEpoch 57/200, Batch 28/45, Loss: 0.38405635952949524\nEpoch 57/200, Batch 29/45, Loss: 0.208387091755867\nEpoch 57/200, Batch 30/45, Loss: 0.5487064123153687\nEpoch 57/200, Batch 31/45, Loss: 0.09749635308980942\nEpoch 57/200, Batch 32/45, Loss: 0.30965113639831543\nEpoch 57/200, Batch 33/45, Loss: 0.4117083251476288\nEpoch 57/200, Batch 34/45, Loss: 0.19644585251808167\nEpoch 57/200, Batch 35/45, Loss: 0.5204172134399414\nEpoch 57/200, Batch 36/45, Loss: 0.2819758355617523\nEpoch 57/200, Batch 37/45, Loss: 0.3506453037261963\nEpoch 57/200, Batch 38/45, Loss: 0.29333311319351196\nEpoch 57/200, Batch 39/45, Loss: 0.11710295081138611\nEpoch 57/200, Batch 40/45, Loss: 0.377453088760376\nEpoch 57/200, Batch 41/45, Loss: 0.3230421841144562\nEpoch 57/200, Batch 42/45, Loss: 0.38441967964172363\nEpoch 57/200, Batch 43/45, Loss: 0.24682806432247162\nEpoch 57/200, Batch 44/45, Loss: 0.28246769309043884\nEpoch 57/200, Batch 45/45, Loss: 0.454591304063797\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  7.137206017971039 Best Val MSE:  6.859336465597153\nEpoch:  58 , Time Elapsed:  9.215075544516246  mins\nEpoch 58/200, Batch 1/45, Loss: 0.32524144649505615\nEpoch 58/200, Batch 2/45, Loss: 0.2726553678512573\nEpoch 58/200, Batch 3/45, Loss: 0.17447012662887573\nEpoch 58/200, Batch 4/45, Loss: 0.25832754373550415\nEpoch 58/200, Batch 5/45, Loss: 0.2967225909233093\nEpoch 58/200, Batch 6/45, Loss: 0.1692732572555542\nEpoch 58/200, Batch 7/45, Loss: 0.45285654067993164\nEpoch 58/200, Batch 8/45, Loss: 0.4294114410877228\nEpoch 58/200, Batch 9/45, Loss: 0.1997002810239792\nEpoch 58/200, Batch 10/45, Loss: 0.8187771439552307\nEpoch 58/200, Batch 11/45, Loss: 0.3317162096500397\nEpoch 58/200, Batch 12/45, Loss: 0.37656864523887634\nEpoch 58/200, Batch 13/45, Loss: 0.7247223854064941\nEpoch 58/200, Batch 14/45, Loss: 0.3909236192703247\nEpoch 58/200, Batch 15/45, Loss: 0.2651975154876709\nEpoch 58/200, Batch 16/45, Loss: 0.12739190459251404\nEpoch 58/200, Batch 17/45, Loss: 0.24189116060733795\nEpoch 58/200, Batch 18/45, Loss: 0.26475539803504944\nEpoch 58/200, Batch 19/45, Loss: 0.31305181980133057\nEpoch 58/200, Batch 20/45, Loss: 0.4169435501098633\nEpoch 58/200, Batch 21/45, Loss: 0.23503801226615906\nEpoch 58/200, Batch 22/45, Loss: 0.17500180006027222\nEpoch 58/200, Batch 23/45, Loss: 0.31252920627593994\nEpoch 58/200, Batch 24/45, Loss: 0.2997645139694214\nEpoch 58/200, Batch 25/45, Loss: 0.2226354032754898\nEpoch 58/200, Batch 26/45, Loss: 0.36332547664642334\nEpoch 58/200, Batch 27/45, Loss: 0.3004454970359802\nEpoch 58/200, Batch 28/45, Loss: 0.19497789442539215\nEpoch 58/200, Batch 29/45, Loss: 0.2863977551460266\nEpoch 58/200, Batch 30/45, Loss: 0.26695215702056885\nEpoch 58/200, Batch 31/45, Loss: 0.3152129352092743\nEpoch 58/200, Batch 32/45, Loss: 0.2591492235660553\nEpoch 58/200, Batch 33/45, Loss: 0.3880232870578766\nEpoch 58/200, Batch 34/45, Loss: 0.32533201575279236\nEpoch 58/200, Batch 35/45, Loss: 0.18812419474124908\nEpoch 58/200, Batch 36/45, Loss: 0.2950994372367859\nEpoch 58/200, Batch 37/45, Loss: 0.18681907653808594\nEpoch 58/200, Batch 38/45, Loss: 0.31833747029304504\nEpoch 58/200, Batch 39/45, Loss: 0.13248278200626373\nEpoch 58/200, Batch 40/45, Loss: 0.36339902877807617\nEpoch 58/200, Batch 41/45, Loss: 0.24401462078094482\nEpoch 58/200, Batch 42/45, Loss: 0.22121408581733704\nEpoch 58/200, Batch 43/45, Loss: 0.3123593330383301\nEpoch 58/200, Batch 44/45, Loss: 0.16625940799713135\nEpoch 58/200, Batch 45/45, Loss: 0.1948116421699524\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  7.748829126358032 Best Val MSE:  6.859336465597153\nEpoch:  59 , Time Elapsed:  9.380455597241719  mins\nEpoch 59/200, Batch 1/45, Loss: 0.25953084230422974\nEpoch 59/200, Batch 2/45, Loss: 0.1896202266216278\nEpoch 59/200, Batch 3/45, Loss: 0.23484119772911072\nEpoch 59/200, Batch 4/45, Loss: 0.9397820234298706\nEpoch 59/200, Batch 5/45, Loss: 0.1337190568447113\nEpoch 59/200, Batch 6/45, Loss: 0.8221145272254944\nEpoch 59/200, Batch 7/45, Loss: 0.22830522060394287\nEpoch 59/200, Batch 8/45, Loss: 0.6221099495887756\nEpoch 59/200, Batch 9/45, Loss: 0.18861916661262512\nEpoch 59/200, Batch 10/45, Loss: 0.3364717364311218\nEpoch 59/200, Batch 11/45, Loss: 0.2415265589952469\nEpoch 59/200, Batch 12/45, Loss: 0.20704597234725952\nEpoch 59/200, Batch 13/45, Loss: 0.4357491135597229\nEpoch 59/200, Batch 14/45, Loss: 0.23309609293937683\nEpoch 59/200, Batch 15/45, Loss: 0.43380942940711975\nEpoch 59/200, Batch 16/45, Loss: 0.3414999842643738\nEpoch 59/200, Batch 17/45, Loss: 0.34784841537475586\nEpoch 59/200, Batch 18/45, Loss: 0.3184469938278198\nEpoch 59/200, Batch 19/45, Loss: 0.19790077209472656\nEpoch 59/200, Batch 20/45, Loss: 0.26131612062454224\nEpoch 59/200, Batch 21/45, Loss: 0.3318035900592804\nEpoch 59/200, Batch 22/45, Loss: 0.36847612261772156\nEpoch 59/200, Batch 23/45, Loss: 0.5145212411880493\nEpoch 59/200, Batch 24/45, Loss: 0.18639467656612396\nEpoch 59/200, Batch 25/45, Loss: 0.2306673377752304\nEpoch 59/200, Batch 26/45, Loss: 0.236160010099411\nEpoch 59/200, Batch 27/45, Loss: 0.3088749647140503\nEpoch 59/200, Batch 28/45, Loss: 0.25372010469436646\nEpoch 59/200, Batch 29/45, Loss: 0.2995757758617401\nEpoch 59/200, Batch 30/45, Loss: 0.16164615750312805\nEpoch 59/200, Batch 31/45, Loss: 0.27330413460731506\nEpoch 59/200, Batch 32/45, Loss: 0.247239887714386\nEpoch 59/200, Batch 33/45, Loss: 0.2770533561706543\nEpoch 59/200, Batch 34/45, Loss: 0.37595072388648987\nEpoch 59/200, Batch 35/45, Loss: 0.28938278555870056\nEpoch 59/200, Batch 36/45, Loss: 0.4291747510433197\nEpoch 59/200, Batch 37/45, Loss: 0.20545022189617157\nEpoch 59/200, Batch 38/45, Loss: 0.2930329144001007\nEpoch 59/200, Batch 39/45, Loss: 0.19828161597251892\nEpoch 59/200, Batch 40/45, Loss: 0.2460576593875885\nEpoch 59/200, Batch 41/45, Loss: 0.7690340280532837\nEpoch 59/200, Batch 42/45, Loss: 0.27900266647338867\nEpoch 59/200, Batch 43/45, Loss: 0.2334362268447876\nEpoch 59/200, Batch 44/45, Loss: 0.24673126637935638\nEpoch 59/200, Batch 45/45, Loss: 0.22196394205093384\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  10.479192972183228 Best Val MSE:  6.859336465597153\nEpoch:  60 , Time Elapsed:  9.538631614049276  mins\nEpoch 60/200, Batch 1/45, Loss: 0.33775708079338074\nEpoch 60/200, Batch 2/45, Loss: 0.47466832399368286\nEpoch 60/200, Batch 3/45, Loss: 0.3269343972206116\nEpoch 60/200, Batch 4/45, Loss: 0.2061169594526291\nEpoch 60/200, Batch 5/45, Loss: 0.21119260787963867\nEpoch 60/200, Batch 6/45, Loss: 0.3007773458957672\nEpoch 60/200, Batch 7/45, Loss: 0.2643425464630127\nEpoch 60/200, Batch 8/45, Loss: 0.22175975143909454\nEpoch 60/200, Batch 9/45, Loss: 0.18220847845077515\nEpoch 60/200, Batch 10/45, Loss: 0.3918544054031372\nEpoch 60/200, Batch 11/45, Loss: 0.2891724705696106\nEpoch 60/200, Batch 12/45, Loss: 0.2821347713470459\nEpoch 60/200, Batch 13/45, Loss: 0.29352933168411255\nEpoch 60/200, Batch 14/45, Loss: 0.2845851182937622\nEpoch 60/200, Batch 15/45, Loss: 0.19335252046585083\nEpoch 60/200, Batch 16/45, Loss: 0.15781040489673615\nEpoch 60/200, Batch 17/45, Loss: 0.12013423442840576\nEpoch 60/200, Batch 18/45, Loss: 0.2975274920463562\nEpoch 60/200, Batch 19/45, Loss: 0.2253943681716919\nEpoch 60/200, Batch 20/45, Loss: 0.26115497946739197\nEpoch 60/200, Batch 21/45, Loss: 0.13961206376552582\nEpoch 60/200, Batch 22/45, Loss: 0.49601662158966064\nEpoch 60/200, Batch 23/45, Loss: 0.6548383235931396\nEpoch 60/200, Batch 24/45, Loss: 0.23513418436050415\nEpoch 60/200, Batch 25/45, Loss: 0.2854272723197937\nEpoch 60/200, Batch 26/45, Loss: 0.24451838433742523\nEpoch 60/200, Batch 27/45, Loss: 0.4265716075897217\nEpoch 60/200, Batch 28/45, Loss: 0.3665660619735718\nEpoch 60/200, Batch 29/45, Loss: 0.22094188630580902\nEpoch 60/200, Batch 30/45, Loss: 0.17174193263053894\nEpoch 60/200, Batch 31/45, Loss: 0.36070793867111206\nEpoch 60/200, Batch 32/45, Loss: 0.19396568834781647\nEpoch 60/200, Batch 33/45, Loss: 0.2610241174697876\nEpoch 60/200, Batch 34/45, Loss: 0.19601327180862427\nEpoch 60/200, Batch 35/45, Loss: 0.3504912853240967\nEpoch 60/200, Batch 36/45, Loss: 0.3711983561515808\nEpoch 60/200, Batch 37/45, Loss: 0.20268109440803528\nEpoch 60/200, Batch 38/45, Loss: 0.21797551214694977\nEpoch 60/200, Batch 39/45, Loss: 0.16796272993087769\nEpoch 60/200, Batch 40/45, Loss: 0.25965768098831177\nEpoch 60/200, Batch 41/45, Loss: 0.29883426427841187\nEpoch 60/200, Batch 42/45, Loss: 0.24857467412948608\nEpoch 60/200, Batch 43/45, Loss: 0.33107298612594604\nEpoch 60/200, Batch 44/45, Loss: 0.20598550140857697\nEpoch 60/200, Batch 45/45, Loss: 0.24302735924720764\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  7.861817359924316 Best Val MSE:  6.859336465597153\nEpoch:  61 , Time Elapsed:  9.697351849079132  mins\nEpoch 61/200, Batch 1/45, Loss: 0.3925192654132843\nEpoch 61/200, Batch 2/45, Loss: 0.10505297034978867\nEpoch 61/200, Batch 3/45, Loss: 0.23102763295173645\nEpoch 61/200, Batch 4/45, Loss: 0.09840017557144165\nEpoch 61/200, Batch 5/45, Loss: 0.22329720854759216\nEpoch 61/200, Batch 6/45, Loss: 0.27599579095840454\nEpoch 61/200, Batch 7/45, Loss: 0.11268126964569092\nEpoch 61/200, Batch 8/45, Loss: 0.19061912596225739\nEpoch 61/200, Batch 9/45, Loss: 0.47842031717300415\nEpoch 61/200, Batch 10/45, Loss: 0.36895042657852173\nEpoch 61/200, Batch 11/45, Loss: 0.3158990442752838\nEpoch 61/200, Batch 12/45, Loss: 0.38900020718574524\nEpoch 61/200, Batch 13/45, Loss: 0.2356068640947342\nEpoch 61/200, Batch 14/45, Loss: 0.28336912393569946\nEpoch 61/200, Batch 15/45, Loss: 0.2012610137462616\nEpoch 61/200, Batch 16/45, Loss: 0.23817835748195648\nEpoch 61/200, Batch 17/45, Loss: 0.35550668835639954\nEpoch 61/200, Batch 18/45, Loss: 0.3557041883468628\nEpoch 61/200, Batch 19/45, Loss: 0.20945540070533752\nEpoch 61/200, Batch 20/45, Loss: 0.3006216585636139\nEpoch 61/200, Batch 21/45, Loss: 0.4170451760292053\nEpoch 61/200, Batch 22/45, Loss: 0.2844468355178833\nEpoch 61/200, Batch 23/45, Loss: 0.32806912064552307\nEpoch 61/200, Batch 24/45, Loss: 0.2429032325744629\nEpoch 61/200, Batch 25/45, Loss: 0.21476879715919495\nEpoch 61/200, Batch 26/45, Loss: 0.2976193428039551\nEpoch 61/200, Batch 27/45, Loss: 0.14250880479812622\nEpoch 61/200, Batch 28/45, Loss: 0.3088151216506958\nEpoch 61/200, Batch 29/45, Loss: 0.35817793011665344\nEpoch 61/200, Batch 30/45, Loss: 0.3316417932510376\nEpoch 61/200, Batch 31/45, Loss: 0.32017892599105835\nEpoch 61/200, Batch 32/45, Loss: 0.19856299459934235\nEpoch 61/200, Batch 33/45, Loss: 0.4495445787906647\nEpoch 61/200, Batch 34/45, Loss: 0.2847944498062134\nEpoch 61/200, Batch 35/45, Loss: 0.32626017928123474\nEpoch 61/200, Batch 36/45, Loss: 0.2387509047985077\nEpoch 61/200, Batch 37/45, Loss: 0.1732998788356781\nEpoch 61/200, Batch 38/45, Loss: 0.2198505699634552\nEpoch 61/200, Batch 39/45, Loss: 0.4188166856765747\nEpoch 61/200, Batch 40/45, Loss: 0.6258059740066528\nEpoch 61/200, Batch 41/45, Loss: 0.5147507190704346\nEpoch 61/200, Batch 42/45, Loss: 0.2561725974082947\nEpoch 61/200, Batch 43/45, Loss: 0.5484497547149658\nEpoch 61/200, Batch 44/45, Loss: 0.22247393429279327\nEpoch 61/200, Batch 45/45, Loss: 0.14297160506248474\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  8.275920510292053 Best Val MSE:  6.859336465597153\nEpoch:  62 , Time Elapsed:  9.860958647727966  mins\nEpoch 62/200, Batch 1/45, Loss: 0.34966668486595154\nEpoch 62/200, Batch 2/45, Loss: 0.21469572186470032\nEpoch 62/200, Batch 3/45, Loss: 0.13885590434074402\nEpoch 62/200, Batch 4/45, Loss: 0.19253291189670563\nEpoch 62/200, Batch 5/45, Loss: 0.22930015623569489\nEpoch 62/200, Batch 6/45, Loss: 0.27242618799209595\nEpoch 62/200, Batch 7/45, Loss: 0.43536001443862915\nEpoch 62/200, Batch 8/45, Loss: 0.4173908829689026\nEpoch 62/200, Batch 9/45, Loss: 0.2955504059791565\nEpoch 62/200, Batch 10/45, Loss: 0.3164207339286804\nEpoch 62/200, Batch 11/45, Loss: 0.29615819454193115\nEpoch 62/200, Batch 12/45, Loss: 0.33085954189300537\nEpoch 62/200, Batch 13/45, Loss: 0.23465889692306519\nEpoch 62/200, Batch 14/45, Loss: 0.270285964012146\nEpoch 62/200, Batch 15/45, Loss: 0.35278165340423584\nEpoch 62/200, Batch 16/45, Loss: 0.2910492718219757\nEpoch 62/200, Batch 17/45, Loss: 0.5393564701080322\nEpoch 62/200, Batch 18/45, Loss: 0.3287689685821533\nEpoch 62/200, Batch 19/45, Loss: 0.2757341265678406\nEpoch 62/200, Batch 20/45, Loss: 0.2984524369239807\nEpoch 62/200, Batch 21/45, Loss: 0.25456002354621887\nEpoch 62/200, Batch 22/45, Loss: 0.26171255111694336\nEpoch 62/200, Batch 23/45, Loss: 0.7166677713394165\nEpoch 62/200, Batch 24/45, Loss: 0.3601691722869873\nEpoch 62/200, Batch 25/45, Loss: 0.5492304563522339\nEpoch 62/200, Batch 26/45, Loss: 0.43030238151550293\nEpoch 62/200, Batch 27/45, Loss: 0.2874370217323303\nEpoch 62/200, Batch 28/45, Loss: 0.22581714391708374\nEpoch 62/200, Batch 29/45, Loss: 0.3774367570877075\nEpoch 62/200, Batch 30/45, Loss: 0.6097595691680908\nEpoch 62/200, Batch 31/45, Loss: 0.25643008947372437\nEpoch 62/200, Batch 32/45, Loss: 0.3605799078941345\nEpoch 62/200, Batch 33/45, Loss: 0.30895134806632996\nEpoch 62/200, Batch 34/45, Loss: 0.5102641582489014\nEpoch 62/200, Batch 35/45, Loss: 0.2630724012851715\nEpoch 62/200, Batch 36/45, Loss: 0.29496580362319946\nEpoch 62/200, Batch 37/45, Loss: 0.36652839183807373\nEpoch 62/200, Batch 38/45, Loss: 0.27781155705451965\nEpoch 62/200, Batch 39/45, Loss: 0.3452768623828888\nEpoch 62/200, Batch 40/45, Loss: 0.3133871853351593\nEpoch 62/200, Batch 41/45, Loss: 0.4047843813896179\nEpoch 62/200, Batch 42/45, Loss: 0.2978150248527527\nEpoch 62/200, Batch 43/45, Loss: 0.16450580954551697\nEpoch 62/200, Batch 44/45, Loss: 0.24965417385101318\nEpoch 62/200, Batch 45/45, Loss: 0.19154483079910278\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  8.59133666753769 Best Val MSE:  6.859336465597153\nEpoch:  63 , Time Elapsed:  10.01901748975118  mins\nEpoch 63/200, Batch 1/45, Loss: 0.2915995419025421\nEpoch 63/200, Batch 2/45, Loss: 0.4491425156593323\nEpoch 63/200, Batch 3/45, Loss: 0.22951000928878784\nEpoch 63/200, Batch 4/45, Loss: 0.3203747570514679\nEpoch 63/200, Batch 5/45, Loss: 0.45398378372192383\nEpoch 63/200, Batch 6/45, Loss: 0.22999289631843567\nEpoch 63/200, Batch 7/45, Loss: 0.371150940656662\nEpoch 63/200, Batch 8/45, Loss: 0.19979098439216614\nEpoch 63/200, Batch 9/45, Loss: 0.36372044682502747\nEpoch 63/200, Batch 10/45, Loss: 0.32194995880126953\nEpoch 63/200, Batch 11/45, Loss: 0.31635645031929016\nEpoch 63/200, Batch 12/45, Loss: 0.4328126907348633\nEpoch 63/200, Batch 13/45, Loss: 0.2319883108139038\nEpoch 63/200, Batch 14/45, Loss: 0.32186490297317505\nEpoch 63/200, Batch 15/45, Loss: 0.19940699636936188\nEpoch 63/200, Batch 16/45, Loss: 0.3253592550754547\nEpoch 63/200, Batch 17/45, Loss: 0.18069002032279968\nEpoch 63/200, Batch 18/45, Loss: 0.5403404235839844\nEpoch 63/200, Batch 19/45, Loss: 0.3009558320045471\nEpoch 63/200, Batch 20/45, Loss: 0.16034825146198273\nEpoch 63/200, Batch 21/45, Loss: 0.2725585103034973\nEpoch 63/200, Batch 22/45, Loss: 0.1901683807373047\nEpoch 63/200, Batch 23/45, Loss: 0.15415531396865845\nEpoch 63/200, Batch 24/45, Loss: 0.2855679392814636\nEpoch 63/200, Batch 25/45, Loss: 0.22244040668010712\nEpoch 63/200, Batch 26/45, Loss: 0.294697642326355\nEpoch 63/200, Batch 27/45, Loss: 0.1444169133901596\nEpoch 63/200, Batch 28/45, Loss: 0.5074942111968994\nEpoch 63/200, Batch 29/45, Loss: 0.6344423294067383\nEpoch 63/200, Batch 30/45, Loss: 0.24013519287109375\nEpoch 63/200, Batch 31/45, Loss: 0.2952590584754944\nEpoch 63/200, Batch 32/45, Loss: 0.3340151906013489\nEpoch 63/200, Batch 33/45, Loss: 0.24677136540412903\nEpoch 63/200, Batch 34/45, Loss: 0.28646600246429443\nEpoch 63/200, Batch 35/45, Loss: 0.24937668442726135\nEpoch 63/200, Batch 36/45, Loss: 0.25177961587905884\nEpoch 63/200, Batch 37/45, Loss: 0.2891988158226013\nEpoch 63/200, Batch 38/45, Loss: 0.36213427782058716\nEpoch 63/200, Batch 39/45, Loss: 0.41921746730804443\nEpoch 63/200, Batch 40/45, Loss: 0.2811812162399292\nEpoch 63/200, Batch 41/45, Loss: 0.32706040143966675\nEpoch 63/200, Batch 42/45, Loss: 0.18924368917942047\nEpoch 63/200, Batch 43/45, Loss: 0.25463956594467163\nEpoch 63/200, Batch 44/45, Loss: 0.18318399786949158\nEpoch 63/200, Batch 45/45, Loss: 0.17721056938171387\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  8.339430630207062 Best Val MSE:  6.859336465597153\nEpoch:  64 , Time Elapsed:  10.175622951984405  mins\nEpoch 64/200, Batch 1/45, Loss: 0.15659993886947632\nEpoch 64/200, Batch 2/45, Loss: 0.2575750946998596\nEpoch 64/200, Batch 3/45, Loss: 0.24987947940826416\nEpoch 64/200, Batch 4/45, Loss: 0.23176473379135132\nEpoch 64/200, Batch 5/45, Loss: 0.23858387768268585\nEpoch 64/200, Batch 6/45, Loss: 0.26359888911247253\nEpoch 64/200, Batch 7/45, Loss: 0.3071718215942383\nEpoch 64/200, Batch 8/45, Loss: 0.3063582479953766\nEpoch 64/200, Batch 9/45, Loss: 0.22327977418899536\nEpoch 64/200, Batch 10/45, Loss: 0.19869542121887207\nEpoch 64/200, Batch 11/45, Loss: 0.18096670508384705\nEpoch 64/200, Batch 12/45, Loss: 0.22040444612503052\nEpoch 64/200, Batch 13/45, Loss: 0.19746394455432892\nEpoch 64/200, Batch 14/45, Loss: 0.1811370849609375\nEpoch 64/200, Batch 15/45, Loss: 0.19324718415737152\nEpoch 64/200, Batch 16/45, Loss: 0.22200241684913635\nEpoch 64/200, Batch 17/45, Loss: 0.4097760021686554\nEpoch 64/200, Batch 18/45, Loss: 0.47162768244743347\nEpoch 64/200, Batch 19/45, Loss: 0.3085760772228241\nEpoch 64/200, Batch 20/45, Loss: 0.2639722228050232\nEpoch 64/200, Batch 21/45, Loss: 1.3038257360458374\nEpoch 64/200, Batch 22/45, Loss: 0.1907668113708496\nEpoch 64/200, Batch 23/45, Loss: 0.35157448053359985\nEpoch 64/200, Batch 24/45, Loss: 0.3403969407081604\nEpoch 64/200, Batch 25/45, Loss: 0.25967031717300415\nEpoch 64/200, Batch 26/45, Loss: 0.2806408107280731\nEpoch 64/200, Batch 27/45, Loss: 0.32655635476112366\nEpoch 64/200, Batch 28/45, Loss: 0.26484134793281555\nEpoch 64/200, Batch 29/45, Loss: 0.6217946410179138\nEpoch 64/200, Batch 30/45, Loss: 0.26600781083106995\nEpoch 64/200, Batch 31/45, Loss: 0.580504298210144\nEpoch 64/200, Batch 32/45, Loss: 0.40055471658706665\nEpoch 64/200, Batch 33/45, Loss: 0.2962171137332916\nEpoch 64/200, Batch 34/45, Loss: 0.22980216145515442\nEpoch 64/200, Batch 35/45, Loss: 0.3076978027820587\nEpoch 64/200, Batch 36/45, Loss: 0.33330515027046204\nEpoch 64/200, Batch 37/45, Loss: 0.47594600915908813\nEpoch 64/200, Batch 38/45, Loss: 0.24772699177265167\nEpoch 64/200, Batch 39/45, Loss: 0.3649277091026306\nEpoch 64/200, Batch 40/45, Loss: 0.4730892777442932\nEpoch 64/200, Batch 41/45, Loss: 0.33652129769325256\nEpoch 64/200, Batch 42/45, Loss: 0.31946486234664917\nEpoch 64/200, Batch 43/45, Loss: 0.4043920040130615\nEpoch 64/200, Batch 44/45, Loss: 0.1840638667345047\nEpoch 64/200, Batch 45/45, Loss: 0.20103463530540466\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  13.190251171588898 Best Val MSE:  6.859336465597153\nEpoch:  65 , Time Elapsed:  10.337822798887888  mins\nEpoch 65/200, Batch 1/45, Loss: 0.24270181357860565\nEpoch 65/200, Batch 2/45, Loss: 0.3423463702201843\nEpoch 65/200, Batch 3/45, Loss: 0.19333410263061523\nEpoch 65/200, Batch 4/45, Loss: 0.44482630491256714\nEpoch 65/200, Batch 5/45, Loss: 0.2965999245643616\nEpoch 65/200, Batch 6/45, Loss: 0.21627193689346313\nEpoch 65/200, Batch 7/45, Loss: 0.32136815786361694\nEpoch 65/200, Batch 8/45, Loss: 0.4427028298377991\nEpoch 65/200, Batch 9/45, Loss: 0.22367903590202332\nEpoch 65/200, Batch 10/45, Loss: 0.2900713086128235\nEpoch 65/200, Batch 11/45, Loss: 0.1690371334552765\nEpoch 65/200, Batch 12/45, Loss: 0.2833351492881775\nEpoch 65/200, Batch 13/45, Loss: 0.3567814826965332\nEpoch 65/200, Batch 14/45, Loss: 0.4224322736263275\nEpoch 65/200, Batch 15/45, Loss: 0.29090896248817444\nEpoch 65/200, Batch 16/45, Loss: 0.46325671672821045\nEpoch 65/200, Batch 17/45, Loss: 0.2528025805950165\nEpoch 65/200, Batch 18/45, Loss: 0.26635587215423584\nEpoch 65/200, Batch 19/45, Loss: 0.2214025855064392\nEpoch 65/200, Batch 20/45, Loss: 0.3526787757873535\nEpoch 65/200, Batch 21/45, Loss: 0.4315897226333618\nEpoch 65/200, Batch 22/45, Loss: 0.2563686668872833\nEpoch 65/200, Batch 23/45, Loss: 0.21947869658470154\nEpoch 65/200, Batch 24/45, Loss: 0.4102627635002136\nEpoch 65/200, Batch 25/45, Loss: 0.2831953763961792\nEpoch 65/200, Batch 26/45, Loss: 0.7040393352508545\nEpoch 65/200, Batch 27/45, Loss: 0.1816892772912979\nEpoch 65/200, Batch 28/45, Loss: 0.2805801033973694\nEpoch 65/200, Batch 29/45, Loss: 0.3149483799934387\nEpoch 65/200, Batch 30/45, Loss: 0.21143610775470734\nEpoch 65/200, Batch 31/45, Loss: 0.2603004276752472\nEpoch 65/200, Batch 32/45, Loss: 0.326513409614563\nEpoch 65/200, Batch 33/45, Loss: 0.29889094829559326\nEpoch 65/200, Batch 34/45, Loss: 0.20606260001659393\nEpoch 65/200, Batch 35/45, Loss: 0.14485222101211548\nEpoch 65/200, Batch 36/45, Loss: 0.2581312954425812\nEpoch 65/200, Batch 37/45, Loss: 0.28750163316726685\nEpoch 65/200, Batch 38/45, Loss: 0.3962770104408264\nEpoch 65/200, Batch 39/45, Loss: 0.2717621922492981\nEpoch 65/200, Batch 40/45, Loss: 0.45091110467910767\nEpoch 65/200, Batch 41/45, Loss: 0.270555317401886\nEpoch 65/200, Batch 42/45, Loss: 0.21266359090805054\nEpoch 65/200, Batch 43/45, Loss: 0.2553802728652954\nEpoch 65/200, Batch 44/45, Loss: 0.30372726917266846\nEpoch 65/200, Batch 45/45, Loss: 0.4685809016227722\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  10.597585141658783 Best Val MSE:  6.859336465597153\nEpoch:  66 , Time Elapsed:  10.496020555496216  mins\nEpoch 66/200, Batch 1/45, Loss: 0.2997777760028839\nEpoch 66/200, Batch 2/45, Loss: 0.36462557315826416\nEpoch 66/200, Batch 3/45, Loss: 0.4847486913204193\nEpoch 66/200, Batch 4/45, Loss: 0.3598424196243286\nEpoch 66/200, Batch 5/45, Loss: 0.24526846408843994\nEpoch 66/200, Batch 6/45, Loss: 0.33613815903663635\nEpoch 66/200, Batch 7/45, Loss: 0.28881728649139404\nEpoch 66/200, Batch 8/45, Loss: 0.2339150309562683\nEpoch 66/200, Batch 9/45, Loss: 0.2152888923883438\nEpoch 66/200, Batch 10/45, Loss: 0.22776329517364502\nEpoch 66/200, Batch 11/45, Loss: 0.3289951682090759\nEpoch 66/200, Batch 12/45, Loss: 0.6784363985061646\nEpoch 66/200, Batch 13/45, Loss: 0.2227545529603958\nEpoch 66/200, Batch 14/45, Loss: 0.17993400990962982\nEpoch 66/200, Batch 15/45, Loss: 0.31953319907188416\nEpoch 66/200, Batch 16/45, Loss: 0.28955331444740295\nEpoch 66/200, Batch 17/45, Loss: 0.3955378830432892\nEpoch 66/200, Batch 18/45, Loss: 1.1742244958877563\nEpoch 66/200, Batch 19/45, Loss: 0.33984172344207764\nEpoch 66/200, Batch 20/45, Loss: 0.31186237931251526\nEpoch 66/200, Batch 21/45, Loss: 0.40025264024734497\nEpoch 66/200, Batch 22/45, Loss: 0.46060246229171753\nEpoch 66/200, Batch 23/45, Loss: 0.32836809754371643\nEpoch 66/200, Batch 24/45, Loss: 0.21488884091377258\nEpoch 66/200, Batch 25/45, Loss: 0.23907917737960815\nEpoch 66/200, Batch 26/45, Loss: 0.3670777976512909\nEpoch 66/200, Batch 27/45, Loss: 0.4233247935771942\nEpoch 66/200, Batch 28/45, Loss: 0.14171472191810608\nEpoch 66/200, Batch 29/45, Loss: 0.25971004366874695\nEpoch 66/200, Batch 30/45, Loss: 0.49352017045021057\nEpoch 66/200, Batch 31/45, Loss: 0.4419724941253662\nEpoch 66/200, Batch 32/45, Loss: 0.28717920184135437\nEpoch 66/200, Batch 33/45, Loss: 0.23069068789482117\nEpoch 66/200, Batch 34/45, Loss: 0.1459694653749466\nEpoch 66/200, Batch 35/45, Loss: 0.4403321146965027\nEpoch 66/200, Batch 36/45, Loss: 0.2812776565551758\nEpoch 66/200, Batch 37/45, Loss: 0.43657076358795166\nEpoch 66/200, Batch 38/45, Loss: 0.42866116762161255\nEpoch 66/200, Batch 39/45, Loss: 0.5588409900665283\nEpoch 66/200, Batch 40/45, Loss: 0.16174480319023132\nEpoch 66/200, Batch 41/45, Loss: 0.3987638056278229\nEpoch 66/200, Batch 42/45, Loss: 0.14753258228302002\nEpoch 66/200, Batch 43/45, Loss: 0.24165141582489014\nEpoch 66/200, Batch 44/45, Loss: 0.3592277765274048\nEpoch 66/200, Batch 45/45, Loss: 0.25319164991378784\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  7.42925950884819 Best Val MSE:  6.859336465597153\nEpoch:  67 , Time Elapsed:  10.651814492543538  mins\nEpoch 67/200, Batch 1/45, Loss: 0.28682905435562134\nEpoch 67/200, Batch 2/45, Loss: 0.4702695310115814\nEpoch 67/200, Batch 3/45, Loss: 0.36987531185150146\nEpoch 67/200, Batch 4/45, Loss: 0.4898739457130432\nEpoch 67/200, Batch 5/45, Loss: 0.20629726350307465\nEpoch 67/200, Batch 6/45, Loss: 0.35271432995796204\nEpoch 67/200, Batch 7/45, Loss: 0.2248360961675644\nEpoch 67/200, Batch 8/45, Loss: 0.2479223906993866\nEpoch 67/200, Batch 9/45, Loss: 0.20588915050029755\nEpoch 67/200, Batch 10/45, Loss: 0.4510614275932312\nEpoch 67/200, Batch 11/45, Loss: 0.2818507254123688\nEpoch 67/200, Batch 12/45, Loss: 0.1845967322587967\nEpoch 67/200, Batch 13/45, Loss: 0.21527495980262756\nEpoch 67/200, Batch 14/45, Loss: 0.47573646903038025\nEpoch 67/200, Batch 15/45, Loss: 0.391940176486969\nEpoch 67/200, Batch 16/45, Loss: 0.2723577320575714\nEpoch 67/200, Batch 17/45, Loss: 0.2745751142501831\nEpoch 67/200, Batch 18/45, Loss: 0.23764333128929138\nEpoch 67/200, Batch 19/45, Loss: 0.2870299816131592\nEpoch 67/200, Batch 20/45, Loss: 0.2835085690021515\nEpoch 67/200, Batch 21/45, Loss: 0.3556438982486725\nEpoch 67/200, Batch 22/45, Loss: 0.22957774996757507\nEpoch 67/200, Batch 23/45, Loss: 0.15271441638469696\nEpoch 67/200, Batch 24/45, Loss: 0.2981657385826111\nEpoch 67/200, Batch 25/45, Loss: 0.2828538715839386\nEpoch 67/200, Batch 26/45, Loss: 0.33912980556488037\nEpoch 67/200, Batch 27/45, Loss: 0.1520247757434845\nEpoch 67/200, Batch 28/45, Loss: 0.1538638174533844\nEpoch 67/200, Batch 29/45, Loss: 0.2086525857448578\nEpoch 67/200, Batch 30/45, Loss: 0.12305986881256104\nEpoch 67/200, Batch 31/45, Loss: 0.277884304523468\nEpoch 67/200, Batch 32/45, Loss: 0.5720805525779724\nEpoch 67/200, Batch 33/45, Loss: 0.15628233551979065\nEpoch 67/200, Batch 34/45, Loss: 0.26329728960990906\nEpoch 67/200, Batch 35/45, Loss: 0.4339052736759186\nEpoch 67/200, Batch 36/45, Loss: 0.360842764377594\nEpoch 67/200, Batch 37/45, Loss: 0.4018916189670563\nEpoch 67/200, Batch 38/45, Loss: 0.23062607645988464\nEpoch 67/200, Batch 39/45, Loss: 0.27117717266082764\nEpoch 67/200, Batch 40/45, Loss: 0.2750658094882965\nEpoch 67/200, Batch 41/45, Loss: 0.21054434776306152\nEpoch 67/200, Batch 42/45, Loss: 0.2244553118944168\nEpoch 67/200, Batch 43/45, Loss: 0.3697323203086853\nEpoch 67/200, Batch 44/45, Loss: 0.2669558525085449\nEpoch 67/200, Batch 45/45, Loss: 0.2612225115299225\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  8.387945830821991 Best Val MSE:  6.859336465597153\nEpoch:  68 , Time Elapsed:  10.809333336353301  mins\nEpoch 68/200, Batch 1/45, Loss: 0.2108195275068283\nEpoch 68/200, Batch 2/45, Loss: 0.3790164291858673\nEpoch 68/200, Batch 3/45, Loss: 0.2801651358604431\nEpoch 68/200, Batch 4/45, Loss: 0.20612341165542603\nEpoch 68/200, Batch 5/45, Loss: 0.20543968677520752\nEpoch 68/200, Batch 6/45, Loss: 0.21166583895683289\nEpoch 68/200, Batch 7/45, Loss: 0.34619268774986267\nEpoch 68/200, Batch 8/45, Loss: 0.16792462766170502\nEpoch 68/200, Batch 9/45, Loss: 0.28497880697250366\nEpoch 68/200, Batch 10/45, Loss: 0.25433480739593506\nEpoch 68/200, Batch 11/45, Loss: 0.21860410273075104\nEpoch 68/200, Batch 12/45, Loss: 0.25895917415618896\nEpoch 68/200, Batch 13/45, Loss: 0.439400315284729\nEpoch 68/200, Batch 14/45, Loss: 0.4653993844985962\nEpoch 68/200, Batch 15/45, Loss: 0.29664039611816406\nEpoch 68/200, Batch 16/45, Loss: 0.26489055156707764\nEpoch 68/200, Batch 17/45, Loss: 0.2720593214035034\nEpoch 68/200, Batch 18/45, Loss: 0.13444264233112335\nEpoch 68/200, Batch 19/45, Loss: 0.27735114097595215\nEpoch 68/200, Batch 20/45, Loss: 0.19570785760879517\nEpoch 68/200, Batch 21/45, Loss: 0.299146831035614\nEpoch 68/200, Batch 22/45, Loss: 0.38169974088668823\nEpoch 68/200, Batch 23/45, Loss: 0.2665516138076782\nEpoch 68/200, Batch 24/45, Loss: 0.2599944770336151\nEpoch 68/200, Batch 25/45, Loss: 0.334229052066803\nEpoch 68/200, Batch 26/45, Loss: 0.30180346965789795\nEpoch 68/200, Batch 27/45, Loss: 0.30841460824012756\nEpoch 68/200, Batch 28/45, Loss: 0.17531107366085052\nEpoch 68/200, Batch 29/45, Loss: 0.21675866842269897\nEpoch 68/200, Batch 30/45, Loss: 0.2730330228805542\nEpoch 68/200, Batch 31/45, Loss: 0.23991933465003967\nEpoch 68/200, Batch 32/45, Loss: 0.39522236585617065\nEpoch 68/200, Batch 33/45, Loss: 0.3343557119369507\nEpoch 68/200, Batch 34/45, Loss: 0.2953958511352539\nEpoch 68/200, Batch 35/45, Loss: 0.3797014355659485\nEpoch 68/200, Batch 36/45, Loss: 0.4319506883621216\nEpoch 68/200, Batch 37/45, Loss: 0.35813814401626587\nEpoch 68/200, Batch 38/45, Loss: 0.18501293659210205\nEpoch 68/200, Batch 39/45, Loss: 0.18228289484977722\nEpoch 68/200, Batch 40/45, Loss: 0.22342419624328613\nEpoch 68/200, Batch 41/45, Loss: 0.2001899778842926\nEpoch 68/200, Batch 42/45, Loss: 0.5124770402908325\nEpoch 68/200, Batch 43/45, Loss: 0.6967422962188721\nEpoch 68/200, Batch 44/45, Loss: 0.2762511372566223\nEpoch 68/200, Batch 45/45, Loss: 0.41235852241516113\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  7.614724576473236 Best Val MSE:  6.859336465597153\nEpoch:  69 , Time Elapsed:  10.977363264560699  mins\nEpoch 69/200, Batch 1/45, Loss: 0.27970391511917114\nEpoch 69/200, Batch 2/45, Loss: 0.24725493788719177\nEpoch 69/200, Batch 3/45, Loss: 0.1526317447423935\nEpoch 69/200, Batch 4/45, Loss: 0.3111193776130676\nEpoch 69/200, Batch 5/45, Loss: 0.24633798003196716\nEpoch 69/200, Batch 6/45, Loss: 0.4592120349407196\nEpoch 69/200, Batch 7/45, Loss: 0.2371818721294403\nEpoch 69/200, Batch 8/45, Loss: 0.16531842947006226\nEpoch 69/200, Batch 9/45, Loss: 0.33246734738349915\nEpoch 69/200, Batch 10/45, Loss: 0.15820682048797607\nEpoch 69/200, Batch 11/45, Loss: 0.32218560576438904\nEpoch 69/200, Batch 12/45, Loss: 0.1841144859790802\nEpoch 69/200, Batch 13/45, Loss: 0.14991700649261475\nEpoch 69/200, Batch 14/45, Loss: 1.1217310428619385\nEpoch 69/200, Batch 15/45, Loss: 0.12697838246822357\nEpoch 69/200, Batch 16/45, Loss: 0.38228461146354675\nEpoch 69/200, Batch 17/45, Loss: 0.19249172508716583\nEpoch 69/200, Batch 18/45, Loss: 0.28007328510284424\nEpoch 69/200, Batch 19/45, Loss: 0.2588081955909729\nEpoch 69/200, Batch 20/45, Loss: 0.3232548236846924\nEpoch 69/200, Batch 21/45, Loss: 0.3649483323097229\nEpoch 69/200, Batch 22/45, Loss: 0.2243831753730774\nEpoch 69/200, Batch 23/45, Loss: 0.23424206674098969\nEpoch 69/200, Batch 24/45, Loss: 0.15469491481781006\nEpoch 69/200, Batch 25/45, Loss: 0.17345744371414185\nEpoch 69/200, Batch 26/45, Loss: 0.2515731751918793\nEpoch 69/200, Batch 27/45, Loss: 0.12125153839588165\nEpoch 69/200, Batch 28/45, Loss: 0.21462225914001465\nEpoch 69/200, Batch 29/45, Loss: 0.5341378450393677\nEpoch 69/200, Batch 30/45, Loss: 0.62971031665802\nEpoch 69/200, Batch 31/45, Loss: 0.21855412423610687\nEpoch 69/200, Batch 32/45, Loss: 0.21167021989822388\nEpoch 69/200, Batch 33/45, Loss: 0.3973279297351837\nEpoch 69/200, Batch 34/45, Loss: 0.42626261711120605\nEpoch 69/200, Batch 35/45, Loss: 0.3411256670951843\nEpoch 69/200, Batch 36/45, Loss: 0.3317343294620514\nEpoch 69/200, Batch 37/45, Loss: 0.2449096143245697\nEpoch 69/200, Batch 38/45, Loss: 0.34258705377578735\nEpoch 69/200, Batch 39/45, Loss: 0.5146709680557251\nEpoch 69/200, Batch 40/45, Loss: 0.40847182273864746\nEpoch 69/200, Batch 41/45, Loss: 0.444263219833374\nEpoch 69/200, Batch 42/45, Loss: 0.3105633854866028\nEpoch 69/200, Batch 43/45, Loss: 0.18015114963054657\nEpoch 69/200, Batch 44/45, Loss: 0.6570892333984375\nEpoch 69/200, Batch 45/45, Loss: 0.22852250933647156\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  7.569894313812256 Best Val MSE:  6.859336465597153\nEpoch:  70 , Time Elapsed:  11.138715028762817  mins\nEpoch 70/200, Batch 1/45, Loss: 0.23992760479450226\nEpoch 70/200, Batch 2/45, Loss: 0.3889474868774414\nEpoch 70/200, Batch 3/45, Loss: 0.3119142949581146\nEpoch 70/200, Batch 4/45, Loss: 0.20994357764720917\nEpoch 70/200, Batch 5/45, Loss: 0.24555937945842743\nEpoch 70/200, Batch 6/45, Loss: 0.20115281641483307\nEpoch 70/200, Batch 7/45, Loss: 0.25545865297317505\nEpoch 70/200, Batch 8/45, Loss: 0.16178521513938904\nEpoch 70/200, Batch 9/45, Loss: 0.3928394913673401\nEpoch 70/200, Batch 10/45, Loss: 0.28649377822875977\nEpoch 70/200, Batch 11/45, Loss: 0.412151962518692\nEpoch 70/200, Batch 12/45, Loss: 0.4990431070327759\nEpoch 70/200, Batch 13/45, Loss: 0.4458954930305481\nEpoch 70/200, Batch 14/45, Loss: 0.5394078493118286\nEpoch 70/200, Batch 15/45, Loss: 0.4240614175796509\nEpoch 70/200, Batch 16/45, Loss: 0.35077965259552\nEpoch 70/200, Batch 17/45, Loss: 0.45673102140426636\nEpoch 70/200, Batch 18/45, Loss: 0.17501477897167206\nEpoch 70/200, Batch 19/45, Loss: 0.21589887142181396\nEpoch 70/200, Batch 20/45, Loss: 0.28354954719543457\nEpoch 70/200, Batch 21/45, Loss: 0.32444584369659424\nEpoch 70/200, Batch 22/45, Loss: 0.3373044729232788\nEpoch 70/200, Batch 23/45, Loss: 0.22355081140995026\nEpoch 70/200, Batch 24/45, Loss: 0.31334418058395386\nEpoch 70/200, Batch 25/45, Loss: 0.6589834094047546\nEpoch 70/200, Batch 26/45, Loss: 0.5216372013092041\nEpoch 70/200, Batch 27/45, Loss: 0.26175907254219055\nEpoch 70/200, Batch 28/45, Loss: 0.15661948919296265\nEpoch 70/200, Batch 29/45, Loss: 0.4459226131439209\nEpoch 70/200, Batch 30/45, Loss: 0.5557222366333008\nEpoch 70/200, Batch 31/45, Loss: 0.3268609941005707\nEpoch 70/200, Batch 32/45, Loss: 0.3605949878692627\nEpoch 70/200, Batch 33/45, Loss: 0.3870914876461029\nEpoch 70/200, Batch 34/45, Loss: 0.391223281621933\nEpoch 70/200, Batch 35/45, Loss: 0.2470875233411789\nEpoch 70/200, Batch 36/45, Loss: 0.23855158686637878\nEpoch 70/200, Batch 37/45, Loss: 0.24214240908622742\nEpoch 70/200, Batch 38/45, Loss: 0.2780001759529114\nEpoch 70/200, Batch 39/45, Loss: 0.381443589925766\nEpoch 70/200, Batch 40/45, Loss: 0.3380405902862549\nEpoch 70/200, Batch 41/45, Loss: 0.20973125100135803\nEpoch 70/200, Batch 42/45, Loss: 0.28130871057510376\nEpoch 70/200, Batch 43/45, Loss: 0.3160625100135803\nEpoch 70/200, Batch 44/45, Loss: 0.23752333223819733\nEpoch 70/200, Batch 45/45, Loss: 0.20638522505760193\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  6.9714624881744385 Best Val MSE:  6.859336465597153\nEpoch:  71 , Time Elapsed:  11.297062134742736  mins\nEpoch 71/200, Batch 1/45, Loss: 0.41205698251724243\nEpoch 71/200, Batch 2/45, Loss: 0.30716049671173096\nEpoch 71/200, Batch 3/45, Loss: 0.3218159079551697\nEpoch 71/200, Batch 4/45, Loss: 0.26768290996551514\nEpoch 71/200, Batch 5/45, Loss: 0.18915049731731415\nEpoch 71/200, Batch 6/45, Loss: 0.21187157928943634\nEpoch 71/200, Batch 7/45, Loss: 0.18157270550727844\nEpoch 71/200, Batch 8/45, Loss: 0.4143289625644684\nEpoch 71/200, Batch 9/45, Loss: 0.16360560059547424\nEpoch 71/200, Batch 10/45, Loss: 0.25052881240844727\nEpoch 71/200, Batch 11/45, Loss: 0.3077485263347626\nEpoch 71/200, Batch 12/45, Loss: 0.3494630455970764\nEpoch 71/200, Batch 13/45, Loss: 0.35334667563438416\nEpoch 71/200, Batch 14/45, Loss: 0.26065555214881897\nEpoch 71/200, Batch 15/45, Loss: 0.6076167821884155\nEpoch 71/200, Batch 16/45, Loss: 0.46792250871658325\nEpoch 71/200, Batch 17/45, Loss: 0.19546431303024292\nEpoch 71/200, Batch 18/45, Loss: 0.2883484959602356\nEpoch 71/200, Batch 19/45, Loss: 0.242571160197258\nEpoch 71/200, Batch 20/45, Loss: 0.1495012789964676\nEpoch 71/200, Batch 21/45, Loss: 0.26432162523269653\nEpoch 71/200, Batch 22/45, Loss: 0.3306768536567688\nEpoch 71/200, Batch 23/45, Loss: 0.27379050850868225\nEpoch 71/200, Batch 24/45, Loss: 0.549146294593811\nEpoch 71/200, Batch 25/45, Loss: 0.2784934341907501\nEpoch 71/200, Batch 26/45, Loss: 0.3828253149986267\nEpoch 71/200, Batch 27/45, Loss: 0.43972575664520264\nEpoch 71/200, Batch 28/45, Loss: 0.2541186511516571\nEpoch 71/200, Batch 29/45, Loss: 0.24264834821224213\nEpoch 71/200, Batch 30/45, Loss: 0.3510301411151886\nEpoch 71/200, Batch 31/45, Loss: 0.22513946890830994\nEpoch 71/200, Batch 32/45, Loss: 0.13169775903224945\nEpoch 71/200, Batch 33/45, Loss: 0.24064934253692627\nEpoch 71/200, Batch 34/45, Loss: 0.3122348189353943\nEpoch 71/200, Batch 35/45, Loss: 0.2289566844701767\nEpoch 71/200, Batch 36/45, Loss: 0.22429224848747253\nEpoch 71/200, Batch 37/45, Loss: 0.24310067296028137\nEpoch 71/200, Batch 38/45, Loss: 0.19016042351722717\nEpoch 71/200, Batch 39/45, Loss: 0.3749358654022217\nEpoch 71/200, Batch 40/45, Loss: 0.1783299744129181\nEpoch 71/200, Batch 41/45, Loss: 0.23172099888324738\nEpoch 71/200, Batch 42/45, Loss: 0.4457147717475891\nEpoch 71/200, Batch 43/45, Loss: 0.26634296774864197\nEpoch 71/200, Batch 44/45, Loss: 0.42711085081100464\nEpoch 71/200, Batch 45/45, Loss: 0.3262398838996887\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  8.909722328186035 Best Val MSE:  6.859336465597153\nEpoch:  72 , Time Elapsed:  11.461604762077332  mins\nEpoch 72/200, Batch 1/45, Loss: 0.34429776668548584\nEpoch 72/200, Batch 2/45, Loss: 0.22989794611930847\nEpoch 72/200, Batch 3/45, Loss: 0.1547052562236786\nEpoch 72/200, Batch 4/45, Loss: 0.23930475115776062\nEpoch 72/200, Batch 5/45, Loss: 0.2915521264076233\nEpoch 72/200, Batch 6/45, Loss: 0.3019319772720337\nEpoch 72/200, Batch 7/45, Loss: 0.4471985995769501\nEpoch 72/200, Batch 8/45, Loss: 0.8047344088554382\nEpoch 72/200, Batch 9/45, Loss: 0.32437747716903687\nEpoch 72/200, Batch 10/45, Loss: 0.27436986565589905\nEpoch 72/200, Batch 11/45, Loss: 0.12529316544532776\nEpoch 72/200, Batch 12/45, Loss: 0.3903830051422119\nEpoch 72/200, Batch 13/45, Loss: 0.33009958267211914\nEpoch 72/200, Batch 14/45, Loss: 0.24067480862140656\nEpoch 72/200, Batch 15/45, Loss: 0.3092661499977112\nEpoch 72/200, Batch 16/45, Loss: 0.2789685130119324\nEpoch 72/200, Batch 17/45, Loss: 0.7356839179992676\nEpoch 72/200, Batch 18/45, Loss: 0.30723056197166443\nEpoch 72/200, Batch 19/45, Loss: 0.2927516996860504\nEpoch 72/200, Batch 20/45, Loss: 0.2968292236328125\nEpoch 72/200, Batch 21/45, Loss: 0.301391065120697\nEpoch 72/200, Batch 22/45, Loss: 0.26108914613723755\nEpoch 72/200, Batch 23/45, Loss: 0.2308110147714615\nEpoch 72/200, Batch 24/45, Loss: 0.3640707731246948\nEpoch 72/200, Batch 25/45, Loss: 0.2800142467021942\nEpoch 72/200, Batch 26/45, Loss: 0.2715539038181305\nEpoch 72/200, Batch 27/45, Loss: 0.21703684329986572\nEpoch 72/200, Batch 28/45, Loss: 0.27183982729911804\nEpoch 72/200, Batch 29/45, Loss: 0.23919577896595\nEpoch 72/200, Batch 30/45, Loss: 0.29148754477500916\nEpoch 72/200, Batch 31/45, Loss: 0.18595600128173828\nEpoch 72/200, Batch 32/45, Loss: 0.5333657264709473\nEpoch 72/200, Batch 33/45, Loss: 0.28238457441329956\nEpoch 72/200, Batch 34/45, Loss: 0.18577437102794647\nEpoch 72/200, Batch 35/45, Loss: 0.14203858375549316\nEpoch 72/200, Batch 36/45, Loss: 0.30524611473083496\nEpoch 72/200, Batch 37/45, Loss: 0.18559736013412476\nEpoch 72/200, Batch 38/45, Loss: 0.24407653510570526\nEpoch 72/200, Batch 39/45, Loss: 0.2567531168460846\nEpoch 72/200, Batch 40/45, Loss: 0.32945138216018677\nEpoch 72/200, Batch 41/45, Loss: 0.19730016589164734\nEpoch 72/200, Batch 42/45, Loss: 0.3914681077003479\nEpoch 72/200, Batch 43/45, Loss: 0.18168792128562927\nEpoch 72/200, Batch 44/45, Loss: 0.20062078535556793\nEpoch 72/200, Batch 45/45, Loss: 0.10260579735040665\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  8.106155782938004 Best Val MSE:  6.859336465597153\nEpoch:  73 , Time Elapsed:  11.619793764750163  mins\nEpoch 73/200, Batch 1/45, Loss: 0.20453572273254395\nEpoch 73/200, Batch 2/45, Loss: 0.32951879501342773\nEpoch 73/200, Batch 3/45, Loss: 0.25199195742607117\nEpoch 73/200, Batch 4/45, Loss: 0.6238267421722412\nEpoch 73/200, Batch 5/45, Loss: 0.5052858591079712\nEpoch 73/200, Batch 6/45, Loss: 0.28750914335250854\nEpoch 73/200, Batch 7/45, Loss: 0.16710801422595978\nEpoch 73/200, Batch 8/45, Loss: 0.3005760908126831\nEpoch 73/200, Batch 9/45, Loss: 0.21601691842079163\nEpoch 73/200, Batch 10/45, Loss: 0.20449288189411163\nEpoch 73/200, Batch 11/45, Loss: 0.23757235705852509\nEpoch 73/200, Batch 12/45, Loss: 0.37434476613998413\nEpoch 73/200, Batch 13/45, Loss: 0.6500766277313232\nEpoch 73/200, Batch 14/45, Loss: 0.5416643619537354\nEpoch 73/200, Batch 15/45, Loss: 0.2750406861305237\nEpoch 73/200, Batch 16/45, Loss: 0.42171505093574524\nEpoch 73/200, Batch 17/45, Loss: 0.3015873432159424\nEpoch 73/200, Batch 18/45, Loss: 0.21227578818798065\nEpoch 73/200, Batch 19/45, Loss: 0.42270901799201965\nEpoch 73/200, Batch 20/45, Loss: 0.7994804978370667\nEpoch 73/200, Batch 21/45, Loss: 0.1977563053369522\nEpoch 73/200, Batch 22/45, Loss: 0.6008182764053345\nEpoch 73/200, Batch 23/45, Loss: 0.428668737411499\nEpoch 73/200, Batch 24/45, Loss: 0.39550352096557617\nEpoch 73/200, Batch 25/45, Loss: 0.32834911346435547\nEpoch 73/200, Batch 26/45, Loss: 0.2800544798374176\nEpoch 73/200, Batch 27/45, Loss: 0.2698236405849457\nEpoch 73/200, Batch 28/45, Loss: 0.17004995048046112\nEpoch 73/200, Batch 29/45, Loss: 0.5220226645469666\nEpoch 73/200, Batch 30/45, Loss: 0.34487783908843994\nEpoch 73/200, Batch 31/45, Loss: 0.33459949493408203\nEpoch 73/200, Batch 32/45, Loss: 0.35173922777175903\nEpoch 73/200, Batch 33/45, Loss: 0.32364344596862793\nEpoch 73/200, Batch 34/45, Loss: 0.1482527256011963\nEpoch 73/200, Batch 35/45, Loss: 0.39865249395370483\nEpoch 73/200, Batch 36/45, Loss: 0.2643984258174896\nEpoch 73/200, Batch 37/45, Loss: 0.3911070227622986\nEpoch 73/200, Batch 38/45, Loss: 0.30578362941741943\nEpoch 73/200, Batch 39/45, Loss: 0.2314855307340622\nEpoch 73/200, Batch 40/45, Loss: 0.30451706051826477\nEpoch 73/200, Batch 41/45, Loss: 0.4069077670574188\nEpoch 73/200, Batch 42/45, Loss: 0.6466922163963318\nEpoch 73/200, Batch 43/45, Loss: 0.3029228150844574\nEpoch 73/200, Batch 44/45, Loss: 0.211798757314682\nEpoch 73/200, Batch 45/45, Loss: 0.21338972449302673\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  10.971362590789795 Best Val MSE:  6.859336465597153\nEpoch:  74 , Time Elapsed:  11.778274889787038  mins\nEpoch 74/200, Batch 1/45, Loss: 0.2790001928806305\nEpoch 74/200, Batch 2/45, Loss: 0.3418169617652893\nEpoch 74/200, Batch 3/45, Loss: 0.430163711309433\nEpoch 74/200, Batch 4/45, Loss: 0.6839235424995422\nEpoch 74/200, Batch 5/45, Loss: 0.29762503504753113\nEpoch 74/200, Batch 6/45, Loss: 0.20854249596595764\nEpoch 74/200, Batch 7/45, Loss: 0.24987433850765228\nEpoch 74/200, Batch 8/45, Loss: 0.3463267683982849\nEpoch 74/200, Batch 9/45, Loss: 0.2728527784347534\nEpoch 74/200, Batch 10/45, Loss: 0.24155718088150024\nEpoch 74/200, Batch 11/45, Loss: 0.5919167995452881\nEpoch 74/200, Batch 12/45, Loss: 0.24423402547836304\nEpoch 74/200, Batch 13/45, Loss: 0.1963113248348236\nEpoch 74/200, Batch 14/45, Loss: 0.17886218428611755\nEpoch 74/200, Batch 15/45, Loss: 0.20343373715877533\nEpoch 74/200, Batch 16/45, Loss: 0.26840940117836\nEpoch 74/200, Batch 17/45, Loss: 0.2283325344324112\nEpoch 74/200, Batch 18/45, Loss: 0.40595656633377075\nEpoch 74/200, Batch 19/45, Loss: 0.1519293487071991\nEpoch 74/200, Batch 20/45, Loss: 0.281923770904541\nEpoch 74/200, Batch 21/45, Loss: 0.2718433439731598\nEpoch 74/200, Batch 22/45, Loss: 0.5688420534133911\nEpoch 74/200, Batch 23/45, Loss: 0.2024364173412323\nEpoch 74/200, Batch 24/45, Loss: 0.3600350320339203\nEpoch 74/200, Batch 25/45, Loss: 0.3110537827014923\nEpoch 74/200, Batch 26/45, Loss: 0.1524227261543274\nEpoch 74/200, Batch 27/45, Loss: 0.2877998948097229\nEpoch 74/200, Batch 28/45, Loss: 0.27828001976013184\nEpoch 74/200, Batch 29/45, Loss: 0.19373130798339844\nEpoch 74/200, Batch 30/45, Loss: 0.3175479471683502\nEpoch 74/200, Batch 31/45, Loss: 0.17315737903118134\nEpoch 74/200, Batch 32/45, Loss: 0.188674658536911\nEpoch 74/200, Batch 33/45, Loss: 0.2135399580001831\nEpoch 74/200, Batch 34/45, Loss: 0.32395100593566895\nEpoch 74/200, Batch 35/45, Loss: 0.2211732417345047\nEpoch 74/200, Batch 36/45, Loss: 0.257582426071167\nEpoch 74/200, Batch 37/45, Loss: 0.37377142906188965\nEpoch 74/200, Batch 38/45, Loss: 0.16230913996696472\nEpoch 74/200, Batch 39/45, Loss: 0.1500508189201355\nEpoch 74/200, Batch 40/45, Loss: 0.24948599934577942\nEpoch 74/200, Batch 41/45, Loss: 0.1891661286354065\nEpoch 74/200, Batch 42/45, Loss: 0.3688991665840149\nEpoch 74/200, Batch 43/45, Loss: 0.3038794696331024\nEpoch 74/200, Batch 44/45, Loss: 0.25207287073135376\nEpoch 74/200, Batch 45/45, Loss: 0.21323317289352417\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  8.8711599111557 Best Val MSE:  6.859336465597153\nEpoch:  75 , Time Elapsed:  11.943048286437989  mins\nEpoch 75/200, Batch 1/45, Loss: 0.24735988676548004\nEpoch 75/200, Batch 2/45, Loss: 0.20407447218894958\nEpoch 75/200, Batch 3/45, Loss: 0.28695523738861084\nEpoch 75/200, Batch 4/45, Loss: 0.20610927045345306\nEpoch 75/200, Batch 5/45, Loss: 0.22091665863990784\nEpoch 75/200, Batch 6/45, Loss: 0.16753210127353668\nEpoch 75/200, Batch 7/45, Loss: 0.3312733769416809\nEpoch 75/200, Batch 8/45, Loss: 0.25264301896095276\nEpoch 75/200, Batch 9/45, Loss: 0.22814452648162842\nEpoch 75/200, Batch 10/45, Loss: 0.22238381206989288\nEpoch 75/200, Batch 11/45, Loss: 0.16591209173202515\nEpoch 75/200, Batch 12/45, Loss: 0.1598665565252304\nEpoch 75/200, Batch 13/45, Loss: 0.2394290268421173\nEpoch 75/200, Batch 14/45, Loss: 0.32492727041244507\nEpoch 75/200, Batch 15/45, Loss: 0.2584468722343445\nEpoch 75/200, Batch 16/45, Loss: 0.38439100980758667\nEpoch 75/200, Batch 17/45, Loss: 0.29168185591697693\nEpoch 75/200, Batch 18/45, Loss: 0.4293450713157654\nEpoch 75/200, Batch 19/45, Loss: 0.25840917229652405\nEpoch 75/200, Batch 20/45, Loss: 0.2275809943675995\nEpoch 75/200, Batch 21/45, Loss: 0.1871137022972107\nEpoch 75/200, Batch 22/45, Loss: 0.2595858573913574\nEpoch 75/200, Batch 23/45, Loss: 0.22381290793418884\nEpoch 75/200, Batch 24/45, Loss: 0.17936524748802185\nEpoch 75/200, Batch 25/45, Loss: 0.24715396761894226\nEpoch 75/200, Batch 26/45, Loss: 0.2360159158706665\nEpoch 75/200, Batch 27/45, Loss: 0.26034432649612427\nEpoch 75/200, Batch 28/45, Loss: 0.23979441821575165\nEpoch 75/200, Batch 29/45, Loss: 0.41394442319869995\nEpoch 75/200, Batch 30/45, Loss: 0.2215535044670105\nEpoch 75/200, Batch 31/45, Loss: 0.2326871156692505\nEpoch 75/200, Batch 32/45, Loss: 0.2910011410713196\nEpoch 75/200, Batch 33/45, Loss: 0.1681479811668396\nEpoch 75/200, Batch 34/45, Loss: 0.2368648648262024\nEpoch 75/200, Batch 35/45, Loss: 0.22067123651504517\nEpoch 75/200, Batch 36/45, Loss: 0.25880399346351624\nEpoch 75/200, Batch 37/45, Loss: 0.388351172208786\nEpoch 75/200, Batch 38/45, Loss: 0.22946423292160034\nEpoch 75/200, Batch 39/45, Loss: 0.1331321746110916\nEpoch 75/200, Batch 40/45, Loss: 0.152366042137146\nEpoch 75/200, Batch 41/45, Loss: 0.2392701506614685\nEpoch 75/200, Batch 42/45, Loss: 0.3256300091743469\nEpoch 75/200, Batch 43/45, Loss: 0.5872616767883301\nEpoch 75/200, Batch 44/45, Loss: 0.1576099395751953\nEpoch 75/200, Batch 45/45, Loss: 0.16980141401290894\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  8.446699321269989 Best Val MSE:  6.859336465597153\nEpoch:  76 , Time Elapsed:  12.100942011674245  mins\nEpoch 76/200, Batch 1/45, Loss: 0.27271202206611633\nEpoch 76/200, Batch 2/45, Loss: 0.10295271873474121\nEpoch 76/200, Batch 3/45, Loss: 0.5696378350257874\nEpoch 76/200, Batch 4/45, Loss: 0.24058252573013306\nEpoch 76/200, Batch 5/45, Loss: 0.21489718556404114\nEpoch 76/200, Batch 6/45, Loss: 0.27289021015167236\nEpoch 76/200, Batch 7/45, Loss: 0.20225179195404053\nEpoch 76/200, Batch 8/45, Loss: 0.2616863548755646\nEpoch 76/200, Batch 9/45, Loss: 0.16589966416358948\nEpoch 76/200, Batch 10/45, Loss: 0.23817124962806702\nEpoch 76/200, Batch 11/45, Loss: 0.3326577842235565\nEpoch 76/200, Batch 12/45, Loss: 0.2228895127773285\nEpoch 76/200, Batch 13/45, Loss: 0.23210518062114716\nEpoch 76/200, Batch 14/45, Loss: 0.262455552816391\nEpoch 76/200, Batch 15/45, Loss: 0.19631165266036987\nEpoch 76/200, Batch 16/45, Loss: 0.30592310428619385\nEpoch 76/200, Batch 17/45, Loss: 0.1275874823331833\nEpoch 76/200, Batch 18/45, Loss: 0.14621931314468384\nEpoch 76/200, Batch 19/45, Loss: 0.31977230310440063\nEpoch 76/200, Batch 20/45, Loss: 0.31424257159233093\nEpoch 76/200, Batch 21/45, Loss: 0.5596760511398315\nEpoch 76/200, Batch 22/45, Loss: 0.12812648713588715\nEpoch 76/200, Batch 23/45, Loss: 0.2641756236553192\nEpoch 76/200, Batch 24/45, Loss: 0.4508252441883087\nEpoch 76/200, Batch 25/45, Loss: 0.15688556432724\nEpoch 76/200, Batch 26/45, Loss: 0.23844316601753235\nEpoch 76/200, Batch 27/45, Loss: 0.24953044950962067\nEpoch 76/200, Batch 28/45, Loss: 0.1683288961648941\nEpoch 76/200, Batch 29/45, Loss: 0.41292035579681396\nEpoch 76/200, Batch 30/45, Loss: 0.28783994913101196\nEpoch 76/200, Batch 31/45, Loss: 0.26973283290863037\nEpoch 76/200, Batch 32/45, Loss: 0.16798749566078186\nEpoch 76/200, Batch 33/45, Loss: 0.22691303491592407\nEpoch 76/200, Batch 34/45, Loss: 0.2546088695526123\nEpoch 76/200, Batch 35/45, Loss: 0.27946868538856506\nEpoch 76/200, Batch 36/45, Loss: 0.42229145765304565\nEpoch 76/200, Batch 37/45, Loss: 0.2659856975078583\nEpoch 76/200, Batch 38/45, Loss: 0.41664254665374756\nEpoch 76/200, Batch 39/45, Loss: 0.3998466730117798\nEpoch 76/200, Batch 40/45, Loss: 0.24223805963993073\nEpoch 76/200, Batch 41/45, Loss: 0.15697336196899414\nEpoch 76/200, Batch 42/45, Loss: 0.256386935710907\nEpoch 76/200, Batch 43/45, Loss: 0.2654799818992615\nEpoch 76/200, Batch 44/45, Loss: 0.24603568017482758\nEpoch 76/200, Batch 45/45, Loss: 0.23465675115585327\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  8.297223508358002 Best Val MSE:  6.859336465597153\nEpoch:  77 , Time Elapsed:  12.261054718494416  mins\nEpoch 77/200, Batch 1/45, Loss: 0.30718159675598145\nEpoch 77/200, Batch 2/45, Loss: 0.28042930364608765\nEpoch 77/200, Batch 3/45, Loss: 0.2588013708591461\nEpoch 77/200, Batch 4/45, Loss: 0.30086952447891235\nEpoch 77/200, Batch 5/45, Loss: 0.21742618083953857\nEpoch 77/200, Batch 6/45, Loss: 0.20321477949619293\nEpoch 77/200, Batch 7/45, Loss: 0.133730947971344\nEpoch 77/200, Batch 8/45, Loss: 0.11496242135763168\nEpoch 77/200, Batch 9/45, Loss: 0.4285775423049927\nEpoch 77/200, Batch 10/45, Loss: 0.16861136257648468\nEpoch 77/200, Batch 11/45, Loss: 0.2259722799062729\nEpoch 77/200, Batch 12/45, Loss: 0.24221085011959076\nEpoch 77/200, Batch 13/45, Loss: 0.28956666588783264\nEpoch 77/200, Batch 14/45, Loss: 0.21433290839195251\nEpoch 77/200, Batch 15/45, Loss: 0.24059990048408508\nEpoch 77/200, Batch 16/45, Loss: 0.31137487292289734\nEpoch 77/200, Batch 17/45, Loss: 0.1591036468744278\nEpoch 77/200, Batch 18/45, Loss: 0.37726956605911255\nEpoch 77/200, Batch 19/45, Loss: 0.1985578089952469\nEpoch 77/200, Batch 20/45, Loss: 0.25987356901168823\nEpoch 77/200, Batch 21/45, Loss: 0.2187858521938324\nEpoch 77/200, Batch 22/45, Loss: 0.21776926517486572\nEpoch 77/200, Batch 23/45, Loss: 0.20195060968399048\nEpoch 77/200, Batch 24/45, Loss: 0.2407960742712021\nEpoch 77/200, Batch 25/45, Loss: 0.27677762508392334\nEpoch 77/200, Batch 26/45, Loss: 0.15047216415405273\nEpoch 77/200, Batch 27/45, Loss: 0.266671359539032\nEpoch 77/200, Batch 28/45, Loss: 0.29548370838165283\nEpoch 77/200, Batch 29/45, Loss: 0.2953455150127411\nEpoch 77/200, Batch 30/45, Loss: 0.21137715876102448\nEpoch 77/200, Batch 31/45, Loss: 0.2349081039428711\nEpoch 77/200, Batch 32/45, Loss: 0.27459901571273804\nEpoch 77/200, Batch 33/45, Loss: 0.31526458263397217\nEpoch 77/200, Batch 34/45, Loss: 0.20699800550937653\nEpoch 77/200, Batch 35/45, Loss: 0.24553723633289337\nEpoch 77/200, Batch 36/45, Loss: 0.19369560480117798\nEpoch 77/200, Batch 37/45, Loss: 0.21483658254146576\nEpoch 77/200, Batch 38/45, Loss: 0.21329626441001892\nEpoch 77/200, Batch 39/45, Loss: 0.2913910448551178\nEpoch 77/200, Batch 40/45, Loss: 0.3444248139858246\nEpoch 77/200, Batch 41/45, Loss: 0.18983736634254456\nEpoch 77/200, Batch 42/45, Loss: 0.4208220839500427\nEpoch 77/200, Batch 43/45, Loss: 0.1308235228061676\nEpoch 77/200, Batch 44/45, Loss: 0.2546684443950653\nEpoch 77/200, Batch 45/45, Loss: 0.22683778405189514\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  7.595277786254883 Best Val MSE:  6.859336465597153\nEpoch:  78 , Time Elapsed:  12.422615547974905  mins\nEpoch 78/200, Batch 1/45, Loss: 0.3063022494316101\nEpoch 78/200, Batch 2/45, Loss: 0.21927201747894287\nEpoch 78/200, Batch 3/45, Loss: 0.14521539211273193\nEpoch 78/200, Batch 4/45, Loss: 0.29422974586486816\nEpoch 78/200, Batch 5/45, Loss: 0.16905979812145233\nEpoch 78/200, Batch 6/45, Loss: 0.31014394760131836\nEpoch 78/200, Batch 7/45, Loss: 0.1440182626247406\nEpoch 78/200, Batch 8/45, Loss: 0.3571915626525879\nEpoch 78/200, Batch 9/45, Loss: 0.2624102830886841\nEpoch 78/200, Batch 10/45, Loss: 0.3114129304885864\nEpoch 78/200, Batch 11/45, Loss: 0.42159199714660645\nEpoch 78/200, Batch 12/45, Loss: 0.21857859194278717\nEpoch 78/200, Batch 13/45, Loss: 0.3650504946708679\nEpoch 78/200, Batch 14/45, Loss: 0.17953966557979584\nEpoch 78/200, Batch 15/45, Loss: 0.21504496037960052\nEpoch 78/200, Batch 16/45, Loss: 0.2767948806285858\nEpoch 78/200, Batch 17/45, Loss: 0.2708108425140381\nEpoch 78/200, Batch 18/45, Loss: 0.48319754004478455\nEpoch 78/200, Batch 19/45, Loss: 0.3701668381690979\nEpoch 78/200, Batch 20/45, Loss: 0.30378881096839905\nEpoch 78/200, Batch 21/45, Loss: 0.4040873646736145\nEpoch 78/200, Batch 22/45, Loss: 0.4833982288837433\nEpoch 78/200, Batch 23/45, Loss: 0.3915728032588959\nEpoch 78/200, Batch 24/45, Loss: 0.20837582647800446\nEpoch 78/200, Batch 25/45, Loss: 0.5834460258483887\nEpoch 78/200, Batch 26/45, Loss: 0.39643141627311707\nEpoch 78/200, Batch 27/45, Loss: 0.16362729668617249\nEpoch 78/200, Batch 28/45, Loss: 0.3357182741165161\nEpoch 78/200, Batch 29/45, Loss: 0.3072201609611511\nEpoch 78/200, Batch 30/45, Loss: 0.25013938546180725\nEpoch 78/200, Batch 31/45, Loss: 0.16405144333839417\nEpoch 78/200, Batch 32/45, Loss: 0.23890753090381622\nEpoch 78/200, Batch 33/45, Loss: 0.19138307869434357\nEpoch 78/200, Batch 34/45, Loss: 0.33550825715065\nEpoch 78/200, Batch 35/45, Loss: 0.252528578042984\nEpoch 78/200, Batch 36/45, Loss: 0.36644247174263\nEpoch 78/200, Batch 37/45, Loss: 0.3482186496257782\nEpoch 78/200, Batch 38/45, Loss: 0.24065683782100677\nEpoch 78/200, Batch 39/45, Loss: 0.34376394748687744\nEpoch 78/200, Batch 40/45, Loss: 0.2540042996406555\nEpoch 78/200, Batch 41/45, Loss: 0.3788515329360962\nEpoch 78/200, Batch 42/45, Loss: 0.17155557870864868\nEpoch 78/200, Batch 43/45, Loss: 0.1970328539609909\nEpoch 78/200, Batch 44/45, Loss: 0.3127763569355011\nEpoch 78/200, Batch 45/45, Loss: 0.2099342793226242\nValidating and Checkpointing!\nBest model Saved! Val MSE:  6.725237637758255\nEpoch:  79 , Time Elapsed:  12.600125726064046  mins\nEpoch 79/200, Batch 1/45, Loss: 0.1577167958021164\nEpoch 79/200, Batch 2/45, Loss: 0.25986453890800476\nEpoch 79/200, Batch 3/45, Loss: 0.19633695483207703\nEpoch 79/200, Batch 4/45, Loss: 0.2958168387413025\nEpoch 79/200, Batch 5/45, Loss: 0.1853332370519638\nEpoch 79/200, Batch 6/45, Loss: 0.3453906774520874\nEpoch 79/200, Batch 7/45, Loss: 0.2645968794822693\nEpoch 79/200, Batch 8/45, Loss: 0.20332804322242737\nEpoch 79/200, Batch 9/45, Loss: 0.3819873034954071\nEpoch 79/200, Batch 10/45, Loss: 0.21787074208259583\nEpoch 79/200, Batch 11/45, Loss: 0.17130278050899506\nEpoch 79/200, Batch 12/45, Loss: 1.2529524564743042\nEpoch 79/200, Batch 13/45, Loss: 0.15816688537597656\nEpoch 79/200, Batch 14/45, Loss: 0.30913686752319336\nEpoch 79/200, Batch 15/45, Loss: 0.23749478161334991\nEpoch 79/200, Batch 16/45, Loss: 0.14764142036437988\nEpoch 79/200, Batch 17/45, Loss: 0.3682878017425537\nEpoch 79/200, Batch 18/45, Loss: 0.3798879384994507\nEpoch 79/200, Batch 19/45, Loss: 0.2879696190357208\nEpoch 79/200, Batch 20/45, Loss: 0.267825722694397\nEpoch 79/200, Batch 21/45, Loss: 0.2407945692539215\nEpoch 79/200, Batch 22/45, Loss: 0.38502341508865356\nEpoch 79/200, Batch 23/45, Loss: 0.2616663873195648\nEpoch 79/200, Batch 24/45, Loss: 0.2050228863954544\nEpoch 79/200, Batch 25/45, Loss: 0.3481934070587158\nEpoch 79/200, Batch 26/45, Loss: 0.18871787190437317\nEpoch 79/200, Batch 27/45, Loss: 0.4918897747993469\nEpoch 79/200, Batch 28/45, Loss: 0.39698606729507446\nEpoch 79/200, Batch 29/45, Loss: 0.3345261812210083\nEpoch 79/200, Batch 30/45, Loss: 0.42097705602645874\nEpoch 79/200, Batch 31/45, Loss: 0.1753237247467041\nEpoch 79/200, Batch 32/45, Loss: 0.3125377893447876\nEpoch 79/200, Batch 33/45, Loss: 0.3219037353992462\nEpoch 79/200, Batch 34/45, Loss: 0.3132288455963135\nEpoch 79/200, Batch 35/45, Loss: 0.15778613090515137\nEpoch 79/200, Batch 36/45, Loss: 0.17755082249641418\nEpoch 79/200, Batch 37/45, Loss: 0.33993491530418396\nEpoch 79/200, Batch 38/45, Loss: 0.34511154890060425\nEpoch 79/200, Batch 39/45, Loss: 0.1359659731388092\nEpoch 79/200, Batch 40/45, Loss: 0.2122994363307953\nEpoch 79/200, Batch 41/45, Loss: 0.24263319373130798\nEpoch 79/200, Batch 42/45, Loss: 0.9966447949409485\nEpoch 79/200, Batch 43/45, Loss: 0.4200819134712219\nEpoch 79/200, Batch 44/45, Loss: 0.18389669060707092\nEpoch 79/200, Batch 45/45, Loss: 0.29249897599220276\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  8.22280365228653 Best Val MSE:  6.725237637758255\nEpoch:  80 , Time Elapsed:  12.761566090583802  mins\nEpoch 80/200, Batch 1/45, Loss: 0.28635674715042114\nEpoch 80/200, Batch 2/45, Loss: 0.17681080102920532\nEpoch 80/200, Batch 3/45, Loss: 0.17089612782001495\nEpoch 80/200, Batch 4/45, Loss: 0.1932958960533142\nEpoch 80/200, Batch 5/45, Loss: 0.19003017246723175\nEpoch 80/200, Batch 6/45, Loss: 0.2025216817855835\nEpoch 80/200, Batch 7/45, Loss: 0.38055509328842163\nEpoch 80/200, Batch 8/45, Loss: 0.49281319975852966\nEpoch 80/200, Batch 9/45, Loss: 0.2671205997467041\nEpoch 80/200, Batch 10/45, Loss: 0.2880259156227112\nEpoch 80/200, Batch 11/45, Loss: 0.4520810842514038\nEpoch 80/200, Batch 12/45, Loss: 0.23234868049621582\nEpoch 80/200, Batch 13/45, Loss: 0.2534060478210449\nEpoch 80/200, Batch 14/45, Loss: 0.15483641624450684\nEpoch 80/200, Batch 15/45, Loss: 0.19328096508979797\nEpoch 80/200, Batch 16/45, Loss: 0.3037130832672119\nEpoch 80/200, Batch 17/45, Loss: 0.21975961327552795\nEpoch 80/200, Batch 18/45, Loss: 0.25110024213790894\nEpoch 80/200, Batch 19/45, Loss: 0.21602791547775269\nEpoch 80/200, Batch 20/45, Loss: 0.44310489296913147\nEpoch 80/200, Batch 21/45, Loss: 0.12160694599151611\nEpoch 80/200, Batch 22/45, Loss: 0.20656029880046844\nEpoch 80/200, Batch 23/45, Loss: 0.26886630058288574\nEpoch 80/200, Batch 24/45, Loss: 0.2969198226928711\nEpoch 80/200, Batch 25/45, Loss: 0.27439409494400024\nEpoch 80/200, Batch 26/45, Loss: 0.2364775836467743\nEpoch 80/200, Batch 27/45, Loss: 0.29720842838287354\nEpoch 80/200, Batch 28/45, Loss: 0.37296414375305176\nEpoch 80/200, Batch 29/45, Loss: 0.31021422147750854\nEpoch 80/200, Batch 30/45, Loss: 0.6445987820625305\nEpoch 80/200, Batch 31/45, Loss: 0.24465996026992798\nEpoch 80/200, Batch 32/45, Loss: 0.28940290212631226\nEpoch 80/200, Batch 33/45, Loss: 0.2879418134689331\nEpoch 80/200, Batch 34/45, Loss: 0.26909807324409485\nEpoch 80/200, Batch 35/45, Loss: 0.1325187087059021\nEpoch 80/200, Batch 36/45, Loss: 0.1514483094215393\nEpoch 80/200, Batch 37/45, Loss: 0.1774384081363678\nEpoch 80/200, Batch 38/45, Loss: 0.17460669577121735\nEpoch 80/200, Batch 39/45, Loss: 0.711999773979187\nEpoch 80/200, Batch 40/45, Loss: 0.34932905435562134\nEpoch 80/200, Batch 41/45, Loss: 0.6638575792312622\nEpoch 80/200, Batch 42/45, Loss: 0.20980051159858704\nEpoch 80/200, Batch 43/45, Loss: 0.20866045355796814\nEpoch 80/200, Batch 44/45, Loss: 0.19525779783725739\nEpoch 80/200, Batch 45/45, Loss: 0.3461492955684662\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  11.496807873249054 Best Val MSE:  6.725237637758255\nEpoch:  81 , Time Elapsed:  12.922962029774984  mins\nEpoch 81/200, Batch 1/45, Loss: 0.2756541967391968\nEpoch 81/200, Batch 2/45, Loss: 0.38158273696899414\nEpoch 81/200, Batch 3/45, Loss: 0.136702761054039\nEpoch 81/200, Batch 4/45, Loss: 0.2118331789970398\nEpoch 81/200, Batch 5/45, Loss: 0.3344801068305969\nEpoch 81/200, Batch 6/45, Loss: 0.26033592224121094\nEpoch 81/200, Batch 7/45, Loss: 0.4026617407798767\nEpoch 81/200, Batch 8/45, Loss: 0.3899843692779541\nEpoch 81/200, Batch 9/45, Loss: 0.37932899594306946\nEpoch 81/200, Batch 10/45, Loss: 0.3533481955528259\nEpoch 81/200, Batch 11/45, Loss: 0.34177839756011963\nEpoch 81/200, Batch 12/45, Loss: 0.2121814787387848\nEpoch 81/200, Batch 13/45, Loss: 0.20552249252796173\nEpoch 81/200, Batch 14/45, Loss: 0.3172477185726166\nEpoch 81/200, Batch 15/45, Loss: 0.18427756428718567\nEpoch 81/200, Batch 16/45, Loss: 0.3123742938041687\nEpoch 81/200, Batch 17/45, Loss: 0.32956328988075256\nEpoch 81/200, Batch 18/45, Loss: 0.22357326745986938\nEpoch 81/200, Batch 19/45, Loss: 0.2735722064971924\nEpoch 81/200, Batch 20/45, Loss: 0.26989758014678955\nEpoch 81/200, Batch 21/45, Loss: 0.26206308603286743\nEpoch 81/200, Batch 22/45, Loss: 0.10508133471012115\nEpoch 81/200, Batch 23/45, Loss: 0.29842066764831543\nEpoch 81/200, Batch 24/45, Loss: 0.21959085762500763\nEpoch 81/200, Batch 25/45, Loss: 0.43052536249160767\nEpoch 81/200, Batch 26/45, Loss: 0.32640206813812256\nEpoch 81/200, Batch 27/45, Loss: 0.2580517828464508\nEpoch 81/200, Batch 28/45, Loss: 0.20938870310783386\nEpoch 81/200, Batch 29/45, Loss: 0.2498122751712799\nEpoch 81/200, Batch 30/45, Loss: 0.35655948519706726\nEpoch 81/200, Batch 31/45, Loss: 0.4355913996696472\nEpoch 81/200, Batch 32/45, Loss: 0.29872703552246094\nEpoch 81/200, Batch 33/45, Loss: 0.4698595404624939\nEpoch 81/200, Batch 34/45, Loss: 0.27031123638153076\nEpoch 81/200, Batch 35/45, Loss: 0.4888887405395508\nEpoch 81/200, Batch 36/45, Loss: 0.5811103582382202\nEpoch 81/200, Batch 37/45, Loss: 0.09444623440504074\nEpoch 81/200, Batch 38/45, Loss: 0.2223876714706421\nEpoch 81/200, Batch 39/45, Loss: 0.5155484676361084\nEpoch 81/200, Batch 40/45, Loss: 0.35475462675094604\nEpoch 81/200, Batch 41/45, Loss: 1.2681933641433716\nEpoch 81/200, Batch 42/45, Loss: 0.23344650864601135\nEpoch 81/200, Batch 43/45, Loss: 0.31700536608695984\nEpoch 81/200, Batch 44/45, Loss: 0.28561973571777344\nEpoch 81/200, Batch 45/45, Loss: 0.38025930523872375\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  7.975886166095734 Best Val MSE:  6.725237637758255\nEpoch:  82 , Time Elapsed:  13.088782691955567  mins\nEpoch 82/200, Batch 1/45, Loss: 0.23384352028369904\nEpoch 82/200, Batch 2/45, Loss: 0.21350231766700745\nEpoch 82/200, Batch 3/45, Loss: 0.16970841586589813\nEpoch 82/200, Batch 4/45, Loss: 0.128167524933815\nEpoch 82/200, Batch 5/45, Loss: 0.19464261829853058\nEpoch 82/200, Batch 6/45, Loss: 0.18944552540779114\nEpoch 82/200, Batch 7/45, Loss: 0.3656426668167114\nEpoch 82/200, Batch 8/45, Loss: 0.3490670919418335\nEpoch 82/200, Batch 9/45, Loss: 0.20609524846076965\nEpoch 82/200, Batch 10/45, Loss: 0.18817874789237976\nEpoch 82/200, Batch 11/45, Loss: 0.21229058504104614\nEpoch 82/200, Batch 12/45, Loss: 0.217329204082489\nEpoch 82/200, Batch 13/45, Loss: 0.24023544788360596\nEpoch 82/200, Batch 14/45, Loss: 0.279835969209671\nEpoch 82/200, Batch 15/45, Loss: 0.25614407658576965\nEpoch 82/200, Batch 16/45, Loss: 0.29128706455230713\nEpoch 82/200, Batch 17/45, Loss: 0.2751809358596802\nEpoch 82/200, Batch 18/45, Loss: 0.22227105498313904\nEpoch 82/200, Batch 19/45, Loss: 0.41715702414512634\nEpoch 82/200, Batch 20/45, Loss: 0.23993746936321259\nEpoch 82/200, Batch 21/45, Loss: 0.21257366240024567\nEpoch 82/200, Batch 22/45, Loss: 0.3197409510612488\nEpoch 82/200, Batch 23/45, Loss: 0.33444511890411377\nEpoch 82/200, Batch 24/45, Loss: 0.23715835809707642\nEpoch 82/200, Batch 25/45, Loss: 0.21114103496074677\nEpoch 82/200, Batch 26/45, Loss: 0.25986599922180176\nEpoch 82/200, Batch 27/45, Loss: 0.49638915061950684\nEpoch 82/200, Batch 28/45, Loss: 0.21131211519241333\nEpoch 82/200, Batch 29/45, Loss: 0.3522898554801941\nEpoch 82/200, Batch 30/45, Loss: 0.2777567207813263\nEpoch 82/200, Batch 31/45, Loss: 0.31138697266578674\nEpoch 82/200, Batch 32/45, Loss: 0.14804598689079285\nEpoch 82/200, Batch 33/45, Loss: 0.3963954746723175\nEpoch 82/200, Batch 34/45, Loss: 0.5702660083770752\nEpoch 82/200, Batch 35/45, Loss: 0.26278966665267944\nEpoch 82/200, Batch 36/45, Loss: 0.2225348949432373\nEpoch 82/200, Batch 37/45, Loss: 0.2741737961769104\nEpoch 82/200, Batch 38/45, Loss: 0.24351182579994202\nEpoch 82/200, Batch 39/45, Loss: 0.4945302903652191\nEpoch 82/200, Batch 40/45, Loss: 0.307964950799942\nEpoch 82/200, Batch 41/45, Loss: 0.22102141380310059\nEpoch 82/200, Batch 42/45, Loss: 0.36838051676750183\nEpoch 82/200, Batch 43/45, Loss: 0.12827301025390625\nEpoch 82/200, Batch 44/45, Loss: 0.24726635217666626\nEpoch 82/200, Batch 45/45, Loss: 0.38693395256996155\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  8.954230070114136 Best Val MSE:  6.725237637758255\nEpoch:  83 , Time Elapsed:  13.24636507431666  mins\nEpoch 83/200, Batch 1/45, Loss: 0.2111850380897522\nEpoch 83/200, Batch 2/45, Loss: 0.32242351770401\nEpoch 83/200, Batch 3/45, Loss: 0.4471582770347595\nEpoch 83/200, Batch 4/45, Loss: 0.35983315110206604\nEpoch 83/200, Batch 5/45, Loss: 0.22777605056762695\nEpoch 83/200, Batch 6/45, Loss: 0.0879555344581604\nEpoch 83/200, Batch 7/45, Loss: 0.3735010623931885\nEpoch 83/200, Batch 8/45, Loss: 0.2965813875198364\nEpoch 83/200, Batch 9/45, Loss: 0.25452423095703125\nEpoch 83/200, Batch 10/45, Loss: 0.40122130513191223\nEpoch 83/200, Batch 11/45, Loss: 0.230473130941391\nEpoch 83/200, Batch 12/45, Loss: 0.3216283321380615\nEpoch 83/200, Batch 13/45, Loss: 0.14339838922023773\nEpoch 83/200, Batch 14/45, Loss: 0.4178677201271057\nEpoch 83/200, Batch 15/45, Loss: 0.2131500095129013\nEpoch 83/200, Batch 16/45, Loss: 0.25388213992118835\nEpoch 83/200, Batch 17/45, Loss: 0.1433517336845398\nEpoch 83/200, Batch 18/45, Loss: 0.20795376598834991\nEpoch 83/200, Batch 19/45, Loss: 0.16429480910301208\nEpoch 83/200, Batch 20/45, Loss: 0.18958914279937744\nEpoch 83/200, Batch 21/45, Loss: 0.39932727813720703\nEpoch 83/200, Batch 22/45, Loss: 0.8607478141784668\nEpoch 83/200, Batch 23/45, Loss: 0.13482265174388885\nEpoch 83/200, Batch 24/45, Loss: 0.18094894289970398\nEpoch 83/200, Batch 25/45, Loss: 0.24033400416374207\nEpoch 83/200, Batch 26/45, Loss: 0.288097083568573\nEpoch 83/200, Batch 27/45, Loss: 0.3148368000984192\nEpoch 83/200, Batch 28/45, Loss: 0.23349443078041077\nEpoch 83/200, Batch 29/45, Loss: 0.3103333115577698\nEpoch 83/200, Batch 30/45, Loss: 0.22045716643333435\nEpoch 83/200, Batch 31/45, Loss: 0.3115874230861664\nEpoch 83/200, Batch 32/45, Loss: 0.39636391401290894\nEpoch 83/200, Batch 33/45, Loss: 0.26168856024742126\nEpoch 83/200, Batch 34/45, Loss: 0.28880709409713745\nEpoch 83/200, Batch 35/45, Loss: 0.23702335357666016\nEpoch 83/200, Batch 36/45, Loss: 0.2918693423271179\nEpoch 83/200, Batch 37/45, Loss: 0.6561812162399292\nEpoch 83/200, Batch 38/45, Loss: 0.22926883399486542\nEpoch 83/200, Batch 39/45, Loss: 0.2263774573802948\nEpoch 83/200, Batch 40/45, Loss: 0.2812800407409668\nEpoch 83/200, Batch 41/45, Loss: 0.18942221999168396\nEpoch 83/200, Batch 42/45, Loss: 0.25837260484695435\nEpoch 83/200, Batch 43/45, Loss: 0.27693116664886475\nEpoch 83/200, Batch 44/45, Loss: 0.2601611018180847\nEpoch 83/200, Batch 45/45, Loss: 0.25335434079170227\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  7.229165971279144 Best Val MSE:  6.725237637758255\nEpoch:  84 , Time Elapsed:  13.403404835859934  mins\nEpoch 84/200, Batch 1/45, Loss: 0.2775973379611969\nEpoch 84/200, Batch 2/45, Loss: 0.16567766666412354\nEpoch 84/200, Batch 3/45, Loss: 0.27086785435676575\nEpoch 84/200, Batch 4/45, Loss: 0.20163342356681824\nEpoch 84/200, Batch 5/45, Loss: 0.17158478498458862\nEpoch 84/200, Batch 6/45, Loss: 0.2313331514596939\nEpoch 84/200, Batch 7/45, Loss: 0.1149844229221344\nEpoch 84/200, Batch 8/45, Loss: 0.25443896651268005\nEpoch 84/200, Batch 9/45, Loss: 0.30901938676834106\nEpoch 84/200, Batch 10/45, Loss: 0.2757474184036255\nEpoch 84/200, Batch 11/45, Loss: 0.2870151400566101\nEpoch 84/200, Batch 12/45, Loss: 0.3351215124130249\nEpoch 84/200, Batch 13/45, Loss: 0.19245828688144684\nEpoch 84/200, Batch 14/45, Loss: 0.3691299259662628\nEpoch 84/200, Batch 15/45, Loss: 0.20767664909362793\nEpoch 84/200, Batch 16/45, Loss: 0.19617176055908203\nEpoch 84/200, Batch 17/45, Loss: 0.2548918128013611\nEpoch 84/200, Batch 18/45, Loss: 0.19208182394504547\nEpoch 84/200, Batch 19/45, Loss: 0.3162916898727417\nEpoch 84/200, Batch 20/45, Loss: 0.33177027106285095\nEpoch 84/200, Batch 21/45, Loss: 0.11085554957389832\nEpoch 84/200, Batch 22/45, Loss: 0.1341644823551178\nEpoch 84/200, Batch 23/45, Loss: 0.24252404272556305\nEpoch 84/200, Batch 24/45, Loss: 0.2641142010688782\nEpoch 84/200, Batch 25/45, Loss: 0.17326349020004272\nEpoch 84/200, Batch 26/45, Loss: 0.16559091210365295\nEpoch 84/200, Batch 27/45, Loss: 0.2934480309486389\nEpoch 84/200, Batch 28/45, Loss: 0.4172970652580261\nEpoch 84/200, Batch 29/45, Loss: 0.6386294364929199\nEpoch 84/200, Batch 30/45, Loss: 0.4242154359817505\nEpoch 84/200, Batch 31/45, Loss: 0.3303852677345276\nEpoch 84/200, Batch 32/45, Loss: 0.30342769622802734\nEpoch 84/200, Batch 33/45, Loss: 0.3516104221343994\nEpoch 84/200, Batch 34/45, Loss: 0.3743477463722229\nEpoch 84/200, Batch 35/45, Loss: 0.5920603275299072\nEpoch 84/200, Batch 36/45, Loss: 0.310769259929657\nEpoch 84/200, Batch 37/45, Loss: 0.27354535460472107\nEpoch 84/200, Batch 38/45, Loss: 0.47873494029045105\nEpoch 84/200, Batch 39/45, Loss: 0.2460857480764389\nEpoch 84/200, Batch 40/45, Loss: 0.212507963180542\nEpoch 84/200, Batch 41/45, Loss: 0.2958438992500305\nEpoch 84/200, Batch 42/45, Loss: 0.2741348147392273\nEpoch 84/200, Batch 43/45, Loss: 0.31197628378868103\nEpoch 84/200, Batch 44/45, Loss: 0.38370370864868164\nEpoch 84/200, Batch 45/45, Loss: 0.19481457769870758\nValidating and Checkpointing!\nBest model Saved! Val MSE:  5.2462038397789\nEpoch:  85 , Time Elapsed:  13.575618314743043  mins\nEpoch 85/200, Batch 1/45, Loss: 0.28095656633377075\nEpoch 85/200, Batch 2/45, Loss: 0.25083377957344055\nEpoch 85/200, Batch 3/45, Loss: 0.37101855874061584\nEpoch 85/200, Batch 4/45, Loss: 0.21040093898773193\nEpoch 85/200, Batch 5/45, Loss: 0.17072945833206177\nEpoch 85/200, Batch 6/45, Loss: 0.3335847556591034\nEpoch 85/200, Batch 7/45, Loss: 0.337973028421402\nEpoch 85/200, Batch 8/45, Loss: 0.2621685862541199\nEpoch 85/200, Batch 9/45, Loss: 0.3572266399860382\nEpoch 85/200, Batch 10/45, Loss: 0.4046727418899536\nEpoch 85/200, Batch 11/45, Loss: 0.29352837800979614\nEpoch 85/200, Batch 12/45, Loss: 0.3759908676147461\nEpoch 85/200, Batch 13/45, Loss: 0.1676192730665207\nEpoch 85/200, Batch 14/45, Loss: 0.3217160105705261\nEpoch 85/200, Batch 15/45, Loss: 0.1800684779882431\nEpoch 85/200, Batch 16/45, Loss: 0.6809647083282471\nEpoch 85/200, Batch 17/45, Loss: 0.24023021757602692\nEpoch 85/200, Batch 18/45, Loss: 0.25052228569984436\nEpoch 85/200, Batch 19/45, Loss: 0.1487564742565155\nEpoch 85/200, Batch 20/45, Loss: 0.3039226233959198\nEpoch 85/200, Batch 21/45, Loss: 0.21561850607395172\nEpoch 85/200, Batch 22/45, Loss: 0.2866772711277008\nEpoch 85/200, Batch 23/45, Loss: 0.3328738212585449\nEpoch 85/200, Batch 24/45, Loss: 0.5147415995597839\nEpoch 85/200, Batch 25/45, Loss: 0.39228564500808716\nEpoch 85/200, Batch 26/45, Loss: 0.3437930941581726\nEpoch 85/200, Batch 27/45, Loss: 0.3734387755393982\nEpoch 85/200, Batch 28/45, Loss: 0.2725982666015625\nEpoch 85/200, Batch 29/45, Loss: 0.17677538096904755\nEpoch 85/200, Batch 30/45, Loss: 0.2401590794324875\nEpoch 85/200, Batch 31/45, Loss: 0.29166290163993835\nEpoch 85/200, Batch 32/45, Loss: 0.18449239432811737\nEpoch 85/200, Batch 33/45, Loss: 0.282015860080719\nEpoch 85/200, Batch 34/45, Loss: 0.16703897714614868\nEpoch 85/200, Batch 35/45, Loss: 0.47382909059524536\nEpoch 85/200, Batch 36/45, Loss: 0.3800037205219269\nEpoch 85/200, Batch 37/45, Loss: 0.3904786705970764\nEpoch 85/200, Batch 38/45, Loss: 0.29877930879592896\nEpoch 85/200, Batch 39/45, Loss: 0.09088246524333954\nEpoch 85/200, Batch 40/45, Loss: 0.35179397463798523\nEpoch 85/200, Batch 41/45, Loss: 0.20593857765197754\nEpoch 85/200, Batch 42/45, Loss: 0.2368249148130417\nEpoch 85/200, Batch 43/45, Loss: 0.105701744556427\nEpoch 85/200, Batch 44/45, Loss: 0.4341241121292114\nEpoch 85/200, Batch 45/45, Loss: 0.210071861743927\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  7.878176122903824 Best Val MSE:  5.2462038397789\nEpoch:  86 , Time Elapsed:  13.731658836205801  mins\nEpoch 86/200, Batch 1/45, Loss: 0.20600059628486633\nEpoch 86/200, Batch 2/45, Loss: 0.35032105445861816\nEpoch 86/200, Batch 3/45, Loss: 0.3584529161453247\nEpoch 86/200, Batch 4/45, Loss: 0.15544261038303375\nEpoch 86/200, Batch 5/45, Loss: 0.3606710433959961\nEpoch 86/200, Batch 6/45, Loss: 0.2218983769416809\nEpoch 86/200, Batch 7/45, Loss: 0.24646958708763123\nEpoch 86/200, Batch 8/45, Loss: 0.24625760316848755\nEpoch 86/200, Batch 9/45, Loss: 0.25677454471588135\nEpoch 86/200, Batch 10/45, Loss: 0.23934653401374817\nEpoch 86/200, Batch 11/45, Loss: 0.13106009364128113\nEpoch 86/200, Batch 12/45, Loss: 0.24245689809322357\nEpoch 86/200, Batch 13/45, Loss: 0.4801102876663208\nEpoch 86/200, Batch 14/45, Loss: 0.3220396935939789\nEpoch 86/200, Batch 15/45, Loss: 0.24367490410804749\nEpoch 86/200, Batch 16/45, Loss: 0.2529395818710327\nEpoch 86/200, Batch 17/45, Loss: 0.5304654836654663\nEpoch 86/200, Batch 18/45, Loss: 0.34610795974731445\nEpoch 86/200, Batch 19/45, Loss: 0.2271997630596161\nEpoch 86/200, Batch 20/45, Loss: 0.24228926002979279\nEpoch 86/200, Batch 21/45, Loss: 0.30811455845832825\nEpoch 86/200, Batch 22/45, Loss: 0.46526098251342773\nEpoch 86/200, Batch 23/45, Loss: 0.20681625604629517\nEpoch 86/200, Batch 24/45, Loss: 0.29398593306541443\nEpoch 86/200, Batch 25/45, Loss: 0.3137478828430176\nEpoch 86/200, Batch 26/45, Loss: 0.24214573204517365\nEpoch 86/200, Batch 27/45, Loss: 0.293422132730484\nEpoch 86/200, Batch 28/45, Loss: 0.26951420307159424\nEpoch 86/200, Batch 29/45, Loss: 0.25035953521728516\nEpoch 86/200, Batch 30/45, Loss: 0.10488548874855042\nEpoch 86/200, Batch 31/45, Loss: 0.20682218670845032\nEpoch 86/200, Batch 32/45, Loss: 0.22191260755062103\nEpoch 86/200, Batch 33/45, Loss: 0.4702921509742737\nEpoch 86/200, Batch 34/45, Loss: 0.24661359190940857\nEpoch 86/200, Batch 35/45, Loss: 0.28838637471199036\nEpoch 86/200, Batch 36/45, Loss: 0.3864487111568451\nEpoch 86/200, Batch 37/45, Loss: 0.21954935789108276\nEpoch 86/200, Batch 38/45, Loss: 0.20571377873420715\nEpoch 86/200, Batch 39/45, Loss: 0.1779085397720337\nEpoch 86/200, Batch 40/45, Loss: 0.267352819442749\nEpoch 86/200, Batch 41/45, Loss: 0.28476712107658386\nEpoch 86/200, Batch 42/45, Loss: 0.20065507292747498\nEpoch 86/200, Batch 43/45, Loss: 0.19245225191116333\nEpoch 86/200, Batch 44/45, Loss: 0.15234798192977905\nEpoch 86/200, Batch 45/45, Loss: 0.13613246381282806\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  6.945106685161591 Best Val MSE:  5.2462038397789\nEpoch:  87 , Time Elapsed:  13.889544475078583  mins\nEpoch 87/200, Batch 1/45, Loss: 0.12463642656803131\nEpoch 87/200, Batch 2/45, Loss: 0.17984850704669952\nEpoch 87/200, Batch 3/45, Loss: 0.3415786027908325\nEpoch 87/200, Batch 4/45, Loss: 0.26986247301101685\nEpoch 87/200, Batch 5/45, Loss: 0.22255092859268188\nEpoch 87/200, Batch 6/45, Loss: 0.1262160837650299\nEpoch 87/200, Batch 7/45, Loss: 0.26460617780685425\nEpoch 87/200, Batch 8/45, Loss: 0.2360040247440338\nEpoch 87/200, Batch 9/45, Loss: 0.3566020727157593\nEpoch 87/200, Batch 10/45, Loss: 0.24100549519062042\nEpoch 87/200, Batch 11/45, Loss: 0.1813925951719284\nEpoch 87/200, Batch 12/45, Loss: 0.29073551297187805\nEpoch 87/200, Batch 13/45, Loss: 0.16038227081298828\nEpoch 87/200, Batch 14/45, Loss: 0.28157705068588257\nEpoch 87/200, Batch 15/45, Loss: 0.16265778243541718\nEpoch 87/200, Batch 16/45, Loss: 0.24435833096504211\nEpoch 87/200, Batch 17/45, Loss: 0.38846689462661743\nEpoch 87/200, Batch 18/45, Loss: 0.19425421953201294\nEpoch 87/200, Batch 19/45, Loss: 0.24956679344177246\nEpoch 87/200, Batch 20/45, Loss: 0.19617033004760742\nEpoch 87/200, Batch 21/45, Loss: 0.24161174893379211\nEpoch 87/200, Batch 22/45, Loss: 0.1541718691587448\nEpoch 87/200, Batch 23/45, Loss: 0.3474735617637634\nEpoch 87/200, Batch 24/45, Loss: 0.265675812959671\nEpoch 87/200, Batch 25/45, Loss: 0.47391968965530396\nEpoch 87/200, Batch 26/45, Loss: 0.4344637393951416\nEpoch 87/200, Batch 27/45, Loss: 0.15123631060123444\nEpoch 87/200, Batch 28/45, Loss: 0.296894907951355\nEpoch 87/200, Batch 29/45, Loss: 0.2957044541835785\nEpoch 87/200, Batch 30/45, Loss: 0.28486260771751404\nEpoch 87/200, Batch 31/45, Loss: 0.24892300367355347\nEpoch 87/200, Batch 32/45, Loss: 0.457420289516449\nEpoch 87/200, Batch 33/45, Loss: 0.2688644528388977\nEpoch 87/200, Batch 34/45, Loss: 0.22086113691329956\nEpoch 87/200, Batch 35/45, Loss: 0.36726582050323486\nEpoch 87/200, Batch 36/45, Loss: 0.39835721254348755\nEpoch 87/200, Batch 37/45, Loss: 0.33522868156433105\nEpoch 87/200, Batch 38/45, Loss: 0.20238924026489258\nEpoch 87/200, Batch 39/45, Loss: 0.30702465772628784\nEpoch 87/200, Batch 40/45, Loss: 1.6686761379241943\nEpoch 87/200, Batch 41/45, Loss: 0.21922622621059418\nEpoch 87/200, Batch 42/45, Loss: 0.5778559446334839\nEpoch 87/200, Batch 43/45, Loss: 0.46565574407577515\nEpoch 87/200, Batch 44/45, Loss: 0.551230788230896\nEpoch 87/200, Batch 45/45, Loss: 0.7410233020782471\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  16.229220032691956 Best Val MSE:  5.2462038397789\nEpoch:  88 , Time Elapsed:  14.045782911777497  mins\nEpoch 88/200, Batch 1/45, Loss: 0.3417791426181793\nEpoch 88/200, Batch 2/45, Loss: 0.44681689143180847\nEpoch 88/200, Batch 3/45, Loss: 0.4366414546966553\nEpoch 88/200, Batch 4/45, Loss: 0.33679619431495667\nEpoch 88/200, Batch 5/45, Loss: 0.3737340569496155\nEpoch 88/200, Batch 6/45, Loss: 0.5918283462524414\nEpoch 88/200, Batch 7/45, Loss: 0.7265549898147583\nEpoch 88/200, Batch 8/45, Loss: 0.4555315673351288\nEpoch 88/200, Batch 9/45, Loss: 0.35616201162338257\nEpoch 88/200, Batch 10/45, Loss: 0.41752585768699646\nEpoch 88/200, Batch 11/45, Loss: 0.5097682476043701\nEpoch 88/200, Batch 12/45, Loss: 3.4867048263549805\nEpoch 88/200, Batch 13/45, Loss: 0.38136106729507446\nEpoch 88/200, Batch 14/45, Loss: 0.6131325960159302\nEpoch 88/200, Batch 15/45, Loss: 1.0697431564331055\nEpoch 88/200, Batch 16/45, Loss: 1.4416792392730713\nEpoch 88/200, Batch 17/45, Loss: 1.102725863456726\nEpoch 88/200, Batch 18/45, Loss: 1.322801947593689\nEpoch 88/200, Batch 19/45, Loss: 0.8921617269515991\nEpoch 88/200, Batch 20/45, Loss: 0.6074507832527161\nEpoch 88/200, Batch 21/45, Loss: 0.8199535608291626\nEpoch 88/200, Batch 22/45, Loss: 1.6563396453857422\nEpoch 88/200, Batch 23/45, Loss: 0.6535910367965698\nEpoch 88/200, Batch 24/45, Loss: 0.5616123676300049\nEpoch 88/200, Batch 25/45, Loss: 0.4650968015193939\nEpoch 88/200, Batch 26/45, Loss: 0.44948920607566833\nEpoch 88/200, Batch 27/45, Loss: 0.372718870639801\nEpoch 88/200, Batch 28/45, Loss: 2.7240817546844482\nEpoch 88/200, Batch 29/45, Loss: 0.3852676749229431\nEpoch 88/200, Batch 30/45, Loss: 0.6947202086448669\nEpoch 88/200, Batch 31/45, Loss: 0.4960157573223114\nEpoch 88/200, Batch 32/45, Loss: 0.3000827431678772\nEpoch 88/200, Batch 33/45, Loss: 0.6103394031524658\nEpoch 88/200, Batch 34/45, Loss: 0.5241761803627014\nEpoch 88/200, Batch 35/45, Loss: 0.6710103750228882\nEpoch 88/200, Batch 36/45, Loss: 0.5361862182617188\nEpoch 88/200, Batch 37/45, Loss: 0.39842572808265686\nEpoch 88/200, Batch 38/45, Loss: 0.3937045931816101\nEpoch 88/200, Batch 39/45, Loss: 0.2001955360174179\nEpoch 88/200, Batch 40/45, Loss: 0.6072118878364563\nEpoch 88/200, Batch 41/45, Loss: 0.3277398347854614\nEpoch 88/200, Batch 42/45, Loss: 0.276388943195343\nEpoch 88/200, Batch 43/45, Loss: 0.5200610160827637\nEpoch 88/200, Batch 44/45, Loss: 0.33995354175567627\nEpoch 88/200, Batch 45/45, Loss: 0.47124284505844116\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  9.398935556411743 Best Val MSE:  5.2462038397789\nEpoch:  89 , Time Elapsed:  14.209044329325359  mins\nEpoch 89/200, Batch 1/45, Loss: 0.38754329085350037\nEpoch 89/200, Batch 2/45, Loss: 0.34661567211151123\nEpoch 89/200, Batch 3/45, Loss: 0.26202014088630676\nEpoch 89/200, Batch 4/45, Loss: 0.44235873222351074\nEpoch 89/200, Batch 5/45, Loss: 0.38836681842803955\nEpoch 89/200, Batch 6/45, Loss: 0.4112699627876282\nEpoch 89/200, Batch 7/45, Loss: 0.45239126682281494\nEpoch 89/200, Batch 8/45, Loss: 0.2544766068458557\nEpoch 89/200, Batch 9/45, Loss: 0.23630715906620026\nEpoch 89/200, Batch 10/45, Loss: 0.5832918882369995\nEpoch 89/200, Batch 11/45, Loss: 0.17640851438045502\nEpoch 89/200, Batch 12/45, Loss: 0.20089969038963318\nEpoch 89/200, Batch 13/45, Loss: 0.34683024883270264\nEpoch 89/200, Batch 14/45, Loss: 0.4289519786834717\nEpoch 89/200, Batch 15/45, Loss: 0.7410068511962891\nEpoch 89/200, Batch 16/45, Loss: 0.2513103783130646\nEpoch 89/200, Batch 17/45, Loss: 0.181433767080307\nEpoch 89/200, Batch 18/45, Loss: 0.68815016746521\nEpoch 89/200, Batch 19/45, Loss: 0.4624474048614502\nEpoch 89/200, Batch 20/45, Loss: 0.37345990538597107\nEpoch 89/200, Batch 21/45, Loss: 0.4083174169063568\nEpoch 89/200, Batch 22/45, Loss: 0.670006275177002\nEpoch 89/200, Batch 23/45, Loss: 0.540640652179718\nEpoch 89/200, Batch 24/45, Loss: 0.33016377687454224\nEpoch 89/200, Batch 25/45, Loss: 0.3763403296470642\nEpoch 89/200, Batch 26/45, Loss: 0.3426477909088135\nEpoch 89/200, Batch 27/45, Loss: 0.35736995935440063\nEpoch 89/200, Batch 28/45, Loss: 0.354383647441864\nEpoch 89/200, Batch 29/45, Loss: 0.25647982954978943\nEpoch 89/200, Batch 30/45, Loss: 0.3346313536167145\nEpoch 89/200, Batch 31/45, Loss: 0.54679274559021\nEpoch 89/200, Batch 32/45, Loss: 0.4082971215248108\nEpoch 89/200, Batch 33/45, Loss: 1.0197480916976929\nEpoch 89/200, Batch 34/45, Loss: 0.520677924156189\nEpoch 89/200, Batch 35/45, Loss: 0.3123304843902588\nEpoch 89/200, Batch 36/45, Loss: 0.2863820791244507\nEpoch 89/200, Batch 37/45, Loss: 0.4566553235054016\nEpoch 89/200, Batch 38/45, Loss: 1.209102988243103\nEpoch 89/200, Batch 39/45, Loss: 0.2720096707344055\nEpoch 89/200, Batch 40/45, Loss: 0.2509523928165436\nEpoch 89/200, Batch 41/45, Loss: 0.313287615776062\nEpoch 89/200, Batch 42/45, Loss: 0.7751309871673584\nEpoch 89/200, Batch 43/45, Loss: 0.5632438659667969\nEpoch 89/200, Batch 44/45, Loss: 0.6431145668029785\nEpoch 89/200, Batch 45/45, Loss: 0.9229195713996887\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  18.54972416162491 Best Val MSE:  5.2462038397789\nEpoch:  90 , Time Elapsed:  14.367218617598216  mins\nEpoch 90/200, Batch 1/45, Loss: 0.9128466844558716\nEpoch 90/200, Batch 2/45, Loss: 1.0100957155227661\nEpoch 90/200, Batch 3/45, Loss: 0.48210880160331726\nEpoch 90/200, Batch 4/45, Loss: 1.3475803136825562\nEpoch 90/200, Batch 5/45, Loss: 0.6558101177215576\nEpoch 90/200, Batch 6/45, Loss: 0.2684747874736786\nEpoch 90/200, Batch 7/45, Loss: 0.20101378858089447\nEpoch 90/200, Batch 8/45, Loss: 0.3359041213989258\nEpoch 90/200, Batch 9/45, Loss: 0.48347342014312744\nEpoch 90/200, Batch 10/45, Loss: 1.2876274585723877\nEpoch 90/200, Batch 11/45, Loss: 0.5379214286804199\nEpoch 90/200, Batch 12/45, Loss: 0.2345229834318161\nEpoch 90/200, Batch 13/45, Loss: 0.3387010097503662\nEpoch 90/200, Batch 14/45, Loss: 0.4494975209236145\nEpoch 90/200, Batch 15/45, Loss: 0.4529050588607788\nEpoch 90/200, Batch 16/45, Loss: 0.3920312225818634\nEpoch 90/200, Batch 17/45, Loss: 0.7269620895385742\nEpoch 90/200, Batch 18/45, Loss: 0.562323808670044\nEpoch 90/200, Batch 19/45, Loss: 0.36390870809555054\nEpoch 90/200, Batch 20/45, Loss: 0.4620767831802368\nEpoch 90/200, Batch 21/45, Loss: 0.5232297778129578\nEpoch 90/200, Batch 22/45, Loss: 0.34318846464157104\nEpoch 90/200, Batch 23/45, Loss: 0.3369467258453369\nEpoch 90/200, Batch 24/45, Loss: 0.2838120758533478\nEpoch 90/200, Batch 25/45, Loss: 0.5158200263977051\nEpoch 90/200, Batch 26/45, Loss: 0.22781294584274292\nEpoch 90/200, Batch 27/45, Loss: 0.7950888872146606\nEpoch 90/200, Batch 28/45, Loss: 0.5678153038024902\nEpoch 90/200, Batch 29/45, Loss: 0.326546311378479\nEpoch 90/200, Batch 30/45, Loss: 0.2266908884048462\nEpoch 90/200, Batch 31/45, Loss: 0.23806533217430115\nEpoch 90/200, Batch 32/45, Loss: 0.2912910580635071\nEpoch 90/200, Batch 33/45, Loss: 0.3796607553958893\nEpoch 90/200, Batch 34/45, Loss: 0.15799608826637268\nEpoch 90/200, Batch 35/45, Loss: 0.21729755401611328\nEpoch 90/200, Batch 36/45, Loss: 0.3117981553077698\nEpoch 90/200, Batch 37/45, Loss: 0.21002760529518127\nEpoch 90/200, Batch 38/45, Loss: 0.13688214123249054\nEpoch 90/200, Batch 39/45, Loss: 0.34085068106651306\nEpoch 90/200, Batch 40/45, Loss: 0.35309362411499023\nEpoch 90/200, Batch 41/45, Loss: 0.15027673542499542\nEpoch 90/200, Batch 42/45, Loss: 0.33229032158851624\nEpoch 90/200, Batch 43/45, Loss: 0.37707188725471497\nEpoch 90/200, Batch 44/45, Loss: 0.18889600038528442\nEpoch 90/200, Batch 45/45, Loss: 0.22506488859653473\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  6.4800529181957245 Best Val MSE:  5.2462038397789\nEpoch:  91 , Time Elapsed:  14.527536833286286  mins\nEpoch 91/200, Batch 1/45, Loss: 0.5108980536460876\nEpoch 91/200, Batch 2/45, Loss: 0.15982908010482788\nEpoch 91/200, Batch 3/45, Loss: 0.23834854364395142\nEpoch 91/200, Batch 4/45, Loss: 0.1800331473350525\nEpoch 91/200, Batch 5/45, Loss: 0.23806214332580566\nEpoch 91/200, Batch 6/45, Loss: 0.2902138829231262\nEpoch 91/200, Batch 7/45, Loss: 0.3436279892921448\nEpoch 91/200, Batch 8/45, Loss: 0.4232693314552307\nEpoch 91/200, Batch 9/45, Loss: 0.5630230903625488\nEpoch 91/200, Batch 10/45, Loss: 1.4048261642456055\nEpoch 91/200, Batch 11/45, Loss: 0.24730932712554932\nEpoch 91/200, Batch 12/45, Loss: 0.1895921528339386\nEpoch 91/200, Batch 13/45, Loss: 0.26458609104156494\nEpoch 91/200, Batch 14/45, Loss: 0.5549049377441406\nEpoch 91/200, Batch 15/45, Loss: 0.14998380839824677\nEpoch 91/200, Batch 16/45, Loss: 0.32210540771484375\nEpoch 91/200, Batch 17/45, Loss: 0.21789294481277466\nEpoch 91/200, Batch 18/45, Loss: 0.3185710906982422\nEpoch 91/200, Batch 19/45, Loss: 0.2953606843948364\nEpoch 91/200, Batch 20/45, Loss: 0.2830219566822052\nEpoch 91/200, Batch 21/45, Loss: 0.3221440315246582\nEpoch 91/200, Batch 22/45, Loss: 0.2651885151863098\nEpoch 91/200, Batch 23/45, Loss: 0.20480112731456757\nEpoch 91/200, Batch 24/45, Loss: 0.23097386956214905\nEpoch 91/200, Batch 25/45, Loss: 0.24138329923152924\nEpoch 91/200, Batch 26/45, Loss: 0.149283766746521\nEpoch 91/200, Batch 27/45, Loss: 0.34312325716018677\nEpoch 91/200, Batch 28/45, Loss: 0.20609834790229797\nEpoch 91/200, Batch 29/45, Loss: 0.2971990704536438\nEpoch 91/200, Batch 30/45, Loss: 0.4286736249923706\nEpoch 91/200, Batch 31/45, Loss: 0.2395433485507965\nEpoch 91/200, Batch 32/45, Loss: 0.302923321723938\nEpoch 91/200, Batch 33/45, Loss: 0.4235922694206238\nEpoch 91/200, Batch 34/45, Loss: 0.09911905974149704\nEpoch 91/200, Batch 35/45, Loss: 0.18141907453536987\nEpoch 91/200, Batch 36/45, Loss: 0.41714584827423096\nEpoch 91/200, Batch 37/45, Loss: 0.23963485658168793\nEpoch 91/200, Batch 38/45, Loss: 0.3611214756965637\nEpoch 91/200, Batch 39/45, Loss: 0.28474733233451843\nEpoch 91/200, Batch 40/45, Loss: 0.16064408421516418\nEpoch 91/200, Batch 41/45, Loss: 1.3635358810424805\nEpoch 91/200, Batch 42/45, Loss: 0.3830640912055969\nEpoch 91/200, Batch 43/45, Loss: 0.20360495150089264\nEpoch 91/200, Batch 44/45, Loss: 0.28477877378463745\nEpoch 91/200, Batch 45/45, Loss: 0.3018934726715088\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  11.347378492355347 Best Val MSE:  5.2462038397789\nEpoch:  92 , Time Elapsed:  14.69216829140981  mins\nEpoch 92/200, Batch 1/45, Loss: 0.22582246363162994\nEpoch 92/200, Batch 2/45, Loss: 0.34318673610687256\nEpoch 92/200, Batch 3/45, Loss: 0.4579165577888489\nEpoch 92/200, Batch 4/45, Loss: 0.3024960160255432\nEpoch 92/200, Batch 5/45, Loss: 0.33445143699645996\nEpoch 92/200, Batch 6/45, Loss: 0.3298453986644745\nEpoch 92/200, Batch 7/45, Loss: 0.348061203956604\nEpoch 92/200, Batch 8/45, Loss: 0.3825179636478424\nEpoch 92/200, Batch 9/45, Loss: 0.29217854142189026\nEpoch 92/200, Batch 10/45, Loss: 0.39407509565353394\nEpoch 92/200, Batch 11/45, Loss: 0.4760393500328064\nEpoch 92/200, Batch 12/45, Loss: 0.32864147424697876\nEpoch 92/200, Batch 13/45, Loss: 0.3118956685066223\nEpoch 92/200, Batch 14/45, Loss: 0.5436474084854126\nEpoch 92/200, Batch 15/45, Loss: 0.31417402625083923\nEpoch 92/200, Batch 16/45, Loss: 0.20807909965515137\nEpoch 92/200, Batch 17/45, Loss: 0.25654810667037964\nEpoch 92/200, Batch 18/45, Loss: 0.24732288718223572\nEpoch 92/200, Batch 19/45, Loss: 0.26067692041397095\nEpoch 92/200, Batch 20/45, Loss: 0.3389521837234497\nEpoch 92/200, Batch 21/45, Loss: 0.28724247217178345\nEpoch 92/200, Batch 22/45, Loss: 0.3447490632534027\nEpoch 92/200, Batch 23/45, Loss: 0.3423163890838623\nEpoch 92/200, Batch 24/45, Loss: 0.4094753563404083\nEpoch 92/200, Batch 25/45, Loss: 3.6081931591033936\nEpoch 92/200, Batch 26/45, Loss: 0.3429507613182068\nEpoch 92/200, Batch 27/45, Loss: 0.36924731731414795\nEpoch 92/200, Batch 28/45, Loss: 0.44058775901794434\nEpoch 92/200, Batch 29/45, Loss: 0.5313422679901123\nEpoch 92/200, Batch 30/45, Loss: 0.5948027968406677\nEpoch 92/200, Batch 31/45, Loss: 0.554628849029541\nEpoch 92/200, Batch 32/45, Loss: 0.42533764243125916\nEpoch 92/200, Batch 33/45, Loss: 1.9435021877288818\nEpoch 92/200, Batch 34/45, Loss: 1.1006340980529785\nEpoch 92/200, Batch 35/45, Loss: 0.5253328680992126\nEpoch 92/200, Batch 36/45, Loss: 0.3792349696159363\nEpoch 92/200, Batch 37/45, Loss: 0.3557290732860565\nEpoch 92/200, Batch 38/45, Loss: 1.0326025485992432\nEpoch 92/200, Batch 39/45, Loss: 0.43423616886138916\nEpoch 92/200, Batch 40/45, Loss: 0.46378040313720703\nEpoch 92/200, Batch 41/45, Loss: 1.4540166854858398\nEpoch 92/200, Batch 42/45, Loss: 0.5376676321029663\nEpoch 92/200, Batch 43/45, Loss: 0.35155218839645386\nEpoch 92/200, Batch 44/45, Loss: 0.3433381915092468\nEpoch 92/200, Batch 45/45, Loss: 0.4992188811302185\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  10.480111002922058 Best Val MSE:  5.2462038397789\nEpoch:  93 , Time Elapsed:  14.850691707928975  mins\nEpoch 93/200, Batch 1/45, Loss: 0.7880648374557495\nEpoch 93/200, Batch 2/45, Loss: 0.6312093734741211\nEpoch 93/200, Batch 3/45, Loss: 0.49824488162994385\nEpoch 93/200, Batch 4/45, Loss: 0.2987704277038574\nEpoch 93/200, Batch 5/45, Loss: 0.3471076488494873\nEpoch 93/200, Batch 6/45, Loss: 0.3506878614425659\nEpoch 93/200, Batch 7/45, Loss: 0.6658822298049927\nEpoch 93/200, Batch 8/45, Loss: 0.8733301162719727\nEpoch 93/200, Batch 9/45, Loss: 0.5253221392631531\nEpoch 93/200, Batch 10/45, Loss: 0.7463266849517822\nEpoch 93/200, Batch 11/45, Loss: 0.20007392764091492\nEpoch 93/200, Batch 12/45, Loss: 0.27133601903915405\nEpoch 93/200, Batch 13/45, Loss: 0.2575514316558838\nEpoch 93/200, Batch 14/45, Loss: 0.38460761308670044\nEpoch 93/200, Batch 15/45, Loss: 0.3800545632839203\nEpoch 93/200, Batch 16/45, Loss: 0.28241896629333496\nEpoch 93/200, Batch 17/45, Loss: 0.7681341171264648\nEpoch 93/200, Batch 18/45, Loss: 0.3673311173915863\nEpoch 93/200, Batch 19/45, Loss: 0.2871425151824951\nEpoch 93/200, Batch 20/45, Loss: 0.44753968715667725\nEpoch 93/200, Batch 21/45, Loss: 1.3106558322906494\nEpoch 93/200, Batch 22/45, Loss: 0.25885510444641113\nEpoch 93/200, Batch 23/45, Loss: 0.5759953260421753\nEpoch 93/200, Batch 24/45, Loss: 0.21158088743686676\nEpoch 93/200, Batch 25/45, Loss: 0.3072318434715271\nEpoch 93/200, Batch 26/45, Loss: 0.4380037188529968\nEpoch 93/200, Batch 27/45, Loss: 0.6346009373664856\nEpoch 93/200, Batch 28/45, Loss: 0.43148165941238403\nEpoch 93/200, Batch 29/45, Loss: 0.4154805541038513\nEpoch 93/200, Batch 30/45, Loss: 0.4868180751800537\nEpoch 93/200, Batch 31/45, Loss: 0.3406902849674225\nEpoch 93/200, Batch 32/45, Loss: 0.7643066644668579\nEpoch 93/200, Batch 33/45, Loss: 0.3973267376422882\nEpoch 93/200, Batch 34/45, Loss: 0.3627166748046875\nEpoch 93/200, Batch 35/45, Loss: 1.031990885734558\nEpoch 93/200, Batch 36/45, Loss: 1.2258509397506714\nEpoch 93/200, Batch 37/45, Loss: 0.3899696469306946\nEpoch 93/200, Batch 38/45, Loss: 0.28548312187194824\nEpoch 93/200, Batch 39/45, Loss: 0.26738181710243225\nEpoch 93/200, Batch 40/45, Loss: 0.591821551322937\nEpoch 93/200, Batch 41/45, Loss: 0.830878734588623\nEpoch 93/200, Batch 42/45, Loss: 1.4812278747558594\nEpoch 93/200, Batch 43/45, Loss: 0.2981882691383362\nEpoch 93/200, Batch 44/45, Loss: 1.2937536239624023\nEpoch 93/200, Batch 45/45, Loss: 0.9211779832839966\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  18.698850274086 Best Val MSE:  5.2462038397789\nEpoch:  94 , Time Elapsed:  15.012542672952016  mins\nEpoch 94/200, Batch 1/45, Loss: 0.4693794846534729\nEpoch 94/200, Batch 2/45, Loss: 0.6359494924545288\nEpoch 94/200, Batch 3/45, Loss: 0.5044550895690918\nEpoch 94/200, Batch 4/45, Loss: 0.5668888092041016\nEpoch 94/200, Batch 5/45, Loss: 0.5702869892120361\nEpoch 94/200, Batch 6/45, Loss: 0.3403976261615753\nEpoch 94/200, Batch 7/45, Loss: 0.31403762102127075\nEpoch 94/200, Batch 8/45, Loss: 0.3541055917739868\nEpoch 94/200, Batch 9/45, Loss: 0.5335814952850342\nEpoch 94/200, Batch 10/45, Loss: 0.3989258110523224\nEpoch 94/200, Batch 11/45, Loss: 1.4567646980285645\nEpoch 94/200, Batch 12/45, Loss: 0.3197134733200073\nEpoch 94/200, Batch 13/45, Loss: 1.2381690740585327\nEpoch 94/200, Batch 14/45, Loss: 0.943305253982544\nEpoch 94/200, Batch 15/45, Loss: 0.3943246006965637\nEpoch 94/200, Batch 16/45, Loss: 0.33215001225471497\nEpoch 94/200, Batch 17/45, Loss: 0.49933692812919617\nEpoch 94/200, Batch 18/45, Loss: 0.28440290689468384\nEpoch 94/200, Batch 19/45, Loss: 0.1320434808731079\nEpoch 94/200, Batch 20/45, Loss: 0.5082347393035889\nEpoch 94/200, Batch 21/45, Loss: 0.1175004243850708\nEpoch 94/200, Batch 22/45, Loss: 0.48169851303100586\nEpoch 94/200, Batch 23/45, Loss: 0.47292956709861755\nEpoch 94/200, Batch 24/45, Loss: 0.40928584337234497\nEpoch 94/200, Batch 25/45, Loss: 0.4402002692222595\nEpoch 94/200, Batch 26/45, Loss: 0.7372226119041443\nEpoch 94/200, Batch 27/45, Loss: 1.2193489074707031\nEpoch 94/200, Batch 28/45, Loss: 0.5450959205627441\nEpoch 94/200, Batch 29/45, Loss: 0.19983847439289093\nEpoch 94/200, Batch 30/45, Loss: 0.6714053750038147\nEpoch 94/200, Batch 31/45, Loss: 0.5343772172927856\nEpoch 94/200, Batch 32/45, Loss: 0.5859428644180298\nEpoch 94/200, Batch 33/45, Loss: 0.6898388266563416\nEpoch 94/200, Batch 34/45, Loss: 1.6819807291030884\nEpoch 94/200, Batch 35/45, Loss: 0.5613512992858887\nEpoch 94/200, Batch 36/45, Loss: 0.3992093801498413\nEpoch 94/200, Batch 37/45, Loss: 0.546200156211853\nEpoch 94/200, Batch 38/45, Loss: 1.07609224319458\nEpoch 94/200, Batch 39/45, Loss: 0.5201206207275391\nEpoch 94/200, Batch 40/45, Loss: 0.1497611552476883\nEpoch 94/200, Batch 41/45, Loss: 0.6102904081344604\nEpoch 94/200, Batch 42/45, Loss: 0.3643217980861664\nEpoch 94/200, Batch 43/45, Loss: 0.32341060042381287\nEpoch 94/200, Batch 44/45, Loss: 0.3904097080230713\nEpoch 94/200, Batch 45/45, Loss: 0.09906899929046631\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  9.369902282953262 Best Val MSE:  5.2462038397789\nEpoch:  95 , Time Elapsed:  15.175717099507649  mins\nEpoch 95/200, Batch 1/45, Loss: 0.28945666551589966\nEpoch 95/200, Batch 2/45, Loss: 0.2738267183303833\nEpoch 95/200, Batch 3/45, Loss: 0.3195227384567261\nEpoch 95/200, Batch 4/45, Loss: 0.5033735036849976\nEpoch 95/200, Batch 5/45, Loss: 0.6607832908630371\nEpoch 95/200, Batch 6/45, Loss: 0.5756521821022034\nEpoch 95/200, Batch 7/45, Loss: 0.5357820391654968\nEpoch 95/200, Batch 8/45, Loss: 0.5004756450653076\nEpoch 95/200, Batch 9/45, Loss: 0.431963711977005\nEpoch 95/200, Batch 10/45, Loss: 0.5354057550430298\nEpoch 95/200, Batch 11/45, Loss: 0.8814742565155029\nEpoch 95/200, Batch 12/45, Loss: 0.48682186007499695\nEpoch 95/200, Batch 13/45, Loss: 0.358654260635376\nEpoch 95/200, Batch 14/45, Loss: 0.536652684211731\nEpoch 95/200, Batch 15/45, Loss: 0.4311273694038391\nEpoch 95/200, Batch 16/45, Loss: 0.5787251591682434\nEpoch 95/200, Batch 17/45, Loss: 0.3908626437187195\nEpoch 95/200, Batch 18/45, Loss: 0.28337693214416504\nEpoch 95/200, Batch 19/45, Loss: 0.5325063467025757\nEpoch 95/200, Batch 20/45, Loss: 0.27590110898017883\nEpoch 95/200, Batch 21/45, Loss: 0.712145209312439\nEpoch 95/200, Batch 22/45, Loss: 0.3095282316207886\nEpoch 95/200, Batch 23/45, Loss: 0.6969919204711914\nEpoch 95/200, Batch 24/45, Loss: 0.3292824327945709\nEpoch 95/200, Batch 25/45, Loss: 0.24640965461730957\nEpoch 95/200, Batch 26/45, Loss: 0.3591234087944031\nEpoch 95/200, Batch 27/45, Loss: 0.40117913484573364\nEpoch 95/200, Batch 28/45, Loss: 0.26759761571884155\nEpoch 95/200, Batch 29/45, Loss: 0.79326331615448\nEpoch 95/200, Batch 30/45, Loss: 0.2576456069946289\nEpoch 95/200, Batch 31/45, Loss: 0.5132313370704651\nEpoch 95/200, Batch 32/45, Loss: 0.5305464267730713\nEpoch 95/200, Batch 33/45, Loss: 0.44733500480651855\nEpoch 95/200, Batch 34/45, Loss: 0.6724050045013428\nEpoch 95/200, Batch 35/45, Loss: 0.3269960582256317\nEpoch 95/200, Batch 36/45, Loss: 0.2699548006057739\nEpoch 95/200, Batch 37/45, Loss: 0.19221735000610352\nEpoch 95/200, Batch 38/45, Loss: 0.39119017124176025\nEpoch 95/200, Batch 39/45, Loss: 0.1852874904870987\nEpoch 95/200, Batch 40/45, Loss: 0.4042385220527649\nEpoch 95/200, Batch 41/45, Loss: 4.658941268920898\nEpoch 95/200, Batch 42/45, Loss: 0.4271999001502991\nEpoch 95/200, Batch 43/45, Loss: 0.3681931495666504\nEpoch 95/200, Batch 44/45, Loss: 0.4788808822631836\nEpoch 95/200, Batch 45/45, Loss: 0.6392496228218079\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  16.965823888778687 Best Val MSE:  5.2462038397789\nEpoch:  96 , Time Elapsed:  15.33319532473882  mins\nEpoch 96/200, Batch 1/45, Loss: 1.2974610328674316\nEpoch 96/200, Batch 2/45, Loss: 1.094543218612671\nEpoch 96/200, Batch 3/45, Loss: 0.9290186166763306\nEpoch 96/200, Batch 4/45, Loss: 1.8294893503189087\nEpoch 96/200, Batch 5/45, Loss: 0.6032329797744751\nEpoch 96/200, Batch 6/45, Loss: 0.6496553421020508\nEpoch 96/200, Batch 7/45, Loss: 0.8028641939163208\nEpoch 96/200, Batch 8/45, Loss: 0.6701211929321289\nEpoch 96/200, Batch 9/45, Loss: 0.3589079976081848\nEpoch 96/200, Batch 10/45, Loss: 0.36569273471832275\nEpoch 96/200, Batch 11/45, Loss: 0.5302666425704956\nEpoch 96/200, Batch 12/45, Loss: 0.3901750445365906\nEpoch 96/200, Batch 13/45, Loss: 0.2221972495317459\nEpoch 96/200, Batch 14/45, Loss: 0.33998388051986694\nEpoch 96/200, Batch 15/45, Loss: 0.34437862038612366\nEpoch 96/200, Batch 16/45, Loss: 0.6279790997505188\nEpoch 96/200, Batch 17/45, Loss: 0.6777552366256714\nEpoch 96/200, Batch 18/45, Loss: 0.5114263296127319\nEpoch 96/200, Batch 19/45, Loss: 0.7462052702903748\nEpoch 96/200, Batch 20/45, Loss: 0.5640143752098083\nEpoch 96/200, Batch 21/45, Loss: 0.2831457853317261\nEpoch 96/200, Batch 22/45, Loss: 0.30351507663726807\nEpoch 96/200, Batch 23/45, Loss: 0.3946295380592346\nEpoch 96/200, Batch 24/45, Loss: 0.6252500414848328\nEpoch 96/200, Batch 25/45, Loss: 1.2982738018035889\nEpoch 96/200, Batch 26/45, Loss: 1.069077730178833\nEpoch 96/200, Batch 27/45, Loss: 0.4795282483100891\nEpoch 96/200, Batch 28/45, Loss: 0.3743990957736969\nEpoch 96/200, Batch 29/45, Loss: 1.7751673460006714\nEpoch 96/200, Batch 30/45, Loss: 0.30509188771247864\nEpoch 96/200, Batch 31/45, Loss: 0.23159733414649963\nEpoch 96/200, Batch 32/45, Loss: 0.23065344989299774\nEpoch 96/200, Batch 33/45, Loss: 0.35461169481277466\nEpoch 96/200, Batch 34/45, Loss: 0.9242802858352661\nEpoch 96/200, Batch 35/45, Loss: 0.28231555223464966\nEpoch 96/200, Batch 36/45, Loss: 0.49956297874450684\nEpoch 96/200, Batch 37/45, Loss: 0.6717296242713928\nEpoch 96/200, Batch 38/45, Loss: 0.44407570362091064\nEpoch 96/200, Batch 39/45, Loss: 0.34797120094299316\nEpoch 96/200, Batch 40/45, Loss: 0.8401808738708496\nEpoch 96/200, Batch 41/45, Loss: 0.42705032229423523\nEpoch 96/200, Batch 42/45, Loss: 0.39231783151626587\nEpoch 96/200, Batch 43/45, Loss: 0.6541965007781982\nEpoch 96/200, Batch 44/45, Loss: 0.3859888017177582\nEpoch 96/200, Batch 45/45, Loss: 0.6124699115753174\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  9.592246294021606 Best Val MSE:  5.2462038397789\nEpoch:  97 , Time Elapsed:  15.488232568899791  mins\nEpoch 97/200, Batch 1/45, Loss: 0.5036261081695557\nEpoch 97/200, Batch 2/45, Loss: 0.6402914524078369\nEpoch 97/200, Batch 3/45, Loss: 0.8214518427848816\nEpoch 97/200, Batch 4/45, Loss: 0.42150551080703735\nEpoch 97/200, Batch 5/45, Loss: 0.5562807321548462\nEpoch 97/200, Batch 6/45, Loss: 0.8158409595489502\nEpoch 97/200, Batch 7/45, Loss: 0.4595188498497009\nEpoch 97/200, Batch 8/45, Loss: 0.5335057973861694\nEpoch 97/200, Batch 9/45, Loss: 0.4337853789329529\nEpoch 97/200, Batch 10/45, Loss: 0.26215851306915283\nEpoch 97/200, Batch 11/45, Loss: 0.6310251951217651\nEpoch 97/200, Batch 12/45, Loss: 0.32627835869789124\nEpoch 97/200, Batch 13/45, Loss: 0.3161008954048157\nEpoch 97/200, Batch 14/45, Loss: 0.6451231241226196\nEpoch 97/200, Batch 15/45, Loss: 0.6551436185836792\nEpoch 97/200, Batch 16/45, Loss: 0.3976343274116516\nEpoch 97/200, Batch 17/45, Loss: 0.47174036502838135\nEpoch 97/200, Batch 18/45, Loss: 0.42034921050071716\nEpoch 97/200, Batch 19/45, Loss: 1.1064436435699463\nEpoch 97/200, Batch 20/45, Loss: 0.30296021699905396\nEpoch 97/200, Batch 21/45, Loss: 0.3378438949584961\nEpoch 97/200, Batch 22/45, Loss: 0.3754369914531708\nEpoch 97/200, Batch 23/45, Loss: 0.3748578429222107\nEpoch 97/200, Batch 24/45, Loss: 0.9757740497589111\nEpoch 97/200, Batch 25/45, Loss: 0.24392512440681458\nEpoch 97/200, Batch 26/45, Loss: 0.5109406113624573\nEpoch 97/200, Batch 27/45, Loss: 0.4078506827354431\nEpoch 97/200, Batch 28/45, Loss: 0.27053022384643555\nEpoch 97/200, Batch 29/45, Loss: 0.26031357049942017\nEpoch 97/200, Batch 30/45, Loss: 0.3417855501174927\nEpoch 97/200, Batch 31/45, Loss: 0.3795439302921295\nEpoch 97/200, Batch 32/45, Loss: 0.37478452920913696\nEpoch 97/200, Batch 33/45, Loss: 0.41098201274871826\nEpoch 97/200, Batch 34/45, Loss: 0.17459845542907715\nEpoch 97/200, Batch 35/45, Loss: 0.16735126078128815\nEpoch 97/200, Batch 36/45, Loss: 0.21365073323249817\nEpoch 97/200, Batch 37/45, Loss: 0.2679302990436554\nEpoch 97/200, Batch 38/45, Loss: 0.5720707178115845\nEpoch 97/200, Batch 39/45, Loss: 0.1765855848789215\nEpoch 97/200, Batch 40/45, Loss: 0.1410028636455536\nEpoch 97/200, Batch 41/45, Loss: 0.5629163980484009\nEpoch 97/200, Batch 42/45, Loss: 0.8026365041732788\nEpoch 97/200, Batch 43/45, Loss: 0.5937720537185669\nEpoch 97/200, Batch 44/45, Loss: 0.5566754341125488\nEpoch 97/200, Batch 45/45, Loss: 0.26630866527557373\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  10.698996663093567 Best Val MSE:  5.2462038397789\nEpoch:  98 , Time Elapsed:  15.642987008889516  mins\nEpoch 98/200, Batch 1/45, Loss: 0.9118211269378662\nEpoch 98/200, Batch 2/45, Loss: 0.6298909783363342\nEpoch 98/200, Batch 3/45, Loss: 0.3239930272102356\nEpoch 98/200, Batch 4/45, Loss: 0.8447439074516296\nEpoch 98/200, Batch 5/45, Loss: 0.4054146409034729\nEpoch 98/200, Batch 6/45, Loss: 1.5188162326812744\nEpoch 98/200, Batch 7/45, Loss: 0.4014483392238617\nEpoch 98/200, Batch 8/45, Loss: 0.1772199124097824\nEpoch 98/200, Batch 9/45, Loss: 0.3374999165534973\nEpoch 98/200, Batch 10/45, Loss: 0.16891878843307495\nEpoch 98/200, Batch 11/45, Loss: 0.25367626547813416\nEpoch 98/200, Batch 12/45, Loss: 0.24085068702697754\nEpoch 98/200, Batch 13/45, Loss: 0.32707837224006653\nEpoch 98/200, Batch 14/45, Loss: 0.3495558202266693\nEpoch 98/200, Batch 15/45, Loss: 0.21332663297653198\nEpoch 98/200, Batch 16/45, Loss: 0.1888982355594635\nEpoch 98/200, Batch 17/45, Loss: 1.2320865392684937\nEpoch 98/200, Batch 18/45, Loss: 0.696109414100647\nEpoch 98/200, Batch 19/45, Loss: 0.4139486253261566\nEpoch 98/200, Batch 20/45, Loss: 0.33079296350479126\nEpoch 98/200, Batch 21/45, Loss: 0.23379114270210266\nEpoch 98/200, Batch 22/45, Loss: 0.5110502243041992\nEpoch 98/200, Batch 23/45, Loss: 0.2598767876625061\nEpoch 98/200, Batch 24/45, Loss: 0.2950587570667267\nEpoch 98/200, Batch 25/45, Loss: 0.36340004205703735\nEpoch 98/200, Batch 26/45, Loss: 0.552834689617157\nEpoch 98/200, Batch 27/45, Loss: 1.4725897312164307\nEpoch 98/200, Batch 28/45, Loss: 0.27086615562438965\nEpoch 98/200, Batch 29/45, Loss: 0.3211488723754883\nEpoch 98/200, Batch 30/45, Loss: 0.18325065076351166\nEpoch 98/200, Batch 31/45, Loss: 1.5122579336166382\nEpoch 98/200, Batch 32/45, Loss: 0.3928435146808624\nEpoch 98/200, Batch 33/45, Loss: 0.4166504740715027\nEpoch 98/200, Batch 34/45, Loss: 1.491906762123108\nEpoch 98/200, Batch 35/45, Loss: 0.4613420069217682\nEpoch 98/200, Batch 36/45, Loss: 0.35369160771369934\nEpoch 98/200, Batch 37/45, Loss: 0.27441293001174927\nEpoch 98/200, Batch 38/45, Loss: 0.30447906255722046\nEpoch 98/200, Batch 39/45, Loss: 0.5120207071304321\nEpoch 98/200, Batch 40/45, Loss: 0.18992675840854645\nEpoch 98/200, Batch 41/45, Loss: 0.2965207099914551\nEpoch 98/200, Batch 42/45, Loss: 0.31675565242767334\nEpoch 98/200, Batch 43/45, Loss: 0.1289958357810974\nEpoch 98/200, Batch 44/45, Loss: 0.24957343935966492\nEpoch 98/200, Batch 45/45, Loss: 0.36107736825942993\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  10.676242172718048 Best Val MSE:  5.2462038397789\nEpoch:  99 , Time Elapsed:  15.80638993581136  mins\nEpoch 99/200, Batch 1/45, Loss: 0.29442769289016724\nEpoch 99/200, Batch 2/45, Loss: 0.17452678084373474\nEpoch 99/200, Batch 3/45, Loss: 0.24812883138656616\nEpoch 99/200, Batch 4/45, Loss: 1.636714220046997\nEpoch 99/200, Batch 5/45, Loss: 0.9971093535423279\nEpoch 99/200, Batch 6/45, Loss: 0.21516487002372742\nEpoch 99/200, Batch 7/45, Loss: 0.18078862130641937\nEpoch 99/200, Batch 8/45, Loss: 0.6473113298416138\nEpoch 99/200, Batch 9/45, Loss: 1.814136028289795\nEpoch 99/200, Batch 10/45, Loss: 1.1221301555633545\nEpoch 99/200, Batch 11/45, Loss: 0.25468721985816956\nEpoch 99/200, Batch 12/45, Loss: 0.5737314820289612\nEpoch 99/200, Batch 13/45, Loss: 0.33380141854286194\nEpoch 99/200, Batch 14/45, Loss: 0.5065878629684448\nEpoch 99/200, Batch 15/45, Loss: 0.4279361963272095\nEpoch 99/200, Batch 16/45, Loss: 0.3889347314834595\nEpoch 99/200, Batch 17/45, Loss: 0.40571391582489014\nEpoch 99/200, Batch 18/45, Loss: 0.3440108895301819\nEpoch 99/200, Batch 19/45, Loss: 0.5153639316558838\nEpoch 99/200, Batch 20/45, Loss: 0.1947547197341919\nEpoch 99/200, Batch 21/45, Loss: 0.38346466422080994\nEpoch 99/200, Batch 22/45, Loss: 0.3783114552497864\nEpoch 99/200, Batch 23/45, Loss: 0.38899534940719604\nEpoch 99/200, Batch 24/45, Loss: 0.21392613649368286\nEpoch 99/200, Batch 25/45, Loss: 0.296474426984787\nEpoch 99/200, Batch 26/45, Loss: 0.16259843111038208\nEpoch 99/200, Batch 27/45, Loss: 0.29680120944976807\nEpoch 99/200, Batch 28/45, Loss: 0.3525737524032593\nEpoch 99/200, Batch 29/45, Loss: 0.25281602144241333\nEpoch 99/200, Batch 30/45, Loss: 0.4011702239513397\nEpoch 99/200, Batch 31/45, Loss: 0.3829292058944702\nEpoch 99/200, Batch 32/45, Loss: 0.21330195665359497\nEpoch 99/200, Batch 33/45, Loss: 0.24769720435142517\nEpoch 99/200, Batch 34/45, Loss: 0.3515223264694214\nEpoch 99/200, Batch 35/45, Loss: 0.21979710459709167\nEpoch 99/200, Batch 36/45, Loss: 0.1905083954334259\nEpoch 99/200, Batch 37/45, Loss: 0.2007521539926529\nEpoch 99/200, Batch 38/45, Loss: 0.21732987463474274\nEpoch 99/200, Batch 39/45, Loss: 0.2745589315891266\nEpoch 99/200, Batch 40/45, Loss: 0.5112589001655579\nEpoch 99/200, Batch 41/45, Loss: 0.28104168176651\nEpoch 99/200, Batch 42/45, Loss: 0.270662784576416\nEpoch 99/200, Batch 43/45, Loss: 0.48782676458358765\nEpoch 99/200, Batch 44/45, Loss: 0.3893181085586548\nEpoch 99/200, Batch 45/45, Loss: 0.3209282159805298\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  7.99983024597168 Best Val MSE:  5.2462038397789\nEpoch:  100 , Time Elapsed:  15.96693328221639  mins\nEpoch 100/200, Batch 1/45, Loss: 0.20288603007793427\nEpoch 100/200, Batch 2/45, Loss: 0.335101842880249\nEpoch 100/200, Batch 3/45, Loss: 0.20461182296276093\nEpoch 100/200, Batch 4/45, Loss: 0.27175092697143555\nEpoch 100/200, Batch 5/45, Loss: 0.22045539319515228\nEpoch 100/200, Batch 6/45, Loss: 0.24727293848991394\nEpoch 100/200, Batch 7/45, Loss: 0.38300222158432007\nEpoch 100/200, Batch 8/45, Loss: 0.30136430263519287\nEpoch 100/200, Batch 9/45, Loss: 0.3094247281551361\nEpoch 100/200, Batch 10/45, Loss: 0.48868051171302795\nEpoch 100/200, Batch 11/45, Loss: 0.1414526104927063\nEpoch 100/200, Batch 12/45, Loss: 0.26991453766822815\nEpoch 100/200, Batch 13/45, Loss: 0.2895672023296356\nEpoch 100/200, Batch 14/45, Loss: 0.23834866285324097\nEpoch 100/200, Batch 15/45, Loss: 0.33807164430618286\nEpoch 100/200, Batch 16/45, Loss: 0.31571853160858154\nEpoch 100/200, Batch 17/45, Loss: 0.5629513263702393\nEpoch 100/200, Batch 18/45, Loss: 0.3432779908180237\nEpoch 100/200, Batch 19/45, Loss: 0.3572481572628021\nEpoch 100/200, Batch 20/45, Loss: 0.37145867943763733\nEpoch 100/200, Batch 21/45, Loss: 0.3274124562740326\nEpoch 100/200, Batch 22/45, Loss: 0.3129384219646454\nEpoch 100/200, Batch 23/45, Loss: 0.26968374848365784\nEpoch 100/200, Batch 24/45, Loss: 0.6198282241821289\nEpoch 100/200, Batch 25/45, Loss: 0.41222137212753296\nEpoch 100/200, Batch 26/45, Loss: 0.12686628103256226\nEpoch 100/200, Batch 27/45, Loss: 0.2650219798088074\nEpoch 100/200, Batch 28/45, Loss: 0.4118306040763855\nEpoch 100/200, Batch 29/45, Loss: 0.220892533659935\nEpoch 100/200, Batch 30/45, Loss: 0.35593199729919434\nEpoch 100/200, Batch 31/45, Loss: 0.17060154676437378\nEpoch 100/200, Batch 32/45, Loss: 0.28773197531700134\nEpoch 100/200, Batch 33/45, Loss: 0.47655677795410156\nEpoch 100/200, Batch 34/45, Loss: 0.22962231934070587\nEpoch 100/200, Batch 35/45, Loss: 0.3915022611618042\nEpoch 100/200, Batch 36/45, Loss: 0.2985894978046417\nEpoch 100/200, Batch 37/45, Loss: 0.2197454273700714\nEpoch 100/200, Batch 38/45, Loss: 0.3094019293785095\nEpoch 100/200, Batch 39/45, Loss: 0.23653240501880646\nEpoch 100/200, Batch 40/45, Loss: 0.22925719618797302\nEpoch 100/200, Batch 41/45, Loss: 0.1867285668849945\nEpoch 100/200, Batch 42/45, Loss: 0.2570216953754425\nEpoch 100/200, Batch 43/45, Loss: 0.25862109661102295\nEpoch 100/200, Batch 44/45, Loss: 0.27789002656936646\nEpoch 100/200, Batch 45/45, Loss: 0.17345264554023743\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  8.099415719509125 Best Val MSE:  5.2462038397789\nEpoch:  101 , Time Elapsed:  16.12332441806793  mins\nEpoch 101/200, Batch 1/45, Loss: 1.2397938966751099\nEpoch 101/200, Batch 2/45, Loss: 0.29365378618240356\nEpoch 101/200, Batch 3/45, Loss: 0.40609660744667053\nEpoch 101/200, Batch 4/45, Loss: 0.3467511534690857\nEpoch 101/200, Batch 5/45, Loss: 0.4198094606399536\nEpoch 101/200, Batch 6/45, Loss: 0.2630726397037506\nEpoch 101/200, Batch 7/45, Loss: 0.42243894934654236\nEpoch 101/200, Batch 8/45, Loss: 0.23685690760612488\nEpoch 101/200, Batch 9/45, Loss: 0.20505499839782715\nEpoch 101/200, Batch 10/45, Loss: 0.33718758821487427\nEpoch 101/200, Batch 11/45, Loss: 0.2548615038394928\nEpoch 101/200, Batch 12/45, Loss: 0.5405243635177612\nEpoch 101/200, Batch 13/45, Loss: 0.2234545797109604\nEpoch 101/200, Batch 14/45, Loss: 0.27493956685066223\nEpoch 101/200, Batch 15/45, Loss: 0.7053616046905518\nEpoch 101/200, Batch 16/45, Loss: 0.6471883058547974\nEpoch 101/200, Batch 17/45, Loss: 0.6110506653785706\nEpoch 101/200, Batch 18/45, Loss: 0.20571544766426086\nEpoch 101/200, Batch 19/45, Loss: 0.3773464858531952\nEpoch 101/200, Batch 20/45, Loss: 0.1965089738368988\nEpoch 101/200, Batch 21/45, Loss: 0.2360488474369049\nEpoch 101/200, Batch 22/45, Loss: 0.2956773638725281\nEpoch 101/200, Batch 23/45, Loss: 0.3750276565551758\nEpoch 101/200, Batch 24/45, Loss: 0.2822917699813843\nEpoch 101/200, Batch 25/45, Loss: 0.3350817561149597\nEpoch 101/200, Batch 26/45, Loss: 0.4014206528663635\nEpoch 101/200, Batch 27/45, Loss: 0.27020061016082764\nEpoch 101/200, Batch 28/45, Loss: 0.39215338230133057\nEpoch 101/200, Batch 29/45, Loss: 0.48253753781318665\nEpoch 101/200, Batch 30/45, Loss: 0.22142164409160614\nEpoch 101/200, Batch 31/45, Loss: 0.3564597964286804\nEpoch 101/200, Batch 32/45, Loss: 0.1494176685810089\nEpoch 101/200, Batch 33/45, Loss: 0.33430662751197815\nEpoch 101/200, Batch 34/45, Loss: 0.22130748629570007\nEpoch 101/200, Batch 35/45, Loss: 0.6897972822189331\nEpoch 101/200, Batch 36/45, Loss: 0.22001245617866516\nEpoch 101/200, Batch 37/45, Loss: 0.6835270524024963\nEpoch 101/200, Batch 38/45, Loss: 0.2711467444896698\nEpoch 101/200, Batch 39/45, Loss: 0.21238607168197632\nEpoch 101/200, Batch 40/45, Loss: 0.1979476809501648\nEpoch 101/200, Batch 41/45, Loss: 0.31321388483047485\nEpoch 101/200, Batch 42/45, Loss: 0.2376248985528946\nEpoch 101/200, Batch 43/45, Loss: 0.42559027671813965\nEpoch 101/200, Batch 44/45, Loss: 0.4049302041530609\nEpoch 101/200, Batch 45/45, Loss: 0.19723600149154663\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  8.586740016937256 Best Val MSE:  5.2462038397789\nEpoch:  102 , Time Elapsed:  16.28150595029195  mins\nEpoch 102/200, Batch 1/45, Loss: 0.49571549892425537\nEpoch 102/200, Batch 2/45, Loss: 0.1995878517627716\nEpoch 102/200, Batch 3/45, Loss: 0.5687602162361145\nEpoch 102/200, Batch 4/45, Loss: 0.2850310206413269\nEpoch 102/200, Batch 5/45, Loss: 0.2579934597015381\nEpoch 102/200, Batch 6/45, Loss: 0.6159504055976868\nEpoch 102/200, Batch 7/45, Loss: 0.20346756279468536\nEpoch 102/200, Batch 8/45, Loss: 0.40931230783462524\nEpoch 102/200, Batch 9/45, Loss: 0.2631218731403351\nEpoch 102/200, Batch 10/45, Loss: 0.23907342553138733\nEpoch 102/200, Batch 11/45, Loss: 0.29344528913497925\nEpoch 102/200, Batch 12/45, Loss: 0.33059120178222656\nEpoch 102/200, Batch 13/45, Loss: 0.2018108069896698\nEpoch 102/200, Batch 14/45, Loss: 0.2381514459848404\nEpoch 102/200, Batch 15/45, Loss: 0.16054081916809082\nEpoch 102/200, Batch 16/45, Loss: 0.25811469554901123\nEpoch 102/200, Batch 17/45, Loss: 0.3202168941497803\nEpoch 102/200, Batch 18/45, Loss: 0.38987404108047485\nEpoch 102/200, Batch 19/45, Loss: 0.37667736411094666\nEpoch 102/200, Batch 20/45, Loss: 0.23705719411373138\nEpoch 102/200, Batch 21/45, Loss: 0.4100881516933441\nEpoch 102/200, Batch 22/45, Loss: 0.25609296560287476\nEpoch 102/200, Batch 23/45, Loss: 0.33049049973487854\nEpoch 102/200, Batch 24/45, Loss: 0.34080392122268677\nEpoch 102/200, Batch 25/45, Loss: 0.13108986616134644\nEpoch 102/200, Batch 26/45, Loss: 0.38906699419021606\nEpoch 102/200, Batch 27/45, Loss: 0.2810983657836914\nEpoch 102/200, Batch 28/45, Loss: 0.4366319179534912\nEpoch 102/200, Batch 29/45, Loss: 0.10360375046730042\nEpoch 102/200, Batch 30/45, Loss: 0.18189838528633118\nEpoch 102/200, Batch 31/45, Loss: 0.19213813543319702\nEpoch 102/200, Batch 32/45, Loss: 0.3681733012199402\nEpoch 102/200, Batch 33/45, Loss: 0.14256800711154938\nEpoch 102/200, Batch 34/45, Loss: 0.19312545657157898\nEpoch 102/200, Batch 35/45, Loss: 0.3394043445587158\nEpoch 102/200, Batch 36/45, Loss: 0.30678391456604004\nEpoch 102/200, Batch 37/45, Loss: 0.4180796146392822\nEpoch 102/200, Batch 38/45, Loss: 0.32500115036964417\nEpoch 102/200, Batch 39/45, Loss: 0.35704731941223145\nEpoch 102/200, Batch 40/45, Loss: 0.12294583022594452\nEpoch 102/200, Batch 41/45, Loss: 0.4124351739883423\nEpoch 102/200, Batch 42/45, Loss: 0.4886566400527954\nEpoch 102/200, Batch 43/45, Loss: 0.1616506278514862\nEpoch 102/200, Batch 44/45, Loss: 0.2793123722076416\nEpoch 102/200, Batch 45/45, Loss: 0.24884319305419922\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  8.406638979911804 Best Val MSE:  5.2462038397789\nEpoch:  103 , Time Elapsed:  16.43713626464208  mins\nEpoch 103/200, Batch 1/45, Loss: 0.15423807501792908\nEpoch 103/200, Batch 2/45, Loss: 0.36007773876190186\nEpoch 103/200, Batch 3/45, Loss: 0.2664142847061157\nEpoch 103/200, Batch 4/45, Loss: 0.30584847927093506\nEpoch 103/200, Batch 5/45, Loss: 0.3146352469921112\nEpoch 103/200, Batch 6/45, Loss: 0.33149388432502747\nEpoch 103/200, Batch 7/45, Loss: 0.16325141489505768\nEpoch 103/200, Batch 8/45, Loss: 0.642760694026947\nEpoch 103/200, Batch 9/45, Loss: 0.1941589117050171\nEpoch 103/200, Batch 10/45, Loss: 0.36980703473091125\nEpoch 103/200, Batch 11/45, Loss: 0.25342512130737305\nEpoch 103/200, Batch 12/45, Loss: 0.2760268747806549\nEpoch 103/200, Batch 13/45, Loss: 0.24358631670475006\nEpoch 103/200, Batch 14/45, Loss: 0.1830502450466156\nEpoch 103/200, Batch 15/45, Loss: 0.12981897592544556\nEpoch 103/200, Batch 16/45, Loss: 0.17610576748847961\nEpoch 103/200, Batch 17/45, Loss: 0.1864175796508789\nEpoch 103/200, Batch 18/45, Loss: 0.2773749530315399\nEpoch 103/200, Batch 19/45, Loss: 0.1662822812795639\nEpoch 103/200, Batch 20/45, Loss: 0.33610403537750244\nEpoch 103/200, Batch 21/45, Loss: 0.23570513725280762\nEpoch 103/200, Batch 22/45, Loss: 0.11502705514431\nEpoch 103/200, Batch 23/45, Loss: 0.24004100263118744\nEpoch 103/200, Batch 24/45, Loss: 0.44368666410446167\nEpoch 103/200, Batch 25/45, Loss: 0.30600249767303467\nEpoch 103/200, Batch 26/45, Loss: 0.1980736255645752\nEpoch 103/200, Batch 27/45, Loss: 0.21732598543167114\nEpoch 103/200, Batch 28/45, Loss: 0.2002055048942566\nEpoch 103/200, Batch 29/45, Loss: 0.3379126787185669\nEpoch 103/200, Batch 30/45, Loss: 0.3160368800163269\nEpoch 103/200, Batch 31/45, Loss: 0.2773194909095764\nEpoch 103/200, Batch 32/45, Loss: 0.210113525390625\nEpoch 103/200, Batch 33/45, Loss: 0.5645262002944946\nEpoch 103/200, Batch 34/45, Loss: 0.5419308543205261\nEpoch 103/200, Batch 35/45, Loss: 0.39296385645866394\nEpoch 103/200, Batch 36/45, Loss: 0.5106756091117859\nEpoch 103/200, Batch 37/45, Loss: 0.6921733617782593\nEpoch 103/200, Batch 38/45, Loss: 0.22315117716789246\nEpoch 103/200, Batch 39/45, Loss: 0.3189312517642975\nEpoch 103/200, Batch 40/45, Loss: 0.2851865291595459\nEpoch 103/200, Batch 41/45, Loss: 0.3160622715950012\nEpoch 103/200, Batch 42/45, Loss: 0.2372381091117859\nEpoch 103/200, Batch 43/45, Loss: 0.3314719498157501\nEpoch 103/200, Batch 44/45, Loss: 0.2946351170539856\nEpoch 103/200, Batch 45/45, Loss: 0.3364589214324951\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  6.672615930438042 Best Val MSE:  5.2462038397789\nEpoch:  104 , Time Elapsed:  16.59619767665863  mins\nEpoch 104/200, Batch 1/45, Loss: 0.28958404064178467\nEpoch 104/200, Batch 2/45, Loss: 0.34991568326950073\nEpoch 104/200, Batch 3/45, Loss: 0.3183871805667877\nEpoch 104/200, Batch 4/45, Loss: 0.5530455708503723\nEpoch 104/200, Batch 5/45, Loss: 0.19435325264930725\nEpoch 104/200, Batch 6/45, Loss: 0.11476599425077438\nEpoch 104/200, Batch 7/45, Loss: 0.270186185836792\nEpoch 104/200, Batch 8/45, Loss: 0.6093253493309021\nEpoch 104/200, Batch 9/45, Loss: 0.35201388597488403\nEpoch 104/200, Batch 10/45, Loss: 0.4590661823749542\nEpoch 104/200, Batch 11/45, Loss: 0.19511635601520538\nEpoch 104/200, Batch 12/45, Loss: 0.2640807330608368\nEpoch 104/200, Batch 13/45, Loss: 0.17986488342285156\nEpoch 104/200, Batch 14/45, Loss: 0.3112795352935791\nEpoch 104/200, Batch 15/45, Loss: 0.41272103786468506\nEpoch 104/200, Batch 16/45, Loss: 0.2077699899673462\nEpoch 104/200, Batch 17/45, Loss: 0.4555523991584778\nEpoch 104/200, Batch 18/45, Loss: 0.2996585965156555\nEpoch 104/200, Batch 19/45, Loss: 0.10968680679798126\nEpoch 104/200, Batch 20/45, Loss: 0.2682185471057892\nEpoch 104/200, Batch 21/45, Loss: 0.13791179656982422\nEpoch 104/200, Batch 22/45, Loss: 0.2674559950828552\nEpoch 104/200, Batch 23/45, Loss: 0.1925385743379593\nEpoch 104/200, Batch 24/45, Loss: 0.31517845392227173\nEpoch 104/200, Batch 25/45, Loss: 0.21195252239704132\nEpoch 104/200, Batch 26/45, Loss: 0.20230409502983093\nEpoch 104/200, Batch 27/45, Loss: 0.3489415645599365\nEpoch 104/200, Batch 28/45, Loss: 0.25412625074386597\nEpoch 104/200, Batch 29/45, Loss: 0.2827022075653076\nEpoch 104/200, Batch 30/45, Loss: 0.22052645683288574\nEpoch 104/200, Batch 31/45, Loss: 0.2814565896987915\nEpoch 104/200, Batch 32/45, Loss: 0.1770726442337036\nEpoch 104/200, Batch 33/45, Loss: 0.4084062874317169\nEpoch 104/200, Batch 34/45, Loss: 0.11820152401924133\nEpoch 104/200, Batch 35/45, Loss: 0.1939917653799057\nEpoch 104/200, Batch 36/45, Loss: 0.29051804542541504\nEpoch 104/200, Batch 37/45, Loss: 0.19792069494724274\nEpoch 104/200, Batch 38/45, Loss: 0.856081485748291\nEpoch 104/200, Batch 39/45, Loss: 0.2437693029642105\nEpoch 104/200, Batch 40/45, Loss: 0.3857783079147339\nEpoch 104/200, Batch 41/45, Loss: 0.14882811903953552\nEpoch 104/200, Batch 42/45, Loss: 0.21599656343460083\nEpoch 104/200, Batch 43/45, Loss: 0.1647336184978485\nEpoch 104/200, Batch 44/45, Loss: 0.16362150013446808\nEpoch 104/200, Batch 45/45, Loss: 0.3066963851451874\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  8.519672393798828 Best Val MSE:  5.2462038397789\nEpoch:  105 , Time Elapsed:  16.752991978327433  mins\nEpoch 105/200, Batch 1/45, Loss: 0.4100741147994995\nEpoch 105/200, Batch 2/45, Loss: 0.20976710319519043\nEpoch 105/200, Batch 3/45, Loss: 0.3007154166698456\nEpoch 105/200, Batch 4/45, Loss: 0.2566111087799072\nEpoch 105/200, Batch 5/45, Loss: 0.20066499710083008\nEpoch 105/200, Batch 6/45, Loss: 0.24494075775146484\nEpoch 105/200, Batch 7/45, Loss: 0.284782737493515\nEpoch 105/200, Batch 8/45, Loss: 0.2846052944660187\nEpoch 105/200, Batch 9/45, Loss: 0.23464161157608032\nEpoch 105/200, Batch 10/45, Loss: 0.17288434505462646\nEpoch 105/200, Batch 11/45, Loss: 0.24779748916625977\nEpoch 105/200, Batch 12/45, Loss: 0.17520378530025482\nEpoch 105/200, Batch 13/45, Loss: 0.3809855282306671\nEpoch 105/200, Batch 14/45, Loss: 0.5053640604019165\nEpoch 105/200, Batch 15/45, Loss: 0.4799961447715759\nEpoch 105/200, Batch 16/45, Loss: 0.20368880033493042\nEpoch 105/200, Batch 17/45, Loss: 0.2214554101228714\nEpoch 105/200, Batch 18/45, Loss: 0.35785067081451416\nEpoch 105/200, Batch 19/45, Loss: 0.28646647930145264\nEpoch 105/200, Batch 20/45, Loss: 0.40244609117507935\nEpoch 105/200, Batch 21/45, Loss: 0.28678426146507263\nEpoch 105/200, Batch 22/45, Loss: 0.38450828194618225\nEpoch 105/200, Batch 23/45, Loss: 0.36764612793922424\nEpoch 105/200, Batch 24/45, Loss: 0.1450953185558319\nEpoch 105/200, Batch 25/45, Loss: 0.2712550759315491\nEpoch 105/200, Batch 26/45, Loss: 0.3876604735851288\nEpoch 105/200, Batch 27/45, Loss: 0.20264717936515808\nEpoch 105/200, Batch 28/45, Loss: 0.200015589594841\nEpoch 105/200, Batch 29/45, Loss: 0.22548577189445496\nEpoch 105/200, Batch 30/45, Loss: 0.24098066985607147\nEpoch 105/200, Batch 31/45, Loss: 0.3983425796031952\nEpoch 105/200, Batch 32/45, Loss: 0.1903194785118103\nEpoch 105/200, Batch 33/45, Loss: 0.19192752242088318\nEpoch 105/200, Batch 34/45, Loss: 0.2160625010728836\nEpoch 105/200, Batch 35/45, Loss: 0.21262548863887787\nEpoch 105/200, Batch 36/45, Loss: 0.2958126366138458\nEpoch 105/200, Batch 37/45, Loss: 0.3586606979370117\nEpoch 105/200, Batch 38/45, Loss: 0.35030651092529297\nEpoch 105/200, Batch 39/45, Loss: 0.2739163637161255\nEpoch 105/200, Batch 40/45, Loss: 0.36410287022590637\nEpoch 105/200, Batch 41/45, Loss: 0.32478395104408264\nEpoch 105/200, Batch 42/45, Loss: 0.27681899070739746\nEpoch 105/200, Batch 43/45, Loss: 0.44987887144088745\nEpoch 105/200, Batch 44/45, Loss: 0.2545608580112457\nEpoch 105/200, Batch 45/45, Loss: 0.3012835383415222\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  7.449301332235336 Best Val MSE:  5.2462038397789\nEpoch:  106 , Time Elapsed:  16.911263338724773  mins\nEpoch 106/200, Batch 1/45, Loss: 0.19466166198253632\nEpoch 106/200, Batch 2/45, Loss: 0.28290998935699463\nEpoch 106/200, Batch 3/45, Loss: 0.3744496703147888\nEpoch 106/200, Batch 4/45, Loss: 0.31831735372543335\nEpoch 106/200, Batch 5/45, Loss: 0.3047875761985779\nEpoch 106/200, Batch 6/45, Loss: 0.25017204880714417\nEpoch 106/200, Batch 7/45, Loss: 0.3369957208633423\nEpoch 106/200, Batch 8/45, Loss: 0.3063178062438965\nEpoch 106/200, Batch 9/45, Loss: 0.7698379158973694\nEpoch 106/200, Batch 10/45, Loss: 0.5960260033607483\nEpoch 106/200, Batch 11/45, Loss: 0.21608389914035797\nEpoch 106/200, Batch 12/45, Loss: 0.49394991993904114\nEpoch 106/200, Batch 13/45, Loss: 0.3098256587982178\nEpoch 106/200, Batch 14/45, Loss: 0.3572573959827423\nEpoch 106/200, Batch 15/45, Loss: 0.2564740777015686\nEpoch 106/200, Batch 16/45, Loss: 0.3592432141304016\nEpoch 106/200, Batch 17/45, Loss: 0.3076793849468231\nEpoch 106/200, Batch 18/45, Loss: 0.22401458024978638\nEpoch 106/200, Batch 19/45, Loss: 0.3228411078453064\nEpoch 106/200, Batch 20/45, Loss: 0.38915231823921204\nEpoch 106/200, Batch 21/45, Loss: 0.14948248863220215\nEpoch 106/200, Batch 22/45, Loss: 0.3585069179534912\nEpoch 106/200, Batch 23/45, Loss: 0.23006543517112732\nEpoch 106/200, Batch 24/45, Loss: 0.5037533640861511\nEpoch 106/200, Batch 25/45, Loss: 0.20871561765670776\nEpoch 106/200, Batch 26/45, Loss: 0.22090168297290802\nEpoch 106/200, Batch 27/45, Loss: 0.30754512548446655\nEpoch 106/200, Batch 28/45, Loss: 0.4887963831424713\nEpoch 106/200, Batch 29/45, Loss: 0.23706132173538208\nEpoch 106/200, Batch 30/45, Loss: 0.2862396538257599\nEpoch 106/200, Batch 31/45, Loss: 0.3447735905647278\nEpoch 106/200, Batch 32/45, Loss: 0.1923244595527649\nEpoch 106/200, Batch 33/45, Loss: 0.3519403338432312\nEpoch 106/200, Batch 34/45, Loss: 0.3132879436016083\nEpoch 106/200, Batch 35/45, Loss: 0.24488729238510132\nEpoch 106/200, Batch 36/45, Loss: 0.34833836555480957\nEpoch 106/200, Batch 37/45, Loss: 0.24918177723884583\nEpoch 106/200, Batch 38/45, Loss: 0.1372247338294983\nEpoch 106/200, Batch 39/45, Loss: 0.1639859974384308\nEpoch 106/200, Batch 40/45, Loss: 0.27446919679641724\nEpoch 106/200, Batch 41/45, Loss: 0.31884706020355225\nEpoch 106/200, Batch 42/45, Loss: 0.4450565278530121\nEpoch 106/200, Batch 43/45, Loss: 0.21384747326374054\nEpoch 106/200, Batch 44/45, Loss: 0.26104435324668884\nEpoch 106/200, Batch 45/45, Loss: 0.2022320181131363\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  7.174461126327515 Best Val MSE:  5.2462038397789\nEpoch:  107 , Time Elapsed:  17.067217922210695  mins\nEpoch 107/200, Batch 1/45, Loss: 0.20369234681129456\nEpoch 107/200, Batch 2/45, Loss: 0.15829777717590332\nEpoch 107/200, Batch 3/45, Loss: 0.36868971586227417\nEpoch 107/200, Batch 4/45, Loss: 0.21118032932281494\nEpoch 107/200, Batch 5/45, Loss: 0.6381581425666809\nEpoch 107/200, Batch 6/45, Loss: 0.25789496302604675\nEpoch 107/200, Batch 7/45, Loss: 0.5324922800064087\nEpoch 107/200, Batch 8/45, Loss: 0.32652485370635986\nEpoch 107/200, Batch 9/45, Loss: 0.38728511333465576\nEpoch 107/200, Batch 10/45, Loss: 0.15190020203590393\nEpoch 107/200, Batch 11/45, Loss: 0.36018434166908264\nEpoch 107/200, Batch 12/45, Loss: 0.4162144958972931\nEpoch 107/200, Batch 13/45, Loss: 0.21643519401550293\nEpoch 107/200, Batch 14/45, Loss: 0.8524613976478577\nEpoch 107/200, Batch 15/45, Loss: 0.19764278829097748\nEpoch 107/200, Batch 16/45, Loss: 0.23260194063186646\nEpoch 107/200, Batch 17/45, Loss: 0.1931304931640625\nEpoch 107/200, Batch 18/45, Loss: 0.31166496872901917\nEpoch 107/200, Batch 19/45, Loss: 0.12915576994419098\nEpoch 107/200, Batch 20/45, Loss: 0.28139111399650574\nEpoch 107/200, Batch 21/45, Loss: 0.36047056317329407\nEpoch 107/200, Batch 22/45, Loss: 0.2995009422302246\nEpoch 107/200, Batch 23/45, Loss: 0.43762198090553284\nEpoch 107/200, Batch 24/45, Loss: 0.2451227307319641\nEpoch 107/200, Batch 25/45, Loss: 0.25664466619491577\nEpoch 107/200, Batch 26/45, Loss: 0.25720804929733276\nEpoch 107/200, Batch 27/45, Loss: 0.16515126824378967\nEpoch 107/200, Batch 28/45, Loss: 0.34221869707107544\nEpoch 107/200, Batch 29/45, Loss: 0.40793532133102417\nEpoch 107/200, Batch 30/45, Loss: 0.21240684390068054\nEpoch 107/200, Batch 31/45, Loss: 0.2484663724899292\nEpoch 107/200, Batch 32/45, Loss: 0.33012855052948\nEpoch 107/200, Batch 33/45, Loss: 0.50968998670578\nEpoch 107/200, Batch 34/45, Loss: 0.25974714756011963\nEpoch 107/200, Batch 35/45, Loss: 0.21965080499649048\nEpoch 107/200, Batch 36/45, Loss: 0.7883970737457275\nEpoch 107/200, Batch 37/45, Loss: 0.17181003093719482\nEpoch 107/200, Batch 38/45, Loss: 0.17342785000801086\nEpoch 107/200, Batch 39/45, Loss: 0.5098816156387329\nEpoch 107/200, Batch 40/45, Loss: 0.5500115156173706\nEpoch 107/200, Batch 41/45, Loss: 0.45558932423591614\nEpoch 107/200, Batch 42/45, Loss: 0.21451134979724884\nEpoch 107/200, Batch 43/45, Loss: 0.23835834860801697\nEpoch 107/200, Batch 44/45, Loss: 0.330111563205719\nEpoch 107/200, Batch 45/45, Loss: 0.329630970954895\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  8.085251688957214 Best Val MSE:  5.2462038397789\nEpoch:  108 , Time Elapsed:  17.220193473498025  mins\nEpoch 108/200, Batch 1/45, Loss: 0.23840785026550293\nEpoch 108/200, Batch 2/45, Loss: 0.2884145975112915\nEpoch 108/200, Batch 3/45, Loss: 0.18787828087806702\nEpoch 108/200, Batch 4/45, Loss: 0.21216712892055511\nEpoch 108/200, Batch 5/45, Loss: 0.23002484440803528\nEpoch 108/200, Batch 6/45, Loss: 0.3231028616428375\nEpoch 108/200, Batch 7/45, Loss: 0.19473928213119507\nEpoch 108/200, Batch 8/45, Loss: 0.4849388003349304\nEpoch 108/200, Batch 9/45, Loss: 0.26337987184524536\nEpoch 108/200, Batch 10/45, Loss: 0.4649038314819336\nEpoch 108/200, Batch 11/45, Loss: 0.2046239674091339\nEpoch 108/200, Batch 12/45, Loss: 0.6990095376968384\nEpoch 108/200, Batch 13/45, Loss: 0.42587438225746155\nEpoch 108/200, Batch 14/45, Loss: 0.6104665994644165\nEpoch 108/200, Batch 15/45, Loss: 0.4286839962005615\nEpoch 108/200, Batch 16/45, Loss: 0.5492597222328186\nEpoch 108/200, Batch 17/45, Loss: 0.529059886932373\nEpoch 108/200, Batch 18/45, Loss: 0.19659657776355743\nEpoch 108/200, Batch 19/45, Loss: 0.2287898063659668\nEpoch 108/200, Batch 20/45, Loss: 0.27599596977233887\nEpoch 108/200, Batch 21/45, Loss: 0.22359797358512878\nEpoch 108/200, Batch 22/45, Loss: 0.19489037990570068\nEpoch 108/200, Batch 23/45, Loss: 0.2882999777793884\nEpoch 108/200, Batch 24/45, Loss: 0.6359034180641174\nEpoch 108/200, Batch 25/45, Loss: 0.46954214572906494\nEpoch 108/200, Batch 26/45, Loss: 0.2698686718940735\nEpoch 108/200, Batch 27/45, Loss: 0.26197269558906555\nEpoch 108/200, Batch 28/45, Loss: 0.39038780331611633\nEpoch 108/200, Batch 29/45, Loss: 0.27387329936027527\nEpoch 108/200, Batch 30/45, Loss: 0.28907305002212524\nEpoch 108/200, Batch 31/45, Loss: 0.6404988765716553\nEpoch 108/200, Batch 32/45, Loss: 0.8481001257896423\nEpoch 108/200, Batch 33/45, Loss: 0.4390624761581421\nEpoch 108/200, Batch 34/45, Loss: 0.8605587482452393\nEpoch 108/200, Batch 35/45, Loss: 0.3180254101753235\nEpoch 108/200, Batch 36/45, Loss: 0.2444794774055481\nEpoch 108/200, Batch 37/45, Loss: 0.30704832077026367\nEpoch 108/200, Batch 38/45, Loss: 0.2821515202522278\nEpoch 108/200, Batch 39/45, Loss: 1.9455161094665527\nEpoch 108/200, Batch 40/45, Loss: 0.28206148743629456\nEpoch 108/200, Batch 41/45, Loss: 0.48565518856048584\nEpoch 108/200, Batch 42/45, Loss: 0.2505134642124176\nEpoch 108/200, Batch 43/45, Loss: 0.35483789443969727\nEpoch 108/200, Batch 44/45, Loss: 0.30289143323898315\nEpoch 108/200, Batch 45/45, Loss: 0.45661279559135437\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  8.150895774364471 Best Val MSE:  5.2462038397789\nEpoch:  109 , Time Elapsed:  17.3819340467453  mins\nEpoch 109/200, Batch 1/45, Loss: 0.5745896100997925\nEpoch 109/200, Batch 2/45, Loss: 0.23921877145767212\nEpoch 109/200, Batch 3/45, Loss: 0.18302373588085175\nEpoch 109/200, Batch 4/45, Loss: 0.32123273611068726\nEpoch 109/200, Batch 5/45, Loss: 0.19899898767471313\nEpoch 109/200, Batch 6/45, Loss: 0.3666667938232422\nEpoch 109/200, Batch 7/45, Loss: 0.4585039019584656\nEpoch 109/200, Batch 8/45, Loss: 0.5164819955825806\nEpoch 109/200, Batch 9/45, Loss: 0.29208946228027344\nEpoch 109/200, Batch 10/45, Loss: 0.1926310807466507\nEpoch 109/200, Batch 11/45, Loss: 0.37106844782829285\nEpoch 109/200, Batch 12/45, Loss: 0.4951208829879761\nEpoch 109/200, Batch 13/45, Loss: 0.15549498796463013\nEpoch 109/200, Batch 14/45, Loss: 0.37399911880493164\nEpoch 109/200, Batch 15/45, Loss: 0.26535701751708984\nEpoch 109/200, Batch 16/45, Loss: 0.35261017084121704\nEpoch 109/200, Batch 17/45, Loss: 0.2100626528263092\nEpoch 109/200, Batch 18/45, Loss: 0.21284526586532593\nEpoch 109/200, Batch 19/45, Loss: 0.20072823762893677\nEpoch 109/200, Batch 20/45, Loss: 0.24046170711517334\nEpoch 109/200, Batch 21/45, Loss: 0.18167276680469513\nEpoch 109/200, Batch 22/45, Loss: 0.1927502155303955\nEpoch 109/200, Batch 23/45, Loss: 0.17748390138149261\nEpoch 109/200, Batch 24/45, Loss: 0.2646771967411041\nEpoch 109/200, Batch 25/45, Loss: 0.3245190382003784\nEpoch 109/200, Batch 26/45, Loss: 0.2834530770778656\nEpoch 109/200, Batch 27/45, Loss: 0.1794779747724533\nEpoch 109/200, Batch 28/45, Loss: 0.2349802553653717\nEpoch 109/200, Batch 29/45, Loss: 0.2630201280117035\nEpoch 109/200, Batch 30/45, Loss: 0.17540597915649414\nEpoch 109/200, Batch 31/45, Loss: 0.3513205051422119\nEpoch 109/200, Batch 32/45, Loss: 0.27552205324172974\nEpoch 109/200, Batch 33/45, Loss: 0.34471890330314636\nEpoch 109/200, Batch 34/45, Loss: 0.4133536219596863\nEpoch 109/200, Batch 35/45, Loss: 0.21655891835689545\nEpoch 109/200, Batch 36/45, Loss: 0.2322070598602295\nEpoch 109/200, Batch 37/45, Loss: 0.5212116837501526\nEpoch 109/200, Batch 38/45, Loss: 0.21175909042358398\nEpoch 109/200, Batch 39/45, Loss: 0.25130119919776917\nEpoch 109/200, Batch 40/45, Loss: 0.2187139391899109\nEpoch 109/200, Batch 41/45, Loss: 0.2777521014213562\nEpoch 109/200, Batch 42/45, Loss: 0.2324727177619934\nEpoch 109/200, Batch 43/45, Loss: 0.18692216277122498\nEpoch 109/200, Batch 44/45, Loss: 0.193023681640625\nEpoch 109/200, Batch 45/45, Loss: 0.2967369258403778\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  7.940217077732086 Best Val MSE:  5.2462038397789\nEpoch:  110 , Time Elapsed:  17.539444343249002  mins\nEpoch 110/200, Batch 1/45, Loss: 0.6420955061912537\nEpoch 110/200, Batch 2/45, Loss: 0.2741430401802063\nEpoch 110/200, Batch 3/45, Loss: 0.20232275128364563\nEpoch 110/200, Batch 4/45, Loss: 0.1786103993654251\nEpoch 110/200, Batch 5/45, Loss: 0.1264488697052002\nEpoch 110/200, Batch 6/45, Loss: 0.25969958305358887\nEpoch 110/200, Batch 7/45, Loss: 0.6740639209747314\nEpoch 110/200, Batch 8/45, Loss: 0.4004135727882385\nEpoch 110/200, Batch 9/45, Loss: 0.31745293736457825\nEpoch 110/200, Batch 10/45, Loss: 0.34718501567840576\nEpoch 110/200, Batch 11/45, Loss: 0.16304200887680054\nEpoch 110/200, Batch 12/45, Loss: 0.301950603723526\nEpoch 110/200, Batch 13/45, Loss: 0.26248645782470703\nEpoch 110/200, Batch 14/45, Loss: 0.3281272351741791\nEpoch 110/200, Batch 15/45, Loss: 0.2740028202533722\nEpoch 110/200, Batch 16/45, Loss: 0.21662697196006775\nEpoch 110/200, Batch 17/45, Loss: 0.30246901512145996\nEpoch 110/200, Batch 18/45, Loss: 0.18842637538909912\nEpoch 110/200, Batch 19/45, Loss: 0.3126106262207031\nEpoch 110/200, Batch 20/45, Loss: 0.2636835277080536\nEpoch 110/200, Batch 21/45, Loss: 0.26782503724098206\nEpoch 110/200, Batch 22/45, Loss: 0.199584499001503\nEpoch 110/200, Batch 23/45, Loss: 0.23515960574150085\nEpoch 110/200, Batch 24/45, Loss: 0.25932884216308594\nEpoch 110/200, Batch 25/45, Loss: 0.2942464351654053\nEpoch 110/200, Batch 26/45, Loss: 0.18213041126728058\nEpoch 110/200, Batch 27/45, Loss: 0.33259516954421997\nEpoch 110/200, Batch 28/45, Loss: 0.4242731034755707\nEpoch 110/200, Batch 29/45, Loss: 0.4127518832683563\nEpoch 110/200, Batch 30/45, Loss: 0.2564380168914795\nEpoch 110/200, Batch 31/45, Loss: 0.4122699797153473\nEpoch 110/200, Batch 32/45, Loss: 0.17261874675750732\nEpoch 110/200, Batch 33/45, Loss: 0.2477349042892456\nEpoch 110/200, Batch 34/45, Loss: 0.2687855660915375\nEpoch 110/200, Batch 35/45, Loss: 0.1812533438205719\nEpoch 110/200, Batch 36/45, Loss: 0.1993427276611328\nEpoch 110/200, Batch 37/45, Loss: 0.566409170627594\nEpoch 110/200, Batch 38/45, Loss: 0.4828318655490875\nEpoch 110/200, Batch 39/45, Loss: 0.26365411281585693\nEpoch 110/200, Batch 40/45, Loss: 0.1638127863407135\nEpoch 110/200, Batch 41/45, Loss: 0.24635088443756104\nEpoch 110/200, Batch 42/45, Loss: 0.2700780928134918\nEpoch 110/200, Batch 43/45, Loss: 0.30012214183807373\nEpoch 110/200, Batch 44/45, Loss: 0.39933449029922485\nEpoch 110/200, Batch 45/45, Loss: 0.23715560138225555\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  9.204656720161438 Best Val MSE:  5.2462038397789\nEpoch:  111 , Time Elapsed:  17.693203977743785  mins\nEpoch 111/200, Batch 1/45, Loss: 0.5031555891036987\nEpoch 111/200, Batch 2/45, Loss: 0.20251676440238953\nEpoch 111/200, Batch 3/45, Loss: 0.2602255940437317\nEpoch 111/200, Batch 4/45, Loss: 0.8850515484809875\nEpoch 111/200, Batch 5/45, Loss: 0.19031588733196259\nEpoch 111/200, Batch 6/45, Loss: 0.2322712540626526\nEpoch 111/200, Batch 7/45, Loss: 0.3349853754043579\nEpoch 111/200, Batch 8/45, Loss: 0.2819545567035675\nEpoch 111/200, Batch 9/45, Loss: 0.3263659179210663\nEpoch 111/200, Batch 10/45, Loss: 0.42040902376174927\nEpoch 111/200, Batch 11/45, Loss: 0.2932579517364502\nEpoch 111/200, Batch 12/45, Loss: 0.2589217722415924\nEpoch 111/200, Batch 13/45, Loss: 0.4526658058166504\nEpoch 111/200, Batch 14/45, Loss: 0.2660228908061981\nEpoch 111/200, Batch 15/45, Loss: 0.24230721592903137\nEpoch 111/200, Batch 16/45, Loss: 0.21791744232177734\nEpoch 111/200, Batch 17/45, Loss: 0.2858026623725891\nEpoch 111/200, Batch 18/45, Loss: 0.28091055154800415\nEpoch 111/200, Batch 19/45, Loss: 0.3734709620475769\nEpoch 111/200, Batch 20/45, Loss: 0.2795448303222656\nEpoch 111/200, Batch 21/45, Loss: 0.5981183052062988\nEpoch 111/200, Batch 22/45, Loss: 0.2392829954624176\nEpoch 111/200, Batch 23/45, Loss: 0.13170932233333588\nEpoch 111/200, Batch 24/45, Loss: 0.24958933889865875\nEpoch 111/200, Batch 25/45, Loss: 0.1490001678466797\nEpoch 111/200, Batch 26/45, Loss: 0.18598687648773193\nEpoch 111/200, Batch 27/45, Loss: 0.18050619959831238\nEpoch 111/200, Batch 28/45, Loss: 0.42554014921188354\nEpoch 111/200, Batch 29/45, Loss: 0.1891244798898697\nEpoch 111/200, Batch 30/45, Loss: 0.18530356884002686\nEpoch 111/200, Batch 31/45, Loss: 0.19790704548358917\nEpoch 111/200, Batch 32/45, Loss: 0.14329180121421814\nEpoch 111/200, Batch 33/45, Loss: 0.2885723114013672\nEpoch 111/200, Batch 34/45, Loss: 0.2547650933265686\nEpoch 111/200, Batch 35/45, Loss: 0.20312093198299408\nEpoch 111/200, Batch 36/45, Loss: 0.388213187456131\nEpoch 111/200, Batch 37/45, Loss: 1.1087629795074463\nEpoch 111/200, Batch 38/45, Loss: 0.3463079333305359\nEpoch 111/200, Batch 39/45, Loss: 0.2617235481739044\nEpoch 111/200, Batch 40/45, Loss: 0.22849516570568085\nEpoch 111/200, Batch 41/45, Loss: 0.34912052750587463\nEpoch 111/200, Batch 42/45, Loss: 0.31711000204086304\nEpoch 111/200, Batch 43/45, Loss: 0.18047165870666504\nEpoch 111/200, Batch 44/45, Loss: 0.3724067807197571\nEpoch 111/200, Batch 45/45, Loss: 0.2519901394844055\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  8.60072386264801 Best Val MSE:  5.2462038397789\nEpoch:  112 , Time Elapsed:  17.85405988295873  mins\nEpoch 112/200, Batch 1/45, Loss: 0.304021954536438\nEpoch 112/200, Batch 2/45, Loss: 0.34251585602760315\nEpoch 112/200, Batch 3/45, Loss: 0.20531141757965088\nEpoch 112/200, Batch 4/45, Loss: 0.30379176139831543\nEpoch 112/200, Batch 5/45, Loss: 0.27550750970840454\nEpoch 112/200, Batch 6/45, Loss: 0.27622148394584656\nEpoch 112/200, Batch 7/45, Loss: 0.22365865111351013\nEpoch 112/200, Batch 8/45, Loss: 0.3333233594894409\nEpoch 112/200, Batch 9/45, Loss: 0.3031945824623108\nEpoch 112/200, Batch 10/45, Loss: 0.30335134267807007\nEpoch 112/200, Batch 11/45, Loss: 0.7571191787719727\nEpoch 112/200, Batch 12/45, Loss: 0.5432531833648682\nEpoch 112/200, Batch 13/45, Loss: 0.31712958216667175\nEpoch 112/200, Batch 14/45, Loss: 0.19387449324131012\nEpoch 112/200, Batch 15/45, Loss: 0.35611292719841003\nEpoch 112/200, Batch 16/45, Loss: 0.3006460666656494\nEpoch 112/200, Batch 17/45, Loss: 0.17012172937393188\nEpoch 112/200, Batch 18/45, Loss: 0.37252604961395264\nEpoch 112/200, Batch 19/45, Loss: 0.34380626678466797\nEpoch 112/200, Batch 20/45, Loss: 0.15487618744373322\nEpoch 112/200, Batch 21/45, Loss: 0.21268385648727417\nEpoch 112/200, Batch 22/45, Loss: 0.39811286330223083\nEpoch 112/200, Batch 23/45, Loss: 0.39290332794189453\nEpoch 112/200, Batch 24/45, Loss: 0.5035735964775085\nEpoch 112/200, Batch 25/45, Loss: 0.20096254348754883\nEpoch 112/200, Batch 26/45, Loss: 0.2584129571914673\nEpoch 112/200, Batch 27/45, Loss: 0.19651982188224792\nEpoch 112/200, Batch 28/45, Loss: 0.21145471930503845\nEpoch 112/200, Batch 29/45, Loss: 0.6464569568634033\nEpoch 112/200, Batch 30/45, Loss: 0.5141448974609375\nEpoch 112/200, Batch 31/45, Loss: 0.281294584274292\nEpoch 112/200, Batch 32/45, Loss: 0.2619178295135498\nEpoch 112/200, Batch 33/45, Loss: 0.23225495219230652\nEpoch 112/200, Batch 34/45, Loss: 0.26412317156791687\nEpoch 112/200, Batch 35/45, Loss: 0.2461526244878769\nEpoch 112/200, Batch 36/45, Loss: 0.20205873250961304\nEpoch 112/200, Batch 37/45, Loss: 0.6168858408927917\nEpoch 112/200, Batch 38/45, Loss: 0.2588242292404175\nEpoch 112/200, Batch 39/45, Loss: 0.20216727256774902\nEpoch 112/200, Batch 40/45, Loss: 0.1999981701374054\nEpoch 112/200, Batch 41/45, Loss: 0.27922123670578003\nEpoch 112/200, Batch 42/45, Loss: 0.28051289916038513\nEpoch 112/200, Batch 43/45, Loss: 0.49713316559791565\nEpoch 112/200, Batch 44/45, Loss: 0.14346876740455627\nEpoch 112/200, Batch 45/45, Loss: 0.3610653877258301\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  5.8934871554374695 Best Val MSE:  5.2462038397789\nEpoch:  113 , Time Elapsed:  18.009069335460662  mins\nEpoch 113/200, Batch 1/45, Loss: 0.24208074808120728\nEpoch 113/200, Batch 2/45, Loss: 0.3241974711418152\nEpoch 113/200, Batch 3/45, Loss: 0.2081427127122879\nEpoch 113/200, Batch 4/45, Loss: 0.40082651376724243\nEpoch 113/200, Batch 5/45, Loss: 0.26599085330963135\nEpoch 113/200, Batch 6/45, Loss: 0.4733286499977112\nEpoch 113/200, Batch 7/45, Loss: 0.2917020916938782\nEpoch 113/200, Batch 8/45, Loss: 0.35563310980796814\nEpoch 113/200, Batch 9/45, Loss: 0.24146747589111328\nEpoch 113/200, Batch 10/45, Loss: 0.3209467828273773\nEpoch 113/200, Batch 11/45, Loss: 0.23818379640579224\nEpoch 113/200, Batch 12/45, Loss: 0.2059982270002365\nEpoch 113/200, Batch 13/45, Loss: 0.22810952365398407\nEpoch 113/200, Batch 14/45, Loss: 0.3358418345451355\nEpoch 113/200, Batch 15/45, Loss: 0.34463071823120117\nEpoch 113/200, Batch 16/45, Loss: 0.3676173686981201\nEpoch 113/200, Batch 17/45, Loss: 0.2761602997779846\nEpoch 113/200, Batch 18/45, Loss: 0.2252732813358307\nEpoch 113/200, Batch 19/45, Loss: 0.2823178470134735\nEpoch 113/200, Batch 20/45, Loss: 0.20674259960651398\nEpoch 113/200, Batch 21/45, Loss: 0.2282971888780594\nEpoch 113/200, Batch 22/45, Loss: 0.41252046823501587\nEpoch 113/200, Batch 23/45, Loss: 0.742594838142395\nEpoch 113/200, Batch 24/45, Loss: 0.23728908598423004\nEpoch 113/200, Batch 25/45, Loss: 0.2252710461616516\nEpoch 113/200, Batch 26/45, Loss: 0.18070700764656067\nEpoch 113/200, Batch 27/45, Loss: 0.19556644558906555\nEpoch 113/200, Batch 28/45, Loss: 0.3623964786529541\nEpoch 113/200, Batch 29/45, Loss: 0.223436176776886\nEpoch 113/200, Batch 30/45, Loss: 0.19211676716804504\nEpoch 113/200, Batch 31/45, Loss: 0.6048043370246887\nEpoch 113/200, Batch 32/45, Loss: 0.21136295795440674\nEpoch 113/200, Batch 33/45, Loss: 0.42180562019348145\nEpoch 113/200, Batch 34/45, Loss: 0.22280654311180115\nEpoch 113/200, Batch 35/45, Loss: 0.18895302712917328\nEpoch 113/200, Batch 36/45, Loss: 0.24420498311519623\nEpoch 113/200, Batch 37/45, Loss: 0.28091517090797424\nEpoch 113/200, Batch 38/45, Loss: 0.2499508559703827\nEpoch 113/200, Batch 39/45, Loss: 0.18013125658035278\nEpoch 113/200, Batch 40/45, Loss: 0.21929951012134552\nEpoch 113/200, Batch 41/45, Loss: 0.22411352396011353\nEpoch 113/200, Batch 42/45, Loss: 0.17114140093326569\nEpoch 113/200, Batch 43/45, Loss: 0.2997872829437256\nEpoch 113/200, Batch 44/45, Loss: 0.2757355570793152\nEpoch 113/200, Batch 45/45, Loss: 0.1569463014602661\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  7.7334389090538025 Best Val MSE:  5.2462038397789\nEpoch:  114 , Time Elapsed:  18.162781604131062  mins\nEpoch 114/200, Batch 1/45, Loss: 0.25592291355133057\nEpoch 114/200, Batch 2/45, Loss: 0.19262707233428955\nEpoch 114/200, Batch 3/45, Loss: 0.2659427523612976\nEpoch 114/200, Batch 4/45, Loss: 0.217683345079422\nEpoch 114/200, Batch 5/45, Loss: 0.39769479632377625\nEpoch 114/200, Batch 6/45, Loss: 0.13190563023090363\nEpoch 114/200, Batch 7/45, Loss: 0.2014807015657425\nEpoch 114/200, Batch 8/45, Loss: 0.22020858526229858\nEpoch 114/200, Batch 9/45, Loss: 0.264614999294281\nEpoch 114/200, Batch 10/45, Loss: 0.3307521939277649\nEpoch 114/200, Batch 11/45, Loss: 0.1665225625038147\nEpoch 114/200, Batch 12/45, Loss: 0.18035092949867249\nEpoch 114/200, Batch 13/45, Loss: 0.4868735671043396\nEpoch 114/200, Batch 14/45, Loss: 0.5659443140029907\nEpoch 114/200, Batch 15/45, Loss: 0.5046859979629517\nEpoch 114/200, Batch 16/45, Loss: 0.29985207319259644\nEpoch 114/200, Batch 17/45, Loss: 0.1608860343694687\nEpoch 114/200, Batch 18/45, Loss: 0.3703846335411072\nEpoch 114/200, Batch 19/45, Loss: 0.2381618320941925\nEpoch 114/200, Batch 20/45, Loss: 0.07114492356777191\nEpoch 114/200, Batch 21/45, Loss: 0.28786009550094604\nEpoch 114/200, Batch 22/45, Loss: 0.2330160140991211\nEpoch 114/200, Batch 23/45, Loss: 0.13455718755722046\nEpoch 114/200, Batch 24/45, Loss: 0.28856194019317627\nEpoch 114/200, Batch 25/45, Loss: 0.32541555166244507\nEpoch 114/200, Batch 26/45, Loss: 0.21730253100395203\nEpoch 114/200, Batch 27/45, Loss: 0.26432833075523376\nEpoch 114/200, Batch 28/45, Loss: 0.1538352519273758\nEpoch 114/200, Batch 29/45, Loss: 0.198903888463974\nEpoch 114/200, Batch 30/45, Loss: 0.34855884313583374\nEpoch 114/200, Batch 31/45, Loss: 0.33654171228408813\nEpoch 114/200, Batch 32/45, Loss: 0.3481268286705017\nEpoch 114/200, Batch 33/45, Loss: 0.2132817655801773\nEpoch 114/200, Batch 34/45, Loss: 0.23193518817424774\nEpoch 114/200, Batch 35/45, Loss: 0.19718775153160095\nEpoch 114/200, Batch 36/45, Loss: 0.34247294068336487\nEpoch 114/200, Batch 37/45, Loss: 0.17676709592342377\nEpoch 114/200, Batch 38/45, Loss: 0.49850529432296753\nEpoch 114/200, Batch 39/45, Loss: 0.16970400512218475\nEpoch 114/200, Batch 40/45, Loss: 0.8405027389526367\nEpoch 114/200, Batch 41/45, Loss: 0.16696713864803314\nEpoch 114/200, Batch 42/45, Loss: 0.4326428771018982\nEpoch 114/200, Batch 43/45, Loss: 0.20409546792507172\nEpoch 114/200, Batch 44/45, Loss: 0.5085610151290894\nEpoch 114/200, Batch 45/45, Loss: 0.49694907665252686\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  12.691134214401245 Best Val MSE:  5.2462038397789\nEpoch:  115 , Time Elapsed:  18.317189761002858  mins\nEpoch 115/200, Batch 1/45, Loss: 0.24536290764808655\nEpoch 115/200, Batch 2/45, Loss: 0.4322167932987213\nEpoch 115/200, Batch 3/45, Loss: 0.2656280994415283\nEpoch 115/200, Batch 4/45, Loss: 0.5226125717163086\nEpoch 115/200, Batch 5/45, Loss: 0.3536970019340515\nEpoch 115/200, Batch 6/45, Loss: 0.36189043521881104\nEpoch 115/200, Batch 7/45, Loss: 0.1907910406589508\nEpoch 115/200, Batch 8/45, Loss: 0.24960604310035706\nEpoch 115/200, Batch 9/45, Loss: 0.26980677247047424\nEpoch 115/200, Batch 10/45, Loss: 0.14911772310733795\nEpoch 115/200, Batch 11/45, Loss: 0.2418408989906311\nEpoch 115/200, Batch 12/45, Loss: 0.39624303579330444\nEpoch 115/200, Batch 13/45, Loss: 0.3319609761238098\nEpoch 115/200, Batch 14/45, Loss: 0.2558991014957428\nEpoch 115/200, Batch 15/45, Loss: 0.362737238407135\nEpoch 115/200, Batch 16/45, Loss: 0.5139782428741455\nEpoch 115/200, Batch 17/45, Loss: 0.4454553723335266\nEpoch 115/200, Batch 18/45, Loss: 0.16823378205299377\nEpoch 115/200, Batch 19/45, Loss: 0.30932027101516724\nEpoch 115/200, Batch 20/45, Loss: 0.6587163209915161\nEpoch 115/200, Batch 21/45, Loss: 0.251107782125473\nEpoch 115/200, Batch 22/45, Loss: 0.38131001591682434\nEpoch 115/200, Batch 23/45, Loss: 0.1976536512374878\nEpoch 115/200, Batch 24/45, Loss: 0.38922959566116333\nEpoch 115/200, Batch 25/45, Loss: 0.3570926785469055\nEpoch 115/200, Batch 26/45, Loss: 0.2430819720029831\nEpoch 115/200, Batch 27/45, Loss: 0.3861953616142273\nEpoch 115/200, Batch 28/45, Loss: 0.2782602310180664\nEpoch 115/200, Batch 29/45, Loss: 0.2955976128578186\nEpoch 115/200, Batch 30/45, Loss: 0.20425522327423096\nEpoch 115/200, Batch 31/45, Loss: 0.21400123834609985\nEpoch 115/200, Batch 32/45, Loss: 0.2822403311729431\nEpoch 115/200, Batch 33/45, Loss: 0.23795557022094727\nEpoch 115/200, Batch 34/45, Loss: 0.27619192004203796\nEpoch 115/200, Batch 35/45, Loss: 0.46531516313552856\nEpoch 115/200, Batch 36/45, Loss: 0.11780349910259247\nEpoch 115/200, Batch 37/45, Loss: 0.2540111243724823\nEpoch 115/200, Batch 38/45, Loss: 0.27403661608695984\nEpoch 115/200, Batch 39/45, Loss: 0.3599332571029663\nEpoch 115/200, Batch 40/45, Loss: 0.2145083248615265\nEpoch 115/200, Batch 41/45, Loss: 0.30536335706710815\nEpoch 115/200, Batch 42/45, Loss: 0.2849307954311371\nEpoch 115/200, Batch 43/45, Loss: 0.2106306552886963\nEpoch 115/200, Batch 44/45, Loss: 0.18634524941444397\nEpoch 115/200, Batch 45/45, Loss: 0.21960170567035675\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  14.565146118402481 Best Val MSE:  5.2462038397789\nEpoch:  116 , Time Elapsed:  18.479149429003396  mins\nEpoch 116/200, Batch 1/45, Loss: 0.15122219920158386\nEpoch 116/200, Batch 2/45, Loss: 0.3207983374595642\nEpoch 116/200, Batch 3/45, Loss: 0.6750076413154602\nEpoch 116/200, Batch 4/45, Loss: 0.283639132976532\nEpoch 116/200, Batch 5/45, Loss: 0.35833412408828735\nEpoch 116/200, Batch 6/45, Loss: 0.15550075471401215\nEpoch 116/200, Batch 7/45, Loss: 0.17373508214950562\nEpoch 116/200, Batch 8/45, Loss: 0.2435581237077713\nEpoch 116/200, Batch 9/45, Loss: 0.29686254262924194\nEpoch 116/200, Batch 10/45, Loss: 0.21000464260578156\nEpoch 116/200, Batch 11/45, Loss: 0.18531270325183868\nEpoch 116/200, Batch 12/45, Loss: 0.5130676627159119\nEpoch 116/200, Batch 13/45, Loss: 0.18354849517345428\nEpoch 116/200, Batch 14/45, Loss: 0.30833700299263\nEpoch 116/200, Batch 15/45, Loss: 0.2516670227050781\nEpoch 116/200, Batch 16/45, Loss: 0.2497992366552353\nEpoch 116/200, Batch 17/45, Loss: 0.28265368938446045\nEpoch 116/200, Batch 18/45, Loss: 0.2167748510837555\nEpoch 116/200, Batch 19/45, Loss: 0.20334503054618835\nEpoch 116/200, Batch 20/45, Loss: 0.12048210203647614\nEpoch 116/200, Batch 21/45, Loss: 0.2756640911102295\nEpoch 116/200, Batch 22/45, Loss: 0.22986982762813568\nEpoch 116/200, Batch 23/45, Loss: 0.12873812019824982\nEpoch 116/200, Batch 24/45, Loss: 0.31734979152679443\nEpoch 116/200, Batch 25/45, Loss: 0.2930276691913605\nEpoch 116/200, Batch 26/45, Loss: 0.3110758066177368\nEpoch 116/200, Batch 27/45, Loss: 0.16517534852027893\nEpoch 116/200, Batch 28/45, Loss: 0.2953440546989441\nEpoch 116/200, Batch 29/45, Loss: 0.26723361015319824\nEpoch 116/200, Batch 30/45, Loss: 0.3713178038597107\nEpoch 116/200, Batch 31/45, Loss: 0.14163503050804138\nEpoch 116/200, Batch 32/45, Loss: 0.3023059666156769\nEpoch 116/200, Batch 33/45, Loss: 0.5170718431472778\nEpoch 116/200, Batch 34/45, Loss: 0.1532459557056427\nEpoch 116/200, Batch 35/45, Loss: 0.3196755051612854\nEpoch 116/200, Batch 36/45, Loss: 0.40526872873306274\nEpoch 116/200, Batch 37/45, Loss: 0.24773359298706055\nEpoch 116/200, Batch 38/45, Loss: 0.22760136425495148\nEpoch 116/200, Batch 39/45, Loss: 0.2681945562362671\nEpoch 116/200, Batch 40/45, Loss: 0.29155996441841125\nEpoch 116/200, Batch 41/45, Loss: 0.2251264750957489\nEpoch 116/200, Batch 42/45, Loss: 0.26669690012931824\nEpoch 116/200, Batch 43/45, Loss: 0.3201615810394287\nEpoch 116/200, Batch 44/45, Loss: 0.20583869516849518\nEpoch 116/200, Batch 45/45, Loss: 0.14639964699745178\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  8.142037987709045 Best Val MSE:  5.2462038397789\nEpoch:  117 , Time Elapsed:  18.635698107878365  mins\nEpoch 117/200, Batch 1/45, Loss: 0.24872426688671112\nEpoch 117/200, Batch 2/45, Loss: 0.31459325551986694\nEpoch 117/200, Batch 3/45, Loss: 0.2077215164899826\nEpoch 117/200, Batch 4/45, Loss: 0.19880129396915436\nEpoch 117/200, Batch 5/45, Loss: 0.21456560492515564\nEpoch 117/200, Batch 6/45, Loss: 0.2597282826900482\nEpoch 117/200, Batch 7/45, Loss: 0.2655101716518402\nEpoch 117/200, Batch 8/45, Loss: 0.19338317215442657\nEpoch 117/200, Batch 9/45, Loss: 0.23617497086524963\nEpoch 117/200, Batch 10/45, Loss: 0.3363254964351654\nEpoch 117/200, Batch 11/45, Loss: 0.2886752486228943\nEpoch 117/200, Batch 12/45, Loss: 0.270158976316452\nEpoch 117/200, Batch 13/45, Loss: 0.28622716665267944\nEpoch 117/200, Batch 14/45, Loss: 0.26167917251586914\nEpoch 117/200, Batch 15/45, Loss: 0.3515295088291168\nEpoch 117/200, Batch 16/45, Loss: 0.23438948392868042\nEpoch 117/200, Batch 17/45, Loss: 0.36360669136047363\nEpoch 117/200, Batch 18/45, Loss: 0.26214417815208435\nEpoch 117/200, Batch 19/45, Loss: 0.2676846981048584\nEpoch 117/200, Batch 20/45, Loss: 0.11778096109628677\nEpoch 117/200, Batch 21/45, Loss: 0.35457319021224976\nEpoch 117/200, Batch 22/45, Loss: 0.5478247404098511\nEpoch 117/200, Batch 23/45, Loss: 0.22211989760398865\nEpoch 117/200, Batch 24/45, Loss: 0.24020612239837646\nEpoch 117/200, Batch 25/45, Loss: 0.3644668459892273\nEpoch 117/200, Batch 26/45, Loss: 0.2308720350265503\nEpoch 117/200, Batch 27/45, Loss: 0.14495782554149628\nEpoch 117/200, Batch 28/45, Loss: 0.30285483598709106\nEpoch 117/200, Batch 29/45, Loss: 0.4251312017440796\nEpoch 117/200, Batch 30/45, Loss: 0.9817568063735962\nEpoch 117/200, Batch 31/45, Loss: 0.37771719694137573\nEpoch 117/200, Batch 32/45, Loss: 0.2901669144630432\nEpoch 117/200, Batch 33/45, Loss: 0.24499879777431488\nEpoch 117/200, Batch 34/45, Loss: 0.1867518424987793\nEpoch 117/200, Batch 35/45, Loss: 0.37516653537750244\nEpoch 117/200, Batch 36/45, Loss: 0.29640573263168335\nEpoch 117/200, Batch 37/45, Loss: 0.1615864634513855\nEpoch 117/200, Batch 38/45, Loss: 0.20691435039043427\nEpoch 117/200, Batch 39/45, Loss: 0.21070989966392517\nEpoch 117/200, Batch 40/45, Loss: 0.15322840213775635\nEpoch 117/200, Batch 41/45, Loss: 0.20198434591293335\nEpoch 117/200, Batch 42/45, Loss: 0.2577725052833557\nEpoch 117/200, Batch 43/45, Loss: 0.24295395612716675\nEpoch 117/200, Batch 44/45, Loss: 0.414558082818985\nEpoch 117/200, Batch 45/45, Loss: 0.1974571794271469\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  9.164335310459137 Best Val MSE:  5.2462038397789\nEpoch:  118 , Time Elapsed:  18.789742656548817  mins\nEpoch 118/200, Batch 1/45, Loss: 0.25878459215164185\nEpoch 118/200, Batch 2/45, Loss: 0.3717978298664093\nEpoch 118/200, Batch 3/45, Loss: 0.23776769638061523\nEpoch 118/200, Batch 4/45, Loss: 0.24367927014827728\nEpoch 118/200, Batch 5/45, Loss: 0.2292734682559967\nEpoch 118/200, Batch 6/45, Loss: 0.4183078408241272\nEpoch 118/200, Batch 7/45, Loss: 0.31898143887519836\nEpoch 118/200, Batch 8/45, Loss: 0.2501903772354126\nEpoch 118/200, Batch 9/45, Loss: 0.25137025117874146\nEpoch 118/200, Batch 10/45, Loss: 0.2811167538166046\nEpoch 118/200, Batch 11/45, Loss: 0.1704002469778061\nEpoch 118/200, Batch 12/45, Loss: 0.19801580905914307\nEpoch 118/200, Batch 13/45, Loss: 0.22965794801712036\nEpoch 118/200, Batch 14/45, Loss: 0.4944203495979309\nEpoch 118/200, Batch 15/45, Loss: 0.1727367639541626\nEpoch 118/200, Batch 16/45, Loss: 0.5459454655647278\nEpoch 118/200, Batch 17/45, Loss: 0.4432942271232605\nEpoch 118/200, Batch 18/45, Loss: 0.18443866074085236\nEpoch 118/200, Batch 19/45, Loss: 0.14809975028038025\nEpoch 118/200, Batch 20/45, Loss: 0.2334175854921341\nEpoch 118/200, Batch 21/45, Loss: 0.29550275206565857\nEpoch 118/200, Batch 22/45, Loss: 0.23021240532398224\nEpoch 118/200, Batch 23/45, Loss: 0.30606284737586975\nEpoch 118/200, Batch 24/45, Loss: 0.38482213020324707\nEpoch 118/200, Batch 25/45, Loss: 0.36795735359191895\nEpoch 118/200, Batch 26/45, Loss: 0.285900354385376\nEpoch 118/200, Batch 27/45, Loss: 0.2447056770324707\nEpoch 118/200, Batch 28/45, Loss: 0.40185943245887756\nEpoch 118/200, Batch 29/45, Loss: 0.360958456993103\nEpoch 118/200, Batch 30/45, Loss: 0.33653539419174194\nEpoch 118/200, Batch 31/45, Loss: 0.35366421937942505\nEpoch 118/200, Batch 32/45, Loss: 0.2378983050584793\nEpoch 118/200, Batch 33/45, Loss: 0.17934566736221313\nEpoch 118/200, Batch 34/45, Loss: 0.2305118441581726\nEpoch 118/200, Batch 35/45, Loss: 0.8449149131774902\nEpoch 118/200, Batch 36/45, Loss: 0.19306235015392303\nEpoch 118/200, Batch 37/45, Loss: 0.2607421576976776\nEpoch 118/200, Batch 38/45, Loss: 0.2653031349182129\nEpoch 118/200, Batch 39/45, Loss: 0.3928285837173462\nEpoch 118/200, Batch 40/45, Loss: 0.2757115960121155\nEpoch 118/200, Batch 41/45, Loss: 0.16351379454135895\nEpoch 118/200, Batch 42/45, Loss: 0.5983772873878479\nEpoch 118/200, Batch 43/45, Loss: 0.31914079189300537\nEpoch 118/200, Batch 44/45, Loss: 0.16993433237075806\nEpoch 118/200, Batch 45/45, Loss: 0.3333047926425934\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  5.835288882255554 Best Val MSE:  5.2462038397789\nEpoch:  119 , Time Elapsed:  18.949530375003814  mins\nEpoch 119/200, Batch 1/45, Loss: 0.16604529321193695\nEpoch 119/200, Batch 2/45, Loss: 0.2919806241989136\nEpoch 119/200, Batch 3/45, Loss: 0.5155692100524902\nEpoch 119/200, Batch 4/45, Loss: 0.16504335403442383\nEpoch 119/200, Batch 5/45, Loss: 0.1781231164932251\nEpoch 119/200, Batch 6/45, Loss: 0.27238139510154724\nEpoch 119/200, Batch 7/45, Loss: 0.3536260724067688\nEpoch 119/200, Batch 8/45, Loss: 0.2341424822807312\nEpoch 119/200, Batch 9/45, Loss: 0.2697281837463379\nEpoch 119/200, Batch 10/45, Loss: 0.2653752863407135\nEpoch 119/200, Batch 11/45, Loss: 0.12764272093772888\nEpoch 119/200, Batch 12/45, Loss: 0.29872971773147583\nEpoch 119/200, Batch 13/45, Loss: 0.395088255405426\nEpoch 119/200, Batch 14/45, Loss: 0.3612942397594452\nEpoch 119/200, Batch 15/45, Loss: 0.4041486382484436\nEpoch 119/200, Batch 16/45, Loss: 0.16269837319850922\nEpoch 119/200, Batch 17/45, Loss: 0.7253201007843018\nEpoch 119/200, Batch 18/45, Loss: 0.2983713746070862\nEpoch 119/200, Batch 19/45, Loss: 0.21835768222808838\nEpoch 119/200, Batch 20/45, Loss: 0.35573405027389526\nEpoch 119/200, Batch 21/45, Loss: 0.38568663597106934\nEpoch 119/200, Batch 22/45, Loss: 0.2807841897010803\nEpoch 119/200, Batch 23/45, Loss: 0.47212839126586914\nEpoch 119/200, Batch 24/45, Loss: 0.1114957258105278\nEpoch 119/200, Batch 25/45, Loss: 0.21938097476959229\nEpoch 119/200, Batch 26/45, Loss: 0.2125064879655838\nEpoch 119/200, Batch 27/45, Loss: 0.2697521448135376\nEpoch 119/200, Batch 28/45, Loss: 0.36712393164634705\nEpoch 119/200, Batch 29/45, Loss: 0.31152409315109253\nEpoch 119/200, Batch 30/45, Loss: 0.2990049719810486\nEpoch 119/200, Batch 31/45, Loss: 0.3500468134880066\nEpoch 119/200, Batch 32/45, Loss: 0.3110487163066864\nEpoch 119/200, Batch 33/45, Loss: 0.14871633052825928\nEpoch 119/200, Batch 34/45, Loss: 0.20294970273971558\nEpoch 119/200, Batch 35/45, Loss: 0.3647507131099701\nEpoch 119/200, Batch 36/45, Loss: 0.16342318058013916\nEpoch 119/200, Batch 37/45, Loss: 0.28969621658325195\nEpoch 119/200, Batch 38/45, Loss: 0.26403486728668213\nEpoch 119/200, Batch 39/45, Loss: 0.15985648334026337\nEpoch 119/200, Batch 40/45, Loss: 0.19437730312347412\nEpoch 119/200, Batch 41/45, Loss: 0.2429734468460083\nEpoch 119/200, Batch 42/45, Loss: 0.29351282119750977\nEpoch 119/200, Batch 43/45, Loss: 0.2565956115722656\nEpoch 119/200, Batch 44/45, Loss: 0.20049813389778137\nEpoch 119/200, Batch 45/45, Loss: 0.30790215730667114\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  8.595061004161835 Best Val MSE:  5.2462038397789\nEpoch:  120 , Time Elapsed:  19.106891663869224  mins\nEpoch 120/200, Batch 1/45, Loss: 0.44101423025131226\nEpoch 120/200, Batch 2/45, Loss: 0.21102403104305267\nEpoch 120/200, Batch 3/45, Loss: 0.19895699620246887\nEpoch 120/200, Batch 4/45, Loss: 0.24310961365699768\nEpoch 120/200, Batch 5/45, Loss: 0.30106571316719055\nEpoch 120/200, Batch 6/45, Loss: 0.18683359026908875\nEpoch 120/200, Batch 7/45, Loss: 0.29978710412979126\nEpoch 120/200, Batch 8/45, Loss: 0.19026491045951843\nEpoch 120/200, Batch 9/45, Loss: 0.16505277156829834\nEpoch 120/200, Batch 10/45, Loss: 0.3206979036331177\nEpoch 120/200, Batch 11/45, Loss: 0.4925028681755066\nEpoch 120/200, Batch 12/45, Loss: 0.24089981615543365\nEpoch 120/200, Batch 13/45, Loss: 0.20796698331832886\nEpoch 120/200, Batch 14/45, Loss: 0.3145587742328644\nEpoch 120/200, Batch 15/45, Loss: 0.19538332521915436\nEpoch 120/200, Batch 16/45, Loss: 0.42450547218322754\nEpoch 120/200, Batch 17/45, Loss: 0.16104623675346375\nEpoch 120/200, Batch 18/45, Loss: 0.23223130404949188\nEpoch 120/200, Batch 19/45, Loss: 0.2719687819480896\nEpoch 120/200, Batch 20/45, Loss: 0.2927287518978119\nEpoch 120/200, Batch 21/45, Loss: 0.19640010595321655\nEpoch 120/200, Batch 22/45, Loss: 0.23381781578063965\nEpoch 120/200, Batch 23/45, Loss: 0.336273193359375\nEpoch 120/200, Batch 24/45, Loss: 0.33728814125061035\nEpoch 120/200, Batch 25/45, Loss: 0.18761879205703735\nEpoch 120/200, Batch 26/45, Loss: 0.24116282165050507\nEpoch 120/200, Batch 27/45, Loss: 0.22327950596809387\nEpoch 120/200, Batch 28/45, Loss: 0.2933952808380127\nEpoch 120/200, Batch 29/45, Loss: 0.16345064342021942\nEpoch 120/200, Batch 30/45, Loss: 0.3545350432395935\nEpoch 120/200, Batch 31/45, Loss: 0.2963411211967468\nEpoch 120/200, Batch 32/45, Loss: 0.33137795329093933\nEpoch 120/200, Batch 33/45, Loss: 0.1544398069381714\nEpoch 120/200, Batch 34/45, Loss: 0.37224280834198\nEpoch 120/200, Batch 35/45, Loss: 0.21451106667518616\nEpoch 120/200, Batch 36/45, Loss: 0.39109453558921814\nEpoch 120/200, Batch 37/45, Loss: 0.2634780704975128\nEpoch 120/200, Batch 38/45, Loss: 0.31652191281318665\nEpoch 120/200, Batch 39/45, Loss: 0.1832851767539978\nEpoch 120/200, Batch 40/45, Loss: 0.2598409652709961\nEpoch 120/200, Batch 41/45, Loss: 0.23710863292217255\nEpoch 120/200, Batch 42/45, Loss: 0.19757984578609467\nEpoch 120/200, Batch 43/45, Loss: 0.23197880387306213\nEpoch 120/200, Batch 44/45, Loss: 0.3075486421585083\nEpoch 120/200, Batch 45/45, Loss: 0.3153003454208374\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  9.759276807308197 Best Val MSE:  5.2462038397789\nEpoch:  121 , Time Elapsed:  19.26458973487218  mins\nEpoch 121/200, Batch 1/45, Loss: 0.33257585763931274\nEpoch 121/200, Batch 2/45, Loss: 0.32007813453674316\nEpoch 121/200, Batch 3/45, Loss: 0.28238430619239807\nEpoch 121/200, Batch 4/45, Loss: 0.22300058603286743\nEpoch 121/200, Batch 5/45, Loss: 0.2818480134010315\nEpoch 121/200, Batch 6/45, Loss: 0.27643585205078125\nEpoch 121/200, Batch 7/45, Loss: 0.13079261779785156\nEpoch 121/200, Batch 8/45, Loss: 0.11713474988937378\nEpoch 121/200, Batch 9/45, Loss: 0.33926719427108765\nEpoch 121/200, Batch 10/45, Loss: 0.3510790765285492\nEpoch 121/200, Batch 11/45, Loss: 0.20640245079994202\nEpoch 121/200, Batch 12/45, Loss: 0.43723803758621216\nEpoch 121/200, Batch 13/45, Loss: 0.1444617509841919\nEpoch 121/200, Batch 14/45, Loss: 0.3267515301704407\nEpoch 121/200, Batch 15/45, Loss: 0.16565626859664917\nEpoch 121/200, Batch 16/45, Loss: 0.1992565393447876\nEpoch 121/200, Batch 17/45, Loss: 0.22182393074035645\nEpoch 121/200, Batch 18/45, Loss: 0.29230284690856934\nEpoch 121/200, Batch 19/45, Loss: 0.21634265780448914\nEpoch 121/200, Batch 20/45, Loss: 0.2589271664619446\nEpoch 121/200, Batch 21/45, Loss: 0.16173776984214783\nEpoch 121/200, Batch 22/45, Loss: 0.28222987055778503\nEpoch 121/200, Batch 23/45, Loss: 0.2966799736022949\nEpoch 121/200, Batch 24/45, Loss: 0.2687743008136749\nEpoch 121/200, Batch 25/45, Loss: 0.4096658527851105\nEpoch 121/200, Batch 26/45, Loss: 0.17940279841423035\nEpoch 121/200, Batch 27/45, Loss: 0.1117870882153511\nEpoch 121/200, Batch 28/45, Loss: 0.16598640382289886\nEpoch 121/200, Batch 29/45, Loss: 0.2568358778953552\nEpoch 121/200, Batch 30/45, Loss: 0.19101592898368835\nEpoch 121/200, Batch 31/45, Loss: 0.14590251445770264\nEpoch 121/200, Batch 32/45, Loss: 0.1770474761724472\nEpoch 121/200, Batch 33/45, Loss: 0.3074564039707184\nEpoch 121/200, Batch 34/45, Loss: 0.2891503572463989\nEpoch 121/200, Batch 35/45, Loss: 0.19575847685337067\nEpoch 121/200, Batch 36/45, Loss: 0.24418391287326813\nEpoch 121/200, Batch 37/45, Loss: 0.21217378973960876\nEpoch 121/200, Batch 38/45, Loss: 0.26953890919685364\nEpoch 121/200, Batch 39/45, Loss: 0.21061934530735016\nEpoch 121/200, Batch 40/45, Loss: 0.19235220551490784\nEpoch 121/200, Batch 41/45, Loss: 0.27126169204711914\nEpoch 121/200, Batch 42/45, Loss: 0.19818811118602753\nEpoch 121/200, Batch 43/45, Loss: 0.29670247435569763\nEpoch 121/200, Batch 44/45, Loss: 0.23612521588802338\nEpoch 121/200, Batch 45/45, Loss: 0.28789061307907104\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  7.683511972427368 Best Val MSE:  5.2462038397789\nEpoch:  122 , Time Elapsed:  19.422534970442452  mins\nEpoch 122/200, Batch 1/45, Loss: 0.2500130236148834\nEpoch 122/200, Batch 2/45, Loss: 0.2597659230232239\nEpoch 122/200, Batch 3/45, Loss: 0.16912809014320374\nEpoch 122/200, Batch 4/45, Loss: 0.14562751352787018\nEpoch 122/200, Batch 5/45, Loss: 0.23117074370384216\nEpoch 122/200, Batch 6/45, Loss: 0.20004749298095703\nEpoch 122/200, Batch 7/45, Loss: 0.21506905555725098\nEpoch 122/200, Batch 8/45, Loss: 0.6329432725906372\nEpoch 122/200, Batch 9/45, Loss: 0.2703339159488678\nEpoch 122/200, Batch 10/45, Loss: 0.08996178209781647\nEpoch 122/200, Batch 11/45, Loss: 0.2339082509279251\nEpoch 122/200, Batch 12/45, Loss: 0.18781647086143494\nEpoch 122/200, Batch 13/45, Loss: 0.29933542013168335\nEpoch 122/200, Batch 14/45, Loss: 0.2834387421607971\nEpoch 122/200, Batch 15/45, Loss: 0.2718169391155243\nEpoch 122/200, Batch 16/45, Loss: 0.2257567048072815\nEpoch 122/200, Batch 17/45, Loss: 0.16999515891075134\nEpoch 122/200, Batch 18/45, Loss: 0.3367305099964142\nEpoch 122/200, Batch 19/45, Loss: 0.28870099782943726\nEpoch 122/200, Batch 20/45, Loss: 0.2862343192100525\nEpoch 122/200, Batch 21/45, Loss: 0.20351466536521912\nEpoch 122/200, Batch 22/45, Loss: 0.2121645212173462\nEpoch 122/200, Batch 23/45, Loss: 0.3339412212371826\nEpoch 122/200, Batch 24/45, Loss: 0.3802257478237152\nEpoch 122/200, Batch 25/45, Loss: 0.26895108819007874\nEpoch 122/200, Batch 26/45, Loss: 0.4426916837692261\nEpoch 122/200, Batch 27/45, Loss: 0.23111973702907562\nEpoch 122/200, Batch 28/45, Loss: 0.2794405221939087\nEpoch 122/200, Batch 29/45, Loss: 0.147854283452034\nEpoch 122/200, Batch 30/45, Loss: 0.3132437467575073\nEpoch 122/200, Batch 31/45, Loss: 0.22449488937854767\nEpoch 122/200, Batch 32/45, Loss: 0.19486454129219055\nEpoch 122/200, Batch 33/45, Loss: 0.25086724758148193\nEpoch 122/200, Batch 34/45, Loss: 0.18203189969062805\nEpoch 122/200, Batch 35/45, Loss: 0.2121610790491104\nEpoch 122/200, Batch 36/45, Loss: 0.1848326176404953\nEpoch 122/200, Batch 37/45, Loss: 0.2156650274991989\nEpoch 122/200, Batch 38/45, Loss: 0.2694517970085144\nEpoch 122/200, Batch 39/45, Loss: 0.38326525688171387\nEpoch 122/200, Batch 40/45, Loss: 0.31220442056655884\nEpoch 122/200, Batch 41/45, Loss: 0.3338715434074402\nEpoch 122/200, Batch 42/45, Loss: 0.2433382272720337\nEpoch 122/200, Batch 43/45, Loss: 0.29366081953048706\nEpoch 122/200, Batch 44/45, Loss: 0.2110174596309662\nEpoch 122/200, Batch 45/45, Loss: 0.1635722517967224\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  8.590037494897842 Best Val MSE:  5.2462038397789\nEpoch:  123 , Time Elapsed:  19.58158957163493  mins\nEpoch 123/200, Batch 1/45, Loss: 0.190398707985878\nEpoch 123/200, Batch 2/45, Loss: 0.2856355309486389\nEpoch 123/200, Batch 3/45, Loss: 0.22735396027565002\nEpoch 123/200, Batch 4/45, Loss: 0.21445390582084656\nEpoch 123/200, Batch 5/45, Loss: 0.19958531856536865\nEpoch 123/200, Batch 6/45, Loss: 0.3129567503929138\nEpoch 123/200, Batch 7/45, Loss: 0.19165831804275513\nEpoch 123/200, Batch 8/45, Loss: 0.2391606867313385\nEpoch 123/200, Batch 9/45, Loss: 0.22169408202171326\nEpoch 123/200, Batch 10/45, Loss: 0.24645328521728516\nEpoch 123/200, Batch 11/45, Loss: 0.1647820770740509\nEpoch 123/200, Batch 12/45, Loss: 0.24696692824363708\nEpoch 123/200, Batch 13/45, Loss: 0.2099027931690216\nEpoch 123/200, Batch 14/45, Loss: 0.3208388686180115\nEpoch 123/200, Batch 15/45, Loss: 0.37469935417175293\nEpoch 123/200, Batch 16/45, Loss: 0.14510385692119598\nEpoch 123/200, Batch 17/45, Loss: 0.254676878452301\nEpoch 123/200, Batch 18/45, Loss: 0.252604603767395\nEpoch 123/200, Batch 19/45, Loss: 0.24597115814685822\nEpoch 123/200, Batch 20/45, Loss: 0.23852527141571045\nEpoch 123/200, Batch 21/45, Loss: 0.2867310047149658\nEpoch 123/200, Batch 22/45, Loss: 0.26391974091529846\nEpoch 123/200, Batch 23/45, Loss: 0.30435773730278015\nEpoch 123/200, Batch 24/45, Loss: 0.20757095515727997\nEpoch 123/200, Batch 25/45, Loss: 0.29756203293800354\nEpoch 123/200, Batch 26/45, Loss: 0.20655669271945953\nEpoch 123/200, Batch 27/45, Loss: 0.3518257737159729\nEpoch 123/200, Batch 28/45, Loss: 1.597267508506775\nEpoch 123/200, Batch 29/45, Loss: 0.4576297998428345\nEpoch 123/200, Batch 30/45, Loss: 0.20065519213676453\nEpoch 123/200, Batch 31/45, Loss: 0.5588206052780151\nEpoch 123/200, Batch 32/45, Loss: 0.24368897080421448\nEpoch 123/200, Batch 33/45, Loss: 0.3140146732330322\nEpoch 123/200, Batch 34/45, Loss: 0.10965801030397415\nEpoch 123/200, Batch 35/45, Loss: 0.21603459119796753\nEpoch 123/200, Batch 36/45, Loss: 0.26427245140075684\nEpoch 123/200, Batch 37/45, Loss: 0.1912945806980133\nEpoch 123/200, Batch 38/45, Loss: 0.5718242526054382\nEpoch 123/200, Batch 39/45, Loss: 0.14663930237293243\nEpoch 123/200, Batch 40/45, Loss: 0.17297962307929993\nEpoch 123/200, Batch 41/45, Loss: 0.3569372594356537\nEpoch 123/200, Batch 42/45, Loss: 0.3106452226638794\nEpoch 123/200, Batch 43/45, Loss: 0.2971823513507843\nEpoch 123/200, Batch 44/45, Loss: 0.21787677705287933\nEpoch 123/200, Batch 45/45, Loss: 0.2534916400909424\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  5.836824476718903 Best Val MSE:  5.2462038397789\nEpoch:  124 , Time Elapsed:  19.734822420279183  mins\nEpoch 124/200, Batch 1/45, Loss: 0.2707427144050598\nEpoch 124/200, Batch 2/45, Loss: 0.2630072832107544\nEpoch 124/200, Batch 3/45, Loss: 0.18924182653427124\nEpoch 124/200, Batch 4/45, Loss: 0.20396876335144043\nEpoch 124/200, Batch 5/45, Loss: 0.13007709383964539\nEpoch 124/200, Batch 6/45, Loss: 0.26229774951934814\nEpoch 124/200, Batch 7/45, Loss: 0.3650486171245575\nEpoch 124/200, Batch 8/45, Loss: 0.2376476377248764\nEpoch 124/200, Batch 9/45, Loss: 0.1425100415945053\nEpoch 124/200, Batch 10/45, Loss: 0.2507319450378418\nEpoch 124/200, Batch 11/45, Loss: 0.3317932188510895\nEpoch 124/200, Batch 12/45, Loss: 0.14450040459632874\nEpoch 124/200, Batch 13/45, Loss: 0.26374465227127075\nEpoch 124/200, Batch 14/45, Loss: 0.23437948524951935\nEpoch 124/200, Batch 15/45, Loss: 0.17814591526985168\nEpoch 124/200, Batch 16/45, Loss: 0.16180741786956787\nEpoch 124/200, Batch 17/45, Loss: 0.22794565558433533\nEpoch 124/200, Batch 18/45, Loss: 0.14827221632003784\nEpoch 124/200, Batch 19/45, Loss: 0.167681485414505\nEpoch 124/200, Batch 20/45, Loss: 0.3049996495246887\nEpoch 124/200, Batch 21/45, Loss: 0.24455788731575012\nEpoch 124/200, Batch 22/45, Loss: 0.3183867037296295\nEpoch 124/200, Batch 23/45, Loss: 0.1905539333820343\nEpoch 124/200, Batch 24/45, Loss: 0.22895145416259766\nEpoch 124/200, Batch 25/45, Loss: 0.28396350145339966\nEpoch 124/200, Batch 26/45, Loss: 0.2639613151550293\nEpoch 124/200, Batch 27/45, Loss: 0.2238788604736328\nEpoch 124/200, Batch 28/45, Loss: 0.2996756136417389\nEpoch 124/200, Batch 29/45, Loss: 0.26700183749198914\nEpoch 124/200, Batch 30/45, Loss: 0.3259526193141937\nEpoch 124/200, Batch 31/45, Loss: 0.22853684425354004\nEpoch 124/200, Batch 32/45, Loss: 0.19781357049942017\nEpoch 124/200, Batch 33/45, Loss: 0.19899490475654602\nEpoch 124/200, Batch 34/45, Loss: 0.20111191272735596\nEpoch 124/200, Batch 35/45, Loss: 0.4091145694255829\nEpoch 124/200, Batch 36/45, Loss: 0.16868241131305695\nEpoch 124/200, Batch 37/45, Loss: 0.15922997891902924\nEpoch 124/200, Batch 38/45, Loss: 0.26051846146583557\nEpoch 124/200, Batch 39/45, Loss: 0.2829953134059906\nEpoch 124/200, Batch 40/45, Loss: 0.2157403528690338\nEpoch 124/200, Batch 41/45, Loss: 0.3002476096153259\nEpoch 124/200, Batch 42/45, Loss: 0.3894648551940918\nEpoch 124/200, Batch 43/45, Loss: 0.20884054899215698\nEpoch 124/200, Batch 44/45, Loss: 0.15182718634605408\nEpoch 124/200, Batch 45/45, Loss: 0.3159556984901428\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  7.891056418418884 Best Val MSE:  5.2462038397789\nEpoch:  125 , Time Elapsed:  19.89128762880961  mins\nEpoch 125/200, Batch 1/45, Loss: 0.35174116492271423\nEpoch 125/200, Batch 2/45, Loss: 0.24623847007751465\nEpoch 125/200, Batch 3/45, Loss: 0.23844501376152039\nEpoch 125/200, Batch 4/45, Loss: 0.3118903636932373\nEpoch 125/200, Batch 5/45, Loss: 0.20805953443050385\nEpoch 125/200, Batch 6/45, Loss: 0.19407175481319427\nEpoch 125/200, Batch 7/45, Loss: 0.27525967359542847\nEpoch 125/200, Batch 8/45, Loss: 0.36323243379592896\nEpoch 125/200, Batch 9/45, Loss: 0.19364716112613678\nEpoch 125/200, Batch 10/45, Loss: 0.2300143837928772\nEpoch 125/200, Batch 11/45, Loss: 0.167366623878479\nEpoch 125/200, Batch 12/45, Loss: 0.35111719369888306\nEpoch 125/200, Batch 13/45, Loss: 0.3024062514305115\nEpoch 125/200, Batch 14/45, Loss: 0.19815456867218018\nEpoch 125/200, Batch 15/45, Loss: 0.21410541236400604\nEpoch 125/200, Batch 16/45, Loss: 0.2907699942588806\nEpoch 125/200, Batch 17/45, Loss: 0.365461528301239\nEpoch 125/200, Batch 18/45, Loss: 0.3007875382900238\nEpoch 125/200, Batch 19/45, Loss: 0.23015636205673218\nEpoch 125/200, Batch 20/45, Loss: 0.19067341089248657\nEpoch 125/200, Batch 21/45, Loss: 0.19570021331310272\nEpoch 125/200, Batch 22/45, Loss: 0.40323156118392944\nEpoch 125/200, Batch 23/45, Loss: 0.2937506437301636\nEpoch 125/200, Batch 24/45, Loss: 0.3236713409423828\nEpoch 125/200, Batch 25/45, Loss: 0.15613117814064026\nEpoch 125/200, Batch 26/45, Loss: 0.20148161053657532\nEpoch 125/200, Batch 27/45, Loss: 0.22836817800998688\nEpoch 125/200, Batch 28/45, Loss: 0.17696857452392578\nEpoch 125/200, Batch 29/45, Loss: 0.37942737340927124\nEpoch 125/200, Batch 30/45, Loss: 0.28137531876564026\nEpoch 125/200, Batch 31/45, Loss: 0.38684898614883423\nEpoch 125/200, Batch 32/45, Loss: 0.23317871987819672\nEpoch 125/200, Batch 33/45, Loss: 0.18209514021873474\nEpoch 125/200, Batch 34/45, Loss: 0.2155570238828659\nEpoch 125/200, Batch 35/45, Loss: 0.19845685362815857\nEpoch 125/200, Batch 36/45, Loss: 0.23225493729114532\nEpoch 125/200, Batch 37/45, Loss: 0.29412418603897095\nEpoch 125/200, Batch 38/45, Loss: 0.22150522470474243\nEpoch 125/200, Batch 39/45, Loss: 0.2910735011100769\nEpoch 125/200, Batch 40/45, Loss: 0.3622012734413147\nEpoch 125/200, Batch 41/45, Loss: 0.34196168184280396\nEpoch 125/200, Batch 42/45, Loss: 0.2755988836288452\nEpoch 125/200, Batch 43/45, Loss: 0.1436932384967804\nEpoch 125/200, Batch 44/45, Loss: 0.15908575057983398\nEpoch 125/200, Batch 45/45, Loss: 0.1745050996541977\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  6.020059943199158 Best Val MSE:  5.2462038397789\nEpoch:  126 , Time Elapsed:  20.05441774924596  mins\nEpoch 126/200, Batch 1/45, Loss: 0.13694074749946594\nEpoch 126/200, Batch 2/45, Loss: 0.18169116973876953\nEpoch 126/200, Batch 3/45, Loss: 0.49746763706207275\nEpoch 126/200, Batch 4/45, Loss: 0.22961610555648804\nEpoch 126/200, Batch 5/45, Loss: 0.2593494951725006\nEpoch 126/200, Batch 6/45, Loss: 0.14338931441307068\nEpoch 126/200, Batch 7/45, Loss: 0.47890955209732056\nEpoch 126/200, Batch 8/45, Loss: 0.30343836545944214\nEpoch 126/200, Batch 9/45, Loss: 0.299222469329834\nEpoch 126/200, Batch 10/45, Loss: 0.22168497741222382\nEpoch 126/200, Batch 11/45, Loss: 0.1913335621356964\nEpoch 126/200, Batch 12/45, Loss: 0.2205008864402771\nEpoch 126/200, Batch 13/45, Loss: 0.12006235867738724\nEpoch 126/200, Batch 14/45, Loss: 0.13162928819656372\nEpoch 126/200, Batch 15/45, Loss: 0.24519021809101105\nEpoch 126/200, Batch 16/45, Loss: 0.16581138968467712\nEpoch 126/200, Batch 17/45, Loss: 0.2037631869316101\nEpoch 126/200, Batch 18/45, Loss: 0.2783176004886627\nEpoch 126/200, Batch 19/45, Loss: 0.24720439314842224\nEpoch 126/200, Batch 20/45, Loss: 0.463739812374115\nEpoch 126/200, Batch 21/45, Loss: 0.24268987774848938\nEpoch 126/200, Batch 22/45, Loss: 0.32806384563446045\nEpoch 126/200, Batch 23/45, Loss: 0.19231972098350525\nEpoch 126/200, Batch 24/45, Loss: 0.10274649411439896\nEpoch 126/200, Batch 25/45, Loss: 0.22497455775737762\nEpoch 126/200, Batch 26/45, Loss: 0.33281421661376953\nEpoch 126/200, Batch 27/45, Loss: 0.15998880565166473\nEpoch 126/200, Batch 28/45, Loss: 0.2502722144126892\nEpoch 126/200, Batch 29/45, Loss: 0.27059900760650635\nEpoch 126/200, Batch 30/45, Loss: 0.1527591347694397\nEpoch 126/200, Batch 31/45, Loss: 0.0698045864701271\nEpoch 126/200, Batch 32/45, Loss: 0.22069290280342102\nEpoch 126/200, Batch 33/45, Loss: 0.27718111872673035\nEpoch 126/200, Batch 34/45, Loss: 0.22487975656986237\nEpoch 126/200, Batch 35/45, Loss: 0.7275841236114502\nEpoch 126/200, Batch 36/45, Loss: 0.18895085155963898\nEpoch 126/200, Batch 37/45, Loss: 1.0071678161621094\nEpoch 126/200, Batch 38/45, Loss: 0.3068915605545044\nEpoch 126/200, Batch 39/45, Loss: 0.22317679226398468\nEpoch 126/200, Batch 40/45, Loss: 0.09195800125598907\nEpoch 126/200, Batch 41/45, Loss: 0.22724619507789612\nEpoch 126/200, Batch 42/45, Loss: 0.23665651679039001\nEpoch 126/200, Batch 43/45, Loss: 0.37385645508766174\nEpoch 126/200, Batch 44/45, Loss: 0.259732186794281\nEpoch 126/200, Batch 45/45, Loss: 0.4232203960418701\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  8.1329765021801 Best Val MSE:  5.2462038397789\nEpoch:  127 , Time Elapsed:  20.20798379977544  mins\nEpoch 127/200, Batch 1/45, Loss: 0.23489119112491608\nEpoch 127/200, Batch 2/45, Loss: 0.22225508093833923\nEpoch 127/200, Batch 3/45, Loss: 0.25857725739479065\nEpoch 127/200, Batch 4/45, Loss: 0.23626352846622467\nEpoch 127/200, Batch 5/45, Loss: 0.26648905873298645\nEpoch 127/200, Batch 6/45, Loss: 0.235295370221138\nEpoch 127/200, Batch 7/45, Loss: 0.12431291490793228\nEpoch 127/200, Batch 8/45, Loss: 0.19352112710475922\nEpoch 127/200, Batch 9/45, Loss: 0.2229824662208557\nEpoch 127/200, Batch 10/45, Loss: 0.38786157965660095\nEpoch 127/200, Batch 11/45, Loss: 0.2665982246398926\nEpoch 127/200, Batch 12/45, Loss: 0.16351544857025146\nEpoch 127/200, Batch 13/45, Loss: 0.4320385456085205\nEpoch 127/200, Batch 14/45, Loss: 0.38953301310539246\nEpoch 127/200, Batch 15/45, Loss: 0.31104224920272827\nEpoch 127/200, Batch 16/45, Loss: 0.15532195568084717\nEpoch 127/200, Batch 17/45, Loss: 0.30639129877090454\nEpoch 127/200, Batch 18/45, Loss: 0.24076861143112183\nEpoch 127/200, Batch 19/45, Loss: 0.36197835206985474\nEpoch 127/200, Batch 20/45, Loss: 0.13263608515262604\nEpoch 127/200, Batch 21/45, Loss: 0.34424445033073425\nEpoch 127/200, Batch 22/45, Loss: 0.23802363872528076\nEpoch 127/200, Batch 23/45, Loss: 0.13560271263122559\nEpoch 127/200, Batch 24/45, Loss: 0.13521668314933777\nEpoch 127/200, Batch 25/45, Loss: 0.36236733198165894\nEpoch 127/200, Batch 26/45, Loss: 0.2708386182785034\nEpoch 127/200, Batch 27/45, Loss: 0.3032277226448059\nEpoch 127/200, Batch 28/45, Loss: 0.323025643825531\nEpoch 127/200, Batch 29/45, Loss: 0.29176104068756104\nEpoch 127/200, Batch 30/45, Loss: 0.2920912802219391\nEpoch 127/200, Batch 31/45, Loss: 0.15971042215824127\nEpoch 127/200, Batch 32/45, Loss: 0.15742523968219757\nEpoch 127/200, Batch 33/45, Loss: 0.18315042555332184\nEpoch 127/200, Batch 34/45, Loss: 0.17089243233203888\nEpoch 127/200, Batch 35/45, Loss: 0.20997747778892517\nEpoch 127/200, Batch 36/45, Loss: 0.48980459570884705\nEpoch 127/200, Batch 37/45, Loss: 0.2748914957046509\nEpoch 127/200, Batch 38/45, Loss: 0.2507149875164032\nEpoch 127/200, Batch 39/45, Loss: 0.24411696195602417\nEpoch 127/200, Batch 40/45, Loss: 0.18931156396865845\nEpoch 127/200, Batch 41/45, Loss: 0.25562652945518494\nEpoch 127/200, Batch 42/45, Loss: 0.1630016565322876\nEpoch 127/200, Batch 43/45, Loss: 0.2550407350063324\nEpoch 127/200, Batch 44/45, Loss: 0.17884384095668793\nEpoch 127/200, Batch 45/45, Loss: 0.31964290142059326\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  7.811068117618561 Best Val MSE:  5.2462038397789\nEpoch:  128 , Time Elapsed:  20.36376754442851  mins\nEpoch 128/200, Batch 1/45, Loss: 0.2503946125507355\nEpoch 128/200, Batch 2/45, Loss: 0.21449103951454163\nEpoch 128/200, Batch 3/45, Loss: 0.21871310472488403\nEpoch 128/200, Batch 4/45, Loss: 0.19089889526367188\nEpoch 128/200, Batch 5/45, Loss: 0.30705007910728455\nEpoch 128/200, Batch 6/45, Loss: 0.4264065623283386\nEpoch 128/200, Batch 7/45, Loss: 0.21578821539878845\nEpoch 128/200, Batch 8/45, Loss: 0.2623133957386017\nEpoch 128/200, Batch 9/45, Loss: 0.21298980712890625\nEpoch 128/200, Batch 10/45, Loss: 0.260235458612442\nEpoch 128/200, Batch 11/45, Loss: 0.2881486415863037\nEpoch 128/200, Batch 12/45, Loss: 0.2278890609741211\nEpoch 128/200, Batch 13/45, Loss: 0.2685036063194275\nEpoch 128/200, Batch 14/45, Loss: 0.39523565769195557\nEpoch 128/200, Batch 15/45, Loss: 0.18288978934288025\nEpoch 128/200, Batch 16/45, Loss: 0.3152259588241577\nEpoch 128/200, Batch 17/45, Loss: 0.25435367226600647\nEpoch 128/200, Batch 18/45, Loss: 0.17440751194953918\nEpoch 128/200, Batch 19/45, Loss: 0.2991034984588623\nEpoch 128/200, Batch 20/45, Loss: 0.25256410241127014\nEpoch 128/200, Batch 21/45, Loss: 0.14678740501403809\nEpoch 128/200, Batch 22/45, Loss: 0.1724734604358673\nEpoch 128/200, Batch 23/45, Loss: 0.25606632232666016\nEpoch 128/200, Batch 24/45, Loss: 0.23984810709953308\nEpoch 128/200, Batch 25/45, Loss: 0.22209545969963074\nEpoch 128/200, Batch 26/45, Loss: 0.21638695895671844\nEpoch 128/200, Batch 27/45, Loss: 0.21805456280708313\nEpoch 128/200, Batch 28/45, Loss: 0.3445124328136444\nEpoch 128/200, Batch 29/45, Loss: 0.15329694747924805\nEpoch 128/200, Batch 30/45, Loss: 0.19506214559078217\nEpoch 128/200, Batch 31/45, Loss: 0.19124901294708252\nEpoch 128/200, Batch 32/45, Loss: 0.2763906717300415\nEpoch 128/200, Batch 33/45, Loss: 0.12055448442697525\nEpoch 128/200, Batch 34/45, Loss: 0.16242650151252747\nEpoch 128/200, Batch 35/45, Loss: 0.20912542939186096\nEpoch 128/200, Batch 36/45, Loss: 0.25692200660705566\nEpoch 128/200, Batch 37/45, Loss: 0.45075294375419617\nEpoch 128/200, Batch 38/45, Loss: 0.25485891103744507\nEpoch 128/200, Batch 39/45, Loss: 0.13335038721561432\nEpoch 128/200, Batch 40/45, Loss: 0.2450035810470581\nEpoch 128/200, Batch 41/45, Loss: 0.3714507520198822\nEpoch 128/200, Batch 42/45, Loss: 0.18126311898231506\nEpoch 128/200, Batch 43/45, Loss: 0.25875750184059143\nEpoch 128/200, Batch 44/45, Loss: 0.2114567756652832\nEpoch 128/200, Batch 45/45, Loss: 0.2315138876438141\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  7.196064352989197 Best Val MSE:  5.2462038397789\nEpoch:  129 , Time Elapsed:  20.52619198560715  mins\nEpoch 129/200, Batch 1/45, Loss: 0.5677573084831238\nEpoch 129/200, Batch 2/45, Loss: 0.21961912512779236\nEpoch 129/200, Batch 3/45, Loss: 0.23258277773857117\nEpoch 129/200, Batch 4/45, Loss: 0.37461525201797485\nEpoch 129/200, Batch 5/45, Loss: 0.2489624172449112\nEpoch 129/200, Batch 6/45, Loss: 0.3150067925453186\nEpoch 129/200, Batch 7/45, Loss: 0.23093098402023315\nEpoch 129/200, Batch 8/45, Loss: 0.1774086058139801\nEpoch 129/200, Batch 9/45, Loss: 0.40090781450271606\nEpoch 129/200, Batch 10/45, Loss: 0.2327883541584015\nEpoch 129/200, Batch 11/45, Loss: 0.1676562875509262\nEpoch 129/200, Batch 12/45, Loss: 0.27139541506767273\nEpoch 129/200, Batch 13/45, Loss: 0.16592907905578613\nEpoch 129/200, Batch 14/45, Loss: 0.2634812295436859\nEpoch 129/200, Batch 15/45, Loss: 0.20875951647758484\nEpoch 129/200, Batch 16/45, Loss: 0.2633739709854126\nEpoch 129/200, Batch 17/45, Loss: 0.517250657081604\nEpoch 129/200, Batch 18/45, Loss: 0.241336852312088\nEpoch 129/200, Batch 19/45, Loss: 0.35910385847091675\nEpoch 129/200, Batch 20/45, Loss: 0.23316539824008942\nEpoch 129/200, Batch 21/45, Loss: 0.28324446082115173\nEpoch 129/200, Batch 22/45, Loss: 0.18472051620483398\nEpoch 129/200, Batch 23/45, Loss: 0.2424379140138626\nEpoch 129/200, Batch 24/45, Loss: 0.10817409306764603\nEpoch 129/200, Batch 25/45, Loss: 0.33614009618759155\nEpoch 129/200, Batch 26/45, Loss: 0.15104399621486664\nEpoch 129/200, Batch 27/45, Loss: 0.2130853235721588\nEpoch 129/200, Batch 28/45, Loss: 0.3800325393676758\nEpoch 129/200, Batch 29/45, Loss: 0.35633817315101624\nEpoch 129/200, Batch 30/45, Loss: 0.3028987944126129\nEpoch 129/200, Batch 31/45, Loss: 0.2516580820083618\nEpoch 129/200, Batch 32/45, Loss: 0.41226276755332947\nEpoch 129/200, Batch 33/45, Loss: 0.07312928885221481\nEpoch 129/200, Batch 34/45, Loss: 0.2730872929096222\nEpoch 129/200, Batch 35/45, Loss: 0.538314700126648\nEpoch 129/200, Batch 36/45, Loss: 0.20400285720825195\nEpoch 129/200, Batch 37/45, Loss: 0.3127419948577881\nEpoch 129/200, Batch 38/45, Loss: 0.2635895609855652\nEpoch 129/200, Batch 39/45, Loss: 0.2553027868270874\nEpoch 129/200, Batch 40/45, Loss: 0.2918349504470825\nEpoch 129/200, Batch 41/45, Loss: 0.11237729340791702\nEpoch 129/200, Batch 42/45, Loss: 0.3104011118412018\nEpoch 129/200, Batch 43/45, Loss: 0.19425362348556519\nEpoch 129/200, Batch 44/45, Loss: 0.252630352973938\nEpoch 129/200, Batch 45/45, Loss: 0.3708721399307251\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  7.783316791057587 Best Val MSE:  5.2462038397789\nEpoch:  130 , Time Elapsed:  20.680426037311555  mins\nEpoch 130/200, Batch 1/45, Loss: 0.21456901729106903\nEpoch 130/200, Batch 2/45, Loss: 0.20570412278175354\nEpoch 130/200, Batch 3/45, Loss: 0.17661023139953613\nEpoch 130/200, Batch 4/45, Loss: 0.1817886233329773\nEpoch 130/200, Batch 5/45, Loss: 0.20825307071208954\nEpoch 130/200, Batch 6/45, Loss: 0.2782260775566101\nEpoch 130/200, Batch 7/45, Loss: 0.1888754963874817\nEpoch 130/200, Batch 8/45, Loss: 0.4851246774196625\nEpoch 130/200, Batch 9/45, Loss: 0.16826322674751282\nEpoch 130/200, Batch 10/45, Loss: 0.24036328494548798\nEpoch 130/200, Batch 11/45, Loss: 0.30944597721099854\nEpoch 130/200, Batch 12/45, Loss: 0.15401455760002136\nEpoch 130/200, Batch 13/45, Loss: 0.2693319618701935\nEpoch 130/200, Batch 14/45, Loss: 0.17594709992408752\nEpoch 130/200, Batch 15/45, Loss: 0.383938193321228\nEpoch 130/200, Batch 16/45, Loss: 0.26346904039382935\nEpoch 130/200, Batch 17/45, Loss: 0.34398478269577026\nEpoch 130/200, Batch 18/45, Loss: 0.1523454636335373\nEpoch 130/200, Batch 19/45, Loss: 0.3263785243034363\nEpoch 130/200, Batch 20/45, Loss: 0.17518851161003113\nEpoch 130/200, Batch 21/45, Loss: 0.30792319774627686\nEpoch 130/200, Batch 22/45, Loss: 0.24198386073112488\nEpoch 130/200, Batch 23/45, Loss: 0.22649988532066345\nEpoch 130/200, Batch 24/45, Loss: 0.23143789172172546\nEpoch 130/200, Batch 25/45, Loss: 0.22320076823234558\nEpoch 130/200, Batch 26/45, Loss: 0.33712834119796753\nEpoch 130/200, Batch 27/45, Loss: 0.13772040605545044\nEpoch 130/200, Batch 28/45, Loss: 0.2259112447500229\nEpoch 130/200, Batch 29/45, Loss: 0.33028578758239746\nEpoch 130/200, Batch 30/45, Loss: 0.30395838618278503\nEpoch 130/200, Batch 31/45, Loss: 0.1804494857788086\nEpoch 130/200, Batch 32/45, Loss: 0.30118024349212646\nEpoch 130/200, Batch 33/45, Loss: 0.3531484007835388\nEpoch 130/200, Batch 34/45, Loss: 0.383188396692276\nEpoch 130/200, Batch 35/45, Loss: 0.133493110537529\nEpoch 130/200, Batch 36/45, Loss: 0.2368188202381134\nEpoch 130/200, Batch 37/45, Loss: 0.21518908441066742\nEpoch 130/200, Batch 38/45, Loss: 0.612992525100708\nEpoch 130/200, Batch 39/45, Loss: 0.2903593182563782\nEpoch 130/200, Batch 40/45, Loss: 0.19234667718410492\nEpoch 130/200, Batch 41/45, Loss: 0.23687492311000824\nEpoch 130/200, Batch 42/45, Loss: 0.21741317212581635\nEpoch 130/200, Batch 43/45, Loss: 0.21202927827835083\nEpoch 130/200, Batch 44/45, Loss: 0.21682852506637573\nEpoch 130/200, Batch 45/45, Loss: 0.24828793108463287\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  7.288129925727844 Best Val MSE:  5.2462038397789\nEpoch:  131 , Time Elapsed:  20.834464299678803  mins\nEpoch 131/200, Batch 1/45, Loss: 0.2998864948749542\nEpoch 131/200, Batch 2/45, Loss: 0.15097102522850037\nEpoch 131/200, Batch 3/45, Loss: 0.2121632993221283\nEpoch 131/200, Batch 4/45, Loss: 0.16475047171115875\nEpoch 131/200, Batch 5/45, Loss: 0.3472679555416107\nEpoch 131/200, Batch 6/45, Loss: 0.19203738868236542\nEpoch 131/200, Batch 7/45, Loss: 0.22026214003562927\nEpoch 131/200, Batch 8/45, Loss: 0.2181602567434311\nEpoch 131/200, Batch 9/45, Loss: 0.219517320394516\nEpoch 131/200, Batch 10/45, Loss: 0.3211953341960907\nEpoch 131/200, Batch 11/45, Loss: 0.2875763177871704\nEpoch 131/200, Batch 12/45, Loss: 0.22434237599372864\nEpoch 131/200, Batch 13/45, Loss: 0.28015682101249695\nEpoch 131/200, Batch 14/45, Loss: 0.20953986048698425\nEpoch 131/200, Batch 15/45, Loss: 0.17938798666000366\nEpoch 131/200, Batch 16/45, Loss: 0.4429323077201843\nEpoch 131/200, Batch 17/45, Loss: 0.21335509419441223\nEpoch 131/200, Batch 18/45, Loss: 0.49982303380966187\nEpoch 131/200, Batch 19/45, Loss: 0.39967551827430725\nEpoch 131/200, Batch 20/45, Loss: 0.28347450494766235\nEpoch 131/200, Batch 21/45, Loss: 0.22635570168495178\nEpoch 131/200, Batch 22/45, Loss: 0.177473247051239\nEpoch 131/200, Batch 23/45, Loss: 0.40719735622406006\nEpoch 131/200, Batch 24/45, Loss: 0.43422964215278625\nEpoch 131/200, Batch 25/45, Loss: 0.2609304189682007\nEpoch 131/200, Batch 26/45, Loss: 0.2407897412776947\nEpoch 131/200, Batch 27/45, Loss: 0.19660699367523193\nEpoch 131/200, Batch 28/45, Loss: 0.2087102234363556\nEpoch 131/200, Batch 29/45, Loss: 0.2725294232368469\nEpoch 131/200, Batch 30/45, Loss: 0.23511555790901184\nEpoch 131/200, Batch 31/45, Loss: 0.22112639248371124\nEpoch 131/200, Batch 32/45, Loss: 0.31111806631088257\nEpoch 131/200, Batch 33/45, Loss: 0.2915279269218445\nEpoch 131/200, Batch 34/45, Loss: 0.3739154636859894\nEpoch 131/200, Batch 35/45, Loss: 0.25284498929977417\nEpoch 131/200, Batch 36/45, Loss: 0.28899767994880676\nEpoch 131/200, Batch 37/45, Loss: 0.4280049800872803\nEpoch 131/200, Batch 38/45, Loss: 0.36549559235572815\nEpoch 131/200, Batch 39/45, Loss: 0.32859617471694946\nEpoch 131/200, Batch 40/45, Loss: 0.2733958065509796\nEpoch 131/200, Batch 41/45, Loss: 0.31748148798942566\nEpoch 131/200, Batch 42/45, Loss: 0.2920967638492584\nEpoch 131/200, Batch 43/45, Loss: 0.19920559227466583\nEpoch 131/200, Batch 44/45, Loss: 0.31062597036361694\nEpoch 131/200, Batch 45/45, Loss: 0.11285535991191864\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  6.830179154872894 Best Val MSE:  5.2462038397789\nEpoch:  132 , Time Elapsed:  20.9936132868131  mins\nEpoch 132/200, Batch 1/45, Loss: 0.17723503708839417\nEpoch 132/200, Batch 2/45, Loss: 0.27996140718460083\nEpoch 132/200, Batch 3/45, Loss: 0.13231021165847778\nEpoch 132/200, Batch 4/45, Loss: 0.25880753993988037\nEpoch 132/200, Batch 5/45, Loss: 0.20091600716114044\nEpoch 132/200, Batch 6/45, Loss: 0.14490383863449097\nEpoch 132/200, Batch 7/45, Loss: 0.3018020987510681\nEpoch 132/200, Batch 8/45, Loss: 0.2887762784957886\nEpoch 132/200, Batch 9/45, Loss: 0.15264883637428284\nEpoch 132/200, Batch 10/45, Loss: 0.33027544617652893\nEpoch 132/200, Batch 11/45, Loss: 0.27946263551712036\nEpoch 132/200, Batch 12/45, Loss: 0.2338990718126297\nEpoch 132/200, Batch 13/45, Loss: 0.20501002669334412\nEpoch 132/200, Batch 14/45, Loss: 0.47860297560691833\nEpoch 132/200, Batch 15/45, Loss: 0.2882276177406311\nEpoch 132/200, Batch 16/45, Loss: 0.2908676862716675\nEpoch 132/200, Batch 17/45, Loss: 0.32359790802001953\nEpoch 132/200, Batch 18/45, Loss: 0.253205269575119\nEpoch 132/200, Batch 19/45, Loss: 0.21286413073539734\nEpoch 132/200, Batch 20/45, Loss: 0.2811610698699951\nEpoch 132/200, Batch 21/45, Loss: 0.2298484444618225\nEpoch 132/200, Batch 22/45, Loss: 0.22044673562049866\nEpoch 132/200, Batch 23/45, Loss: 0.35486334562301636\nEpoch 132/200, Batch 24/45, Loss: 0.1870993971824646\nEpoch 132/200, Batch 25/45, Loss: 0.20631325244903564\nEpoch 132/200, Batch 26/45, Loss: 0.6357246041297913\nEpoch 132/200, Batch 27/45, Loss: 0.1493096947669983\nEpoch 132/200, Batch 28/45, Loss: 0.38462311029434204\nEpoch 132/200, Batch 29/45, Loss: 0.30826929211616516\nEpoch 132/200, Batch 30/45, Loss: 0.35236960649490356\nEpoch 132/200, Batch 31/45, Loss: 0.3298032581806183\nEpoch 132/200, Batch 32/45, Loss: 0.33536142110824585\nEpoch 132/200, Batch 33/45, Loss: 0.2903551757335663\nEpoch 132/200, Batch 34/45, Loss: 0.2499111294746399\nEpoch 132/200, Batch 35/45, Loss: 0.23018315434455872\nEpoch 132/200, Batch 36/45, Loss: 0.3329812288284302\nEpoch 132/200, Batch 37/45, Loss: 0.39642134308815\nEpoch 132/200, Batch 38/45, Loss: 0.29223665595054626\nEpoch 132/200, Batch 39/45, Loss: 0.33286672830581665\nEpoch 132/200, Batch 40/45, Loss: 0.5224820971488953\nEpoch 132/200, Batch 41/45, Loss: 0.18879494071006775\nEpoch 132/200, Batch 42/45, Loss: 0.24878428876399994\nEpoch 132/200, Batch 43/45, Loss: 0.922450602054596\nEpoch 132/200, Batch 44/45, Loss: 0.3071401119232178\nEpoch 132/200, Batch 45/45, Loss: 0.30512627959251404\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  7.8028150498867035 Best Val MSE:  5.2462038397789\nEpoch:  133 , Time Elapsed:  21.160384273529054  mins\nEpoch 133/200, Batch 1/45, Loss: 0.24728906154632568\nEpoch 133/200, Batch 2/45, Loss: 0.34524571895599365\nEpoch 133/200, Batch 3/45, Loss: 0.9292113780975342\nEpoch 133/200, Batch 4/45, Loss: 0.3102281391620636\nEpoch 133/200, Batch 5/45, Loss: 0.22520029544830322\nEpoch 133/200, Batch 6/45, Loss: 0.20374487340450287\nEpoch 133/200, Batch 7/45, Loss: 0.3159324526786804\nEpoch 133/200, Batch 8/45, Loss: 0.16418257355690002\nEpoch 133/200, Batch 9/45, Loss: 0.22245270013809204\nEpoch 133/200, Batch 10/45, Loss: 0.32166534662246704\nEpoch 133/200, Batch 11/45, Loss: 1.2642539739608765\nEpoch 133/200, Batch 12/45, Loss: 0.34589046239852905\nEpoch 133/200, Batch 13/45, Loss: 0.42901745438575745\nEpoch 133/200, Batch 14/45, Loss: 0.3682398200035095\nEpoch 133/200, Batch 15/45, Loss: 0.23630093038082123\nEpoch 133/200, Batch 16/45, Loss: 0.45884573459625244\nEpoch 133/200, Batch 17/45, Loss: 0.5166935324668884\nEpoch 133/200, Batch 18/45, Loss: 0.5385251045227051\nEpoch 133/200, Batch 19/45, Loss: 0.5957636833190918\nEpoch 133/200, Batch 20/45, Loss: 0.493762344121933\nEpoch 133/200, Batch 21/45, Loss: 0.5359489321708679\nEpoch 133/200, Batch 22/45, Loss: 0.4746701717376709\nEpoch 133/200, Batch 23/45, Loss: 0.40917184948921204\nEpoch 133/200, Batch 24/45, Loss: 0.5775623321533203\nEpoch 133/200, Batch 25/45, Loss: 0.2958729565143585\nEpoch 133/200, Batch 26/45, Loss: 0.7363665103912354\nEpoch 133/200, Batch 31/45, Loss: 0.22959598898887634\nEpoch 133/200, Batch 32/45, Loss: 0.2205865979194641\nEpoch 133/200, Batch 33/45, Loss: 0.2487679123878479\nEpoch 133/200, Batch 34/45, Loss: 0.421226441860199\nEpoch 133/200, Batch 35/45, Loss: 0.2769221365451813\nEpoch 133/200, Batch 36/45, Loss: 0.22411756217479706\nEpoch 133/200, Batch 37/45, Loss: 0.3377876877784729\nEpoch 133/200, Batch 38/45, Loss: 0.27616849541664124\nEpoch 133/200, Batch 39/45, Loss: 0.5013617873191833\nEpoch 133/200, Batch 40/45, Loss: 0.17216423153877258\nEpoch 133/200, Batch 41/45, Loss: 0.3719048798084259\nEpoch 133/200, Batch 42/45, Loss: 0.2896662652492523\nEpoch 133/200, Batch 43/45, Loss: 0.24342761933803558\nEpoch 133/200, Batch 44/45, Loss: 0.2981385588645935\nEpoch 133/200, Batch 45/45, Loss: 0.35372471809387207\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  9.814968764781952 Best Val MSE:  5.2462038397789\nEpoch:  134 , Time Elapsed:  21.318814980983735  mins\nEpoch 134/200, Batch 1/45, Loss: 0.3948245346546173\nEpoch 134/200, Batch 2/45, Loss: 0.270084947347641\nEpoch 134/200, Batch 3/45, Loss: 0.4047008752822876\nEpoch 134/200, Batch 4/45, Loss: 6.279279708862305\nEpoch 134/200, Batch 5/45, Loss: 0.8175351619720459\nEpoch 134/200, Batch 6/45, Loss: 0.6631554365158081\nEpoch 134/200, Batch 7/45, Loss: 0.8003605008125305\nEpoch 134/200, Batch 8/45, Loss: 1.0695335865020752\nEpoch 134/200, Batch 9/45, Loss: 0.8723876476287842\nEpoch 134/200, Batch 10/45, Loss: 0.9594491720199585\nEpoch 134/200, Batch 11/45, Loss: 0.9890440702438354\nEpoch 134/200, Batch 12/45, Loss: 0.8522841334342957\nEpoch 134/200, Batch 13/45, Loss: 0.8691949844360352\nEpoch 134/200, Batch 14/45, Loss: 0.7117559313774109\nEpoch 134/200, Batch 15/45, Loss: 0.6125152707099915\nEpoch 134/200, Batch 16/45, Loss: 0.4052790403366089\nEpoch 134/200, Batch 17/45, Loss: 2.286898612976074\nEpoch 134/200, Batch 18/45, Loss: 0.3688088655471802\nEpoch 134/200, Batch 19/45, Loss: 0.19426611065864563\nEpoch 134/200, Batch 20/45, Loss: 0.23462745547294617\nEpoch 134/200, Batch 21/45, Loss: 0.9412084221839905\nEpoch 134/200, Batch 22/45, Loss: 0.5352883338928223\nEpoch 134/200, Batch 23/45, Loss: 0.41368037462234497\nEpoch 134/200, Batch 24/45, Loss: 0.3453303575515747\nEpoch 134/200, Batch 25/45, Loss: 0.644564688205719\nEpoch 134/200, Batch 26/45, Loss: 0.3695333003997803\nEpoch 134/200, Batch 27/45, Loss: 0.5738094449043274\nEpoch 134/200, Batch 28/45, Loss: 0.37197497487068176\nEpoch 134/200, Batch 29/45, Loss: 0.43187621235847473\nEpoch 134/200, Batch 30/45, Loss: 0.3034364879131317\nEpoch 134/200, Batch 31/45, Loss: 0.7176253795623779\nEpoch 134/200, Batch 32/45, Loss: 0.5441771745681763\nEpoch 134/200, Batch 33/45, Loss: 0.6929088234901428\nEpoch 134/200, Batch 34/45, Loss: 1.2889094352722168\nEpoch 134/200, Batch 35/45, Loss: 0.35089796781539917\nEpoch 134/200, Batch 36/45, Loss: 0.7433202862739563\nEpoch 134/200, Batch 37/45, Loss: 4.26345157623291\nEpoch 134/200, Batch 38/45, Loss: 0.4049464464187622\nEpoch 134/200, Batch 39/45, Loss: 0.3583833873271942\nEpoch 134/200, Batch 40/45, Loss: 0.7644184231758118\nEpoch 134/200, Batch 41/45, Loss: 1.2528905868530273\nEpoch 134/200, Batch 42/45, Loss: 1.2211971282958984\nEpoch 134/200, Batch 43/45, Loss: 1.5541400909423828\nEpoch 134/200, Batch 44/45, Loss: 1.4831275939941406\nEpoch 134/200, Batch 45/45, Loss: 1.4306498765945435\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  21.205572366714478 Best Val MSE:  5.2462038397789\nEpoch:  135 , Time Elapsed:  21.47543873389562  mins\nEpoch 135/200, Batch 1/45, Loss: 1.4267280101776123\nEpoch 135/200, Batch 2/45, Loss: 1.6208879947662354\nEpoch 135/200, Batch 3/45, Loss: 1.4852664470672607\nEpoch 135/200, Batch 4/45, Loss: 1.0700392723083496\nEpoch 135/200, Batch 5/45, Loss: 1.0684998035430908\nEpoch 135/200, Batch 6/45, Loss: 0.933302640914917\nEpoch 135/200, Batch 7/45, Loss: 1.178417444229126\nEpoch 135/200, Batch 8/45, Loss: 0.7470375299453735\nEpoch 135/200, Batch 9/45, Loss: 0.4788760244846344\nEpoch 135/200, Batch 10/45, Loss: 0.3548974096775055\nEpoch 135/200, Batch 11/45, Loss: 0.3362284302711487\nEpoch 135/200, Batch 12/45, Loss: 0.3449289798736572\nEpoch 135/200, Batch 13/45, Loss: 0.5223294496536255\nEpoch 135/200, Batch 14/45, Loss: 0.6517510414123535\nEpoch 135/200, Batch 15/45, Loss: 0.6420702338218689\nEpoch 135/200, Batch 16/45, Loss: 0.4849563241004944\nEpoch 135/200, Batch 17/45, Loss: 0.44018128514289856\nEpoch 135/200, Batch 18/45, Loss: 1.1974246501922607\nEpoch 135/200, Batch 19/45, Loss: 0.3449164032936096\nEpoch 135/200, Batch 20/45, Loss: 0.3991248607635498\nEpoch 135/200, Batch 21/45, Loss: 0.3473147749900818\nEpoch 135/200, Batch 22/45, Loss: 0.26619064807891846\nEpoch 135/200, Batch 23/45, Loss: 1.368335247039795\nEpoch 135/200, Batch 24/45, Loss: 0.2457057386636734\nEpoch 135/200, Batch 25/45, Loss: 0.27671027183532715\nEpoch 135/200, Batch 26/45, Loss: 0.3941551744937897\nEpoch 135/200, Batch 27/45, Loss: 0.4487399160861969\nEpoch 135/200, Batch 28/45, Loss: 0.34345918893814087\nEpoch 135/200, Batch 29/45, Loss: 0.5231961607933044\nEpoch 135/200, Batch 30/45, Loss: 0.6157735586166382\nEpoch 135/200, Batch 31/45, Loss: 0.3029639720916748\nEpoch 135/200, Batch 32/45, Loss: 0.3443946838378906\nEpoch 135/200, Batch 33/45, Loss: 0.26229315996170044\nEpoch 135/200, Batch 34/45, Loss: 0.4017735719680786\nEpoch 135/200, Batch 35/45, Loss: 0.2705405652523041\nEpoch 135/200, Batch 36/45, Loss: 0.7292516827583313\nEpoch 135/200, Batch 37/45, Loss: 0.37275028228759766\nEpoch 135/200, Batch 38/45, Loss: 19.88541030883789\nEpoch 135/200, Batch 39/45, Loss: 0.8833475112915039\nEpoch 135/200, Batch 40/45, Loss: 1.3346871137619019\nEpoch 135/200, Batch 41/45, Loss: 1.3575351238250732\nEpoch 135/200, Batch 42/45, Loss: 1.525590181350708\nEpoch 135/200, Batch 43/45, Loss: 1.895430326461792\nEpoch 135/200, Batch 44/45, Loss: 2.0240063667297363\nEpoch 135/200, Batch 45/45, Loss: 2.0904197692871094\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  37.97687602043152 Best Val MSE:  5.2462038397789\nEpoch:  136 , Time Elapsed:  21.64205540815989  mins\nEpoch 136/200, Batch 1/45, Loss: 2.206562042236328\nEpoch 136/200, Batch 2/45, Loss: 2.3948378562927246\nEpoch 136/200, Batch 3/45, Loss: 2.3259706497192383\nEpoch 136/200, Batch 4/45, Loss: 2.658595561981201\nEpoch 136/200, Batch 5/45, Loss: 1.9464035034179688\nEpoch 136/200, Batch 6/45, Loss: 2.0346813201904297\nEpoch 136/200, Batch 7/45, Loss: 1.9761215448379517\nEpoch 136/200, Batch 8/45, Loss: 2.034332036972046\nEpoch 136/200, Batch 9/45, Loss: 1.4074299335479736\nEpoch 136/200, Batch 10/45, Loss: 1.6932897567749023\nEpoch 136/200, Batch 11/45, Loss: 1.62198805809021\nEpoch 136/200, Batch 12/45, Loss: 1.9526845216751099\nEpoch 136/200, Batch 13/45, Loss: 1.776751160621643\nEpoch 136/200, Batch 14/45, Loss: 1.4321461915969849\nEpoch 136/200, Batch 15/45, Loss: 1.5120497941970825\nEpoch 136/200, Batch 16/45, Loss: 1.4561480283737183\nEpoch 136/200, Batch 17/45, Loss: 1.3210113048553467\nEpoch 136/200, Batch 18/45, Loss: 1.291865348815918\nEpoch 136/200, Batch 19/45, Loss: 1.4325823783874512\nEpoch 136/200, Batch 20/45, Loss: 1.0970509052276611\nEpoch 136/200, Batch 21/45, Loss: 1.103745937347412\nEpoch 136/200, Batch 22/45, Loss: 0.9570887088775635\nEpoch 136/200, Batch 23/45, Loss: 0.8321719169616699\nEpoch 136/200, Batch 24/45, Loss: 1.1213057041168213\nEpoch 136/200, Batch 25/45, Loss: 0.8607286214828491\nEpoch 136/200, Batch 26/45, Loss: 0.9404360055923462\nEpoch 136/200, Batch 27/45, Loss: 0.7184956669807434\nEpoch 136/200, Batch 28/45, Loss: 0.980281412601471\nEpoch 136/200, Batch 29/45, Loss: 2.4083449840545654\nEpoch 136/200, Batch 30/45, Loss: 0.9650447368621826\nEpoch 136/200, Batch 31/45, Loss: 0.7751308083534241\nEpoch 136/200, Batch 32/45, Loss: 0.3869503140449524\nEpoch 136/200, Batch 33/45, Loss: 0.4732283055782318\nEpoch 136/200, Batch 34/45, Loss: 0.6311441659927368\nEpoch 136/200, Batch 35/45, Loss: 1.00406813621521\nEpoch 136/200, Batch 36/45, Loss: 0.7304225564002991\nEpoch 136/200, Batch 37/45, Loss: 0.6870900988578796\nEpoch 136/200, Batch 38/45, Loss: 1.234560251235962\nEpoch 136/200, Batch 39/45, Loss: 0.6332052946090698\nEpoch 136/200, Batch 40/45, Loss: 0.8026047945022583\nEpoch 136/200, Batch 41/45, Loss: 0.6341880559921265\nEpoch 136/200, Batch 42/45, Loss: 0.5465448498725891\nEpoch 136/200, Batch 43/45, Loss: 0.7431471347808838\nEpoch 136/200, Batch 44/45, Loss: 0.5237222909927368\nEpoch 136/200, Batch 45/45, Loss: 1.044342041015625\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  768.8048877716064 Best Val MSE:  5.2462038397789\nEpoch:  137 , Time Elapsed:  21.797479943434396  mins\nEpoch 137/200, Batch 1/45, Loss: 0.6027025580406189\nEpoch 137/200, Batch 2/45, Loss: 0.400890588760376\nEpoch 137/200, Batch 3/45, Loss: 0.3447325825691223\nEpoch 137/200, Batch 4/45, Loss: 0.9488787651062012\nEpoch 137/200, Batch 5/45, Loss: 0.6135542988777161\nEpoch 137/200, Batch 6/45, Loss: 0.2131546437740326\nEpoch 137/200, Batch 7/45, Loss: 0.48119956254959106\nEpoch 137/200, Batch 8/45, Loss: 0.8844308257102966\nEpoch 137/200, Batch 9/45, Loss: 0.3359532952308655\nEpoch 137/200, Batch 10/45, Loss: 0.26315611600875854\nEpoch 137/200, Batch 11/45, Loss: 0.38519755005836487\nEpoch 137/200, Batch 12/45, Loss: 0.996131181716919\nEpoch 137/200, Batch 13/45, Loss: 0.40388989448547363\nEpoch 137/200, Batch 14/45, Loss: 0.311957985162735\nEpoch 137/200, Batch 15/45, Loss: 0.44629982113838196\nEpoch 137/200, Batch 16/45, Loss: 0.5625360012054443\nEpoch 137/200, Batch 17/45, Loss: 0.4254794716835022\nEpoch 137/200, Batch 18/45, Loss: 0.338813453912735\nEpoch 137/200, Batch 19/45, Loss: 0.18970349431037903\nEpoch 137/200, Batch 20/45, Loss: 0.6101309061050415\nEpoch 137/200, Batch 21/45, Loss: 0.37578463554382324\nEpoch 137/200, Batch 22/45, Loss: 0.46004921197891235\nEpoch 137/200, Batch 23/45, Loss: 0.2734076678752899\nEpoch 137/200, Batch 24/45, Loss: 0.4648118019104004\nEpoch 137/200, Batch 25/45, Loss: 0.2944602072238922\nEpoch 137/200, Batch 26/45, Loss: 0.4804331064224243\nEpoch 137/200, Batch 27/45, Loss: 0.4368970990180969\nEpoch 137/200, Batch 28/45, Loss: 0.44348618388175964\nEpoch 137/200, Batch 29/45, Loss: 0.17503167688846588\nEpoch 137/200, Batch 30/45, Loss: 0.4169851243495941\nEpoch 137/200, Batch 31/45, Loss: 0.30533432960510254\nEpoch 137/200, Batch 32/45, Loss: 0.34714627265930176\nEpoch 137/200, Batch 33/45, Loss: 0.2913360595703125\nEpoch 137/200, Batch 34/45, Loss: 0.8932431936264038\nEpoch 137/200, Batch 35/45, Loss: 0.5767723917961121\nEpoch 137/200, Batch 36/45, Loss: 0.36046668887138367\nEpoch 137/200, Batch 37/45, Loss: 0.21134188771247864\nEpoch 137/200, Batch 38/45, Loss: 0.12896732985973358\nEpoch 137/200, Batch 39/45, Loss: 0.38804036378860474\nEpoch 137/200, Batch 40/45, Loss: 0.4165874421596527\nEpoch 137/200, Batch 41/45, Loss: 0.6811325550079346\nEpoch 137/200, Batch 42/45, Loss: 0.329582542181015\nEpoch 137/200, Batch 43/45, Loss: 0.42055976390838623\nEpoch 137/200, Batch 44/45, Loss: 0.27048712968826294\nEpoch 137/200, Batch 45/45, Loss: 0.09390128403902054\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  113.45876753330231 Best Val MSE:  5.2462038397789\nEpoch:  138 , Time Elapsed:  21.953195405006408  mins\nEpoch 138/200, Batch 1/45, Loss: 0.7758598327636719\nEpoch 138/200, Batch 2/45, Loss: 0.42868494987487793\nEpoch 138/200, Batch 3/45, Loss: 0.18586137890815735\nEpoch 138/200, Batch 4/45, Loss: 0.39923614263534546\nEpoch 138/200, Batch 5/45, Loss: 1.3465759754180908\nEpoch 138/200, Batch 6/45, Loss: 0.4424827992916107\nEpoch 138/200, Batch 7/45, Loss: 0.25179076194763184\nEpoch 138/200, Batch 8/45, Loss: 0.2499430626630783\nEpoch 138/200, Batch 9/45, Loss: 0.36419981718063354\nEpoch 138/200, Batch 10/45, Loss: 0.4396292567253113\nEpoch 138/200, Batch 11/45, Loss: 0.42792826890945435\nEpoch 138/200, Batch 12/45, Loss: 0.5195299983024597\nEpoch 138/200, Batch 13/45, Loss: 0.24317282438278198\nEpoch 138/200, Batch 14/45, Loss: 0.13546627759933472\nEpoch 138/200, Batch 15/45, Loss: 0.2903344929218292\nEpoch 138/200, Batch 16/45, Loss: 0.2164275050163269\nEpoch 138/200, Batch 17/45, Loss: 0.4586625397205353\nEpoch 138/200, Batch 18/45, Loss: 0.5952503085136414\nEpoch 138/200, Batch 19/45, Loss: 0.5507674217224121\nEpoch 138/200, Batch 20/45, Loss: 0.31222328543663025\nEpoch 138/200, Batch 21/45, Loss: 0.7186317443847656\nEpoch 138/200, Batch 22/45, Loss: 0.59116530418396\nEpoch 138/200, Batch 23/45, Loss: 0.5217584371566772\nEpoch 138/200, Batch 24/45, Loss: 0.4619542360305786\nEpoch 138/200, Batch 25/45, Loss: 0.35513871908187866\nEpoch 138/200, Batch 26/45, Loss: 0.3860177993774414\nEpoch 138/200, Batch 27/45, Loss: 1.5001442432403564\nEpoch 138/200, Batch 28/45, Loss: 0.3184508979320526\nEpoch 138/200, Batch 29/45, Loss: 0.5540609955787659\nEpoch 138/200, Batch 30/45, Loss: 0.3220990002155304\nEpoch 138/200, Batch 31/45, Loss: 0.28854668140411377\nEpoch 138/200, Batch 32/45, Loss: 0.3811461627483368\nEpoch 138/200, Batch 33/45, Loss: 0.9284543395042419\nEpoch 138/200, Batch 34/45, Loss: 0.7981207370758057\nEpoch 138/200, Batch 35/45, Loss: 0.7741649150848389\nEpoch 138/200, Batch 36/45, Loss: 0.5639934539794922\nEpoch 138/200, Batch 37/45, Loss: 0.31580352783203125\nEpoch 138/200, Batch 38/45, Loss: 0.634876012802124\nEpoch 138/200, Batch 39/45, Loss: 0.35259756445884705\nEpoch 138/200, Batch 40/45, Loss: 0.5094826221466064\nEpoch 138/200, Batch 41/45, Loss: 0.5798517465591431\nEpoch 138/200, Batch 42/45, Loss: 0.3929656744003296\nEpoch 138/200, Batch 43/45, Loss: 0.2511749267578125\nEpoch 138/200, Batch 44/45, Loss: 0.34867435693740845\nEpoch 138/200, Batch 45/45, Loss: 0.5381399989128113\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  168.55177092552185 Best Val MSE:  5.2462038397789\nEpoch:  139 , Time Elapsed:  22.11619288921356  mins\nEpoch 139/200, Batch 1/45, Loss: 0.4488249719142914\nEpoch 139/200, Batch 2/45, Loss: 0.9678179621696472\nEpoch 139/200, Batch 3/45, Loss: 0.31977930665016174\nEpoch 139/200, Batch 4/45, Loss: 0.3150685727596283\nEpoch 139/200, Batch 5/45, Loss: 0.21328508853912354\nEpoch 139/200, Batch 6/45, Loss: 0.37120574712753296\nEpoch 139/200, Batch 7/45, Loss: 0.3983413577079773\nEpoch 139/200, Batch 8/45, Loss: 0.6897783279418945\nEpoch 139/200, Batch 9/45, Loss: 0.3279270529747009\nEpoch 139/200, Batch 10/45, Loss: 0.2748766243457794\nEpoch 139/200, Batch 11/45, Loss: 0.3686829209327698\nEpoch 139/200, Batch 12/45, Loss: 0.2647426724433899\nEpoch 139/200, Batch 13/45, Loss: 0.3191569745540619\nEpoch 139/200, Batch 14/45, Loss: 0.39955198764801025\nEpoch 139/200, Batch 15/45, Loss: 0.4334687292575836\nEpoch 139/200, Batch 16/45, Loss: 0.3623025417327881\nEpoch 139/200, Batch 17/45, Loss: 0.3014315962791443\nEpoch 139/200, Batch 18/45, Loss: 0.555331826210022\nEpoch 139/200, Batch 19/45, Loss: 0.27864062786102295\nEpoch 139/200, Batch 20/45, Loss: 0.3525393009185791\nEpoch 139/200, Batch 21/45, Loss: 0.40159285068511963\nEpoch 139/200, Batch 22/45, Loss: 0.23352673649787903\nEpoch 139/200, Batch 23/45, Loss: 0.3808661699295044\nEpoch 139/200, Batch 24/45, Loss: 0.2201138287782669\nEpoch 139/200, Batch 25/45, Loss: 0.5176248550415039\nEpoch 139/200, Batch 26/45, Loss: 0.570002555847168\nEpoch 139/200, Batch 27/45, Loss: 0.31704312562942505\nEpoch 139/200, Batch 28/45, Loss: 0.50543212890625\nEpoch 139/200, Batch 29/45, Loss: 0.5328871607780457\nEpoch 139/200, Batch 30/45, Loss: 0.4268624782562256\nEpoch 139/200, Batch 31/45, Loss: 0.24498037993907928\nEpoch 139/200, Batch 32/45, Loss: 0.3387601375579834\nEpoch 139/200, Batch 33/45, Loss: 0.3016449511051178\nEpoch 139/200, Batch 34/45, Loss: 0.24577836692333221\nEpoch 139/200, Batch 35/45, Loss: 0.2840369939804077\nEpoch 139/200, Batch 36/45, Loss: 0.2834576666355133\nEpoch 139/200, Batch 37/45, Loss: 0.8724642992019653\nEpoch 139/200, Batch 38/45, Loss: 0.726254940032959\nEpoch 139/200, Batch 39/45, Loss: 0.7669248580932617\nEpoch 139/200, Batch 40/45, Loss: 0.19919706881046295\nEpoch 139/200, Batch 41/45, Loss: 0.3136065602302551\nEpoch 139/200, Batch 42/45, Loss: 0.2364913374185562\nEpoch 139/200, Batch 43/45, Loss: 1.07716703414917\nEpoch 139/200, Batch 44/45, Loss: 0.21209287643432617\nEpoch 139/200, Batch 45/45, Loss: 0.3491939902305603\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  12.05872568488121 Best Val MSE:  5.2462038397789\nEpoch:  140 , Time Elapsed:  22.275282561779022  mins\nEpoch 140/200, Batch 1/45, Loss: 0.3226436972618103\nEpoch 140/200, Batch 2/45, Loss: 0.4296824336051941\nEpoch 140/200, Batch 3/45, Loss: 0.265338659286499\nEpoch 140/200, Batch 4/45, Loss: 0.5073434114456177\nEpoch 140/200, Batch 5/45, Loss: 0.3023066818714142\nEpoch 140/200, Batch 6/45, Loss: 0.49693095684051514\nEpoch 140/200, Batch 7/45, Loss: 0.3733499050140381\nEpoch 140/200, Batch 8/45, Loss: 0.40954670310020447\nEpoch 140/200, Batch 9/45, Loss: 0.8373150825500488\nEpoch 140/200, Batch 10/45, Loss: 0.49871766567230225\nEpoch 140/200, Batch 11/45, Loss: 0.25553062558174133\nEpoch 140/200, Batch 12/45, Loss: 0.44897547364234924\nEpoch 140/200, Batch 13/45, Loss: 0.30629345774650574\nEpoch 140/200, Batch 14/45, Loss: 0.2613133192062378\nEpoch 140/200, Batch 15/45, Loss: 0.26394712924957275\nEpoch 140/200, Batch 16/45, Loss: 0.2717548608779907\nEpoch 140/200, Batch 17/45, Loss: 1.0298539400100708\nEpoch 140/200, Batch 18/45, Loss: 1.4813942909240723\nEpoch 140/200, Batch 19/45, Loss: 0.38420116901397705\nEpoch 140/200, Batch 20/45, Loss: 0.4393438994884491\nEpoch 140/200, Batch 21/45, Loss: 0.42512500286102295\nEpoch 140/200, Batch 22/45, Loss: 0.48885953426361084\nEpoch 140/200, Batch 23/45, Loss: 0.4247402250766754\nEpoch 140/200, Batch 24/45, Loss: 1.4181410074234009\nEpoch 140/200, Batch 25/45, Loss: 0.5849873423576355\nEpoch 140/200, Batch 26/45, Loss: 0.5180617570877075\nEpoch 140/200, Batch 27/45, Loss: 0.2675832509994507\nEpoch 140/200, Batch 28/45, Loss: 0.751934289932251\nEpoch 140/200, Batch 29/45, Loss: 0.369401216506958\nEpoch 140/200, Batch 30/45, Loss: 0.28545498847961426\nEpoch 140/200, Batch 31/45, Loss: 0.6421762108802795\nEpoch 140/200, Batch 32/45, Loss: 0.219480961561203\nEpoch 140/200, Batch 33/45, Loss: 0.2251233160495758\nEpoch 140/200, Batch 34/45, Loss: 0.2776135504245758\nEpoch 140/200, Batch 35/45, Loss: 0.48061084747314453\nEpoch 140/200, Batch 36/45, Loss: 0.2886464297771454\nEpoch 140/200, Batch 37/45, Loss: 0.5026647448539734\nEpoch 140/200, Batch 38/45, Loss: 0.6841540336608887\nEpoch 140/200, Batch 39/45, Loss: 0.2602273225784302\nEpoch 140/200, Batch 40/45, Loss: 0.45508500933647156\nEpoch 140/200, Batch 41/45, Loss: 0.4816456735134125\nEpoch 140/200, Batch 42/45, Loss: 0.37529081106185913\nEpoch 140/200, Batch 43/45, Loss: 0.31835317611694336\nEpoch 140/200, Batch 44/45, Loss: 0.2214115858078003\nEpoch 140/200, Batch 45/45, Loss: 0.13609746098518372\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  1658.4783477783203 Best Val MSE:  5.2462038397789\nEpoch:  141 , Time Elapsed:  22.431103912989297  mins\nEpoch 141/200, Batch 1/45, Loss: 0.333443820476532\nEpoch 141/200, Batch 2/45, Loss: 0.34305906295776367\nEpoch 141/200, Batch 3/45, Loss: 0.16078384220600128\nEpoch 141/200, Batch 4/45, Loss: 0.40442579984664917\nEpoch 141/200, Batch 5/45, Loss: 0.27813538908958435\nEpoch 141/200, Batch 6/45, Loss: 0.27914726734161377\nEpoch 141/200, Batch 7/45, Loss: 0.3280982971191406\nEpoch 141/200, Batch 8/45, Loss: 0.5334640741348267\nEpoch 141/200, Batch 9/45, Loss: 0.2530050277709961\nEpoch 141/200, Batch 10/45, Loss: 0.2953733205795288\nEpoch 141/200, Batch 11/45, Loss: 0.27188488841056824\nEpoch 141/200, Batch 12/45, Loss: 0.34151071310043335\nEpoch 141/200, Batch 13/45, Loss: 0.40383610129356384\nEpoch 141/200, Batch 14/45, Loss: 0.28134146332740784\nEpoch 141/200, Batch 15/45, Loss: 0.3753107488155365\nEpoch 141/200, Batch 16/45, Loss: 0.45027095079421997\nEpoch 141/200, Batch 17/45, Loss: 0.2753269672393799\nEpoch 141/200, Batch 18/45, Loss: 0.30841392278671265\nEpoch 141/200, Batch 19/45, Loss: 0.2918809652328491\nEpoch 141/200, Batch 20/45, Loss: 0.40947195887565613\nEpoch 141/200, Batch 21/45, Loss: 0.4906143844127655\nEpoch 141/200, Batch 22/45, Loss: 0.2594835162162781\nEpoch 141/200, Batch 23/45, Loss: 0.15295936167240143\nEpoch 141/200, Batch 24/45, Loss: 0.3096115291118622\nEpoch 141/200, Batch 25/45, Loss: 0.31706148386001587\nEpoch 141/200, Batch 26/45, Loss: 0.3736644387245178\nEpoch 141/200, Batch 27/45, Loss: 0.2317584753036499\nEpoch 141/200, Batch 28/45, Loss: 0.2419712394475937\nEpoch 141/200, Batch 29/45, Loss: 0.47306597232818604\nEpoch 141/200, Batch 30/45, Loss: 0.5205039978027344\nEpoch 141/200, Batch 31/45, Loss: 0.6069110035896301\nEpoch 141/200, Batch 32/45, Loss: 0.19734494388103485\nEpoch 141/200, Batch 33/45, Loss: 0.2745775580406189\nEpoch 141/200, Batch 34/45, Loss: 0.2440236657857895\nEpoch 141/200, Batch 35/45, Loss: 0.4499177932739258\nEpoch 141/200, Batch 36/45, Loss: 0.264956533908844\nEpoch 141/200, Batch 37/45, Loss: 0.32185080647468567\nEpoch 141/200, Batch 38/45, Loss: 0.3762613832950592\nEpoch 141/200, Batch 39/45, Loss: 0.5541368722915649\nEpoch 141/200, Batch 40/45, Loss: 0.2583593726158142\nEpoch 141/200, Batch 41/45, Loss: 0.3266187608242035\nEpoch 141/200, Batch 42/45, Loss: 0.2535300850868225\nEpoch 141/200, Batch 43/45, Loss: 0.43126189708709717\nEpoch 141/200, Batch 44/45, Loss: 0.3120768070220947\nEpoch 141/200, Batch 45/45, Loss: 0.29792046546936035\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  111.09141659736633 Best Val MSE:  5.2462038397789\nEpoch:  142 , Time Elapsed:  22.59118167559306  mins\nEpoch 142/200, Batch 1/45, Loss: 0.3658282160758972\nEpoch 142/200, Batch 2/45, Loss: 0.6775388717651367\nEpoch 142/200, Batch 3/45, Loss: 0.1887488216161728\nEpoch 142/200, Batch 4/45, Loss: 0.16814905405044556\nEpoch 142/200, Batch 5/45, Loss: 0.18053805828094482\nEpoch 142/200, Batch 6/45, Loss: 0.35550031065940857\nEpoch 142/200, Batch 7/45, Loss: 0.33959686756134033\nEpoch 142/200, Batch 8/45, Loss: 0.26385045051574707\nEpoch 142/200, Batch 9/45, Loss: 0.33606889843940735\nEpoch 142/200, Batch 10/45, Loss: 0.33418676257133484\nEpoch 142/200, Batch 11/45, Loss: 0.17388297617435455\nEpoch 142/200, Batch 12/45, Loss: 0.2763165831565857\nEpoch 142/200, Batch 13/45, Loss: 0.4206395447254181\nEpoch 142/200, Batch 14/45, Loss: 0.4110671877861023\nEpoch 142/200, Batch 15/45, Loss: 0.4789585471153259\nEpoch 142/200, Batch 16/45, Loss: 0.2186661660671234\nEpoch 142/200, Batch 17/45, Loss: 0.5476534366607666\nEpoch 142/200, Batch 18/45, Loss: 0.3797363042831421\nEpoch 142/200, Batch 19/45, Loss: 0.2770059406757355\nEpoch 142/200, Batch 20/45, Loss: 0.3609669506549835\nEpoch 142/200, Batch 21/45, Loss: 0.4023336172103882\nEpoch 142/200, Batch 22/45, Loss: 0.273751437664032\nEpoch 142/200, Batch 23/45, Loss: 0.29230183362960815\nEpoch 142/200, Batch 24/45, Loss: 0.28326061367988586\nEpoch 142/200, Batch 25/45, Loss: 0.3361099362373352\nEpoch 142/200, Batch 26/45, Loss: 0.4267163574695587\nEpoch 142/200, Batch 27/45, Loss: 0.19469410181045532\nEpoch 142/200, Batch 28/45, Loss: 0.41346830129623413\nEpoch 142/200, Batch 29/45, Loss: 0.4701455235481262\nEpoch 142/200, Batch 30/45, Loss: 0.35596388578414917\nEpoch 142/200, Batch 31/45, Loss: 0.374357670545578\nEpoch 142/200, Batch 32/45, Loss: 0.5347726345062256\nEpoch 142/200, Batch 33/45, Loss: 0.23957619071006775\nEpoch 142/200, Batch 34/45, Loss: 0.16879698634147644\nEpoch 142/200, Batch 35/45, Loss: 0.2628791034221649\nEpoch 142/200, Batch 36/45, Loss: 0.30963584780693054\nEpoch 142/200, Batch 37/45, Loss: 0.2729771137237549\nEpoch 142/200, Batch 38/45, Loss: 0.25455957651138306\nEpoch 142/200, Batch 39/45, Loss: 0.16139070689678192\nEpoch 142/200, Batch 40/45, Loss: 0.31667375564575195\nEpoch 142/200, Batch 41/45, Loss: 0.21676070988178253\nEpoch 142/200, Batch 42/45, Loss: 0.32061275839805603\nEpoch 142/200, Batch 43/45, Loss: 0.2735203206539154\nEpoch 142/200, Batch 44/45, Loss: 0.5042363405227661\nEpoch 142/200, Batch 45/45, Loss: 0.2134343385696411\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  186.2175531387329 Best Val MSE:  5.2462038397789\nEpoch:  143 , Time Elapsed:  22.759386138121286  mins\nEpoch 143/200, Batch 1/45, Loss: 0.2962222695350647\nEpoch 143/200, Batch 2/45, Loss: 0.3817915916442871\nEpoch 143/200, Batch 3/45, Loss: 0.21264199912548065\nEpoch 143/200, Batch 4/45, Loss: 0.36946189403533936\nEpoch 143/200, Batch 5/45, Loss: 0.28252357244491577\nEpoch 143/200, Batch 6/45, Loss: 0.2592991292476654\nEpoch 143/200, Batch 7/45, Loss: 0.3506685495376587\nEpoch 143/200, Batch 8/45, Loss: 0.23728352785110474\nEpoch 143/200, Batch 9/45, Loss: 0.14671102166175842\nEpoch 143/200, Batch 10/45, Loss: 0.23706524074077606\nEpoch 143/200, Batch 11/45, Loss: 0.2524409890174866\nEpoch 143/200, Batch 12/45, Loss: 0.13215342164039612\nEpoch 143/200, Batch 13/45, Loss: 0.2831159234046936\nEpoch 143/200, Batch 14/45, Loss: 0.16269923746585846\nEpoch 143/200, Batch 15/45, Loss: 0.23647023737430573\nEpoch 143/200, Batch 16/45, Loss: 0.23226536810398102\nEpoch 143/200, Batch 17/45, Loss: 0.27398109436035156\nEpoch 143/200, Batch 18/45, Loss: 0.4832058846950531\nEpoch 143/200, Batch 19/45, Loss: 0.30881911516189575\nEpoch 143/200, Batch 20/45, Loss: 0.25779926776885986\nEpoch 143/200, Batch 21/45, Loss: 2.8653550148010254\nEpoch 143/200, Batch 22/45, Loss: 0.2822750210762024\nEpoch 143/200, Batch 23/45, Loss: 0.837356448173523\nEpoch 143/200, Batch 24/45, Loss: 0.599759578704834\nEpoch 143/200, Batch 25/45, Loss: 0.792006254196167\nEpoch 143/200, Batch 26/45, Loss: 0.5624600648880005\nEpoch 143/200, Batch 27/45, Loss: 0.709907591342926\nEpoch 143/200, Batch 28/45, Loss: 0.8437732458114624\nEpoch 143/200, Batch 29/45, Loss: 0.8872871398925781\nEpoch 143/200, Batch 30/45, Loss: 0.4977709949016571\nEpoch 143/200, Batch 31/45, Loss: 0.5407763719558716\nEpoch 143/200, Batch 32/45, Loss: 0.486274778842926\nEpoch 143/200, Batch 33/45, Loss: 0.4111700654029846\nEpoch 143/200, Batch 34/45, Loss: 0.6934986114501953\nEpoch 143/200, Batch 35/45, Loss: 0.32503730058670044\nEpoch 143/200, Batch 36/45, Loss: 1.1603832244873047\nEpoch 143/200, Batch 37/45, Loss: 0.5193873047828674\nEpoch 143/200, Batch 38/45, Loss: 0.38200411200523376\nEpoch 143/200, Batch 39/45, Loss: 1.3004412651062012\nEpoch 143/200, Batch 40/45, Loss: 0.4322815537452698\nEpoch 143/200, Batch 41/45, Loss: 0.33820363879203796\nEpoch 143/200, Batch 42/45, Loss: 0.2378101646900177\nEpoch 143/200, Batch 43/45, Loss: 0.4902648329734802\nEpoch 143/200, Batch 44/45, Loss: 0.3713333010673523\nEpoch 143/200, Batch 45/45, Loss: 0.25531846284866333\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  15.103046402335167 Best Val MSE:  5.2462038397789\nEpoch:  144 , Time Elapsed:  22.91654380559921  mins\nEpoch 144/200, Batch 1/45, Loss: 0.35576823353767395\nEpoch 144/200, Batch 2/45, Loss: 0.22217567265033722\nEpoch 144/200, Batch 3/45, Loss: 0.32230091094970703\nEpoch 144/200, Batch 4/45, Loss: 0.2981133759021759\nEpoch 144/200, Batch 5/45, Loss: 0.31021589040756226\nEpoch 144/200, Batch 6/45, Loss: 0.44354957342147827\nEpoch 144/200, Batch 7/45, Loss: 0.39380884170532227\nEpoch 144/200, Batch 8/45, Loss: 0.4462662935256958\nEpoch 144/200, Batch 9/45, Loss: 0.3462832272052765\nEpoch 144/200, Batch 10/45, Loss: 0.38184309005737305\nEpoch 144/200, Batch 11/45, Loss: 0.4740191102027893\nEpoch 144/200, Batch 12/45, Loss: 0.3039703071117401\nEpoch 144/200, Batch 13/45, Loss: 0.2787322402000427\nEpoch 144/200, Batch 14/45, Loss: 0.36617642641067505\nEpoch 144/200, Batch 15/45, Loss: 0.1903921365737915\nEpoch 144/200, Batch 16/45, Loss: 0.2665712833404541\nEpoch 144/200, Batch 17/45, Loss: 0.28424695134162903\nEpoch 144/200, Batch 18/45, Loss: 0.7240370512008667\nEpoch 144/200, Batch 19/45, Loss: 1.2561734914779663\nEpoch 144/200, Batch 20/45, Loss: 0.12833383679389954\nEpoch 144/200, Batch 21/45, Loss: 0.9749497175216675\nEpoch 144/200, Batch 22/45, Loss: 0.4684838056564331\nEpoch 144/200, Batch 23/45, Loss: 0.6284867525100708\nEpoch 144/200, Batch 24/45, Loss: 0.30440840125083923\nEpoch 144/200, Batch 25/45, Loss: 0.32257604598999023\nEpoch 144/200, Batch 26/45, Loss: 0.2226077765226364\nEpoch 144/200, Batch 27/45, Loss: 0.33245235681533813\nEpoch 144/200, Batch 28/45, Loss: 0.2578944265842438\nEpoch 144/200, Batch 29/45, Loss: 0.2523878514766693\nEpoch 144/200, Batch 30/45, Loss: 0.2997887134552002\nEpoch 144/200, Batch 31/45, Loss: 0.3017651438713074\nEpoch 144/200, Batch 32/45, Loss: 0.3304794430732727\nEpoch 144/200, Batch 33/45, Loss: 0.30509161949157715\nEpoch 144/200, Batch 34/45, Loss: 0.4317181408405304\nEpoch 144/200, Batch 35/45, Loss: 0.2445458620786667\nEpoch 144/200, Batch 36/45, Loss: 0.3061756193637848\nEpoch 144/200, Batch 37/45, Loss: 0.3424338102340698\nEpoch 144/200, Batch 38/45, Loss: 0.5494827032089233\nEpoch 144/200, Batch 39/45, Loss: 0.3612731993198395\nEpoch 144/200, Batch 40/45, Loss: 0.2608794569969177\nEpoch 144/200, Batch 41/45, Loss: 0.2838362157344818\nEpoch 144/200, Batch 42/45, Loss: 0.17294791340827942\nEpoch 144/200, Batch 43/45, Loss: 0.4502580165863037\nEpoch 144/200, Batch 44/45, Loss: 0.5258816480636597\nEpoch 144/200, Batch 45/45, Loss: 0.5410560965538025\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  8.712890982627869 Best Val MSE:  5.2462038397789\nEpoch:  145 , Time Elapsed:  23.072835036118825  mins\nEpoch 145/200, Batch 1/45, Loss: 0.18378101289272308\nEpoch 145/200, Batch 2/45, Loss: 0.20752748847007751\nEpoch 145/200, Batch 3/45, Loss: 0.2583250403404236\nEpoch 145/200, Batch 4/45, Loss: 0.22177055478096008\nEpoch 145/200, Batch 5/45, Loss: 0.24086561799049377\nEpoch 145/200, Batch 6/45, Loss: 0.31420812010765076\nEpoch 145/200, Batch 7/45, Loss: 0.1913587749004364\nEpoch 145/200, Batch 8/45, Loss: 0.2628955543041229\nEpoch 145/200, Batch 9/45, Loss: 0.31255918741226196\nEpoch 145/200, Batch 10/45, Loss: 0.5713902711868286\nEpoch 145/200, Batch 11/45, Loss: 0.503962516784668\nEpoch 145/200, Batch 12/45, Loss: 0.27159950137138367\nEpoch 145/200, Batch 13/45, Loss: 0.13947990536689758\nEpoch 145/200, Batch 14/45, Loss: 0.31529927253723145\nEpoch 145/200, Batch 15/45, Loss: 0.4117039144039154\nEpoch 145/200, Batch 16/45, Loss: 0.3125208020210266\nEpoch 145/200, Batch 17/45, Loss: 0.23769131302833557\nEpoch 145/200, Batch 18/45, Loss: 0.19545546174049377\nEpoch 145/200, Batch 19/45, Loss: 0.20252498984336853\nEpoch 145/200, Batch 20/45, Loss: 0.38067182898521423\nEpoch 145/200, Batch 21/45, Loss: 0.3714587986469269\nEpoch 145/200, Batch 22/45, Loss: 0.31263500452041626\nEpoch 145/200, Batch 23/45, Loss: 0.2602906823158264\nEpoch 145/200, Batch 24/45, Loss: 0.3235911428928375\nEpoch 145/200, Batch 25/45, Loss: 0.2914353907108307\nEpoch 145/200, Batch 26/45, Loss: 0.2633884847164154\nEpoch 145/200, Batch 27/45, Loss: 0.5027396082878113\nEpoch 145/200, Batch 28/45, Loss: 0.24528568983078003\nEpoch 145/200, Batch 29/45, Loss: 0.24643178284168243\nEpoch 145/200, Batch 30/45, Loss: 0.3168381154537201\nEpoch 145/200, Batch 31/45, Loss: 0.6038685441017151\nEpoch 145/200, Batch 32/45, Loss: 0.2678478956222534\nEpoch 145/200, Batch 33/45, Loss: 0.2649680972099304\nEpoch 145/200, Batch 34/45, Loss: 0.40807926654815674\nEpoch 145/200, Batch 35/45, Loss: 0.25369399785995483\nEpoch 145/200, Batch 36/45, Loss: 0.2581642270088196\nEpoch 145/200, Batch 37/45, Loss: 0.22031280398368835\nEpoch 145/200, Batch 38/45, Loss: 0.5642522573471069\nEpoch 145/200, Batch 39/45, Loss: 0.17892485857009888\nEpoch 145/200, Batch 40/45, Loss: 0.4013751745223999\nEpoch 145/200, Batch 41/45, Loss: 0.3215116858482361\nEpoch 145/200, Batch 42/45, Loss: 0.47378379106521606\nEpoch 145/200, Batch 43/45, Loss: 0.3115922212600708\nEpoch 145/200, Batch 44/45, Loss: 0.2864929139614105\nEpoch 145/200, Batch 45/45, Loss: 0.41901499032974243\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  6.9678043872118 Best Val MSE:  5.2462038397789\nEpoch:  146 , Time Elapsed:  23.2337411125501  mins\nEpoch 146/200, Batch 1/45, Loss: 0.24321529269218445\nEpoch 146/200, Batch 2/45, Loss: 0.2401767075061798\nEpoch 146/200, Batch 3/45, Loss: 0.1625574678182602\nEpoch 146/200, Batch 4/45, Loss: 0.266948401927948\nEpoch 146/200, Batch 5/45, Loss: 0.43800321221351624\nEpoch 146/200, Batch 6/45, Loss: 0.5985631942749023\nEpoch 146/200, Batch 7/45, Loss: 0.21314190328121185\nEpoch 146/200, Batch 8/45, Loss: 0.32209110260009766\nEpoch 146/200, Batch 9/45, Loss: 0.37587231397628784\nEpoch 146/200, Batch 10/45, Loss: 0.20124608278274536\nEpoch 146/200, Batch 11/45, Loss: 0.18958443403244019\nEpoch 146/200, Batch 12/45, Loss: 0.3292263150215149\nEpoch 146/200, Batch 13/45, Loss: 0.89693683385849\nEpoch 146/200, Batch 14/45, Loss: 0.26707983016967773\nEpoch 146/200, Batch 15/45, Loss: 0.26713502407073975\nEpoch 146/200, Batch 16/45, Loss: 0.17867715656757355\nEpoch 146/200, Batch 17/45, Loss: 0.2238175868988037\nEpoch 146/200, Batch 18/45, Loss: 0.32116201519966125\nEpoch 146/200, Batch 19/45, Loss: 0.4777226746082306\nEpoch 146/200, Batch 20/45, Loss: 0.13276079297065735\nEpoch 146/200, Batch 21/45, Loss: 0.2484477013349533\nEpoch 146/200, Batch 22/45, Loss: 0.20563389360904694\nEpoch 146/200, Batch 23/45, Loss: 0.3603411316871643\nEpoch 146/200, Batch 24/45, Loss: 0.37061017751693726\nEpoch 146/200, Batch 25/45, Loss: 0.38809919357299805\nEpoch 146/200, Batch 26/45, Loss: 0.3698939085006714\nEpoch 146/200, Batch 27/45, Loss: 0.20343151688575745\nEpoch 146/200, Batch 28/45, Loss: 0.30354952812194824\nEpoch 146/200, Batch 29/45, Loss: 0.4025036692619324\nEpoch 146/200, Batch 30/45, Loss: 1.0910555124282837\nEpoch 146/200, Batch 31/45, Loss: 0.25726601481437683\nEpoch 146/200, Batch 32/45, Loss: 2.7028756141662598\nEpoch 146/200, Batch 33/45, Loss: 0.23339563608169556\nEpoch 146/200, Batch 34/45, Loss: 0.301656574010849\nEpoch 146/200, Batch 35/45, Loss: 0.21908941864967346\nEpoch 146/200, Batch 36/45, Loss: 0.2632841467857361\nEpoch 146/200, Batch 37/45, Loss: 0.4864925742149353\nEpoch 146/200, Batch 38/45, Loss: 0.2686236500740051\nEpoch 146/200, Batch 39/45, Loss: 0.39083150029182434\nEpoch 146/200, Batch 40/45, Loss: 0.5399808883666992\nEpoch 146/200, Batch 41/45, Loss: 0.3347846269607544\nEpoch 146/200, Batch 42/45, Loss: 0.5130844116210938\nEpoch 146/200, Batch 43/45, Loss: 0.29234883189201355\nEpoch 146/200, Batch 44/45, Loss: 0.360824316740036\nEpoch 146/200, Batch 45/45, Loss: 0.32750943303108215\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  865.4984741210938 Best Val MSE:  5.2462038397789\nEpoch:  147 , Time Elapsed:  23.390742607911427  mins\nEpoch 147/200, Batch 1/45, Loss: 1.9092724323272705\nEpoch 147/200, Batch 2/45, Loss: 0.31552937626838684\nEpoch 147/200, Batch 3/45, Loss: 0.4565998911857605\nEpoch 147/200, Batch 4/45, Loss: 0.8947272300720215\nEpoch 147/200, Batch 5/45, Loss: 0.36728042364120483\nEpoch 147/200, Batch 6/45, Loss: 0.18190087378025055\nEpoch 147/200, Batch 7/45, Loss: 2.2198283672332764\nEpoch 147/200, Batch 8/45, Loss: 0.365031361579895\nEpoch 147/200, Batch 9/45, Loss: 0.6217191219329834\nEpoch 147/200, Batch 10/45, Loss: 0.33578744530677795\nEpoch 147/200, Batch 11/45, Loss: 0.9286538362503052\nEpoch 147/200, Batch 12/45, Loss: 0.6328737139701843\nEpoch 147/200, Batch 13/45, Loss: 1.7995836734771729\nEpoch 147/200, Batch 14/45, Loss: 0.5525110960006714\nEpoch 147/200, Batch 15/45, Loss: 0.5160688757896423\nEpoch 147/200, Batch 16/45, Loss: 0.522361159324646\nEpoch 147/200, Batch 17/45, Loss: 0.4806874394416809\nEpoch 147/200, Batch 18/45, Loss: 0.481991171836853\nEpoch 147/200, Batch 19/45, Loss: 0.7902043461799622\nEpoch 147/200, Batch 20/45, Loss: 0.5335315465927124\nEpoch 147/200, Batch 21/45, Loss: 0.2183207869529724\nEpoch 147/200, Batch 22/45, Loss: 0.39296478033065796\nEpoch 147/200, Batch 23/45, Loss: 0.4613216817378998\nEpoch 147/200, Batch 24/45, Loss: 0.31902816891670227\nEpoch 147/200, Batch 25/45, Loss: 0.379248708486557\nEpoch 147/200, Batch 26/45, Loss: 0.24601992964744568\nEpoch 147/200, Batch 27/45, Loss: 0.3135753273963928\nEpoch 147/200, Batch 28/45, Loss: 0.3495452105998993\nEpoch 147/200, Batch 29/45, Loss: 0.42540663480758667\nEpoch 147/200, Batch 30/45, Loss: 0.4675864577293396\nEpoch 147/200, Batch 31/45, Loss: 0.38633453845977783\nEpoch 147/200, Batch 32/45, Loss: 0.30100858211517334\nEpoch 147/200, Batch 33/45, Loss: 0.49404096603393555\nEpoch 147/200, Batch 34/45, Loss: 0.4144994914531708\nEpoch 147/200, Batch 35/45, Loss: 0.3364354968070984\nEpoch 147/200, Batch 36/45, Loss: 0.21494995057582855\nEpoch 147/200, Batch 37/45, Loss: 0.5965996980667114\nEpoch 147/200, Batch 38/45, Loss: 0.31534692645072937\nEpoch 147/200, Batch 39/45, Loss: 0.22773689031600952\nEpoch 147/200, Batch 40/45, Loss: 0.22890737652778625\nEpoch 147/200, Batch 41/45, Loss: 0.21859446167945862\nEpoch 147/200, Batch 42/45, Loss: 0.28619977831840515\nEpoch 147/200, Batch 43/45, Loss: 0.31627988815307617\nEpoch 147/200, Batch 44/45, Loss: 0.47159188985824585\nEpoch 147/200, Batch 45/45, Loss: 0.4525359869003296\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  9.605272859334946 Best Val MSE:  5.2462038397789\nEpoch:  148 , Time Elapsed:  23.548982071876527  mins\nEpoch 148/200, Batch 1/45, Loss: 0.37538257241249084\nEpoch 148/200, Batch 2/45, Loss: 0.28900155425071716\nEpoch 148/200, Batch 3/45, Loss: 0.4367278516292572\nEpoch 148/200, Batch 4/45, Loss: 0.17804007232189178\nEpoch 148/200, Batch 5/45, Loss: 0.37638241052627563\nEpoch 148/200, Batch 6/45, Loss: 0.3528395891189575\nEpoch 148/200, Batch 7/45, Loss: 0.38392168283462524\nEpoch 148/200, Batch 8/45, Loss: 0.23163259029388428\nEpoch 148/200, Batch 9/45, Loss: 0.231481671333313\nEpoch 148/200, Batch 10/45, Loss: 0.22998830676078796\nEpoch 148/200, Batch 11/45, Loss: 0.3686714768409729\nEpoch 148/200, Batch 12/45, Loss: 0.16238537430763245\nEpoch 148/200, Batch 13/45, Loss: 0.33526331186294556\nEpoch 148/200, Batch 14/45, Loss: 0.45614710450172424\nEpoch 148/200, Batch 15/45, Loss: 0.29070621728897095\nEpoch 148/200, Batch 16/45, Loss: 0.19129371643066406\nEpoch 148/200, Batch 17/45, Loss: 0.6336065530776978\nEpoch 148/200, Batch 18/45, Loss: 0.43632110953330994\nEpoch 148/200, Batch 19/45, Loss: 0.23653781414031982\nEpoch 148/200, Batch 20/45, Loss: 0.6879073977470398\nEpoch 148/200, Batch 21/45, Loss: 0.5047471523284912\nEpoch 148/200, Batch 22/45, Loss: 0.2777886688709259\nEpoch 148/200, Batch 23/45, Loss: 0.2285608947277069\nEpoch 148/200, Batch 24/45, Loss: 0.2563205659389496\nEpoch 148/200, Batch 25/45, Loss: 0.4023802876472473\nEpoch 148/200, Batch 26/45, Loss: 0.29084017872810364\nEpoch 148/200, Batch 27/45, Loss: 0.2221338152885437\nEpoch 148/200, Batch 28/45, Loss: 0.2793833017349243\nEpoch 148/200, Batch 29/45, Loss: 0.371974378824234\nEpoch 148/200, Batch 30/45, Loss: 0.17478080093860626\nEpoch 148/200, Batch 31/45, Loss: 0.27379027009010315\nEpoch 148/200, Batch 32/45, Loss: 0.44655314087867737\nEpoch 148/200, Batch 33/45, Loss: 0.28631851077079773\nEpoch 148/200, Batch 34/45, Loss: 0.19893166422843933\nEpoch 148/200, Batch 35/45, Loss: 0.5316972136497498\nEpoch 148/200, Batch 36/45, Loss: 0.23519137501716614\nEpoch 148/200, Batch 37/45, Loss: 0.16714292764663696\nEpoch 148/200, Batch 38/45, Loss: 0.2899402379989624\nEpoch 148/200, Batch 39/45, Loss: 0.299068808555603\nEpoch 148/200, Batch 40/45, Loss: 0.3652251958847046\nEpoch 148/200, Batch 41/45, Loss: 0.5198901891708374\nEpoch 148/200, Batch 42/45, Loss: 0.23370271921157837\nEpoch 148/200, Batch 43/45, Loss: 1.233394980430603\nEpoch 148/200, Batch 44/45, Loss: 0.19751134514808655\nEpoch 148/200, Batch 45/45, Loss: 0.2235613763332367\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  492.56310844421387 Best Val MSE:  5.2462038397789\nEpoch:  149 , Time Elapsed:  23.71100516319275  mins\nEpoch 149/200, Batch 1/45, Loss: 0.28384384512901306\nEpoch 149/200, Batch 2/45, Loss: 0.3128696382045746\nEpoch 149/200, Batch 3/45, Loss: 0.31756073236465454\nEpoch 149/200, Batch 4/45, Loss: 0.29648125171661377\nEpoch 149/200, Batch 5/45, Loss: 0.4021446704864502\nEpoch 149/200, Batch 6/45, Loss: 0.37321069836616516\nEpoch 149/200, Batch 7/45, Loss: 0.21889697015285492\nEpoch 149/200, Batch 8/45, Loss: 0.23301811516284943\nEpoch 149/200, Batch 9/45, Loss: 0.2708795368671417\nEpoch 149/200, Batch 10/45, Loss: 0.25844982266426086\nEpoch 149/200, Batch 11/45, Loss: 0.2501179277896881\nEpoch 149/200, Batch 12/45, Loss: 0.38508450984954834\nEpoch 149/200, Batch 13/45, Loss: 0.20100165903568268\nEpoch 149/200, Batch 14/45, Loss: 0.202478289604187\nEpoch 149/200, Batch 15/45, Loss: 0.17606592178344727\nEpoch 149/200, Batch 16/45, Loss: 0.30779266357421875\nEpoch 149/200, Batch 17/45, Loss: 0.23191452026367188\nEpoch 149/200, Batch 18/45, Loss: 0.2886609137058258\nEpoch 149/200, Batch 19/45, Loss: 0.4304724633693695\nEpoch 149/200, Batch 20/45, Loss: 0.23290571570396423\nEpoch 149/200, Batch 21/45, Loss: 0.28976714611053467\nEpoch 149/200, Batch 22/45, Loss: 0.9090485572814941\nEpoch 149/200, Batch 23/45, Loss: 0.3708105683326721\nEpoch 149/200, Batch 24/45, Loss: 0.3930891156196594\nEpoch 149/200, Batch 25/45, Loss: 0.23183691501617432\nEpoch 149/200, Batch 26/45, Loss: 0.27618011832237244\nEpoch 149/200, Batch 27/45, Loss: 0.3964976370334625\nEpoch 149/200, Batch 28/45, Loss: 0.17894569039344788\nEpoch 149/200, Batch 29/45, Loss: 0.44356879591941833\nEpoch 149/200, Batch 30/45, Loss: 0.30881762504577637\nEpoch 149/200, Batch 31/45, Loss: 0.20200346410274506\nEpoch 149/200, Batch 32/45, Loss: 0.28879880905151367\nEpoch 149/200, Batch 33/45, Loss: 0.40278908610343933\nEpoch 149/200, Batch 34/45, Loss: 0.29824239015579224\nEpoch 149/200, Batch 35/45, Loss: 0.10878999531269073\nEpoch 149/200, Batch 36/45, Loss: 0.18458706140518188\nEpoch 149/200, Batch 37/45, Loss: 0.17270830273628235\nEpoch 149/200, Batch 38/45, Loss: 0.30217450857162476\nEpoch 149/200, Batch 39/45, Loss: 0.2770957946777344\nEpoch 149/200, Batch 40/45, Loss: 0.2562638521194458\nEpoch 149/200, Batch 41/45, Loss: 0.5640002489089966\nEpoch 149/200, Batch 42/45, Loss: 0.20489944517612457\nEpoch 149/200, Batch 43/45, Loss: 0.3223172128200531\nEpoch 149/200, Batch 44/45, Loss: 0.5253420472145081\nEpoch 149/200, Batch 45/45, Loss: 0.23297631740570068\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  9.194700419902802 Best Val MSE:  5.2462038397789\nEpoch:  150 , Time Elapsed:  23.87008604208628  mins\nEpoch 150/200, Batch 1/45, Loss: 0.23345118761062622\nEpoch 150/200, Batch 2/45, Loss: 0.3886096477508545\nEpoch 150/200, Batch 3/45, Loss: 0.373481422662735\nEpoch 150/200, Batch 4/45, Loss: 0.3761582374572754\nEpoch 150/200, Batch 5/45, Loss: 0.25142911076545715\nEpoch 150/200, Batch 6/45, Loss: 0.3874599039554596\nEpoch 150/200, Batch 7/45, Loss: 0.22490084171295166\nEpoch 150/200, Batch 8/45, Loss: 0.4184283912181854\nEpoch 150/200, Batch 9/45, Loss: 0.3471764922142029\nEpoch 150/200, Batch 10/45, Loss: 0.20969423651695251\nEpoch 150/200, Batch 11/45, Loss: 0.4008958339691162\nEpoch 150/200, Batch 12/45, Loss: 0.3880215287208557\nEpoch 150/200, Batch 13/45, Loss: 0.2371569275856018\nEpoch 150/200, Batch 14/45, Loss: 0.21406376361846924\nEpoch 150/200, Batch 15/45, Loss: 0.3206784725189209\nEpoch 150/200, Batch 16/45, Loss: 0.19227641820907593\nEpoch 150/200, Batch 17/45, Loss: 0.14627686142921448\nEpoch 150/200, Batch 18/45, Loss: 0.2015027105808258\nEpoch 150/200, Batch 19/45, Loss: 0.2782750725746155\nEpoch 150/200, Batch 20/45, Loss: 0.1666417121887207\nEpoch 150/200, Batch 21/45, Loss: 0.272479772567749\nEpoch 150/200, Batch 22/45, Loss: 0.21016807854175568\nEpoch 150/200, Batch 23/45, Loss: 0.25353875756263733\nEpoch 150/200, Batch 24/45, Loss: 0.35745537281036377\nEpoch 150/200, Batch 25/45, Loss: 0.3023383915424347\nEpoch 150/200, Batch 26/45, Loss: 0.26644963026046753\nEpoch 150/200, Batch 27/45, Loss: 0.30552956461906433\nEpoch 150/200, Batch 28/45, Loss: 0.13731035590171814\nEpoch 150/200, Batch 29/45, Loss: 0.2015984207391739\nEpoch 150/200, Batch 30/45, Loss: 0.1805943250656128\nEpoch 150/200, Batch 31/45, Loss: 0.34915757179260254\nEpoch 150/200, Batch 32/45, Loss: 0.4266591966152191\nEpoch 150/200, Batch 33/45, Loss: 0.3229633867740631\nEpoch 150/200, Batch 34/45, Loss: 0.36788278818130493\nEpoch 150/200, Batch 35/45, Loss: 0.5919352173805237\nEpoch 150/200, Batch 36/45, Loss: 0.8905411958694458\nEpoch 150/200, Batch 37/45, Loss: 0.2344624400138855\nEpoch 150/200, Batch 38/45, Loss: 0.2349824607372284\nEpoch 150/200, Batch 39/45, Loss: 0.41716575622558594\nEpoch 150/200, Batch 40/45, Loss: 0.2566452622413635\nEpoch 150/200, Batch 41/45, Loss: 0.3047398030757904\nEpoch 150/200, Batch 42/45, Loss: 0.1704026758670807\nEpoch 150/200, Batch 43/45, Loss: 0.3913436830043793\nEpoch 150/200, Batch 44/45, Loss: 0.2621128261089325\nEpoch 150/200, Batch 45/45, Loss: 0.19383050501346588\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  8.884865686297417 Best Val MSE:  5.2462038397789\nEpoch:  151 , Time Elapsed:  24.023150289058684  mins\nEpoch 151/200, Batch 1/45, Loss: 0.20905491709709167\nEpoch 151/200, Batch 2/45, Loss: 0.4755978584289551\nEpoch 151/200, Batch 3/45, Loss: 0.24741941690444946\nEpoch 151/200, Batch 4/45, Loss: 0.28697729110717773\nEpoch 151/200, Batch 5/45, Loss: 0.18553420901298523\nEpoch 151/200, Batch 6/45, Loss: 0.22206950187683105\nEpoch 151/200, Batch 7/45, Loss: 0.25693514943122864\nEpoch 151/200, Batch 8/45, Loss: 0.328269898891449\nEpoch 151/200, Batch 9/45, Loss: 0.26596295833587646\nEpoch 151/200, Batch 10/45, Loss: 0.1622137725353241\nEpoch 151/200, Batch 11/45, Loss: 0.26393771171569824\nEpoch 151/200, Batch 12/45, Loss: 0.2975314259529114\nEpoch 151/200, Batch 13/45, Loss: 0.3100455403327942\nEpoch 151/200, Batch 14/45, Loss: 0.18515890836715698\nEpoch 151/200, Batch 15/45, Loss: 0.23099076747894287\nEpoch 151/200, Batch 16/45, Loss: 0.6103598475456238\nEpoch 151/200, Batch 17/45, Loss: 0.22976455092430115\nEpoch 151/200, Batch 18/45, Loss: 0.29110294580459595\nEpoch 151/200, Batch 19/45, Loss: 0.31207942962646484\nEpoch 151/200, Batch 20/45, Loss: 0.4982318580150604\nEpoch 151/200, Batch 21/45, Loss: 0.33925026655197144\nEpoch 151/200, Batch 22/45, Loss: 0.3329847455024719\nEpoch 151/200, Batch 23/45, Loss: 0.287828266620636\nEpoch 151/200, Batch 24/45, Loss: 0.4764055907726288\nEpoch 151/200, Batch 25/45, Loss: 0.22211802005767822\nEpoch 151/200, Batch 26/45, Loss: 0.2500537931919098\nEpoch 151/200, Batch 27/45, Loss: 0.26932063698768616\nEpoch 151/200, Batch 28/45, Loss: 0.20761306583881378\nEpoch 151/200, Batch 29/45, Loss: 0.14186912775039673\nEpoch 151/200, Batch 30/45, Loss: 0.3122190833091736\nEpoch 151/200, Batch 31/45, Loss: 0.27423518896102905\nEpoch 151/200, Batch 32/45, Loss: 0.3639632761478424\nEpoch 151/200, Batch 33/45, Loss: 0.21445661783218384\nEpoch 151/200, Batch 34/45, Loss: 0.24858783185482025\nEpoch 151/200, Batch 35/45, Loss: 0.24273544549942017\nEpoch 151/200, Batch 36/45, Loss: 0.18318504095077515\nEpoch 151/200, Batch 37/45, Loss: 0.25011536478996277\nEpoch 151/200, Batch 38/45, Loss: 0.254026859998703\nEpoch 151/200, Batch 39/45, Loss: 0.4750571846961975\nEpoch 151/200, Batch 40/45, Loss: 0.384765625\nEpoch 151/200, Batch 41/45, Loss: 0.36239027976989746\nEpoch 151/200, Batch 42/45, Loss: 0.7687689065933228\nEpoch 151/200, Batch 43/45, Loss: 0.2532987594604492\nEpoch 151/200, Batch 44/45, Loss: 0.345703661441803\nEpoch 151/200, Batch 45/45, Loss: 0.2545025646686554\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  60.94451200962067 Best Val MSE:  5.2462038397789\nEpoch:  152 , Time Elapsed:  24.178951950867972  mins\nEpoch 152/200, Batch 1/45, Loss: 0.12614935636520386\nEpoch 152/200, Batch 2/45, Loss: 0.303855836391449\nEpoch 152/200, Batch 3/45, Loss: 0.26502737402915955\nEpoch 152/200, Batch 4/45, Loss: 0.3673282563686371\nEpoch 152/200, Batch 5/45, Loss: 0.22305771708488464\nEpoch 152/200, Batch 6/45, Loss: 0.27847638726234436\nEpoch 152/200, Batch 7/45, Loss: 0.23023010790348053\nEpoch 152/200, Batch 8/45, Loss: 0.23975202441215515\nEpoch 152/200, Batch 9/45, Loss: 0.3648422360420227\nEpoch 152/200, Batch 10/45, Loss: 0.497649610042572\nEpoch 152/200, Batch 11/45, Loss: 0.28231096267700195\nEpoch 152/200, Batch 12/45, Loss: 0.42491382360458374\nEpoch 152/200, Batch 13/45, Loss: 0.39215564727783203\nEpoch 152/200, Batch 14/45, Loss: 0.22023236751556396\nEpoch 152/200, Batch 15/45, Loss: 0.2496301233768463\nEpoch 152/200, Batch 16/45, Loss: 0.10380633175373077\nEpoch 152/200, Batch 17/45, Loss: 0.3693353533744812\nEpoch 152/200, Batch 18/45, Loss: 0.16238059103488922\nEpoch 152/200, Batch 19/45, Loss: 0.30930572748184204\nEpoch 152/200, Batch 20/45, Loss: 0.2531324625015259\nEpoch 152/200, Batch 21/45, Loss: 0.2000947743654251\nEpoch 152/200, Batch 22/45, Loss: 0.29433301091194153\nEpoch 152/200, Batch 23/45, Loss: 0.2945207357406616\nEpoch 152/200, Batch 24/45, Loss: 0.22833961248397827\nEpoch 152/200, Batch 25/45, Loss: 0.2395031601190567\nEpoch 152/200, Batch 26/45, Loss: 0.624567449092865\nEpoch 152/200, Batch 27/45, Loss: 0.20906399190425873\nEpoch 152/200, Batch 28/45, Loss: 0.25473400950431824\nEpoch 152/200, Batch 29/45, Loss: 0.21168990433216095\nEpoch 152/200, Batch 30/45, Loss: 0.4703063368797302\nEpoch 152/200, Batch 31/45, Loss: 0.23731985688209534\nEpoch 152/200, Batch 32/45, Loss: 0.2536340355873108\nEpoch 152/200, Batch 33/45, Loss: 0.4106588363647461\nEpoch 152/200, Batch 34/45, Loss: 0.2761855721473694\nEpoch 152/200, Batch 35/45, Loss: 0.5041810870170593\nEpoch 152/200, Batch 36/45, Loss: 0.3421247601509094\nEpoch 152/200, Batch 37/45, Loss: 0.2165723443031311\nEpoch 152/200, Batch 38/45, Loss: 0.21249540150165558\nEpoch 152/200, Batch 39/45, Loss: 0.3416998088359833\nEpoch 152/200, Batch 40/45, Loss: 0.19164299964904785\nEpoch 152/200, Batch 41/45, Loss: 0.39447125792503357\nEpoch 152/200, Batch 42/45, Loss: 0.21734264492988586\nEpoch 152/200, Batch 43/45, Loss: 0.21486543118953705\nEpoch 152/200, Batch 44/45, Loss: 0.24920286238193512\nEpoch 152/200, Batch 45/45, Loss: 0.39515137672424316\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  19.73068055510521 Best Val MSE:  5.2462038397789\nEpoch:  153 , Time Elapsed:  24.340119155248008  mins\nEpoch 153/200, Batch 1/45, Loss: 0.32205337285995483\nEpoch 153/200, Batch 2/45, Loss: 0.1928485929965973\nEpoch 153/200, Batch 3/45, Loss: 0.7504682540893555\nEpoch 153/200, Batch 4/45, Loss: 0.24151848256587982\nEpoch 153/200, Batch 5/45, Loss: 0.21591518819332123\nEpoch 153/200, Batch 6/45, Loss: 0.30630600452423096\nEpoch 153/200, Batch 7/45, Loss: 0.2858871519565582\nEpoch 153/200, Batch 8/45, Loss: 0.14729216694831848\nEpoch 153/200, Batch 9/45, Loss: 0.28505635261535645\nEpoch 153/200, Batch 10/45, Loss: 0.26209646463394165\nEpoch 153/200, Batch 11/45, Loss: 0.29250064492225647\nEpoch 153/200, Batch 12/45, Loss: 0.3699241578578949\nEpoch 153/200, Batch 13/45, Loss: 0.3025636672973633\nEpoch 153/200, Batch 14/45, Loss: 0.215162456035614\nEpoch 153/200, Batch 15/45, Loss: 0.4169445335865021\nEpoch 153/200, Batch 16/45, Loss: 0.37448298931121826\nEpoch 153/200, Batch 17/45, Loss: 0.47156497836112976\nEpoch 153/200, Batch 18/45, Loss: 0.21930785477161407\nEpoch 153/200, Batch 19/45, Loss: 0.13848820328712463\nEpoch 153/200, Batch 20/45, Loss: 0.2586560845375061\nEpoch 153/200, Batch 21/45, Loss: 0.18027661740779877\nEpoch 153/200, Batch 22/45, Loss: 0.21171662211418152\nEpoch 153/200, Batch 23/45, Loss: 0.07358451187610626\nEpoch 153/200, Batch 24/45, Loss: 0.2162625938653946\nEpoch 153/200, Batch 25/45, Loss: 0.28193891048431396\nEpoch 153/200, Batch 26/45, Loss: 0.3856251835823059\nEpoch 153/200, Batch 27/45, Loss: 0.23730407655239105\nEpoch 153/200, Batch 28/45, Loss: 0.2598775625228882\nEpoch 153/200, Batch 29/45, Loss: 0.24922910332679749\nEpoch 153/200, Batch 30/45, Loss: 0.2819303274154663\nEpoch 153/200, Batch 31/45, Loss: 0.23626568913459778\nEpoch 153/200, Batch 32/45, Loss: 0.2546921372413635\nEpoch 153/200, Batch 33/45, Loss: 0.18935014307498932\nEpoch 153/200, Batch 34/45, Loss: 0.4460282623767853\nEpoch 153/200, Batch 35/45, Loss: 0.22515437006950378\nEpoch 153/200, Batch 36/45, Loss: 0.2809426188468933\nEpoch 153/200, Batch 37/45, Loss: 0.2678467631340027\nEpoch 153/200, Batch 38/45, Loss: 0.174758180975914\nEpoch 153/200, Batch 39/45, Loss: 0.22462627291679382\nEpoch 153/200, Batch 40/45, Loss: 0.2893788814544678\nEpoch 153/200, Batch 41/45, Loss: 0.20092499256134033\nEpoch 153/200, Batch 42/45, Loss: 0.29251185059547424\nEpoch 153/200, Batch 43/45, Loss: 0.3997125029563904\nEpoch 153/200, Batch 44/45, Loss: 0.23670098185539246\nEpoch 153/200, Batch 45/45, Loss: 0.14507538080215454\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  6.3309880048036575 Best Val MSE:  5.2462038397789\nEpoch:  154 , Time Elapsed:  24.494554181893665  mins\nEpoch 154/200, Batch 1/45, Loss: 0.27863746881484985\nEpoch 154/200, Batch 2/45, Loss: 0.17613822221755981\nEpoch 154/200, Batch 3/45, Loss: 0.22272491455078125\nEpoch 154/200, Batch 4/45, Loss: 0.2836381196975708\nEpoch 154/200, Batch 5/45, Loss: 0.3424665033817291\nEpoch 154/200, Batch 6/45, Loss: 0.21853385865688324\nEpoch 154/200, Batch 7/45, Loss: 0.1723911464214325\nEpoch 154/200, Batch 8/45, Loss: 0.31695428490638733\nEpoch 154/200, Batch 9/45, Loss: 0.20364032685756683\nEpoch 154/200, Batch 10/45, Loss: 0.12708520889282227\nEpoch 154/200, Batch 11/45, Loss: 0.5704159736633301\nEpoch 154/200, Batch 12/45, Loss: 0.38740694522857666\nEpoch 154/200, Batch 13/45, Loss: 0.438822865486145\nEpoch 154/200, Batch 14/45, Loss: 0.3234398066997528\nEpoch 154/200, Batch 15/45, Loss: 0.2963430881500244\nEpoch 154/200, Batch 16/45, Loss: 0.376895010471344\nEpoch 154/200, Batch 17/45, Loss: 0.28263673186302185\nEpoch 154/200, Batch 18/45, Loss: 0.3366939425468445\nEpoch 154/200, Batch 19/45, Loss: 0.2793566584587097\nEpoch 154/200, Batch 20/45, Loss: 0.31545311212539673\nEpoch 154/200, Batch 21/45, Loss: 0.33081239461898804\nEpoch 154/200, Batch 22/45, Loss: 0.3459815979003906\nEpoch 154/200, Batch 23/45, Loss: 0.2390100359916687\nEpoch 154/200, Batch 24/45, Loss: 0.3211505115032196\nEpoch 154/200, Batch 25/45, Loss: 0.35513466596603394\nEpoch 154/200, Batch 26/45, Loss: 0.25139519572257996\nEpoch 154/200, Batch 27/45, Loss: 0.30927568674087524\nEpoch 154/200, Batch 28/45, Loss: 0.2852405905723572\nEpoch 154/200, Batch 29/45, Loss: 0.20894980430603027\nEpoch 154/200, Batch 30/45, Loss: 0.4539996385574341\nEpoch 154/200, Batch 31/45, Loss: 0.30937501788139343\nEpoch 154/200, Batch 32/45, Loss: 0.16195125877857208\nEpoch 154/200, Batch 33/45, Loss: 0.32270121574401855\nEpoch 154/200, Batch 34/45, Loss: 0.7280064821243286\nEpoch 154/200, Batch 35/45, Loss: 0.22103342413902283\nEpoch 154/200, Batch 36/45, Loss: 0.30802303552627563\nEpoch 154/200, Batch 37/45, Loss: 0.2673529088497162\nEpoch 154/200, Batch 38/45, Loss: 0.35143977403640747\nEpoch 154/200, Batch 39/45, Loss: 0.2402118593454361\nEpoch 154/200, Batch 40/45, Loss: 0.20694319903850555\nEpoch 154/200, Batch 41/45, Loss: 0.4538770020008087\nEpoch 154/200, Batch 42/45, Loss: 0.2332497537136078\nEpoch 154/200, Batch 43/45, Loss: 0.2314569652080536\nEpoch 154/200, Batch 44/45, Loss: 0.24340270459651947\nEpoch 154/200, Batch 45/45, Loss: 0.2462453693151474\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  22.66489690542221 Best Val MSE:  5.2462038397789\nEpoch:  155 , Time Elapsed:  24.650330992539725  mins\nEpoch 155/200, Batch 1/45, Loss: 0.36181193590164185\nEpoch 155/200, Batch 2/45, Loss: 0.22481922805309296\nEpoch 155/200, Batch 3/45, Loss: 0.4272439181804657\nEpoch 155/200, Batch 4/45, Loss: 0.41189682483673096\nEpoch 155/200, Batch 5/45, Loss: 0.35217398405075073\nEpoch 155/200, Batch 6/45, Loss: 0.24239808320999146\nEpoch 155/200, Batch 7/45, Loss: 0.3863411247730255\nEpoch 155/200, Batch 8/45, Loss: 0.29747676849365234\nEpoch 155/200, Batch 9/45, Loss: 0.3166460394859314\nEpoch 155/200, Batch 10/45, Loss: 0.15279942750930786\nEpoch 155/200, Batch 11/45, Loss: 0.30707043409347534\nEpoch 155/200, Batch 12/45, Loss: 0.22493278980255127\nEpoch 155/200, Batch 13/45, Loss: 0.17911212146282196\nEpoch 155/200, Batch 14/45, Loss: 0.17978373169898987\nEpoch 155/200, Batch 15/45, Loss: 1.0471569299697876\nEpoch 155/200, Batch 16/45, Loss: 0.3472558856010437\nEpoch 155/200, Batch 17/45, Loss: 0.3627762794494629\nEpoch 155/200, Batch 18/45, Loss: 0.17830535769462585\nEpoch 155/200, Batch 19/45, Loss: 0.3341904282569885\nEpoch 155/200, Batch 20/45, Loss: 0.26942145824432373\nEpoch 155/200, Batch 21/45, Loss: 0.18978029489517212\nEpoch 155/200, Batch 22/45, Loss: 0.2625110447406769\nEpoch 155/200, Batch 23/45, Loss: 0.3192964792251587\nEpoch 155/200, Batch 24/45, Loss: 0.16638129949569702\nEpoch 155/200, Batch 25/45, Loss: 0.17986419796943665\nEpoch 155/200, Batch 26/45, Loss: 0.2737271785736084\nEpoch 155/200, Batch 27/45, Loss: 0.2665691077709198\nEpoch 155/200, Batch 28/45, Loss: 0.23569419980049133\nEpoch 155/200, Batch 29/45, Loss: 0.6039806604385376\nEpoch 155/200, Batch 30/45, Loss: 0.25547054409980774\nEpoch 155/200, Batch 31/45, Loss: 0.2428797036409378\nEpoch 155/200, Batch 32/45, Loss: 0.2967977523803711\nEpoch 155/200, Batch 33/45, Loss: 0.21931184828281403\nEpoch 155/200, Batch 34/45, Loss: 0.4874827563762665\nEpoch 155/200, Batch 35/45, Loss: 0.267769455909729\nEpoch 155/200, Batch 36/45, Loss: 0.20049327611923218\nEpoch 155/200, Batch 37/45, Loss: 0.4755955934524536\nEpoch 155/200, Batch 38/45, Loss: 0.4788949489593506\nEpoch 155/200, Batch 39/45, Loss: 0.2597355842590332\nEpoch 155/200, Batch 40/45, Loss: 0.41668248176574707\nEpoch 155/200, Batch 41/45, Loss: 0.27814584970474243\nEpoch 155/200, Batch 42/45, Loss: 0.21306666731834412\nEpoch 155/200, Batch 43/45, Loss: 0.2906075119972229\nEpoch 155/200, Batch 44/45, Loss: 0.24776387214660645\nEpoch 155/200, Batch 45/45, Loss: 0.2347010225057602\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  246.75244855880737 Best Val MSE:  5.2462038397789\nEpoch:  156 , Time Elapsed:  24.809050198396047  mins\nEpoch 156/200, Batch 1/45, Loss: 0.17254763841629028\nEpoch 156/200, Batch 2/45, Loss: 0.7461065053939819\nEpoch 156/200, Batch 3/45, Loss: 0.2382739633321762\nEpoch 156/200, Batch 4/45, Loss: 0.2682962715625763\nEpoch 156/200, Batch 5/45, Loss: 0.26194602251052856\nEpoch 156/200, Batch 6/45, Loss: 0.1503143012523651\nEpoch 156/200, Batch 7/45, Loss: 0.17921724915504456\nEpoch 156/200, Batch 8/45, Loss: 0.3056703209877014\nEpoch 156/200, Batch 9/45, Loss: 0.39037394523620605\nEpoch 156/200, Batch 10/45, Loss: 0.6251765489578247\nEpoch 156/200, Batch 11/45, Loss: 0.24011141061782837\nEpoch 156/200, Batch 12/45, Loss: 0.31770849227905273\nEpoch 156/200, Batch 13/45, Loss: 0.5402143597602844\nEpoch 156/200, Batch 14/45, Loss: 0.4755162000656128\nEpoch 156/200, Batch 15/45, Loss: 0.3762304186820984\nEpoch 156/200, Batch 16/45, Loss: 0.2979888319969177\nEpoch 156/200, Batch 17/45, Loss: 0.40283945202827454\nEpoch 156/200, Batch 18/45, Loss: 0.5805290937423706\nEpoch 156/200, Batch 19/45, Loss: 0.23457029461860657\nEpoch 156/200, Batch 20/45, Loss: 0.1850820779800415\nEpoch 156/200, Batch 21/45, Loss: 0.5791195034980774\nEpoch 156/200, Batch 22/45, Loss: 0.2975800931453705\nEpoch 156/200, Batch 23/45, Loss: 0.27318722009658813\nEpoch 156/200, Batch 24/45, Loss: 0.30961310863494873\nEpoch 156/200, Batch 25/45, Loss: 0.33354616165161133\nEpoch 156/200, Batch 26/45, Loss: 0.3602694272994995\nEpoch 156/200, Batch 27/45, Loss: 0.33455294370651245\nEpoch 156/200, Batch 28/45, Loss: 0.3399527668952942\nEpoch 156/200, Batch 29/45, Loss: 0.35481348633766174\nEpoch 156/200, Batch 30/45, Loss: 0.22331927716732025\nEpoch 156/200, Batch 31/45, Loss: 0.3117028474807739\nEpoch 156/200, Batch 32/45, Loss: 0.21267904341220856\nEpoch 156/200, Batch 33/45, Loss: 0.28732872009277344\nEpoch 156/200, Batch 34/45, Loss: 0.22482722997665405\nEpoch 156/200, Batch 35/45, Loss: 0.41914689540863037\nEpoch 156/200, Batch 36/45, Loss: 0.1921478509902954\nEpoch 156/200, Batch 37/45, Loss: 0.38556697964668274\nEpoch 156/200, Batch 38/45, Loss: 0.18175595998764038\nEpoch 156/200, Batch 39/45, Loss: 0.3074203431606293\nEpoch 156/200, Batch 40/45, Loss: 0.37369638681411743\nEpoch 156/200, Batch 41/45, Loss: 0.1622864007949829\nEpoch 156/200, Batch 42/45, Loss: 0.2670838236808777\nEpoch 156/200, Batch 43/45, Loss: 0.1845540702342987\nEpoch 156/200, Batch 44/45, Loss: 0.3483737111091614\nEpoch 156/200, Batch 45/45, Loss: 0.20555801689624786\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  23.134657382965088 Best Val MSE:  5.2462038397789\nEpoch:  157 , Time Elapsed:  24.963786554336547  mins\nEpoch 157/200, Batch 1/45, Loss: 0.27609339356422424\nEpoch 157/200, Batch 2/45, Loss: 0.19055314362049103\nEpoch 157/200, Batch 3/45, Loss: 0.1694713979959488\nEpoch 157/200, Batch 4/45, Loss: 0.2558525502681732\nEpoch 157/200, Batch 5/45, Loss: 0.34526073932647705\nEpoch 157/200, Batch 6/45, Loss: 0.37113797664642334\nEpoch 157/200, Batch 7/45, Loss: 0.44627511501312256\nEpoch 157/200, Batch 8/45, Loss: 0.2618589699268341\nEpoch 157/200, Batch 9/45, Loss: 0.18523922562599182\nEpoch 157/200, Batch 10/45, Loss: 0.4150492548942566\nEpoch 157/200, Batch 11/45, Loss: 0.33624550700187683\nEpoch 157/200, Batch 12/45, Loss: 0.3201034665107727\nEpoch 157/200, Batch 13/45, Loss: 0.19863268733024597\nEpoch 157/200, Batch 14/45, Loss: 0.26859694719314575\nEpoch 157/200, Batch 15/45, Loss: 0.22430333495140076\nEpoch 157/200, Batch 16/45, Loss: 0.2480272650718689\nEpoch 157/200, Batch 17/45, Loss: 0.22491028904914856\nEpoch 157/200, Batch 18/45, Loss: 0.2344655692577362\nEpoch 157/200, Batch 19/45, Loss: 0.13997957110404968\nEpoch 157/200, Batch 20/45, Loss: 0.19140127301216125\nEpoch 157/200, Batch 21/45, Loss: 0.25822579860687256\nEpoch 157/200, Batch 22/45, Loss: 0.28846102952957153\nEpoch 157/200, Batch 23/45, Loss: 0.22877314686775208\nEpoch 157/200, Batch 24/45, Loss: 0.23848694562911987\nEpoch 157/200, Batch 25/45, Loss: 0.3081131875514984\nEpoch 157/200, Batch 26/45, Loss: 0.25964996218681335\nEpoch 157/200, Batch 27/45, Loss: 0.2420182079076767\nEpoch 157/200, Batch 28/45, Loss: 0.2896333336830139\nEpoch 157/200, Batch 29/45, Loss: 0.22121475636959076\nEpoch 157/200, Batch 30/45, Loss: 0.2525436580181122\nEpoch 157/200, Batch 31/45, Loss: 0.25409433245658875\nEpoch 157/200, Batch 32/45, Loss: 0.44675272703170776\nEpoch 157/200, Batch 33/45, Loss: 0.3588651716709137\nEpoch 157/200, Batch 34/45, Loss: 0.1864665299654007\nEpoch 157/200, Batch 35/45, Loss: 0.18543240427970886\nEpoch 157/200, Batch 36/45, Loss: 0.35018736124038696\nEpoch 157/200, Batch 37/45, Loss: 0.27425941824913025\nEpoch 157/200, Batch 38/45, Loss: 0.2607586681842804\nEpoch 157/200, Batch 39/45, Loss: 0.34682539105415344\nEpoch 157/200, Batch 40/45, Loss: 0.23222774267196655\nEpoch 157/200, Batch 41/45, Loss: 0.13211899995803833\nEpoch 157/200, Batch 42/45, Loss: 0.2682565450668335\nEpoch 157/200, Batch 43/45, Loss: 0.163102924823761\nEpoch 157/200, Batch 44/45, Loss: 0.1347104161977768\nEpoch 157/200, Batch 45/45, Loss: 0.1657833456993103\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  103.82199454307556 Best Val MSE:  5.2462038397789\nEpoch:  158 , Time Elapsed:  25.118652061621347  mins\nEpoch 158/200, Batch 1/45, Loss: 0.14372460544109344\nEpoch 158/200, Batch 2/45, Loss: 0.17568162083625793\nEpoch 158/200, Batch 3/45, Loss: 0.2568379044532776\nEpoch 158/200, Batch 4/45, Loss: 0.29516732692718506\nEpoch 158/200, Batch 5/45, Loss: 0.28882700204849243\nEpoch 158/200, Batch 6/45, Loss: 0.24810412526130676\nEpoch 158/200, Batch 7/45, Loss: 0.1510528326034546\nEpoch 158/200, Batch 8/45, Loss: 0.303216814994812\nEpoch 158/200, Batch 9/45, Loss: 0.2887299656867981\nEpoch 158/200, Batch 10/45, Loss: 0.3451351523399353\nEpoch 158/200, Batch 11/45, Loss: 0.22808736562728882\nEpoch 158/200, Batch 12/45, Loss: 0.2079259157180786\nEpoch 158/200, Batch 13/45, Loss: 0.256027489900589\nEpoch 158/200, Batch 14/45, Loss: 0.3024805188179016\nEpoch 158/200, Batch 15/45, Loss: 0.37380748987197876\nEpoch 158/200, Batch 16/45, Loss: 0.23114733397960663\nEpoch 158/200, Batch 17/45, Loss: 0.2377694547176361\nEpoch 158/200, Batch 18/45, Loss: 0.147707998752594\nEpoch 158/200, Batch 19/45, Loss: 0.2406410574913025\nEpoch 158/200, Batch 20/45, Loss: 0.23853735625743866\nEpoch 158/200, Batch 21/45, Loss: 0.23791661858558655\nEpoch 158/200, Batch 22/45, Loss: 0.37418046593666077\nEpoch 158/200, Batch 23/45, Loss: 0.22976504266262054\nEpoch 158/200, Batch 24/45, Loss: 0.25889813899993896\nEpoch 158/200, Batch 25/45, Loss: 0.2830908000469208\nEpoch 158/200, Batch 26/45, Loss: 0.16366034746170044\nEpoch 158/200, Batch 27/45, Loss: 0.22131136059761047\nEpoch 158/200, Batch 28/45, Loss: 0.19215810298919678\nEpoch 158/200, Batch 29/45, Loss: 0.43803614377975464\nEpoch 158/200, Batch 30/45, Loss: 0.2802625000476837\nEpoch 158/200, Batch 31/45, Loss: 0.401712030172348\nEpoch 158/200, Batch 32/45, Loss: 0.2860550880432129\nEpoch 158/200, Batch 33/45, Loss: 0.11554023623466492\nEpoch 158/200, Batch 34/45, Loss: 0.33611685037612915\nEpoch 158/200, Batch 35/45, Loss: 0.2697307765483856\nEpoch 158/200, Batch 36/45, Loss: 0.2589966654777527\nEpoch 158/200, Batch 37/45, Loss: 0.30133917927742004\nEpoch 158/200, Batch 38/45, Loss: 0.317674458026886\nEpoch 158/200, Batch 39/45, Loss: 0.261686772108078\nEpoch 158/200, Batch 40/45, Loss: 0.41545116901397705\nEpoch 158/200, Batch 41/45, Loss: 0.21686704456806183\nEpoch 158/200, Batch 42/45, Loss: 0.27424100041389465\nEpoch 158/200, Batch 43/45, Loss: 0.18712198734283447\nEpoch 158/200, Batch 44/45, Loss: 0.26332008838653564\nEpoch 158/200, Batch 45/45, Loss: 0.3112015128135681\nValidating and Checkpointing!\nBest model Saved! Val MSE:  5.151235476136208\nEpoch:  159 , Time Elapsed:  25.277913423379264  mins\nEpoch 159/200, Batch 1/45, Loss: 0.2565709948539734\nEpoch 159/200, Batch 2/45, Loss: 0.25623804330825806\nEpoch 159/200, Batch 3/45, Loss: 0.2821809649467468\nEpoch 159/200, Batch 4/45, Loss: 0.23895004391670227\nEpoch 159/200, Batch 5/45, Loss: 0.3336670398712158\nEpoch 159/200, Batch 6/45, Loss: 0.34474486112594604\nEpoch 159/200, Batch 7/45, Loss: 0.1542401909828186\nEpoch 159/200, Batch 8/45, Loss: 0.2490784376859665\nEpoch 159/200, Batch 9/45, Loss: 0.15105627477169037\nEpoch 159/200, Batch 10/45, Loss: 0.3320498466491699\nEpoch 159/200, Batch 11/45, Loss: 0.13259631395339966\nEpoch 159/200, Batch 12/45, Loss: 0.26579949259757996\nEpoch 159/200, Batch 13/45, Loss: 0.28697916865348816\nEpoch 159/200, Batch 14/45, Loss: 0.15755969285964966\nEpoch 159/200, Batch 15/45, Loss: 0.25184983015060425\nEpoch 159/200, Batch 16/45, Loss: 0.36181291937828064\nEpoch 159/200, Batch 17/45, Loss: 0.10417602956295013\nEpoch 159/200, Batch 18/45, Loss: 0.10064464807510376\nEpoch 159/200, Batch 19/45, Loss: 0.1811775267124176\nEpoch 159/200, Batch 20/45, Loss: 0.33885467052459717\nEpoch 159/200, Batch 21/45, Loss: 0.3539755642414093\nEpoch 159/200, Batch 22/45, Loss: 0.2263653576374054\nEpoch 159/200, Batch 23/45, Loss: 0.3396212160587311\nEpoch 159/200, Batch 24/45, Loss: 0.33917751908302307\nEpoch 159/200, Batch 25/45, Loss: 0.22106848657131195\nEpoch 159/200, Batch 26/45, Loss: 0.20946010947227478\nEpoch 159/200, Batch 27/45, Loss: 0.22945258021354675\nEpoch 159/200, Batch 28/45, Loss: 0.35260340571403503\nEpoch 159/200, Batch 29/45, Loss: 0.25870436429977417\nEpoch 159/200, Batch 30/45, Loss: 0.24364787340164185\nEpoch 159/200, Batch 31/45, Loss: 0.2348250448703766\nEpoch 159/200, Batch 32/45, Loss: 0.16279080510139465\nEpoch 159/200, Batch 33/45, Loss: 0.31763115525245667\nEpoch 159/200, Batch 34/45, Loss: 0.15889126062393188\nEpoch 159/200, Batch 35/45, Loss: 0.23841239511966705\nEpoch 159/200, Batch 36/45, Loss: 0.6121139526367188\nEpoch 159/200, Batch 37/45, Loss: 0.13351818919181824\nEpoch 159/200, Batch 38/45, Loss: 0.21182933449745178\nEpoch 159/200, Batch 39/45, Loss: 0.25898170471191406\nEpoch 159/200, Batch 40/45, Loss: 0.40640780329704285\nEpoch 159/200, Batch 41/45, Loss: 0.21135900914669037\nEpoch 159/200, Batch 42/45, Loss: 0.2324596345424652\nEpoch 159/200, Batch 43/45, Loss: 0.2575308680534363\nEpoch 159/200, Batch 44/45, Loss: 0.34183889627456665\nEpoch 159/200, Batch 45/45, Loss: 0.2729778289794922\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  448.4218053817749 Best Val MSE:  5.151235476136208\nEpoch:  160 , Time Elapsed:  25.43976446787516  mins\nEpoch 160/200, Batch 1/45, Loss: 0.21866363286972046\nEpoch 160/200, Batch 2/45, Loss: 0.1769670695066452\nEpoch 160/200, Batch 3/45, Loss: 0.20852524042129517\nEpoch 160/200, Batch 4/45, Loss: 0.3795334994792938\nEpoch 160/200, Batch 5/45, Loss: 0.2677636742591858\nEpoch 160/200, Batch 6/45, Loss: 0.30598941445350647\nEpoch 160/200, Batch 7/45, Loss: 0.24971160292625427\nEpoch 160/200, Batch 8/45, Loss: 0.12765519320964813\nEpoch 160/200, Batch 9/45, Loss: 0.09212606400251389\nEpoch 160/200, Batch 10/45, Loss: 0.5301245450973511\nEpoch 160/200, Batch 11/45, Loss: 0.258756160736084\nEpoch 160/200, Batch 12/45, Loss: 0.2690580487251282\nEpoch 160/200, Batch 13/45, Loss: 0.171725794672966\nEpoch 160/200, Batch 14/45, Loss: 0.13925200700759888\nEpoch 160/200, Batch 15/45, Loss: 0.18217843770980835\nEpoch 160/200, Batch 16/45, Loss: 0.33903950452804565\nEpoch 160/200, Batch 17/45, Loss: 0.23930785059928894\nEpoch 160/200, Batch 18/45, Loss: 0.2184542715549469\nEpoch 160/200, Batch 19/45, Loss: 0.255215048789978\nEpoch 160/200, Batch 20/45, Loss: 0.7412471771240234\nEpoch 160/200, Batch 21/45, Loss: 0.3265278935432434\nEpoch 160/200, Batch 22/45, Loss: 0.3820652663707733\nEpoch 160/200, Batch 23/45, Loss: 0.32239535450935364\nEpoch 160/200, Batch 24/45, Loss: 0.2954525947570801\nEpoch 160/200, Batch 25/45, Loss: 0.2430398017168045\nEpoch 160/200, Batch 26/45, Loss: 0.16449406743049622\nEpoch 160/200, Batch 27/45, Loss: 0.2746090292930603\nEpoch 160/200, Batch 28/45, Loss: 0.3191898465156555\nEpoch 160/200, Batch 29/45, Loss: 0.26162582635879517\nEpoch 160/200, Batch 30/45, Loss: 0.2974655032157898\nEpoch 160/200, Batch 31/45, Loss: 0.3290834128856659\nEpoch 160/200, Batch 32/45, Loss: 0.4207385778427124\nEpoch 160/200, Batch 33/45, Loss: 0.3543500602245331\nEpoch 160/200, Batch 34/45, Loss: 0.40174829959869385\nEpoch 160/200, Batch 35/45, Loss: 0.18494315445423126\nEpoch 160/200, Batch 36/45, Loss: 0.3459823429584503\nEpoch 160/200, Batch 37/45, Loss: 0.4355393350124359\nEpoch 160/200, Batch 38/45, Loss: 0.24650512635707855\nEpoch 160/200, Batch 39/45, Loss: 0.20557308197021484\nEpoch 160/200, Batch 40/45, Loss: 0.3456876277923584\nEpoch 160/200, Batch 41/45, Loss: 0.3304106593132019\nEpoch 160/200, Batch 42/45, Loss: 0.22108465433120728\nEpoch 160/200, Batch 43/45, Loss: 0.361400306224823\nEpoch 160/200, Batch 44/45, Loss: 0.20512300729751587\nEpoch 160/200, Batch 45/45, Loss: 0.4498061239719391\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  50.72955244779587 Best Val MSE:  5.151235476136208\nEpoch:  161 , Time Elapsed:  25.592994312445324  mins\nEpoch 161/200, Batch 1/45, Loss: 0.2399396449327469\nEpoch 161/200, Batch 2/45, Loss: 0.7789578437805176\nEpoch 161/200, Batch 3/45, Loss: 0.18030999600887299\nEpoch 161/200, Batch 4/45, Loss: 0.43894240260124207\nEpoch 161/200, Batch 5/45, Loss: 0.5288365483283997\nEpoch 161/200, Batch 6/45, Loss: 0.23422029614448547\nEpoch 161/200, Batch 7/45, Loss: 0.3624413013458252\nEpoch 161/200, Batch 8/45, Loss: 0.32103073596954346\nEpoch 161/200, Batch 9/45, Loss: 0.7197179794311523\nEpoch 161/200, Batch 10/45, Loss: 0.38887834548950195\nEpoch 161/200, Batch 11/45, Loss: 0.2253982126712799\nEpoch 161/200, Batch 12/45, Loss: 0.2613430321216583\nEpoch 161/200, Batch 13/45, Loss: 0.36504295468330383\nEpoch 161/200, Batch 14/45, Loss: 0.21329933404922485\nEpoch 161/200, Batch 15/45, Loss: 0.49091440439224243\nEpoch 161/200, Batch 16/45, Loss: 0.24593627452850342\nEpoch 161/200, Batch 17/45, Loss: 0.2590809762477875\nEpoch 161/200, Batch 18/45, Loss: 0.2047397643327713\nEpoch 161/200, Batch 19/45, Loss: 0.2313585877418518\nEpoch 161/200, Batch 20/45, Loss: 0.28947800397872925\nEpoch 161/200, Batch 21/45, Loss: 0.18431693315505981\nEpoch 161/200, Batch 22/45, Loss: 0.28174686431884766\nEpoch 161/200, Batch 23/45, Loss: 0.3338814675807953\nEpoch 161/200, Batch 24/45, Loss: 0.23417341709136963\nEpoch 161/200, Batch 25/45, Loss: 0.30944228172302246\nEpoch 161/200, Batch 26/45, Loss: 0.18218708038330078\nEpoch 161/200, Batch 27/45, Loss: 0.33564817905426025\nEpoch 161/200, Batch 28/45, Loss: 0.36905980110168457\nEpoch 161/200, Batch 29/45, Loss: 0.24858637154102325\nEpoch 161/200, Batch 30/45, Loss: 0.2616194486618042\nEpoch 161/200, Batch 31/45, Loss: 0.43615907430648804\nEpoch 161/200, Batch 32/45, Loss: 0.33281153440475464\nEpoch 161/200, Batch 33/45, Loss: 0.21781226992607117\nEpoch 161/200, Batch 34/45, Loss: 0.41615015268325806\nEpoch 161/200, Batch 35/45, Loss: 0.23082850873470306\nEpoch 161/200, Batch 36/45, Loss: 0.2671911418437958\nEpoch 161/200, Batch 37/45, Loss: 0.2086164355278015\nEpoch 161/200, Batch 38/45, Loss: 0.2566527724266052\nEpoch 161/200, Batch 39/45, Loss: 0.24297600984573364\nEpoch 161/200, Batch 40/45, Loss: 0.2181968241930008\nEpoch 161/200, Batch 41/45, Loss: 0.6081661581993103\nEpoch 161/200, Batch 42/45, Loss: 0.11172283440828323\nEpoch 161/200, Batch 43/45, Loss: 0.5301825404167175\nEpoch 161/200, Batch 44/45, Loss: 0.17078670859336853\nEpoch 161/200, Batch 45/45, Loss: 0.332721471786499\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  8.030152894556522 Best Val MSE:  5.151235476136208\nEpoch:  162 , Time Elapsed:  25.7500368475914  mins\nEpoch 162/200, Batch 1/45, Loss: 0.133428156375885\nEpoch 162/200, Batch 2/45, Loss: 0.1686423271894455\nEpoch 162/200, Batch 3/45, Loss: 0.10774468630552292\nEpoch 162/200, Batch 4/45, Loss: 0.3383379578590393\nEpoch 162/200, Batch 5/45, Loss: 0.35123592615127563\nEpoch 162/200, Batch 6/45, Loss: 0.32838180661201477\nEpoch 162/200, Batch 7/45, Loss: 0.463506281375885\nEpoch 162/200, Batch 8/45, Loss: 0.24650982022285461\nEpoch 162/200, Batch 9/45, Loss: 0.25835299491882324\nEpoch 162/200, Batch 10/45, Loss: 0.3282355070114136\nEpoch 162/200, Batch 11/45, Loss: 0.23984502255916595\nEpoch 162/200, Batch 12/45, Loss: 0.2867538332939148\nEpoch 162/200, Batch 13/45, Loss: 0.13307470083236694\nEpoch 162/200, Batch 14/45, Loss: 0.8820381164550781\nEpoch 162/200, Batch 15/45, Loss: 0.31877878308296204\nEpoch 162/200, Batch 16/45, Loss: 0.5447627902030945\nEpoch 162/200, Batch 17/45, Loss: 0.6748178601264954\nEpoch 162/200, Batch 18/45, Loss: 0.2058248519897461\nEpoch 162/200, Batch 19/45, Loss: 0.12227851152420044\nEpoch 162/200, Batch 20/45, Loss: 0.3123570680618286\nEpoch 162/200, Batch 21/45, Loss: 0.1703176647424698\nEpoch 162/200, Batch 22/45, Loss: 0.2165454924106598\nEpoch 162/200, Batch 23/45, Loss: 0.19755877554416656\nEpoch 162/200, Batch 24/45, Loss: 0.3015859127044678\nEpoch 162/200, Batch 25/45, Loss: 0.29249656200408936\nEpoch 162/200, Batch 26/45, Loss: 0.5370105504989624\nEpoch 162/200, Batch 27/45, Loss: 0.1081857830286026\nEpoch 162/200, Batch 28/45, Loss: 0.3707636594772339\nEpoch 162/200, Batch 29/45, Loss: 0.2405029535293579\nEpoch 162/200, Batch 30/45, Loss: 0.3162499666213989\nEpoch 162/200, Batch 31/45, Loss: 0.2582088112831116\nEpoch 162/200, Batch 32/45, Loss: 0.49830925464630127\nEpoch 162/200, Batch 33/45, Loss: 0.14976947009563446\nEpoch 162/200, Batch 34/45, Loss: 0.3401634097099304\nEpoch 162/200, Batch 35/45, Loss: 0.22588783502578735\nEpoch 162/200, Batch 36/45, Loss: 0.2772371172904968\nEpoch 162/200, Batch 37/45, Loss: 0.31574976444244385\nEpoch 162/200, Batch 38/45, Loss: 0.3812974691390991\nEpoch 162/200, Batch 39/45, Loss: 0.2162972390651703\nEpoch 162/200, Batch 40/45, Loss: 0.2180226892232895\nEpoch 162/200, Batch 41/45, Loss: 0.2551332116127014\nEpoch 162/200, Batch 42/45, Loss: 0.2860829830169678\nEpoch 162/200, Batch 43/45, Loss: 0.25843143463134766\nEpoch 162/200, Batch 44/45, Loss: 0.44667696952819824\nEpoch 162/200, Batch 45/45, Loss: 0.26827821135520935\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  936.7301120758057 Best Val MSE:  5.151235476136208\nEpoch:  163 , Time Elapsed:  25.911618729432423  mins\nEpoch 163/200, Batch 1/45, Loss: 0.2552333474159241\nEpoch 163/200, Batch 2/45, Loss: 0.21366769075393677\nEpoch 163/200, Batch 3/45, Loss: 0.40322017669677734\nEpoch 163/200, Batch 4/45, Loss: 0.20791146159172058\nEpoch 163/200, Batch 5/45, Loss: 0.36517560482025146\nEpoch 163/200, Batch 6/45, Loss: 0.20268651843070984\nEpoch 163/200, Batch 7/45, Loss: 0.2777659296989441\nEpoch 163/200, Batch 8/45, Loss: 0.16054491698741913\nEpoch 163/200, Batch 9/45, Loss: 0.44711369276046753\nEpoch 163/200, Batch 10/45, Loss: 0.2551538348197937\nEpoch 163/200, Batch 11/45, Loss: 0.36149486899375916\nEpoch 163/200, Batch 12/45, Loss: 0.5672351717948914\nEpoch 163/200, Batch 13/45, Loss: 0.34098511934280396\nEpoch 163/200, Batch 14/45, Loss: 0.2017897367477417\nEpoch 163/200, Batch 15/45, Loss: 0.18463236093521118\nEpoch 163/200, Batch 16/45, Loss: 0.24597540497779846\nEpoch 163/200, Batch 17/45, Loss: 0.26557037234306335\nEpoch 163/200, Batch 18/45, Loss: 0.2173786461353302\nEpoch 163/200, Batch 19/45, Loss: 0.3668466806411743\nEpoch 163/200, Batch 20/45, Loss: 0.18615221977233887\nEpoch 163/200, Batch 21/45, Loss: 0.29766908288002014\nEpoch 163/200, Batch 22/45, Loss: 0.25359979271888733\nEpoch 163/200, Batch 23/45, Loss: 0.25723111629486084\nEpoch 163/200, Batch 24/45, Loss: 0.2372753620147705\nEpoch 163/200, Batch 25/45, Loss: 0.36789244413375854\nEpoch 163/200, Batch 26/45, Loss: 0.49109435081481934\nEpoch 163/200, Batch 27/45, Loss: 0.15509666502475739\nEpoch 163/200, Batch 28/45, Loss: 0.13138684630393982\nEpoch 163/200, Batch 29/45, Loss: 0.47308775782585144\nEpoch 163/200, Batch 30/45, Loss: 0.19879309833049774\nEpoch 163/200, Batch 31/45, Loss: 0.6178467869758606\nEpoch 163/200, Batch 32/45, Loss: 0.28336483240127563\nEpoch 163/200, Batch 33/45, Loss: 0.18351691961288452\nEpoch 163/200, Batch 34/45, Loss: 0.2800776958465576\nEpoch 163/200, Batch 35/45, Loss: 0.19638478755950928\nEpoch 163/200, Batch 36/45, Loss: 0.2883657217025757\nEpoch 163/200, Batch 37/45, Loss: 0.28991419076919556\nEpoch 163/200, Batch 38/45, Loss: 0.2700507342815399\nEpoch 163/200, Batch 39/45, Loss: 0.3917897343635559\nEpoch 163/200, Batch 40/45, Loss: 0.2966741919517517\nEpoch 163/200, Batch 41/45, Loss: 0.2528670132160187\nEpoch 163/200, Batch 42/45, Loss: 0.22746074199676514\nEpoch 163/200, Batch 43/45, Loss: 0.10476486384868622\nEpoch 163/200, Batch 44/45, Loss: 0.24542298913002014\nEpoch 163/200, Batch 45/45, Loss: 0.33103811740875244\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  170.82656955718994 Best Val MSE:  5.151235476136208\nEpoch:  164 , Time Elapsed:  26.066842520236968  mins\nEpoch 164/200, Batch 1/45, Loss: 0.2454957365989685\nEpoch 164/200, Batch 2/45, Loss: 0.1720002293586731\nEpoch 164/200, Batch 3/45, Loss: 0.262313574552536\nEpoch 164/200, Batch 4/45, Loss: 0.1951846480369568\nEpoch 164/200, Batch 5/45, Loss: 0.6210311651229858\nEpoch 164/200, Batch 6/45, Loss: 0.2338007390499115\nEpoch 164/200, Batch 7/45, Loss: 0.5144767761230469\nEpoch 164/200, Batch 8/45, Loss: 0.14916172623634338\nEpoch 164/200, Batch 9/45, Loss: 0.48607122898101807\nEpoch 164/200, Batch 10/45, Loss: 0.3476865291595459\nEpoch 164/200, Batch 11/45, Loss: 0.30080991983413696\nEpoch 164/200, Batch 12/45, Loss: 0.24959862232208252\nEpoch 164/200, Batch 13/45, Loss: 0.24729399383068085\nEpoch 164/200, Batch 14/45, Loss: 0.22789375483989716\nEpoch 164/200, Batch 15/45, Loss: 0.2921897768974304\nEpoch 164/200, Batch 16/45, Loss: 0.2354041486978531\nEpoch 164/200, Batch 17/45, Loss: 0.2166774868965149\nEpoch 164/200, Batch 18/45, Loss: 0.17342618107795715\nEpoch 164/200, Batch 19/45, Loss: 0.29469504952430725\nEpoch 164/200, Batch 20/45, Loss: 0.17074836790561676\nEpoch 164/200, Batch 21/45, Loss: 0.1460954248905182\nEpoch 164/200, Batch 22/45, Loss: 0.3539346754550934\nEpoch 164/200, Batch 23/45, Loss: 0.25327205657958984\nEpoch 164/200, Batch 24/45, Loss: 0.32048603892326355\nEpoch 164/200, Batch 25/45, Loss: 0.1825530230998993\nEpoch 164/200, Batch 26/45, Loss: 0.19005581736564636\nEpoch 164/200, Batch 27/45, Loss: 0.43569037318229675\nEpoch 164/200, Batch 28/45, Loss: 0.3122521638870239\nEpoch 164/200, Batch 29/45, Loss: 0.19919800758361816\nEpoch 164/200, Batch 30/45, Loss: 0.30424219369888306\nEpoch 164/200, Batch 31/45, Loss: 0.30108702182769775\nEpoch 164/200, Batch 32/45, Loss: 0.38129857182502747\nEpoch 164/200, Batch 33/45, Loss: 0.11760827898979187\nEpoch 164/200, Batch 34/45, Loss: 0.22420240938663483\nEpoch 164/200, Batch 35/45, Loss: 0.3119676113128662\nEpoch 164/200, Batch 36/45, Loss: 0.11036677658557892\nEpoch 164/200, Batch 37/45, Loss: 0.1688065379858017\nEpoch 164/200, Batch 38/45, Loss: 0.40075093507766724\nEpoch 164/200, Batch 39/45, Loss: 0.15238378942012787\nEpoch 164/200, Batch 40/45, Loss: 0.3390827775001526\nEpoch 164/200, Batch 41/45, Loss: 0.412048876285553\nEpoch 164/200, Batch 42/45, Loss: 0.3408089280128479\nEpoch 164/200, Batch 43/45, Loss: 0.2721073031425476\nEpoch 164/200, Batch 44/45, Loss: 0.24844148755073547\nEpoch 164/200, Batch 45/45, Loss: 0.1535840630531311\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  67.89773488044739 Best Val MSE:  5.151235476136208\nEpoch:  165 , Time Elapsed:  26.222212759653726  mins\nEpoch 165/200, Batch 1/45, Loss: 0.20688098669052124\nEpoch 165/200, Batch 2/45, Loss: 0.26913487911224365\nEpoch 165/200, Batch 3/45, Loss: 0.2503589391708374\nEpoch 165/200, Batch 4/45, Loss: 0.2172783613204956\nEpoch 165/200, Batch 5/45, Loss: 0.6102012395858765\nEpoch 165/200, Batch 6/45, Loss: 0.3722359538078308\nEpoch 165/200, Batch 7/45, Loss: 0.12852764129638672\nEpoch 165/200, Batch 8/45, Loss: 0.21028994023799896\nEpoch 165/200, Batch 9/45, Loss: 0.3522716760635376\nEpoch 165/200, Batch 10/45, Loss: 0.2431555986404419\nEpoch 165/200, Batch 11/45, Loss: 0.6562159061431885\nEpoch 165/200, Batch 12/45, Loss: 0.22554756700992584\nEpoch 165/200, Batch 13/45, Loss: 0.2904837727546692\nEpoch 165/200, Batch 14/45, Loss: 0.24807853996753693\nEpoch 165/200, Batch 15/45, Loss: 0.09333410859107971\nEpoch 165/200, Batch 16/45, Loss: 0.45200657844543457\nEpoch 165/200, Batch 17/45, Loss: 0.16473141312599182\nEpoch 165/200, Batch 18/45, Loss: 0.2854827046394348\nEpoch 165/200, Batch 19/45, Loss: 0.21552270650863647\nEpoch 165/200, Batch 20/45, Loss: 0.21036434173583984\nEpoch 165/200, Batch 21/45, Loss: 0.5459580421447754\nEpoch 165/200, Batch 22/45, Loss: 0.3111274838447571\nEpoch 165/200, Batch 23/45, Loss: 0.1981227993965149\nEpoch 165/200, Batch 24/45, Loss: 0.18035157024860382\nEpoch 165/200, Batch 25/45, Loss: 0.39415091276168823\nEpoch 165/200, Batch 26/45, Loss: 0.2413453757762909\nEpoch 165/200, Batch 27/45, Loss: 0.24770157039165497\nEpoch 165/200, Batch 28/45, Loss: 0.36054444313049316\nEpoch 165/200, Batch 29/45, Loss: 0.2069365680217743\nEpoch 165/200, Batch 30/45, Loss: 0.1905447393655777\nEpoch 165/200, Batch 31/45, Loss: 0.24693140387535095\nEpoch 165/200, Batch 32/45, Loss: 0.22956129908561707\nEpoch 165/200, Batch 33/45, Loss: 0.30490174889564514\nEpoch 165/200, Batch 34/45, Loss: 0.28136128187179565\nEpoch 165/200, Batch 35/45, Loss: 0.2584511637687683\nEpoch 165/200, Batch 36/45, Loss: 0.15066251158714294\nEpoch 165/200, Batch 37/45, Loss: 0.21021738648414612\nEpoch 165/200, Batch 38/45, Loss: 0.2787684500217438\nEpoch 165/200, Batch 39/45, Loss: 0.26842957735061646\nEpoch 165/200, Batch 40/45, Loss: 0.25562942028045654\nEpoch 165/200, Batch 41/45, Loss: 0.43021389842033386\nEpoch 165/200, Batch 42/45, Loss: 0.2600518465042114\nEpoch 165/200, Batch 43/45, Loss: 0.2432681918144226\nEpoch 165/200, Batch 44/45, Loss: 0.32472628355026245\nEpoch 165/200, Batch 45/45, Loss: 0.2936350405216217\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  8.723838806152344 Best Val MSE:  5.151235476136208\nEpoch:  166 , Time Elapsed:  26.380376553535463  mins\nEpoch 166/200, Batch 1/45, Loss: 0.23246589303016663\nEpoch 166/200, Batch 2/45, Loss: 0.3949461579322815\nEpoch 166/200, Batch 3/45, Loss: 0.2468191236257553\nEpoch 166/200, Batch 4/45, Loss: 0.33016371726989746\nEpoch 166/200, Batch 5/45, Loss: 0.28763481974601746\nEpoch 166/200, Batch 6/45, Loss: 0.2845557630062103\nEpoch 166/200, Batch 7/45, Loss: 0.1988055258989334\nEpoch 166/200, Batch 8/45, Loss: 0.1965399831533432\nEpoch 166/200, Batch 9/45, Loss: 0.31667178869247437\nEpoch 166/200, Batch 10/45, Loss: 0.41402700543403625\nEpoch 166/200, Batch 11/45, Loss: 0.2798837423324585\nEpoch 166/200, Batch 12/45, Loss: 0.22034043073654175\nEpoch 166/200, Batch 13/45, Loss: 0.12844693660736084\nEpoch 166/200, Batch 14/45, Loss: 0.2976814806461334\nEpoch 166/200, Batch 15/45, Loss: 0.45293059945106506\nEpoch 166/200, Batch 16/45, Loss: 0.17283275723457336\nEpoch 166/200, Batch 17/45, Loss: 0.38555997610092163\nEpoch 166/200, Batch 18/45, Loss: 0.23016740381717682\nEpoch 166/200, Batch 19/45, Loss: 0.24154503643512726\nEpoch 166/200, Batch 20/45, Loss: 0.19342011213302612\nEpoch 166/200, Batch 21/45, Loss: 0.19812914729118347\nEpoch 166/200, Batch 22/45, Loss: 0.26163047552108765\nEpoch 166/200, Batch 23/45, Loss: 0.19398677349090576\nEpoch 166/200, Batch 24/45, Loss: 0.3357190489768982\nEpoch 166/200, Batch 25/45, Loss: 0.40852561593055725\nEpoch 166/200, Batch 26/45, Loss: 0.16160276532173157\nEpoch 166/200, Batch 27/45, Loss: 0.1907714307308197\nEpoch 166/200, Batch 28/45, Loss: 0.2496650367975235\nEpoch 166/200, Batch 29/45, Loss: 0.2857436537742615\nEpoch 166/200, Batch 30/45, Loss: 0.25161638855934143\nEpoch 166/200, Batch 31/45, Loss: 0.26700833439826965\nEpoch 166/200, Batch 32/45, Loss: 0.2287786602973938\nEpoch 166/200, Batch 33/45, Loss: 0.27776142954826355\nEpoch 166/200, Batch 34/45, Loss: 0.22209367156028748\nEpoch 166/200, Batch 35/45, Loss: 0.3047296404838562\nEpoch 166/200, Batch 36/45, Loss: 0.21008308231830597\nEpoch 166/200, Batch 37/45, Loss: 0.23455336689949036\nEpoch 166/200, Batch 38/45, Loss: 0.24339896440505981\nEpoch 166/200, Batch 39/45, Loss: 0.4756056070327759\nEpoch 166/200, Batch 40/45, Loss: 0.4371500015258789\nEpoch 166/200, Batch 41/45, Loss: 0.2294129580259323\nEpoch 166/200, Batch 42/45, Loss: 0.14518436789512634\nEpoch 166/200, Batch 43/45, Loss: 0.2650448679924011\nEpoch 166/200, Batch 44/45, Loss: 0.20874816179275513\nEpoch 166/200, Batch 45/45, Loss: 0.77500319480896\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  104.79833245277405 Best Val MSE:  5.151235476136208\nEpoch:  167 , Time Elapsed:  26.53670161565145  mins\nEpoch 167/200, Batch 1/45, Loss: 0.1988973766565323\nEpoch 167/200, Batch 2/45, Loss: 0.2244604229927063\nEpoch 167/200, Batch 3/45, Loss: 0.7105315923690796\nEpoch 167/200, Batch 4/45, Loss: 0.3733029365539551\nEpoch 167/200, Batch 5/45, Loss: 0.1361870914697647\nEpoch 167/200, Batch 6/45, Loss: 0.20865654945373535\nEpoch 167/200, Batch 7/45, Loss: 0.21269819140434265\nEpoch 167/200, Batch 8/45, Loss: 0.3228256106376648\nEpoch 167/200, Batch 9/45, Loss: 0.8034030795097351\nEpoch 167/200, Batch 10/45, Loss: 0.17798878252506256\nEpoch 167/200, Batch 11/45, Loss: 0.28337371349334717\nEpoch 167/200, Batch 12/45, Loss: 0.3724035620689392\nEpoch 167/200, Batch 13/45, Loss: 0.39491331577301025\nEpoch 167/200, Batch 14/45, Loss: 0.09831391274929047\nEpoch 167/200, Batch 15/45, Loss: 0.3470054864883423\nEpoch 167/200, Batch 16/45, Loss: 0.3108293116092682\nEpoch 167/200, Batch 17/45, Loss: 0.2403738796710968\nEpoch 167/200, Batch 18/45, Loss: 0.29328542947769165\nEpoch 167/200, Batch 19/45, Loss: 0.2244095504283905\nEpoch 167/200, Batch 20/45, Loss: 0.2126874029636383\nEpoch 167/200, Batch 21/45, Loss: 0.4254971146583557\nEpoch 167/200, Batch 22/45, Loss: 0.2382364273071289\nEpoch 167/200, Batch 23/45, Loss: 0.5631295442581177\nEpoch 167/200, Batch 24/45, Loss: 0.3174605369567871\nEpoch 167/200, Batch 25/45, Loss: 0.4452369809150696\nEpoch 167/200, Batch 26/45, Loss: 0.2709510326385498\nEpoch 167/200, Batch 27/45, Loss: 0.28491920232772827\nEpoch 167/200, Batch 28/45, Loss: 0.19040396809577942\nEpoch 167/200, Batch 29/45, Loss: 0.5167845487594604\nEpoch 167/200, Batch 30/45, Loss: 0.23203012347221375\nEpoch 167/200, Batch 31/45, Loss: 0.19091729819774628\nEpoch 167/200, Batch 32/45, Loss: 0.15973979234695435\nEpoch 167/200, Batch 33/45, Loss: 0.23410087823867798\nEpoch 167/200, Batch 34/45, Loss: 0.17930635809898376\nEpoch 167/200, Batch 35/45, Loss: 0.3050236403942108\nEpoch 167/200, Batch 36/45, Loss: 0.6143261194229126\nEpoch 167/200, Batch 37/45, Loss: 0.5833898186683655\nEpoch 167/200, Batch 38/45, Loss: 0.2128266990184784\nEpoch 167/200, Batch 39/45, Loss: 0.14471641182899475\nEpoch 167/200, Batch 40/45, Loss: 0.411138653755188\nEpoch 167/200, Batch 41/45, Loss: 0.47005364298820496\nEpoch 167/200, Batch 42/45, Loss: 0.5531043410301208\nEpoch 167/200, Batch 43/45, Loss: 0.4407120645046234\nEpoch 167/200, Batch 44/45, Loss: 0.3778279423713684\nEpoch 167/200, Batch 45/45, Loss: 0.49890851974487305\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  399.6997661590576 Best Val MSE:  5.151235476136208\nEpoch:  168 , Time Elapsed:  26.695944559574126  mins\nEpoch 168/200, Batch 1/45, Loss: 0.4390159845352173\nEpoch 168/200, Batch 2/45, Loss: 0.3212209939956665\nEpoch 168/200, Batch 3/45, Loss: 0.21304301917552948\nEpoch 168/200, Batch 4/45, Loss: 0.16058748960494995\nEpoch 168/200, Batch 5/45, Loss: 0.2261826992034912\nEpoch 168/200, Batch 6/45, Loss: 0.5102618932723999\nEpoch 168/200, Batch 7/45, Loss: 0.16447243094444275\nEpoch 168/200, Batch 8/45, Loss: 0.2139313817024231\nEpoch 168/200, Batch 9/45, Loss: 0.5255099534988403\nEpoch 168/200, Batch 10/45, Loss: 0.31378456950187683\nEpoch 168/200, Batch 11/45, Loss: 0.21455200016498566\nEpoch 168/200, Batch 12/45, Loss: 0.7384253740310669\nEpoch 168/200, Batch 13/45, Loss: 0.47397372126579285\nEpoch 168/200, Batch 14/45, Loss: 0.22303590178489685\nEpoch 168/200, Batch 15/45, Loss: 0.2550327181816101\nEpoch 168/200, Batch 16/45, Loss: 0.4948485791683197\nEpoch 168/200, Batch 17/45, Loss: 0.3793225884437561\nEpoch 168/200, Batch 18/45, Loss: 0.18369218707084656\nEpoch 168/200, Batch 19/45, Loss: 0.20316991209983826\nEpoch 168/200, Batch 20/45, Loss: 0.3017352223396301\nEpoch 168/200, Batch 21/45, Loss: 0.31583037972450256\nEpoch 168/200, Batch 22/45, Loss: 0.3628317415714264\nEpoch 168/200, Batch 23/45, Loss: 0.2899308502674103\nEpoch 168/200, Batch 24/45, Loss: 0.38642752170562744\nEpoch 168/200, Batch 25/45, Loss: 0.24799194931983948\nEpoch 168/200, Batch 26/45, Loss: 0.5614267587661743\nEpoch 168/200, Batch 27/45, Loss: 0.2326725870370865\nEpoch 168/200, Batch 28/45, Loss: 0.23215334117412567\nEpoch 168/200, Batch 29/45, Loss: 0.2598443627357483\nEpoch 168/200, Batch 30/45, Loss: 0.37006235122680664\nEpoch 168/200, Batch 31/45, Loss: 0.22960321605205536\nEpoch 168/200, Batch 32/45, Loss: 0.2504086196422577\nEpoch 168/200, Batch 33/45, Loss: 0.28681647777557373\nEpoch 168/200, Batch 34/45, Loss: 0.4002854526042938\nEpoch 168/200, Batch 35/45, Loss: 0.22676575183868408\nEpoch 168/200, Batch 36/45, Loss: 0.21990910172462463\nEpoch 168/200, Batch 37/45, Loss: 0.2280125916004181\nEpoch 168/200, Batch 38/45, Loss: 0.28484830260276794\nEpoch 168/200, Batch 39/45, Loss: 0.20872312784194946\nEpoch 168/200, Batch 40/45, Loss: 0.17275413870811462\nEpoch 168/200, Batch 41/45, Loss: 0.20589286088943481\nEpoch 168/200, Batch 42/45, Loss: 0.32449403405189514\nEpoch 168/200, Batch 43/45, Loss: 0.12162810564041138\nEpoch 168/200, Batch 44/45, Loss: 0.3110324740409851\nEpoch 168/200, Batch 45/45, Loss: 0.22695982456207275\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  8.653673224151134 Best Val MSE:  5.151235476136208\nEpoch:  169 , Time Elapsed:  26.854144457976023  mins\nEpoch 169/200, Batch 1/45, Loss: 0.21769094467163086\nEpoch 169/200, Batch 2/45, Loss: 0.33842945098876953\nEpoch 169/200, Batch 3/45, Loss: 0.16676798462867737\nEpoch 169/200, Batch 4/45, Loss: 0.228106290102005\nEpoch 169/200, Batch 5/45, Loss: 0.3106801211833954\nEpoch 169/200, Batch 6/45, Loss: 0.41169798374176025\nEpoch 169/200, Batch 7/45, Loss: 0.32754969596862793\nEpoch 169/200, Batch 8/45, Loss: 0.26636263728141785\nEpoch 169/200, Batch 9/45, Loss: 0.17756369709968567\nEpoch 169/200, Batch 10/45, Loss: 0.26584142446517944\nEpoch 169/200, Batch 11/45, Loss: 0.21703501045703888\nEpoch 169/200, Batch 12/45, Loss: 0.12399338185787201\nEpoch 169/200, Batch 13/45, Loss: 0.20599183440208435\nEpoch 169/200, Batch 14/45, Loss: 0.29377317428588867\nEpoch 169/200, Batch 15/45, Loss: 0.3244664669036865\nEpoch 169/200, Batch 16/45, Loss: 0.31161653995513916\nEpoch 169/200, Batch 17/45, Loss: 0.21566526591777802\nEpoch 169/200, Batch 18/45, Loss: 0.25106149911880493\nEpoch 169/200, Batch 19/45, Loss: 0.27273696660995483\nEpoch 169/200, Batch 20/45, Loss: 0.23962348699569702\nEpoch 169/200, Batch 21/45, Loss: 0.22184249758720398\nEpoch 169/200, Batch 22/45, Loss: 0.39265280961990356\nEpoch 169/200, Batch 23/45, Loss: 0.15206825733184814\nEpoch 169/200, Batch 24/45, Loss: 0.2747214436531067\nEpoch 169/200, Batch 25/45, Loss: 0.2375650554895401\nEpoch 169/200, Batch 26/45, Loss: 0.45243313908576965\nEpoch 169/200, Batch 27/45, Loss: 0.21549057960510254\nEpoch 169/200, Batch 28/45, Loss: 0.18686017394065857\nEpoch 169/200, Batch 29/45, Loss: 0.28730344772338867\nEpoch 169/200, Batch 30/45, Loss: 0.5693395137786865\nEpoch 169/200, Batch 31/45, Loss: 0.18413926661014557\nEpoch 169/200, Batch 32/45, Loss: 0.36357274651527405\nEpoch 169/200, Batch 33/45, Loss: 0.12398785352706909\nEpoch 169/200, Batch 34/45, Loss: 0.12436941266059875\nEpoch 169/200, Batch 35/45, Loss: 0.2187441885471344\nEpoch 169/200, Batch 36/45, Loss: 0.1963474154472351\nEpoch 169/200, Batch 37/45, Loss: 0.20970214903354645\nEpoch 169/200, Batch 38/45, Loss: 0.23475849628448486\nEpoch 169/200, Batch 39/45, Loss: 0.15175512433052063\nEpoch 169/200, Batch 40/45, Loss: 0.1770075261592865\nEpoch 169/200, Batch 41/45, Loss: 0.2727935016155243\nEpoch 169/200, Batch 42/45, Loss: 0.11526542901992798\nEpoch 169/200, Batch 43/45, Loss: 0.36954277753829956\nEpoch 169/200, Batch 44/45, Loss: 0.18112199008464813\nEpoch 169/200, Batch 45/45, Loss: 0.21260420978069305\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  57.16730797290802 Best Val MSE:  5.151235476136208\nEpoch:  170 , Time Elapsed:  27.01621698141098  mins\nEpoch 170/200, Batch 1/45, Loss: 0.3547305464744568\nEpoch 170/200, Batch 2/45, Loss: 0.18108239769935608\nEpoch 170/200, Batch 3/45, Loss: 0.31231656670570374\nEpoch 170/200, Batch 4/45, Loss: 0.3543848991394043\nEpoch 170/200, Batch 5/45, Loss: 0.2718103528022766\nEpoch 170/200, Batch 6/45, Loss: 0.26140013337135315\nEpoch 170/200, Batch 7/45, Loss: 0.29880091547966003\nEpoch 170/200, Batch 8/45, Loss: 0.24931281805038452\nEpoch 170/200, Batch 9/45, Loss: 0.18354211747646332\nEpoch 170/200, Batch 10/45, Loss: 0.15325510501861572\nEpoch 170/200, Batch 11/45, Loss: 0.22702419757843018\nEpoch 170/200, Batch 12/45, Loss: 0.3279173672199249\nEpoch 170/200, Batch 13/45, Loss: 0.1374734342098236\nEpoch 170/200, Batch 14/45, Loss: 0.2261260449886322\nEpoch 170/200, Batch 15/45, Loss: 0.10266567766666412\nEpoch 170/200, Batch 16/45, Loss: 0.16113047301769257\nEpoch 170/200, Batch 17/45, Loss: 0.15671071410179138\nEpoch 170/200, Batch 18/45, Loss: 0.24906811118125916\nEpoch 170/200, Batch 19/45, Loss: 0.25996753573417664\nEpoch 170/200, Batch 20/45, Loss: 0.21706795692443848\nEpoch 170/200, Batch 21/45, Loss: 0.22225436568260193\nEpoch 170/200, Batch 22/45, Loss: 0.2293870747089386\nEpoch 170/200, Batch 23/45, Loss: 0.2774130702018738\nEpoch 170/200, Batch 24/45, Loss: 0.2890620231628418\nEpoch 170/200, Batch 25/45, Loss: 0.253130704164505\nEpoch 170/200, Batch 26/45, Loss: 0.5393707156181335\nEpoch 170/200, Batch 27/45, Loss: 0.31726983189582825\nEpoch 170/200, Batch 28/45, Loss: 0.19326196610927582\nEpoch 170/200, Batch 29/45, Loss: 0.20279398560523987\nEpoch 170/200, Batch 30/45, Loss: 0.16612577438354492\nEpoch 170/200, Batch 31/45, Loss: 0.294160932302475\nEpoch 170/200, Batch 32/45, Loss: 0.4251883625984192\nEpoch 170/200, Batch 33/45, Loss: 0.29115569591522217\nEpoch 170/200, Batch 34/45, Loss: 0.2138357013463974\nEpoch 170/200, Batch 35/45, Loss: 0.32304370403289795\nEpoch 170/200, Batch 36/45, Loss: 0.29653841257095337\nEpoch 170/200, Batch 37/45, Loss: 0.12728454172611237\nEpoch 170/200, Batch 38/45, Loss: 0.31657785177230835\nEpoch 170/200, Batch 39/45, Loss: 0.26575595140457153\nEpoch 170/200, Batch 40/45, Loss: 0.2804240882396698\nEpoch 170/200, Batch 41/45, Loss: 0.289089560508728\nEpoch 170/200, Batch 42/45, Loss: 0.13712552189826965\nEpoch 170/200, Batch 43/45, Loss: 0.19411331415176392\nEpoch 170/200, Batch 44/45, Loss: 0.3684350848197937\nEpoch 170/200, Batch 45/45, Loss: 0.2779619097709656\nValidating and Checkpointing!\nBest model Saved! Val MSE:  4.714497983455658\nEpoch:  171 , Time Elapsed:  27.18079056739807  mins\nEpoch 171/200, Batch 1/45, Loss: 0.230706125497818\nEpoch 171/200, Batch 2/45, Loss: 0.26205840706825256\nEpoch 171/200, Batch 3/45, Loss: 0.2678196430206299\nEpoch 171/200, Batch 4/45, Loss: 0.1874900460243225\nEpoch 171/200, Batch 5/45, Loss: 0.17975860834121704\nEpoch 171/200, Batch 6/45, Loss: 0.24423477053642273\nEpoch 171/200, Batch 7/45, Loss: 0.2795639634132385\nEpoch 171/200, Batch 8/45, Loss: 0.4729970693588257\nEpoch 171/200, Batch 9/45, Loss: 0.20798543095588684\nEpoch 171/200, Batch 10/45, Loss: 0.2218971848487854\nEpoch 171/200, Batch 11/45, Loss: 0.2448958307504654\nEpoch 171/200, Batch 12/45, Loss: 0.1791902780532837\nEpoch 171/200, Batch 13/45, Loss: 0.14615902304649353\nEpoch 171/200, Batch 14/45, Loss: 0.2639824151992798\nEpoch 171/200, Batch 15/45, Loss: 0.5898897647857666\nEpoch 171/200, Batch 16/45, Loss: 0.1322912573814392\nEpoch 171/200, Batch 17/45, Loss: 0.2468743920326233\nEpoch 171/200, Batch 18/45, Loss: 0.4533257782459259\nEpoch 171/200, Batch 19/45, Loss: 0.13174286484718323\nEpoch 171/200, Batch 20/45, Loss: 0.2545948028564453\nEpoch 171/200, Batch 21/45, Loss: 0.2219383716583252\nEpoch 171/200, Batch 22/45, Loss: 0.27598410844802856\nEpoch 171/200, Batch 23/45, Loss: 0.1678886115550995\nEpoch 171/200, Batch 24/45, Loss: 0.28638747334480286\nEpoch 171/200, Batch 25/45, Loss: 0.30459898710250854\nEpoch 171/200, Batch 26/45, Loss: 0.24757181107997894\nEpoch 171/200, Batch 27/45, Loss: 0.1912335753440857\nEpoch 171/200, Batch 28/45, Loss: 0.24883493781089783\nEpoch 171/200, Batch 29/45, Loss: 0.24599900841712952\nEpoch 171/200, Batch 30/45, Loss: 0.22288092970848083\nEpoch 171/200, Batch 31/45, Loss: 0.2589304447174072\nEpoch 171/200, Batch 32/45, Loss: 0.3566036820411682\nEpoch 171/200, Batch 33/45, Loss: 0.5402654409408569\nEpoch 171/200, Batch 34/45, Loss: 0.2629007399082184\nEpoch 171/200, Batch 35/45, Loss: 0.2054806500673294\nEpoch 171/200, Batch 36/45, Loss: 0.11727782338857651\nEpoch 171/200, Batch 37/45, Loss: 0.2815296947956085\nEpoch 171/200, Batch 38/45, Loss: 0.27212992310523987\nEpoch 171/200, Batch 39/45, Loss: 0.24608132243156433\nEpoch 171/200, Batch 40/45, Loss: 0.14836812019348145\nEpoch 171/200, Batch 41/45, Loss: 0.27290812134742737\nEpoch 171/200, Batch 42/45, Loss: 0.19865253567695618\nEpoch 171/200, Batch 43/45, Loss: 0.19716161489486694\nEpoch 171/200, Batch 44/45, Loss: 0.1365552693605423\nEpoch 171/200, Batch 45/45, Loss: 0.22247907519340515\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  10.697609230875969 Best Val MSE:  4.714497983455658\nEpoch:  172 , Time Elapsed:  27.33845647573471  mins\nEpoch 172/200, Batch 1/45, Loss: 0.396295964717865\nEpoch 172/200, Batch 2/45, Loss: 0.3877931833267212\nEpoch 172/200, Batch 3/45, Loss: 0.37308990955352783\nEpoch 172/200, Batch 4/45, Loss: 0.2503010630607605\nEpoch 172/200, Batch 5/45, Loss: 0.16306427121162415\nEpoch 172/200, Batch 6/45, Loss: 0.15961243212223053\nEpoch 172/200, Batch 7/45, Loss: 0.29482948780059814\nEpoch 172/200, Batch 8/45, Loss: 0.24373629689216614\nEpoch 172/200, Batch 9/45, Loss: 0.21277612447738647\nEpoch 172/200, Batch 10/45, Loss: 0.2296711653470993\nEpoch 172/200, Batch 11/45, Loss: 0.24966737627983093\nEpoch 172/200, Batch 12/45, Loss: 0.3746454119682312\nEpoch 172/200, Batch 13/45, Loss: 0.13801531493663788\nEpoch 172/200, Batch 14/45, Loss: 0.21850532293319702\nEpoch 172/200, Batch 15/45, Loss: 0.19672521948814392\nEpoch 172/200, Batch 16/45, Loss: 0.19838054478168488\nEpoch 172/200, Batch 17/45, Loss: 0.3501095175743103\nEpoch 172/200, Batch 18/45, Loss: 0.9537816643714905\nEpoch 172/200, Batch 19/45, Loss: 0.4961705207824707\nEpoch 172/200, Batch 20/45, Loss: 0.2839394211769104\nEpoch 172/200, Batch 21/45, Loss: 0.32710421085357666\nEpoch 172/200, Batch 22/45, Loss: 0.31448090076446533\nEpoch 172/200, Batch 23/45, Loss: 0.2858704924583435\nEpoch 172/200, Batch 24/45, Loss: 0.24971848726272583\nEpoch 172/200, Batch 25/45, Loss: 0.28268975019454956\nEpoch 172/200, Batch 26/45, Loss: 0.2651151120662689\nEpoch 172/200, Batch 27/45, Loss: 0.5383405089378357\nEpoch 172/200, Batch 28/45, Loss: 0.4319649040699005\nEpoch 172/200, Batch 29/45, Loss: 0.3172335922718048\nEpoch 172/200, Batch 30/45, Loss: 0.2941248416900635\nEpoch 172/200, Batch 31/45, Loss: 0.19354021549224854\nEpoch 172/200, Batch 32/45, Loss: 0.2419470250606537\nEpoch 172/200, Batch 33/45, Loss: 0.47641146183013916\nEpoch 172/200, Batch 34/45, Loss: 0.3383961021900177\nEpoch 172/200, Batch 35/45, Loss: 0.2481864094734192\nEpoch 172/200, Batch 36/45, Loss: 0.2966262698173523\nEpoch 172/200, Batch 37/45, Loss: 0.3539925217628479\nEpoch 172/200, Batch 38/45, Loss: 0.23119521141052246\nEpoch 172/200, Batch 39/45, Loss: 0.18248701095581055\nEpoch 172/200, Batch 40/45, Loss: 0.18917804956436157\nEpoch 172/200, Batch 41/45, Loss: 0.3030039966106415\nEpoch 172/200, Batch 42/45, Loss: 0.37733030319213867\nEpoch 172/200, Batch 43/45, Loss: 0.21792180836200714\nEpoch 172/200, Batch 44/45, Loss: 0.21220539510250092\nEpoch 172/200, Batch 45/45, Loss: 0.281728059053421\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  17.455842077732086 Best Val MSE:  4.714497983455658\nEpoch:  173 , Time Elapsed:  27.50027686357498  mins\nEpoch 173/200, Batch 1/45, Loss: 0.21250256896018982\nEpoch 173/200, Batch 2/45, Loss: 0.13567855954170227\nEpoch 173/200, Batch 3/45, Loss: 0.13620483875274658\nEpoch 173/200, Batch 4/45, Loss: 0.24282708764076233\nEpoch 173/200, Batch 5/45, Loss: 0.18935856223106384\nEpoch 173/200, Batch 6/45, Loss: 0.23520736396312714\nEpoch 173/200, Batch 7/45, Loss: 0.23664885759353638\nEpoch 173/200, Batch 8/45, Loss: 0.17997491359710693\nEpoch 173/200, Batch 9/45, Loss: 0.2021053582429886\nEpoch 173/200, Batch 10/45, Loss: 0.20347952842712402\nEpoch 173/200, Batch 11/45, Loss: 0.2484472095966339\nEpoch 173/200, Batch 12/45, Loss: 0.28820279240608215\nEpoch 173/200, Batch 13/45, Loss: 0.2716767191886902\nEpoch 173/200, Batch 14/45, Loss: 0.26570308208465576\nEpoch 173/200, Batch 15/45, Loss: 0.16575129330158234\nEpoch 173/200, Batch 16/45, Loss: 0.24572888016700745\nEpoch 173/200, Batch 17/45, Loss: 0.1849815547466278\nEpoch 173/200, Batch 18/45, Loss: 0.35145241022109985\nEpoch 173/200, Batch 19/45, Loss: 0.34487056732177734\nEpoch 173/200, Batch 20/45, Loss: 0.5358008146286011\nEpoch 173/200, Batch 21/45, Loss: 0.25632762908935547\nEpoch 173/200, Batch 22/45, Loss: 0.18030792474746704\nEpoch 173/200, Batch 23/45, Loss: 0.3749237656593323\nEpoch 173/200, Batch 24/45, Loss: 0.197980135679245\nEpoch 173/200, Batch 25/45, Loss: 0.18105445802211761\nEpoch 173/200, Batch 26/45, Loss: 0.26574158668518066\nEpoch 173/200, Batch 27/45, Loss: 0.2764085531234741\nEpoch 173/200, Batch 28/45, Loss: 0.20612488687038422\nEpoch 173/200, Batch 29/45, Loss: 0.2944631576538086\nEpoch 173/200, Batch 30/45, Loss: 0.12553973495960236\nEpoch 173/200, Batch 31/45, Loss: 0.22062085568904877\nEpoch 173/200, Batch 32/45, Loss: 0.41259145736694336\nEpoch 173/200, Batch 33/45, Loss: 0.23511426150798798\nEpoch 173/200, Batch 34/45, Loss: 0.28148791193962097\nEpoch 173/200, Batch 35/45, Loss: 0.49964678287506104\nEpoch 173/200, Batch 36/45, Loss: 0.24605897068977356\nEpoch 173/200, Batch 37/45, Loss: 0.15375642478466034\nEpoch 173/200, Batch 38/45, Loss: 0.27383309602737427\nEpoch 173/200, Batch 39/45, Loss: 0.45109134912490845\nEpoch 173/200, Batch 40/45, Loss: 0.22370702028274536\nEpoch 173/200, Batch 41/45, Loss: 0.30986762046813965\nEpoch 173/200, Batch 42/45, Loss: 0.1461000144481659\nEpoch 173/200, Batch 43/45, Loss: 0.26471665501594543\nEpoch 173/200, Batch 44/45, Loss: 0.2809881865978241\nEpoch 173/200, Batch 45/45, Loss: 0.24619027972221375\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  25.18907517194748 Best Val MSE:  4.714497983455658\nEpoch:  174 , Time Elapsed:  27.658836674690246  mins\nEpoch 174/200, Batch 1/45, Loss: 0.22842252254486084\nEpoch 174/200, Batch 2/45, Loss: 0.26406294107437134\nEpoch 174/200, Batch 3/45, Loss: 0.17611350119113922\nEpoch 174/200, Batch 4/45, Loss: 0.3935766816139221\nEpoch 174/200, Batch 5/45, Loss: 0.21238823235034943\nEpoch 174/200, Batch 6/45, Loss: 0.20268963277339935\nEpoch 174/200, Batch 7/45, Loss: 0.2448635697364807\nEpoch 174/200, Batch 8/45, Loss: 0.20927101373672485\nEpoch 174/200, Batch 9/45, Loss: 0.23043355345726013\nEpoch 174/200, Batch 10/45, Loss: 0.3588337004184723\nEpoch 174/200, Batch 11/45, Loss: 0.3000074625015259\nEpoch 174/200, Batch 12/45, Loss: 0.2801266312599182\nEpoch 174/200, Batch 13/45, Loss: 0.19127404689788818\nEpoch 174/200, Batch 14/45, Loss: 0.18681323528289795\nEpoch 174/200, Batch 15/45, Loss: 0.31410372257232666\nEpoch 174/200, Batch 16/45, Loss: 0.23542101681232452\nEpoch 174/200, Batch 17/45, Loss: 0.15329472720623016\nEpoch 174/200, Batch 18/45, Loss: 0.1577153503894806\nEpoch 174/200, Batch 19/45, Loss: 0.27924078702926636\nEpoch 174/200, Batch 20/45, Loss: 0.20244953036308289\nEpoch 174/200, Batch 21/45, Loss: 0.199362650513649\nEpoch 174/200, Batch 22/45, Loss: 0.35821419954299927\nEpoch 174/200, Batch 23/45, Loss: 0.4137815237045288\nEpoch 174/200, Batch 24/45, Loss: 0.41808101534843445\nEpoch 174/200, Batch 25/45, Loss: 0.2885027527809143\nEpoch 174/200, Batch 26/45, Loss: 0.2404911071062088\nEpoch 174/200, Batch 27/45, Loss: 0.2526628077030182\nEpoch 174/200, Batch 28/45, Loss: 0.20563040673732758\nEpoch 174/200, Batch 29/45, Loss: 0.11938586086034775\nEpoch 174/200, Batch 30/45, Loss: 0.3965703547000885\nEpoch 174/200, Batch 31/45, Loss: 0.334093302488327\nEpoch 174/200, Batch 32/45, Loss: 0.2642962634563446\nEpoch 174/200, Batch 33/45, Loss: 0.15301477909088135\nEpoch 174/200, Batch 34/45, Loss: 0.14413779973983765\nEpoch 174/200, Batch 35/45, Loss: 0.23264729976654053\nEpoch 174/200, Batch 36/45, Loss: 0.4020456075668335\nEpoch 174/200, Batch 37/45, Loss: 0.1545822024345398\nEpoch 174/200, Batch 38/45, Loss: 0.15288934111595154\nEpoch 174/200, Batch 39/45, Loss: 0.22408612072467804\nEpoch 174/200, Batch 40/45, Loss: 0.23733364045619965\nEpoch 174/200, Batch 41/45, Loss: 0.24225078523159027\nEpoch 174/200, Batch 42/45, Loss: 0.22503837943077087\nEpoch 174/200, Batch 43/45, Loss: 0.4408762454986572\nEpoch 174/200, Batch 44/45, Loss: 0.24139802157878876\nEpoch 174/200, Batch 45/45, Loss: 0.4176494777202606\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  5.698232375085354 Best Val MSE:  4.714497983455658\nEpoch:  175 , Time Elapsed:  27.817142860094705  mins\nEpoch 175/200, Batch 1/45, Loss: 0.14992478489875793\nEpoch 175/200, Batch 2/45, Loss: 0.267073392868042\nEpoch 175/200, Batch 3/45, Loss: 0.37801221013069153\nEpoch 175/200, Batch 4/45, Loss: 0.2977278232574463\nEpoch 175/200, Batch 5/45, Loss: 0.854912281036377\nEpoch 175/200, Batch 6/45, Loss: 0.2102259397506714\nEpoch 175/200, Batch 7/45, Loss: 0.28387850522994995\nEpoch 175/200, Batch 8/45, Loss: 0.22689059376716614\nEpoch 175/200, Batch 9/45, Loss: 0.26616692543029785\nEpoch 175/200, Batch 10/45, Loss: 0.41301682591438293\nEpoch 175/200, Batch 11/45, Loss: 0.33788323402404785\nEpoch 175/200, Batch 12/45, Loss: 0.24519503116607666\nEpoch 175/200, Batch 13/45, Loss: 0.22521433234214783\nEpoch 175/200, Batch 14/45, Loss: 0.3535066246986389\nEpoch 175/200, Batch 15/45, Loss: 0.2466793954372406\nEpoch 175/200, Batch 16/45, Loss: 0.30673691630363464\nEpoch 175/200, Batch 17/45, Loss: 0.3562469482421875\nEpoch 175/200, Batch 18/45, Loss: 0.2873516380786896\nEpoch 175/200, Batch 19/45, Loss: 0.1742890328168869\nEpoch 175/200, Batch 20/45, Loss: 0.6126918196678162\nEpoch 175/200, Batch 21/45, Loss: 0.2911584973335266\nEpoch 175/200, Batch 22/45, Loss: 0.29027190804481506\nEpoch 175/200, Batch 23/45, Loss: 0.2916529178619385\nEpoch 175/200, Batch 24/45, Loss: 0.328140527009964\nEpoch 175/200, Batch 25/45, Loss: 0.2932729721069336\nEpoch 175/200, Batch 26/45, Loss: 0.473960280418396\nEpoch 175/200, Batch 27/45, Loss: 0.1316324770450592\nEpoch 175/200, Batch 28/45, Loss: 0.2688661813735962\nEpoch 175/200, Batch 29/45, Loss: 0.21385924518108368\nEpoch 175/200, Batch 30/45, Loss: 0.2611984312534332\nEpoch 175/200, Batch 31/45, Loss: 0.27203044295310974\nEpoch 175/200, Batch 32/45, Loss: 0.279802143573761\nEpoch 175/200, Batch 33/45, Loss: 0.2896761894226074\nEpoch 175/200, Batch 34/45, Loss: 0.39235809445381165\nEpoch 175/200, Batch 35/45, Loss: 0.3297756314277649\nEpoch 175/200, Batch 36/45, Loss: 0.3151858448982239\nEpoch 175/200, Batch 37/45, Loss: 0.21722349524497986\nEpoch 175/200, Batch 38/45, Loss: 0.5137823224067688\nEpoch 175/200, Batch 39/45, Loss: 0.19315935671329498\nEpoch 175/200, Batch 40/45, Loss: 0.324634313583374\nEpoch 175/200, Batch 41/45, Loss: 0.19203323125839233\nEpoch 175/200, Batch 42/45, Loss: 0.32645612955093384\nEpoch 175/200, Batch 43/45, Loss: 0.12903237342834473\nEpoch 175/200, Batch 44/45, Loss: 0.3246105909347534\nEpoch 175/200, Batch 45/45, Loss: 0.2302069216966629\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  114.79156589508057 Best Val MSE:  4.714497983455658\nEpoch:  176 , Time Elapsed:  27.975532503922782  mins\nEpoch 176/200, Batch 1/45, Loss: 0.24073565006256104\nEpoch 176/200, Batch 2/45, Loss: 0.25817376375198364\nEpoch 176/200, Batch 3/45, Loss: 0.3499704599380493\nEpoch 176/200, Batch 4/45, Loss: 0.17803330719470978\nEpoch 176/200, Batch 5/45, Loss: 0.36847418546676636\nEpoch 176/200, Batch 6/45, Loss: 0.3777045011520386\nEpoch 176/200, Batch 7/45, Loss: 0.272754430770874\nEpoch 176/200, Batch 8/45, Loss: 0.2805129289627075\nEpoch 176/200, Batch 9/45, Loss: 0.3682118058204651\nEpoch 176/200, Batch 10/45, Loss: 0.3077329397201538\nEpoch 176/200, Batch 11/45, Loss: 0.24557270109653473\nEpoch 176/200, Batch 12/45, Loss: 0.3776305317878723\nEpoch 176/200, Batch 13/45, Loss: 0.19272050261497498\nEpoch 176/200, Batch 14/45, Loss: 0.2073868066072464\nEpoch 176/200, Batch 15/45, Loss: 0.3324844241142273\nEpoch 176/200, Batch 16/45, Loss: 0.273072749376297\nEpoch 176/200, Batch 17/45, Loss: 0.254052072763443\nEpoch 176/200, Batch 18/45, Loss: 0.14609703421592712\nEpoch 176/200, Batch 19/45, Loss: 0.22622330486774445\nEpoch 176/200, Batch 20/45, Loss: 0.21453428268432617\nEpoch 176/200, Batch 21/45, Loss: 0.6894450187683105\nEpoch 176/200, Batch 22/45, Loss: 0.2953087091445923\nEpoch 176/200, Batch 23/45, Loss: 0.14037853479385376\nEpoch 176/200, Batch 24/45, Loss: 0.2587531805038452\nEpoch 176/200, Batch 25/45, Loss: 0.29176342487335205\nEpoch 176/200, Batch 26/45, Loss: 0.2734018862247467\nEpoch 176/200, Batch 27/45, Loss: 0.24669232964515686\nEpoch 176/200, Batch 28/45, Loss: 0.6654042601585388\nEpoch 176/200, Batch 29/45, Loss: 0.3116915225982666\nEpoch 176/200, Batch 30/45, Loss: 0.32629841566085815\nEpoch 176/200, Batch 31/45, Loss: 0.1640264093875885\nEpoch 176/200, Batch 32/45, Loss: 0.46164411306381226\nEpoch 176/200, Batch 33/45, Loss: 0.15013954043388367\nEpoch 176/200, Batch 34/45, Loss: 0.19841448962688446\nEpoch 176/200, Batch 35/45, Loss: 0.27463996410369873\nEpoch 176/200, Batch 36/45, Loss: 0.30522221326828003\nEpoch 176/200, Batch 37/45, Loss: 0.2786107659339905\nEpoch 176/200, Batch 38/45, Loss: 0.31001609563827515\nEpoch 176/200, Batch 39/45, Loss: 0.2638123333454132\nEpoch 176/200, Batch 40/45, Loss: 0.3884112238883972\nEpoch 176/200, Batch 41/45, Loss: 0.48773622512817383\nEpoch 176/200, Batch 42/45, Loss: 0.24006471037864685\nEpoch 176/200, Batch 43/45, Loss: 0.2738717198371887\nEpoch 176/200, Batch 44/45, Loss: 0.30223438143730164\nEpoch 176/200, Batch 45/45, Loss: 0.18951590359210968\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  113.79009342193604 Best Val MSE:  4.714497983455658\nEpoch:  177 , Time Elapsed:  28.133567456404368  mins\nEpoch 177/200, Batch 1/45, Loss: 0.24264100193977356\nEpoch 177/200, Batch 2/45, Loss: 0.24450692534446716\nEpoch 177/200, Batch 3/45, Loss: 0.15167996287345886\nEpoch 177/200, Batch 4/45, Loss: 0.15696310997009277\nEpoch 177/200, Batch 5/45, Loss: 0.240618035197258\nEpoch 177/200, Batch 6/45, Loss: 0.21566444635391235\nEpoch 177/200, Batch 7/45, Loss: 0.18380531668663025\nEpoch 177/200, Batch 8/45, Loss: 0.26794683933258057\nEpoch 177/200, Batch 9/45, Loss: 0.1673833727836609\nEpoch 177/200, Batch 10/45, Loss: 0.1785811185836792\nEpoch 177/200, Batch 11/45, Loss: 0.27042150497436523\nEpoch 177/200, Batch 12/45, Loss: 0.24638418853282928\nEpoch 177/200, Batch 13/45, Loss: 0.2179836481809616\nEpoch 177/200, Batch 14/45, Loss: 0.2519248127937317\nEpoch 177/200, Batch 15/45, Loss: 0.17613987624645233\nEpoch 177/200, Batch 16/45, Loss: 0.16996753215789795\nEpoch 177/200, Batch 17/45, Loss: 0.23017948865890503\nEpoch 177/200, Batch 18/45, Loss: 0.38807213306427\nEpoch 177/200, Batch 19/45, Loss: 0.3709166347980499\nEpoch 177/200, Batch 20/45, Loss: 0.3883780241012573\nEpoch 177/200, Batch 21/45, Loss: 0.4394245445728302\nEpoch 177/200, Batch 22/45, Loss: 0.23444688320159912\nEpoch 177/200, Batch 23/45, Loss: 0.31473103165626526\nEpoch 177/200, Batch 24/45, Loss: 0.21466153860092163\nEpoch 177/200, Batch 25/45, Loss: 0.3357425332069397\nEpoch 177/200, Batch 26/45, Loss: 0.2928601801395416\nEpoch 177/200, Batch 27/45, Loss: 0.23955172300338745\nEpoch 177/200, Batch 28/45, Loss: 0.300239235162735\nEpoch 177/200, Batch 29/45, Loss: 0.2503114938735962\nEpoch 177/200, Batch 30/45, Loss: 0.13167023658752441\nEpoch 177/200, Batch 31/45, Loss: 0.10625004768371582\nEpoch 177/200, Batch 32/45, Loss: 0.2569829523563385\nEpoch 177/200, Batch 33/45, Loss: 0.13076195120811462\nEpoch 177/200, Batch 34/45, Loss: 0.3886961340904236\nEpoch 177/200, Batch 35/45, Loss: 0.3321645259857178\nEpoch 177/200, Batch 36/45, Loss: 0.1821533590555191\nEpoch 177/200, Batch 37/45, Loss: 0.2457745373249054\nEpoch 177/200, Batch 38/45, Loss: 0.29148972034454346\nEpoch 177/200, Batch 39/45, Loss: 0.22367876768112183\nEpoch 177/200, Batch 40/45, Loss: 0.2981131076812744\nEpoch 177/200, Batch 41/45, Loss: 0.2540719509124756\nEpoch 177/200, Batch 42/45, Loss: 0.5064147114753723\nEpoch 177/200, Batch 43/45, Loss: 0.20150449872016907\nEpoch 177/200, Batch 44/45, Loss: 0.20215077698230743\nEpoch 177/200, Batch 45/45, Loss: 0.2240084409713745\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  145.32305335998535 Best Val MSE:  4.714497983455658\nEpoch:  178 , Time Elapsed:  28.292355799674986  mins\nEpoch 178/200, Batch 1/45, Loss: 0.19170373678207397\nEpoch 178/200, Batch 2/45, Loss: 0.12026587128639221\nEpoch 178/200, Batch 3/45, Loss: 0.1628466099500656\nEpoch 178/200, Batch 4/45, Loss: 0.42264610528945923\nEpoch 178/200, Batch 5/45, Loss: 0.18562081456184387\nEpoch 178/200, Batch 6/45, Loss: 0.2017657458782196\nEpoch 178/200, Batch 7/45, Loss: 0.2544478178024292\nEpoch 178/200, Batch 8/45, Loss: 0.23932847380638123\nEpoch 178/200, Batch 9/45, Loss: 0.3440939784049988\nEpoch 178/200, Batch 10/45, Loss: 0.20429249107837677\nEpoch 178/200, Batch 11/45, Loss: 0.14363780617713928\nEpoch 178/200, Batch 12/45, Loss: 0.23737934231758118\nEpoch 178/200, Batch 13/45, Loss: 0.3136599659919739\nEpoch 178/200, Batch 14/45, Loss: 0.3166447579860687\nEpoch 178/200, Batch 15/45, Loss: 0.3037083148956299\nEpoch 178/200, Batch 16/45, Loss: 0.22469592094421387\nEpoch 178/200, Batch 17/45, Loss: 0.2889317572116852\nEpoch 178/200, Batch 18/45, Loss: 0.21587908267974854\nEpoch 178/200, Batch 19/45, Loss: 0.20538893342018127\nEpoch 178/200, Batch 20/45, Loss: 0.12964142858982086\nEpoch 178/200, Batch 21/45, Loss: 0.3961030840873718\nEpoch 178/200, Batch 22/45, Loss: 0.36259832978248596\nEpoch 178/200, Batch 23/45, Loss: 0.2019285410642624\nEpoch 178/200, Batch 24/45, Loss: 0.3077186942100525\nEpoch 178/200, Batch 25/45, Loss: 0.298664391040802\nEpoch 178/200, Batch 26/45, Loss: 0.2787016034126282\nEpoch 178/200, Batch 27/45, Loss: 0.2132454812526703\nEpoch 178/200, Batch 28/45, Loss: 0.2206772267818451\nEpoch 178/200, Batch 29/45, Loss: 0.2623218595981598\nEpoch 178/200, Batch 30/45, Loss: 0.4469831585884094\nEpoch 178/200, Batch 31/45, Loss: 0.2720057964324951\nEpoch 178/200, Batch 32/45, Loss: 0.20216664671897888\nEpoch 178/200, Batch 33/45, Loss: 0.31815454363822937\nEpoch 178/200, Batch 34/45, Loss: 0.1344175934791565\nEpoch 178/200, Batch 35/45, Loss: 0.4360429644584656\nEpoch 178/200, Batch 36/45, Loss: 0.3235524594783783\nEpoch 178/200, Batch 37/45, Loss: 0.20328187942504883\nEpoch 178/200, Batch 38/45, Loss: 0.35936489701271057\nEpoch 178/200, Batch 39/45, Loss: 0.2678315043449402\nEpoch 178/200, Batch 40/45, Loss: 0.26390230655670166\nEpoch 178/200, Batch 41/45, Loss: 0.18829432129859924\nEpoch 178/200, Batch 42/45, Loss: 0.27701306343078613\nEpoch 178/200, Batch 43/45, Loss: 0.2418009638786316\nEpoch 178/200, Batch 44/45, Loss: 0.36069154739379883\nEpoch 178/200, Batch 45/45, Loss: 0.25990891456604004\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  250.67515468597412 Best Val MSE:  4.714497983455658\nEpoch:  179 , Time Elapsed:  28.45001613299052  mins\nEpoch 179/200, Batch 1/45, Loss: 0.153128981590271\nEpoch 179/200, Batch 2/45, Loss: 0.2216157466173172\nEpoch 179/200, Batch 3/45, Loss: 0.20940472185611725\nEpoch 179/200, Batch 4/45, Loss: 0.3761315941810608\nEpoch 179/200, Batch 5/45, Loss: 0.19584867358207703\nEpoch 179/200, Batch 6/45, Loss: 0.3660590648651123\nEpoch 179/200, Batch 7/45, Loss: 0.30636173486709595\nEpoch 179/200, Batch 8/45, Loss: 0.17750585079193115\nEpoch 179/200, Batch 9/45, Loss: 0.24359172582626343\nEpoch 179/200, Batch 10/45, Loss: 0.3546971082687378\nEpoch 179/200, Batch 11/45, Loss: 0.3794797360897064\nEpoch 179/200, Batch 12/45, Loss: 0.26817768812179565\nEpoch 179/200, Batch 13/45, Loss: 0.14096631109714508\nEpoch 179/200, Batch 14/45, Loss: 0.32751554250717163\nEpoch 179/200, Batch 15/45, Loss: 0.20365078747272491\nEpoch 179/200, Batch 16/45, Loss: 0.1808026134967804\nEpoch 179/200, Batch 17/45, Loss: 0.16393762826919556\nEpoch 179/200, Batch 18/45, Loss: 0.19665227830410004\nEpoch 179/200, Batch 19/45, Loss: 0.299704909324646\nEpoch 179/200, Batch 20/45, Loss: 0.45937782526016235\nEpoch 179/200, Batch 21/45, Loss: 0.298061728477478\nEpoch 179/200, Batch 22/45, Loss: 0.23288047313690186\nEpoch 179/200, Batch 23/45, Loss: 0.19454246759414673\nEpoch 179/200, Batch 24/45, Loss: 0.13533928990364075\nEpoch 179/200, Batch 25/45, Loss: 0.14930947124958038\nEpoch 179/200, Batch 26/45, Loss: 0.2849268615245819\nEpoch 179/200, Batch 27/45, Loss: 0.38646402955055237\nEpoch 179/200, Batch 28/45, Loss: 0.27540674805641174\nEpoch 179/200, Batch 29/45, Loss: 0.1849261224269867\nEpoch 179/200, Batch 30/45, Loss: 0.256193608045578\nEpoch 179/200, Batch 31/45, Loss: 0.32744356989860535\nEpoch 179/200, Batch 32/45, Loss: 0.4115268588066101\nEpoch 179/200, Batch 33/45, Loss: 0.21828526258468628\nEpoch 179/200, Batch 34/45, Loss: 0.178961843252182\nEpoch 179/200, Batch 35/45, Loss: 0.19280245900154114\nEpoch 179/200, Batch 36/45, Loss: 0.40221473574638367\nEpoch 179/200, Batch 37/45, Loss: 0.315923810005188\nEpoch 179/200, Batch 38/45, Loss: 0.6225174069404602\nEpoch 179/200, Batch 39/45, Loss: 0.36276546120643616\nEpoch 179/200, Batch 40/45, Loss: 0.369647741317749\nEpoch 179/200, Batch 41/45, Loss: 0.29317164421081543\nEpoch 179/200, Batch 42/45, Loss: 0.18688704073429108\nEpoch 179/200, Batch 43/45, Loss: 0.28367677330970764\nEpoch 179/200, Batch 44/45, Loss: 0.16475440561771393\nEpoch 179/200, Batch 45/45, Loss: 0.28390929102897644\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  83.31642603874207 Best Val MSE:  4.714497983455658\nEpoch:  180 , Time Elapsed:  28.614835214614867  mins\nEpoch 180/200, Batch 1/45, Loss: 0.33181074261665344\nEpoch 180/200, Batch 2/45, Loss: 0.2616407573223114\nEpoch 180/200, Batch 3/45, Loss: 0.24308384954929352\nEpoch 180/200, Batch 4/45, Loss: 0.37855440378189087\nEpoch 180/200, Batch 5/45, Loss: 0.3054400384426117\nEpoch 180/200, Batch 6/45, Loss: 0.3835523724555969\nEpoch 180/200, Batch 7/45, Loss: 0.3916233479976654\nEpoch 180/200, Batch 8/45, Loss: 0.18594472110271454\nEpoch 180/200, Batch 9/45, Loss: 0.20063072443008423\nEpoch 180/200, Batch 10/45, Loss: 0.24782511591911316\nEpoch 180/200, Batch 11/45, Loss: 0.7464019656181335\nEpoch 180/200, Batch 12/45, Loss: 0.30377689003944397\nEpoch 180/200, Batch 13/45, Loss: 0.2170102447271347\nEpoch 180/200, Batch 14/45, Loss: 0.3107389807701111\nEpoch 180/200, Batch 15/45, Loss: 0.18775418400764465\nEpoch 180/200, Batch 16/45, Loss: 0.27110135555267334\nEpoch 180/200, Batch 17/45, Loss: 0.2632256746292114\nEpoch 180/200, Batch 18/45, Loss: 0.3466498553752899\nEpoch 180/200, Batch 19/45, Loss: 0.37649720907211304\nEpoch 180/200, Batch 20/45, Loss: 0.30426621437072754\nEpoch 180/200, Batch 21/45, Loss: 0.2768923342227936\nEpoch 180/200, Batch 22/45, Loss: 0.4222538471221924\nEpoch 180/200, Batch 23/45, Loss: 0.31025469303131104\nEpoch 180/200, Batch 24/45, Loss: 0.31067487597465515\nEpoch 180/200, Batch 25/45, Loss: 0.21617670357227325\nEpoch 180/200, Batch 26/45, Loss: 0.2533796727657318\nEpoch 180/200, Batch 27/45, Loss: 0.3134600818157196\nEpoch 180/200, Batch 28/45, Loss: 0.2755663990974426\nEpoch 180/200, Batch 29/45, Loss: 0.23074376583099365\nEpoch 180/200, Batch 30/45, Loss: 0.24830472469329834\nEpoch 180/200, Batch 31/45, Loss: 0.2365698516368866\nEpoch 180/200, Batch 32/45, Loss: 0.5171469449996948\nEpoch 180/200, Batch 33/45, Loss: 0.2363988310098648\nEpoch 180/200, Batch 34/45, Loss: 0.2503291368484497\nEpoch 180/200, Batch 35/45, Loss: 0.23766019940376282\nEpoch 180/200, Batch 36/45, Loss: 0.44140931963920593\nEpoch 180/200, Batch 37/45, Loss: 0.4919593334197998\nEpoch 180/200, Batch 38/45, Loss: 0.1747060865163803\nEpoch 180/200, Batch 39/45, Loss: 0.20996810495853424\nEpoch 180/200, Batch 40/45, Loss: 0.3107776939868927\nEpoch 180/200, Batch 41/45, Loss: 0.17918837070465088\nEpoch 180/200, Batch 42/45, Loss: 0.20978663861751556\nEpoch 180/200, Batch 43/45, Loss: 0.4098300635814667\nEpoch 180/200, Batch 44/45, Loss: 0.3654109239578247\nEpoch 180/200, Batch 45/45, Loss: 0.227821946144104\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  8.038947194814682 Best Val MSE:  4.714497983455658\nEpoch:  181 , Time Elapsed:  28.773527896404268  mins\nEpoch 181/200, Batch 1/45, Loss: 0.2837861180305481\nEpoch 181/200, Batch 2/45, Loss: 0.2063775658607483\nEpoch 181/200, Batch 3/45, Loss: 0.2306523323059082\nEpoch 181/200, Batch 4/45, Loss: 0.3005969524383545\nEpoch 181/200, Batch 5/45, Loss: 0.29485753178596497\nEpoch 181/200, Batch 6/45, Loss: 0.23908784985542297\nEpoch 181/200, Batch 7/45, Loss: 0.24706551432609558\nEpoch 181/200, Batch 8/45, Loss: 0.18322555720806122\nEpoch 181/200, Batch 9/45, Loss: 0.2592473328113556\nEpoch 181/200, Batch 10/45, Loss: 0.20729775726795197\nEpoch 181/200, Batch 11/45, Loss: 0.23679527640342712\nEpoch 181/200, Batch 12/45, Loss: 0.1812419593334198\nEpoch 181/200, Batch 13/45, Loss: 0.36396580934524536\nEpoch 181/200, Batch 14/45, Loss: 0.25385066866874695\nEpoch 181/200, Batch 15/45, Loss: 0.3739206790924072\nEpoch 181/200, Batch 16/45, Loss: 0.2794265151023865\nEpoch 181/200, Batch 17/45, Loss: 0.21250905096530914\nEpoch 181/200, Batch 18/45, Loss: 0.3165286183357239\nEpoch 181/200, Batch 19/45, Loss: 0.23485486209392548\nEpoch 181/200, Batch 20/45, Loss: 0.21861965954303741\nEpoch 181/200, Batch 21/45, Loss: 0.3219258189201355\nEpoch 181/200, Batch 22/45, Loss: 0.14900268614292145\nEpoch 181/200, Batch 23/45, Loss: 0.21867911517620087\nEpoch 181/200, Batch 24/45, Loss: 0.289927214384079\nEpoch 181/200, Batch 25/45, Loss: 0.1573006808757782\nEpoch 181/200, Batch 26/45, Loss: 0.31278789043426514\nEpoch 181/200, Batch 27/45, Loss: 0.11599934101104736\nEpoch 181/200, Batch 28/45, Loss: 0.1583556830883026\nEpoch 181/200, Batch 29/45, Loss: 0.21707850694656372\nEpoch 181/200, Batch 30/45, Loss: 0.24223893880844116\nEpoch 181/200, Batch 31/45, Loss: 0.2739922106266022\nEpoch 181/200, Batch 32/45, Loss: 0.17158935964107513\nEpoch 181/200, Batch 33/45, Loss: 0.11218886077404022\nEpoch 181/200, Batch 34/45, Loss: 0.25809943675994873\nEpoch 181/200, Batch 35/45, Loss: 0.23222878575325012\nEpoch 181/200, Batch 36/45, Loss: 0.36059820652008057\nEpoch 181/200, Batch 37/45, Loss: 0.1813662201166153\nEpoch 181/200, Batch 38/45, Loss: 0.5509738922119141\nEpoch 181/200, Batch 39/45, Loss: 0.16792938113212585\nEpoch 181/200, Batch 40/45, Loss: 0.22117197513580322\nEpoch 181/200, Batch 41/45, Loss: 0.2830280065536499\nEpoch 181/200, Batch 42/45, Loss: 0.31153595447540283\nEpoch 181/200, Batch 43/45, Loss: 0.22131997346878052\nEpoch 181/200, Batch 44/45, Loss: 0.23702487349510193\nEpoch 181/200, Batch 45/45, Loss: 0.21145744621753693\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  81.40730237960815 Best Val MSE:  4.714497983455658\nEpoch:  182 , Time Elapsed:  28.930794926484428  mins\nEpoch 182/200, Batch 1/45, Loss: 0.4125745892524719\nEpoch 182/200, Batch 2/45, Loss: 0.28816235065460205\nEpoch 182/200, Batch 3/45, Loss: 0.13026607036590576\nEpoch 182/200, Batch 4/45, Loss: 0.2484855055809021\nEpoch 182/200, Batch 5/45, Loss: 0.20808395743370056\nEpoch 182/200, Batch 6/45, Loss: 0.16406233608722687\nEpoch 182/200, Batch 7/45, Loss: 0.2624421715736389\nEpoch 182/200, Batch 8/45, Loss: 1.5393750667572021\nEpoch 182/200, Batch 9/45, Loss: 0.22695529460906982\nEpoch 182/200, Batch 10/45, Loss: 0.31801944971084595\nEpoch 182/200, Batch 11/45, Loss: 0.3501153588294983\nEpoch 182/200, Batch 12/45, Loss: 0.2385312020778656\nEpoch 182/200, Batch 13/45, Loss: 1.0385189056396484\nEpoch 182/200, Batch 14/45, Loss: 0.3123592436313629\nEpoch 182/200, Batch 15/45, Loss: 0.5698844194412231\nEpoch 182/200, Batch 16/45, Loss: 0.3188023567199707\nEpoch 182/200, Batch 17/45, Loss: 0.32685181498527527\nEpoch 182/200, Batch 18/45, Loss: 0.36063122749328613\nEpoch 182/200, Batch 19/45, Loss: 0.33945998549461365\nEpoch 182/200, Batch 20/45, Loss: 0.4337881803512573\nEpoch 182/200, Batch 21/45, Loss: 0.3365715444087982\nEpoch 182/200, Batch 22/45, Loss: 0.1462535411119461\nEpoch 182/200, Batch 23/45, Loss: 0.378293514251709\nEpoch 182/200, Batch 24/45, Loss: 0.19763946533203125\nEpoch 182/200, Batch 25/45, Loss: 0.24084168672561646\nEpoch 182/200, Batch 26/45, Loss: 0.280274361371994\nEpoch 182/200, Batch 27/45, Loss: 0.33144044876098633\nEpoch 182/200, Batch 28/45, Loss: 0.38502293825149536\nEpoch 182/200, Batch 29/45, Loss: 0.36714625358581543\nEpoch 182/200, Batch 30/45, Loss: 0.2717776596546173\nEpoch 182/200, Batch 31/45, Loss: 0.22392044961452484\nEpoch 182/200, Batch 32/45, Loss: 0.17310172319412231\nEpoch 182/200, Batch 33/45, Loss: 0.2144656479358673\nEpoch 182/200, Batch 34/45, Loss: 0.21155628561973572\nEpoch 182/200, Batch 35/45, Loss: 0.15045088529586792\nEpoch 182/200, Batch 36/45, Loss: 0.3843792676925659\nEpoch 182/200, Batch 37/45, Loss: 0.43160927295684814\nEpoch 182/200, Batch 38/45, Loss: 0.3611007332801819\nEpoch 182/200, Batch 39/45, Loss: 0.34034663438796997\nEpoch 182/200, Batch 40/45, Loss: 0.16818112134933472\nEpoch 182/200, Batch 41/45, Loss: 0.350929319858551\nEpoch 182/200, Batch 42/45, Loss: 0.25098109245300293\nEpoch 182/200, Batch 43/45, Loss: 0.18020422756671906\nEpoch 182/200, Batch 44/45, Loss: 0.18750271201133728\nEpoch 182/200, Batch 45/45, Loss: 0.507491409778595\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  76.0123541355133 Best Val MSE:  4.714497983455658\nEpoch:  183 , Time Elapsed:  29.086163822809855  mins\nEpoch 183/200, Batch 1/45, Loss: 0.7811340093612671\nEpoch 183/200, Batch 2/45, Loss: 0.43834835290908813\nEpoch 183/200, Batch 3/45, Loss: 0.3453437089920044\nEpoch 183/200, Batch 4/45, Loss: 0.25616270303726196\nEpoch 183/200, Batch 5/45, Loss: 0.2155715674161911\nEpoch 183/200, Batch 6/45, Loss: 0.29567933082580566\nEpoch 183/200, Batch 7/45, Loss: 0.2357315868139267\nEpoch 183/200, Batch 8/45, Loss: 0.4147220253944397\nEpoch 183/200, Batch 9/45, Loss: 0.19575200974941254\nEpoch 183/200, Batch 10/45, Loss: 0.16740034520626068\nEpoch 183/200, Batch 11/45, Loss: 0.5317880511283875\nEpoch 183/200, Batch 12/45, Loss: 0.19984835386276245\nEpoch 183/200, Batch 13/45, Loss: 0.22117184102535248\nEpoch 183/200, Batch 14/45, Loss: 0.32616016268730164\nEpoch 183/200, Batch 15/45, Loss: 0.1634010672569275\nEpoch 183/200, Batch 16/45, Loss: 0.32355615496635437\nEpoch 183/200, Batch 17/45, Loss: 0.16439785063266754\nEpoch 183/200, Batch 18/45, Loss: 0.21358700096607208\nEpoch 183/200, Batch 19/45, Loss: 0.24569320678710938\nEpoch 183/200, Batch 20/45, Loss: 0.11852581799030304\nEpoch 183/200, Batch 21/45, Loss: 0.26386022567749023\nEpoch 183/200, Batch 22/45, Loss: 0.3782767057418823\nEpoch 183/200, Batch 23/45, Loss: 0.23705264925956726\nEpoch 183/200, Batch 24/45, Loss: 0.30481764674186707\nEpoch 183/200, Batch 25/45, Loss: 0.28215450048446655\nEpoch 183/200, Batch 26/45, Loss: 0.2275063842535019\nEpoch 183/200, Batch 27/45, Loss: 0.2943541705608368\nEpoch 183/200, Batch 28/45, Loss: 0.3063388466835022\nEpoch 183/200, Batch 29/45, Loss: 0.27416446805000305\nEpoch 183/200, Batch 30/45, Loss: 0.1997327208518982\nEpoch 183/200, Batch 31/45, Loss: 0.2688160538673401\nEpoch 183/200, Batch 32/45, Loss: 0.2131383717060089\nEpoch 183/200, Batch 33/45, Loss: 0.21336884796619415\nEpoch 183/200, Batch 34/45, Loss: 1.6161197423934937\nEpoch 183/200, Batch 35/45, Loss: 0.38334178924560547\nEpoch 183/200, Batch 36/45, Loss: 0.1894766092300415\nEpoch 183/200, Batch 37/45, Loss: 0.45001220703125\nEpoch 183/200, Batch 38/45, Loss: 0.2761691212654114\nEpoch 183/200, Batch 39/45, Loss: 0.36116501688957214\nEpoch 183/200, Batch 40/45, Loss: 0.18989914655685425\nEpoch 183/200, Batch 41/45, Loss: 0.2647600471973419\nEpoch 183/200, Batch 42/45, Loss: 0.23116964101791382\nEpoch 183/200, Batch 43/45, Loss: 0.2135249376296997\nEpoch 183/200, Batch 44/45, Loss: 0.19729463756084442\nEpoch 183/200, Batch 45/45, Loss: 0.215940460562706\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  119.93032169342041 Best Val MSE:  4.714497983455658\nEpoch:  184 , Time Elapsed:  29.24756240049998  mins\nEpoch 184/200, Batch 1/45, Loss: 0.17816011607646942\nEpoch 184/200, Batch 2/45, Loss: 0.36057108640670776\nEpoch 184/200, Batch 3/45, Loss: 0.1764398217201233\nEpoch 184/200, Batch 4/45, Loss: 0.13725577294826508\nEpoch 184/200, Batch 5/45, Loss: 0.4121526777744293\nEpoch 184/200, Batch 6/45, Loss: 0.15525773167610168\nEpoch 184/200, Batch 7/45, Loss: 0.21377971768379211\nEpoch 184/200, Batch 8/45, Loss: 0.22087527811527252\nEpoch 184/200, Batch 9/45, Loss: 0.3519565463066101\nEpoch 184/200, Batch 10/45, Loss: 0.2176331728696823\nEpoch 184/200, Batch 11/45, Loss: 0.22286878526210785\nEpoch 184/200, Batch 12/45, Loss: 0.3220045864582062\nEpoch 184/200, Batch 13/45, Loss: 0.21559865772724152\nEpoch 184/200, Batch 14/45, Loss: 0.38216644525527954\nEpoch 184/200, Batch 15/45, Loss: 0.20918360352516174\nEpoch 184/200, Batch 16/45, Loss: 0.2609558701515198\nEpoch 184/200, Batch 17/45, Loss: 0.27860668301582336\nEpoch 184/200, Batch 18/45, Loss: 0.2736864686012268\nEpoch 184/200, Batch 19/45, Loss: 0.10319913923740387\nEpoch 184/200, Batch 20/45, Loss: 0.16975905001163483\nEpoch 184/200, Batch 21/45, Loss: 0.15276259183883667\nEpoch 184/200, Batch 22/45, Loss: 0.2911532521247864\nEpoch 184/200, Batch 23/45, Loss: 0.2596362829208374\nEpoch 184/200, Batch 24/45, Loss: 0.19781939685344696\nEpoch 184/200, Batch 25/45, Loss: 0.2837417423725128\nEpoch 184/200, Batch 26/45, Loss: 0.28473514318466187\nEpoch 184/200, Batch 27/45, Loss: 0.39271095395088196\nEpoch 184/200, Batch 28/45, Loss: 0.25636357069015503\nEpoch 184/200, Batch 29/45, Loss: 0.23615984618663788\nEpoch 184/200, Batch 30/45, Loss: 0.3543581962585449\nEpoch 184/200, Batch 31/45, Loss: 0.2153097689151764\nEpoch 184/200, Batch 32/45, Loss: 0.18755948543548584\nEpoch 184/200, Batch 33/45, Loss: 0.16118785738945007\nEpoch 184/200, Batch 34/45, Loss: 0.29621368646621704\nEpoch 184/200, Batch 35/45, Loss: 0.9455108046531677\nEpoch 184/200, Batch 36/45, Loss: 0.23271682858467102\nEpoch 184/200, Batch 37/45, Loss: 0.2466086745262146\nEpoch 184/200, Batch 38/45, Loss: 0.24413086473941803\nEpoch 184/200, Batch 39/45, Loss: 0.26423195004463196\nEpoch 184/200, Batch 40/45, Loss: 0.2027437388896942\nEpoch 184/200, Batch 41/45, Loss: 0.168570876121521\nEpoch 184/200, Batch 42/45, Loss: 0.11038172245025635\nEpoch 184/200, Batch 43/45, Loss: 0.28558820486068726\nEpoch 184/200, Batch 44/45, Loss: 0.18542739748954773\nEpoch 184/200, Batch 45/45, Loss: 0.29945531487464905\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  4.7516414523124695 Best Val MSE:  4.714497983455658\nEpoch:  185 , Time Elapsed:  29.410065674781798  mins\nEpoch 185/200, Batch 1/45, Loss: 0.2937324047088623\nEpoch 185/200, Batch 2/45, Loss: 0.22817829251289368\nEpoch 185/200, Batch 3/45, Loss: 0.4486415386199951\nEpoch 185/200, Batch 4/45, Loss: 0.2951739430427551\nEpoch 185/200, Batch 5/45, Loss: 0.18467473983764648\nEpoch 185/200, Batch 6/45, Loss: 0.22244715690612793\nEpoch 185/200, Batch 7/45, Loss: 0.2971791625022888\nEpoch 185/200, Batch 8/45, Loss: 0.2601809799671173\nEpoch 185/200, Batch 9/45, Loss: 0.3738659620285034\nEpoch 185/200, Batch 10/45, Loss: 0.22537899017333984\nEpoch 185/200, Batch 11/45, Loss: 0.14313741028308868\nEpoch 185/200, Batch 12/45, Loss: 0.2621936500072479\nEpoch 185/200, Batch 13/45, Loss: 0.27593356370925903\nEpoch 185/200, Batch 14/45, Loss: 0.1620224416255951\nEpoch 185/200, Batch 15/45, Loss: 0.13653242588043213\nEpoch 185/200, Batch 16/45, Loss: 0.21314892172813416\nEpoch 185/200, Batch 17/45, Loss: 0.23812806606292725\nEpoch 185/200, Batch 18/45, Loss: 0.18753159046173096\nEpoch 185/200, Batch 19/45, Loss: 0.23047839105129242\nEpoch 185/200, Batch 20/45, Loss: 0.35840633511543274\nEpoch 185/200, Batch 21/45, Loss: 0.2722848951816559\nEpoch 185/200, Batch 22/45, Loss: 0.19770899415016174\nEpoch 185/200, Batch 23/45, Loss: 0.22164323925971985\nEpoch 185/200, Batch 24/45, Loss: 0.23920297622680664\nEpoch 185/200, Batch 25/45, Loss: 0.22367677092552185\nEpoch 185/200, Batch 26/45, Loss: 0.21136417984962463\nEpoch 185/200, Batch 27/45, Loss: 0.369037926197052\nEpoch 185/200, Batch 28/45, Loss: 0.16549742221832275\nEpoch 185/200, Batch 29/45, Loss: 0.20195990800857544\nEpoch 185/200, Batch 30/45, Loss: 0.2652657628059387\nEpoch 185/200, Batch 31/45, Loss: 0.3127588629722595\nEpoch 185/200, Batch 32/45, Loss: 0.17891588807106018\nEpoch 185/200, Batch 33/45, Loss: 0.2042011022567749\nEpoch 185/200, Batch 34/45, Loss: 0.24745798110961914\nEpoch 185/200, Batch 35/45, Loss: 0.23935818672180176\nEpoch 185/200, Batch 36/45, Loss: 0.12687437236309052\nEpoch 185/200, Batch 37/45, Loss: 0.15956613421440125\nEpoch 185/200, Batch 38/45, Loss: 0.31971797347068787\nEpoch 185/200, Batch 39/45, Loss: 0.23052316904067993\nEpoch 185/200, Batch 40/45, Loss: 0.21072763204574585\nEpoch 185/200, Batch 41/45, Loss: 0.25037580728530884\nEpoch 185/200, Batch 42/45, Loss: 0.203841894865036\nEpoch 185/200, Batch 43/45, Loss: 0.2456876039505005\nEpoch 185/200, Batch 44/45, Loss: 0.49037787318229675\nEpoch 185/200, Batch 45/45, Loss: 0.7912474870681763\nValidating and Checkpointing!\nBest model Saved! Val MSE:  3.566082552075386\nEpoch:  186 , Time Elapsed:  29.57913270791372  mins\nEpoch 186/200, Batch 1/45, Loss: 0.1929146647453308\nEpoch 186/200, Batch 2/45, Loss: 0.28585392236709595\nEpoch 186/200, Batch 3/45, Loss: 0.22570225596427917\nEpoch 186/200, Batch 4/45, Loss: 0.28693926334381104\nEpoch 186/200, Batch 5/45, Loss: 0.20293600857257843\nEpoch 186/200, Batch 6/45, Loss: 0.26657044887542725\nEpoch 186/200, Batch 7/45, Loss: 0.20602931082248688\nEpoch 186/200, Batch 8/45, Loss: 0.2432938516139984\nEpoch 186/200, Batch 9/45, Loss: 0.2963826060295105\nEpoch 186/200, Batch 10/45, Loss: 0.24002239108085632\nEpoch 186/200, Batch 11/45, Loss: 0.23035122454166412\nEpoch 186/200, Batch 12/45, Loss: 0.2645174562931061\nEpoch 186/200, Batch 13/45, Loss: 0.5358597636222839\nEpoch 186/200, Batch 14/45, Loss: 0.435228168964386\nEpoch 186/200, Batch 15/45, Loss: 0.13696259260177612\nEpoch 186/200, Batch 16/45, Loss: 0.16336652636528015\nEpoch 186/200, Batch 17/45, Loss: 0.308724582195282\nEpoch 186/200, Batch 18/45, Loss: 0.17682071030139923\nEpoch 186/200, Batch 19/45, Loss: 0.3457178771495819\nEpoch 186/200, Batch 20/45, Loss: 0.3529965877532959\nEpoch 186/200, Batch 21/45, Loss: 0.379169762134552\nEpoch 186/200, Batch 22/45, Loss: 0.7657612562179565\nEpoch 186/200, Batch 23/45, Loss: 0.22709062695503235\nEpoch 186/200, Batch 24/45, Loss: 0.24401815235614777\nEpoch 186/200, Batch 25/45, Loss: 0.30264851450920105\nEpoch 186/200, Batch 26/45, Loss: 0.2808026075363159\nEpoch 186/200, Batch 27/45, Loss: 0.17216384410858154\nEpoch 186/200, Batch 28/45, Loss: 0.1830926090478897\nEpoch 186/200, Batch 29/45, Loss: 0.1776028275489807\nEpoch 186/200, Batch 30/45, Loss: 0.27406248450279236\nEpoch 186/200, Batch 31/45, Loss: 0.2730145752429962\nEpoch 186/200, Batch 32/45, Loss: 0.5206567645072937\nEpoch 186/200, Batch 33/45, Loss: 0.28305870294570923\nEpoch 186/200, Batch 34/45, Loss: 0.18694613873958588\nEpoch 186/200, Batch 35/45, Loss: 0.20650452375411987\nEpoch 186/200, Batch 36/45, Loss: 0.24896752834320068\nEpoch 186/200, Batch 37/45, Loss: 0.35548263788223267\nEpoch 186/200, Batch 38/45, Loss: 0.20739847421646118\nEpoch 186/200, Batch 39/45, Loss: 0.23109355568885803\nEpoch 186/200, Batch 40/45, Loss: 0.40591737627983093\nEpoch 186/200, Batch 41/45, Loss: 0.15798638761043549\nEpoch 186/200, Batch 42/45, Loss: 0.3074090778827667\nEpoch 186/200, Batch 43/45, Loss: 0.1499481499195099\nEpoch 186/200, Batch 44/45, Loss: 0.3506161570549011\nEpoch 186/200, Batch 45/45, Loss: 0.24206529557704926\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  183.08407306671143 Best Val MSE:  3.566082552075386\nEpoch:  187 , Time Elapsed:  29.73865945736567  mins\nEpoch 187/200, Batch 1/45, Loss: 0.13712018728256226\nEpoch 187/200, Batch 2/45, Loss: 0.34933680295944214\nEpoch 187/200, Batch 3/45, Loss: 0.28746846318244934\nEpoch 187/200, Batch 4/45, Loss: 0.2412564605474472\nEpoch 187/200, Batch 5/45, Loss: 0.12607616186141968\nEpoch 187/200, Batch 6/45, Loss: 0.2382592260837555\nEpoch 187/200, Batch 7/45, Loss: 0.1427530199289322\nEpoch 187/200, Batch 8/45, Loss: 0.41460803151130676\nEpoch 187/200, Batch 9/45, Loss: 0.2229515016078949\nEpoch 187/200, Batch 10/45, Loss: 0.14327013492584229\nEpoch 187/200, Batch 11/45, Loss: 0.37726128101348877\nEpoch 187/200, Batch 12/45, Loss: 0.09564773738384247\nEpoch 187/200, Batch 13/45, Loss: 0.1859847456216812\nEpoch 187/200, Batch 14/45, Loss: 0.1550646722316742\nEpoch 187/200, Batch 15/45, Loss: 0.22498275339603424\nEpoch 187/200, Batch 16/45, Loss: 0.23962731659412384\nEpoch 187/200, Batch 17/45, Loss: 0.3772372901439667\nEpoch 187/200, Batch 18/45, Loss: 0.3456684350967407\nEpoch 187/200, Batch 19/45, Loss: 0.24038217961788177\nEpoch 187/200, Batch 20/45, Loss: 0.35225173830986023\nEpoch 187/200, Batch 21/45, Loss: 0.22313445806503296\nEpoch 187/200, Batch 22/45, Loss: 0.2215910255908966\nEpoch 187/200, Batch 23/45, Loss: 0.08821654319763184\nEpoch 187/200, Batch 24/45, Loss: 0.23639510571956635\nEpoch 187/200, Batch 25/45, Loss: 0.3416084051132202\nEpoch 187/200, Batch 26/45, Loss: 0.42391106486320496\nEpoch 187/200, Batch 27/45, Loss: 0.37527745962142944\nEpoch 187/200, Batch 28/45, Loss: 0.6934719085693359\nEpoch 187/200, Batch 29/45, Loss: 0.09723881632089615\nEpoch 187/200, Batch 30/45, Loss: 0.2390926629304886\nEpoch 187/200, Batch 31/45, Loss: 0.1857890486717224\nEpoch 187/200, Batch 32/45, Loss: 0.3051576614379883\nEpoch 187/200, Batch 33/45, Loss: 0.22681717574596405\nEpoch 187/200, Batch 34/45, Loss: 0.2328871488571167\nEpoch 187/200, Batch 35/45, Loss: 0.3445177674293518\nEpoch 187/200, Batch 36/45, Loss: 0.2877630591392517\nEpoch 187/200, Batch 37/45, Loss: 0.2619338035583496\nEpoch 187/200, Batch 38/45, Loss: 0.22198230028152466\nEpoch 187/200, Batch 39/45, Loss: 0.24754375219345093\nEpoch 187/200, Batch 40/45, Loss: 0.548162043094635\nEpoch 187/200, Batch 41/45, Loss: 0.36705368757247925\nEpoch 187/200, Batch 42/45, Loss: 0.31629008054733276\nEpoch 187/200, Batch 43/45, Loss: 0.24096688628196716\nEpoch 187/200, Batch 44/45, Loss: 0.22163501381874084\nEpoch 187/200, Batch 45/45, Loss: 0.44040369987487793\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  59.54218626022339 Best Val MSE:  3.566082552075386\nEpoch:  188 , Time Elapsed:  29.899587655067442  mins\nEpoch 188/200, Batch 1/45, Loss: 0.28809434175491333\nEpoch 188/200, Batch 2/45, Loss: 0.2761836647987366\nEpoch 188/200, Batch 3/45, Loss: 0.26343703269958496\nEpoch 188/200, Batch 4/45, Loss: 0.1957632601261139\nEpoch 188/200, Batch 5/45, Loss: 0.4564277231693268\nEpoch 188/200, Batch 6/45, Loss: 0.16281002759933472\nEpoch 188/200, Batch 7/45, Loss: 0.27774620056152344\nEpoch 188/200, Batch 8/45, Loss: 0.41116803884506226\nEpoch 188/200, Batch 9/45, Loss: 0.3728437125682831\nEpoch 188/200, Batch 10/45, Loss: 0.2881140410900116\nEpoch 188/200, Batch 11/45, Loss: 0.21703121066093445\nEpoch 188/200, Batch 12/45, Loss: 0.19620104134082794\nEpoch 188/200, Batch 13/45, Loss: 0.2277030348777771\nEpoch 188/200, Batch 14/45, Loss: 0.23152001202106476\nEpoch 188/200, Batch 15/45, Loss: 0.11419718712568283\nEpoch 188/200, Batch 16/45, Loss: 0.5741885900497437\nEpoch 188/200, Batch 17/45, Loss: 0.4164726138114929\nEpoch 188/200, Batch 18/45, Loss: 0.19820158183574677\nEpoch 188/200, Batch 19/45, Loss: 0.19329990446567535\nEpoch 188/200, Batch 20/45, Loss: 0.25150230526924133\nEpoch 188/200, Batch 21/45, Loss: 0.20769482851028442\nEpoch 188/200, Batch 22/45, Loss: 0.20580032467842102\nEpoch 188/200, Batch 23/45, Loss: 0.19868817925453186\nEpoch 188/200, Batch 24/45, Loss: 0.31360217928886414\nEpoch 188/200, Batch 25/45, Loss: 0.3281872570514679\nEpoch 188/200, Batch 26/45, Loss: 0.1884079873561859\nEpoch 188/200, Batch 27/45, Loss: 0.18000397086143494\nEpoch 188/200, Batch 28/45, Loss: 0.2967800498008728\nEpoch 188/200, Batch 29/45, Loss: 0.34816908836364746\nEpoch 188/200, Batch 30/45, Loss: 0.19434994459152222\nEpoch 188/200, Batch 31/45, Loss: 0.2765513062477112\nEpoch 188/200, Batch 32/45, Loss: 0.41862648725509644\nEpoch 188/200, Batch 33/45, Loss: 0.24631771445274353\nEpoch 188/200, Batch 34/45, Loss: 0.22701610624790192\nEpoch 188/200, Batch 35/45, Loss: 0.2561769187450409\nEpoch 188/200, Batch 36/45, Loss: 0.34587863087654114\nEpoch 188/200, Batch 37/45, Loss: 0.21883824467658997\nEpoch 188/200, Batch 38/45, Loss: 0.19581767916679382\nEpoch 188/200, Batch 39/45, Loss: 0.2586125135421753\nEpoch 188/200, Batch 40/45, Loss: 0.32976973056793213\nEpoch 188/200, Batch 41/45, Loss: 0.16427098214626312\nEpoch 188/200, Batch 42/45, Loss: 0.1947135031223297\nEpoch 188/200, Batch 43/45, Loss: 0.2458367943763733\nEpoch 188/200, Batch 44/45, Loss: 0.43652111291885376\nEpoch 188/200, Batch 45/45, Loss: 0.3221975862979889\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  46.10199415683746 Best Val MSE:  3.566082552075386\nEpoch:  189 , Time Elapsed:  30.059796953201293  mins\nEpoch 189/200, Batch 1/45, Loss: 0.2451806664466858\nEpoch 189/200, Batch 2/45, Loss: 0.34609949588775635\nEpoch 189/200, Batch 3/45, Loss: 0.30880022048950195\nEpoch 189/200, Batch 4/45, Loss: 0.1344093382358551\nEpoch 189/200, Batch 5/45, Loss: 0.23561608791351318\nEpoch 189/200, Batch 6/45, Loss: 0.11738593876361847\nEpoch 189/200, Batch 7/45, Loss: 0.47480857372283936\nEpoch 189/200, Batch 8/45, Loss: 0.12720412015914917\nEpoch 189/200, Batch 9/45, Loss: 0.1229935884475708\nEpoch 189/200, Batch 10/45, Loss: 0.30012261867523193\nEpoch 189/200, Batch 11/45, Loss: 0.23598670959472656\nEpoch 189/200, Batch 12/45, Loss: 0.1852445751428604\nEpoch 189/200, Batch 13/45, Loss: 0.19804221391677856\nEpoch 189/200, Batch 14/45, Loss: 0.21501590311527252\nEpoch 189/200, Batch 15/45, Loss: 0.2806010842323303\nEpoch 189/200, Batch 16/45, Loss: 0.23665563762187958\nEpoch 189/200, Batch 17/45, Loss: 0.295416921377182\nEpoch 189/200, Batch 18/45, Loss: 0.20395269989967346\nEpoch 189/200, Batch 19/45, Loss: 0.2930378019809723\nEpoch 189/200, Batch 20/45, Loss: 0.17405490577220917\nEpoch 189/200, Batch 21/45, Loss: 0.25333958864212036\nEpoch 189/200, Batch 22/45, Loss: 1.3279982805252075\nEpoch 189/200, Batch 23/45, Loss: 0.17306609451770782\nEpoch 189/200, Batch 24/45, Loss: 0.46722447872161865\nEpoch 189/200, Batch 25/45, Loss: 0.5180283784866333\nEpoch 189/200, Batch 26/45, Loss: 0.5227121114730835\nEpoch 189/200, Batch 27/45, Loss: 0.35660892724990845\nEpoch 189/200, Batch 28/45, Loss: 0.18726523220539093\nEpoch 189/200, Batch 29/45, Loss: 0.47759556770324707\nEpoch 189/200, Batch 30/45, Loss: 0.7849393486976624\nEpoch 189/200, Batch 31/45, Loss: 0.22549384832382202\nEpoch 189/200, Batch 32/45, Loss: 0.27875787019729614\nEpoch 189/200, Batch 33/45, Loss: 0.24017468094825745\nEpoch 189/200, Batch 34/45, Loss: 0.2833603620529175\nEpoch 189/200, Batch 35/45, Loss: 0.36672917008399963\nEpoch 189/200, Batch 36/45, Loss: 0.24547576904296875\nEpoch 189/200, Batch 37/45, Loss: 0.5739232301712036\nEpoch 189/200, Batch 38/45, Loss: 0.33391380310058594\nEpoch 189/200, Batch 39/45, Loss: 0.16904979944229126\nEpoch 189/200, Batch 40/45, Loss: 0.2727755010128021\nEpoch 189/200, Batch 41/45, Loss: 0.20016896724700928\nEpoch 189/200, Batch 42/45, Loss: 0.40625888109207153\nEpoch 189/200, Batch 43/45, Loss: 0.361594557762146\nEpoch 189/200, Batch 44/45, Loss: 0.3451162576675415\nEpoch 189/200, Batch 45/45, Loss: 0.2814882695674896\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  61.14498329162598 Best Val MSE:  3.566082552075386\nEpoch:  190 , Time Elapsed:  30.22720251083374  mins\nEpoch 190/200, Batch 1/45, Loss: 0.1362871676683426\nEpoch 190/200, Batch 2/45, Loss: 0.5244762897491455\nEpoch 190/200, Batch 3/45, Loss: 0.261502742767334\nEpoch 190/200, Batch 4/45, Loss: 0.3098480701446533\nEpoch 190/200, Batch 5/45, Loss: 0.220768541097641\nEpoch 190/200, Batch 6/45, Loss: 0.22000938653945923\nEpoch 190/200, Batch 7/45, Loss: 0.38056129217147827\nEpoch 190/200, Batch 8/45, Loss: 0.9323921203613281\nEpoch 190/200, Batch 9/45, Loss: 0.2883897125720978\nEpoch 190/200, Batch 10/45, Loss: 0.2092605084180832\nEpoch 190/200, Batch 11/45, Loss: 0.26584330201148987\nEpoch 190/200, Batch 12/45, Loss: 0.3683004677295685\nEpoch 190/200, Batch 13/45, Loss: 0.20856785774230957\nEpoch 190/200, Batch 14/45, Loss: 0.35656091570854187\nEpoch 190/200, Batch 15/45, Loss: 0.4016207754611969\nEpoch 190/200, Batch 16/45, Loss: 0.12981687486171722\nEpoch 190/200, Batch 17/45, Loss: 0.38268500566482544\nEpoch 190/200, Batch 18/45, Loss: 0.1800708770751953\nEpoch 190/200, Batch 19/45, Loss: 0.40697550773620605\nEpoch 190/200, Batch 20/45, Loss: 0.24685178697109222\nEpoch 190/200, Batch 21/45, Loss: 0.26268574595451355\nEpoch 190/200, Batch 22/45, Loss: 0.2195618897676468\nEpoch 190/200, Batch 23/45, Loss: 0.28217434883117676\nEpoch 190/200, Batch 24/45, Loss: 0.3096029460430145\nEpoch 190/200, Batch 25/45, Loss: 0.240565225481987\nEpoch 190/200, Batch 26/45, Loss: 0.301827609539032\nEpoch 190/200, Batch 27/45, Loss: 0.23123204708099365\nEpoch 190/200, Batch 28/45, Loss: 0.25635796785354614\nEpoch 190/200, Batch 29/45, Loss: 0.17944082617759705\nEpoch 190/200, Batch 30/45, Loss: 0.1710490882396698\nEpoch 190/200, Batch 31/45, Loss: 0.28356513381004333\nEpoch 190/200, Batch 32/45, Loss: 0.40744906663894653\nEpoch 190/200, Batch 33/45, Loss: 0.600148618221283\nEpoch 190/200, Batch 34/45, Loss: 0.338380366563797\nEpoch 190/200, Batch 35/45, Loss: 0.2382795810699463\nEpoch 190/200, Batch 36/45, Loss: 0.37626051902770996\nEpoch 190/200, Batch 37/45, Loss: 0.6101319193840027\nEpoch 190/200, Batch 38/45, Loss: 0.2811413109302521\nEpoch 190/200, Batch 39/45, Loss: 0.25522586703300476\nEpoch 190/200, Batch 40/45, Loss: 0.3140619993209839\nEpoch 190/200, Batch 41/45, Loss: 0.19529137015342712\nEpoch 190/200, Batch 42/45, Loss: 0.24991492927074432\nEpoch 190/200, Batch 43/45, Loss: 0.14076898992061615\nEpoch 190/200, Batch 44/45, Loss: 0.20736385881900787\nEpoch 190/200, Batch 45/45, Loss: 0.3156929612159729\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  107.39019918441772 Best Val MSE:  3.566082552075386\nEpoch:  191 , Time Elapsed:  30.38857638835907  mins\nEpoch 191/200, Batch 1/45, Loss: 0.30544692277908325\nEpoch 191/200, Batch 2/45, Loss: 0.33528614044189453\nEpoch 191/200, Batch 3/45, Loss: 0.2079719603061676\nEpoch 191/200, Batch 4/45, Loss: 0.2728823125362396\nEpoch 191/200, Batch 5/45, Loss: 0.31223902106285095\nEpoch 191/200, Batch 6/45, Loss: 0.1253482699394226\nEpoch 191/200, Batch 7/45, Loss: 0.16782614588737488\nEpoch 191/200, Batch 8/45, Loss: 0.3074752688407898\nEpoch 191/200, Batch 9/45, Loss: 0.29673945903778076\nEpoch 191/200, Batch 10/45, Loss: 0.25029611587524414\nEpoch 191/200, Batch 11/45, Loss: 0.49501317739486694\nEpoch 191/200, Batch 12/45, Loss: 0.2579662501811981\nEpoch 191/200, Batch 13/45, Loss: 0.22793787717819214\nEpoch 191/200, Batch 14/45, Loss: 0.3189886212348938\nEpoch 191/200, Batch 15/45, Loss: 0.27109968662261963\nEpoch 191/200, Batch 16/45, Loss: 0.18804138898849487\nEpoch 191/200, Batch 17/45, Loss: 0.31586652994155884\nEpoch 191/200, Batch 18/45, Loss: 0.30763885378837585\nEpoch 191/200, Batch 19/45, Loss: 0.28590595722198486\nEpoch 191/200, Batch 20/45, Loss: 0.19440153241157532\nEpoch 191/200, Batch 21/45, Loss: 0.412522554397583\nEpoch 191/200, Batch 22/45, Loss: 0.15700411796569824\nEpoch 191/200, Batch 23/45, Loss: 0.5421395897865295\nEpoch 191/200, Batch 24/45, Loss: 0.22074121236801147\nEpoch 191/200, Batch 25/45, Loss: 0.1945721060037613\nEpoch 191/200, Batch 26/45, Loss: 0.20794236660003662\nEpoch 191/200, Batch 27/45, Loss: 0.27860090136528015\nEpoch 191/200, Batch 28/45, Loss: 0.24960342049598694\nEpoch 191/200, Batch 29/45, Loss: 0.1879323273897171\nEpoch 191/200, Batch 30/45, Loss: 0.55086749792099\nEpoch 191/200, Batch 31/45, Loss: 0.3119052052497864\nEpoch 191/200, Batch 32/45, Loss: 0.22043552994728088\nEpoch 191/200, Batch 33/45, Loss: 0.2192150205373764\nEpoch 191/200, Batch 34/45, Loss: 0.43343544006347656\nEpoch 191/200, Batch 35/45, Loss: 0.21502482891082764\nEpoch 191/200, Batch 36/45, Loss: 0.24787674844264984\nEpoch 191/200, Batch 37/45, Loss: 0.4050709903240204\nEpoch 191/200, Batch 38/45, Loss: 0.3344411849975586\nEpoch 191/200, Batch 39/45, Loss: 0.4937174916267395\nEpoch 191/200, Batch 40/45, Loss: 0.23466414213180542\nEpoch 191/200, Batch 41/45, Loss: 0.24684396386146545\nEpoch 191/200, Batch 42/45, Loss: 0.4082266688346863\nEpoch 191/200, Batch 43/45, Loss: 0.2763800024986267\nEpoch 191/200, Batch 44/45, Loss: 0.4780231714248657\nEpoch 191/200, Batch 45/45, Loss: 0.2647908329963684\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  1319.3438110351562 Best Val MSE:  3.566082552075386\nEpoch:  192 , Time Elapsed:  30.547852802276612  mins\nEpoch 192/200, Batch 1/45, Loss: 0.1167730912566185\nEpoch 192/200, Batch 2/45, Loss: 0.2473650723695755\nEpoch 192/200, Batch 3/45, Loss: 0.25164806842803955\nEpoch 192/200, Batch 4/45, Loss: 0.22288116812705994\nEpoch 192/200, Batch 5/45, Loss: 0.3450803756713867\nEpoch 192/200, Batch 6/45, Loss: 0.3322126865386963\nEpoch 192/200, Batch 7/45, Loss: 0.17328213155269623\nEpoch 192/200, Batch 8/45, Loss: 0.3207944333553314\nEpoch 192/200, Batch 9/45, Loss: 0.24804699420928955\nEpoch 192/200, Batch 10/45, Loss: 0.30892595648765564\nEpoch 192/200, Batch 11/45, Loss: 0.3059311509132385\nEpoch 192/200, Batch 12/45, Loss: 0.24644842743873596\nEpoch 192/200, Batch 13/45, Loss: 0.258992075920105\nEpoch 192/200, Batch 14/45, Loss: 0.24216565489768982\nEpoch 192/200, Batch 15/45, Loss: 0.17958685755729675\nEpoch 192/200, Batch 16/45, Loss: 0.23137149214744568\nEpoch 192/200, Batch 17/45, Loss: 0.22898319363594055\nEpoch 192/200, Batch 18/45, Loss: 0.26565641164779663\nEpoch 192/200, Batch 19/45, Loss: 0.33729198575019836\nEpoch 192/200, Batch 20/45, Loss: 0.1340831220149994\nEpoch 192/200, Batch 21/45, Loss: 0.25270652770996094\nEpoch 192/200, Batch 22/45, Loss: 0.2402210384607315\nEpoch 192/200, Batch 23/45, Loss: 0.2536320686340332\nEpoch 192/200, Batch 24/45, Loss: 0.19759804010391235\nEpoch 192/200, Batch 25/45, Loss: 0.30761539936065674\nEpoch 192/200, Batch 26/45, Loss: 0.4220723807811737\nEpoch 192/200, Batch 27/45, Loss: 0.1998502016067505\nEpoch 192/200, Batch 28/45, Loss: 0.16008450090885162\nEpoch 192/200, Batch 29/45, Loss: 0.28716790676116943\nEpoch 192/200, Batch 30/45, Loss: 0.2508217394351959\nEpoch 192/200, Batch 31/45, Loss: 0.12702196836471558\nEpoch 192/200, Batch 32/45, Loss: 0.19400395452976227\nEpoch 192/200, Batch 33/45, Loss: 0.26237809658050537\nEpoch 192/200, Batch 34/45, Loss: 0.26493215560913086\nEpoch 192/200, Batch 35/45, Loss: 0.17143124341964722\nEpoch 192/200, Batch 36/45, Loss: 0.2709043622016907\nEpoch 192/200, Batch 37/45, Loss: 0.16171419620513916\nEpoch 192/200, Batch 38/45, Loss: 0.2563023269176483\nEpoch 192/200, Batch 39/45, Loss: 0.19168336689472198\nEpoch 192/200, Batch 40/45, Loss: 0.528978705406189\nEpoch 192/200, Batch 41/45, Loss: 0.42545086145401\nEpoch 192/200, Batch 42/45, Loss: 0.23286014795303345\nEpoch 192/200, Batch 43/45, Loss: 0.28521716594696045\nEpoch 192/200, Batch 44/45, Loss: 0.27888357639312744\nEpoch 192/200, Batch 45/45, Loss: 0.2852669954299927\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  144.20645093917847 Best Val MSE:  3.566082552075386\nEpoch:  193 , Time Elapsed:  30.718158157666526  mins\nEpoch 193/200, Batch 1/45, Loss: 0.14531025290489197\nEpoch 193/200, Batch 2/45, Loss: 0.2000565379858017\nEpoch 193/200, Batch 3/45, Loss: 0.3348466455936432\nEpoch 193/200, Batch 4/45, Loss: 0.2358660250902176\nEpoch 193/200, Batch 5/45, Loss: 0.15792116522789001\nEpoch 193/200, Batch 6/45, Loss: 0.25233304500579834\nEpoch 193/200, Batch 7/45, Loss: 0.3422033190727234\nEpoch 193/200, Batch 8/45, Loss: 0.2119322121143341\nEpoch 193/200, Batch 9/45, Loss: 0.22643941640853882\nEpoch 193/200, Batch 10/45, Loss: 0.19559729099273682\nEpoch 193/200, Batch 11/45, Loss: 0.5855419635772705\nEpoch 193/200, Batch 12/45, Loss: 0.6515005826950073\nEpoch 193/200, Batch 13/45, Loss: 0.2626290023326874\nEpoch 193/200, Batch 14/45, Loss: 0.5256121158599854\nEpoch 193/200, Batch 15/45, Loss: 0.18507534265518188\nEpoch 193/200, Batch 16/45, Loss: 0.16286735236644745\nEpoch 193/200, Batch 17/45, Loss: 0.32564419507980347\nEpoch 193/200, Batch 18/45, Loss: 0.35958331823349\nEpoch 193/200, Batch 19/45, Loss: 0.2354186624288559\nEpoch 193/200, Batch 20/45, Loss: 0.2587246894836426\nEpoch 193/200, Batch 21/45, Loss: 0.49504464864730835\nEpoch 193/200, Batch 22/45, Loss: 0.08956480771303177\nEpoch 193/200, Batch 23/45, Loss: 0.3561108112335205\nEpoch 193/200, Batch 24/45, Loss: 0.30228960514068604\nEpoch 193/200, Batch 25/45, Loss: 0.15871499478816986\nEpoch 193/200, Batch 26/45, Loss: 0.22206860780715942\nEpoch 193/200, Batch 27/45, Loss: 0.37895476818084717\nEpoch 193/200, Batch 28/45, Loss: 0.4079012870788574\nEpoch 193/200, Batch 29/45, Loss: 0.2420952469110489\nEpoch 193/200, Batch 30/45, Loss: 0.336814820766449\nEpoch 193/200, Batch 31/45, Loss: 0.3092969059944153\nEpoch 193/200, Batch 32/45, Loss: 0.19381453096866608\nEpoch 193/200, Batch 33/45, Loss: 0.12323152273893356\nEpoch 193/200, Batch 34/45, Loss: 0.28325924277305603\nEpoch 193/200, Batch 35/45, Loss: 0.2582313120365143\nEpoch 193/200, Batch 36/45, Loss: 0.22300371527671814\nEpoch 193/200, Batch 37/45, Loss: 0.18997591733932495\nEpoch 193/200, Batch 38/45, Loss: 0.26316267251968384\nEpoch 193/200, Batch 39/45, Loss: 0.3597339391708374\nEpoch 193/200, Batch 40/45, Loss: 0.28538012504577637\nEpoch 193/200, Batch 41/45, Loss: 0.2725808322429657\nEpoch 193/200, Batch 42/45, Loss: 0.3137415051460266\nEpoch 193/200, Batch 43/45, Loss: 0.3198322355747223\nEpoch 193/200, Batch 44/45, Loss: 0.22677895426750183\nEpoch 193/200, Batch 45/45, Loss: 0.27215561270713806\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  128.18452072143555 Best Val MSE:  3.566082552075386\nEpoch:  194 , Time Elapsed:  30.88027543624242  mins\nEpoch 194/200, Batch 1/45, Loss: 0.1756461262702942\nEpoch 194/200, Batch 2/45, Loss: 0.11041103303432465\nEpoch 194/200, Batch 3/45, Loss: 0.3117041289806366\nEpoch 194/200, Batch 4/45, Loss: 0.22881701588630676\nEpoch 194/200, Batch 5/45, Loss: 0.22661319375038147\nEpoch 194/200, Batch 6/45, Loss: 0.20711754262447357\nEpoch 194/200, Batch 7/45, Loss: 0.24074383080005646\nEpoch 194/200, Batch 8/45, Loss: 0.1939724087715149\nEpoch 194/200, Batch 9/45, Loss: 0.24354633688926697\nEpoch 194/200, Batch 10/45, Loss: 0.2764570713043213\nEpoch 194/200, Batch 11/45, Loss: 0.19920717179775238\nEpoch 194/200, Batch 12/45, Loss: 0.23380644619464874\nEpoch 194/200, Batch 13/45, Loss: 0.6213588714599609\nEpoch 194/200, Batch 14/45, Loss: 0.18842610716819763\nEpoch 194/200, Batch 15/45, Loss: 0.1900937855243683\nEpoch 194/200, Batch 16/45, Loss: 0.30906593799591064\nEpoch 194/200, Batch 17/45, Loss: 0.15644778311252594\nEpoch 194/200, Batch 18/45, Loss: 0.11745361983776093\nEpoch 194/200, Batch 19/45, Loss: 0.3225520849227905\nEpoch 194/200, Batch 20/45, Loss: 0.15166613459587097\nEpoch 194/200, Batch 21/45, Loss: 0.21942222118377686\nEpoch 194/200, Batch 22/45, Loss: 0.3933113217353821\nEpoch 194/200, Batch 23/45, Loss: 0.24885085225105286\nEpoch 194/200, Batch 24/45, Loss: 0.2718285322189331\nEpoch 194/200, Batch 25/45, Loss: 0.22452367842197418\nEpoch 194/200, Batch 26/45, Loss: 0.1740114688873291\nEpoch 194/200, Batch 27/45, Loss: 0.2949422001838684\nEpoch 194/200, Batch 28/45, Loss: 0.1367567777633667\nEpoch 194/200, Batch 29/45, Loss: 0.1975160390138626\nEpoch 194/200, Batch 30/45, Loss: 0.3793470561504364\nEpoch 194/200, Batch 31/45, Loss: 0.25862911343574524\nEpoch 194/200, Batch 32/45, Loss: 0.25579410791397095\nEpoch 194/200, Batch 33/45, Loss: 0.3820415735244751\nEpoch 194/200, Batch 34/45, Loss: 0.23491261899471283\nEpoch 194/200, Batch 35/45, Loss: 0.2262873649597168\nEpoch 194/200, Batch 36/45, Loss: 0.2007608264684677\nEpoch 194/200, Batch 37/45, Loss: 0.30660954117774963\nEpoch 194/200, Batch 38/45, Loss: 0.3460361063480377\nEpoch 194/200, Batch 39/45, Loss: 0.29367268085479736\nEpoch 194/200, Batch 40/45, Loss: 0.31513214111328125\nEpoch 194/200, Batch 41/45, Loss: 0.2948451638221741\nEpoch 194/200, Batch 42/45, Loss: 0.3412044644355774\nEpoch 194/200, Batch 43/45, Loss: 0.34791100025177\nEpoch 194/200, Batch 44/45, Loss: 0.25909560918807983\nEpoch 194/200, Batch 45/45, Loss: 0.5344064235687256\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  159.5511770248413 Best Val MSE:  3.566082552075386\nEpoch:  195 , Time Elapsed:  31.043182436625163  mins\nEpoch 195/200, Batch 1/45, Loss: 0.30611753463745117\nEpoch 195/200, Batch 2/45, Loss: 0.219499409198761\nEpoch 195/200, Batch 3/45, Loss: 0.4956643283367157\nEpoch 195/200, Batch 4/45, Loss: 0.18918025493621826\nEpoch 195/200, Batch 5/45, Loss: 0.2059866189956665\nEpoch 195/200, Batch 6/45, Loss: 0.2872619032859802\nEpoch 195/200, Batch 7/45, Loss: 0.24327875673770905\nEpoch 195/200, Batch 8/45, Loss: 0.2471628487110138\nEpoch 195/200, Batch 9/45, Loss: 0.33773353695869446\nEpoch 195/200, Batch 10/45, Loss: 0.24286746978759766\nEpoch 195/200, Batch 11/45, Loss: 0.21529534459114075\nEpoch 195/200, Batch 12/45, Loss: 0.5683788061141968\nEpoch 195/200, Batch 13/45, Loss: 0.16614729166030884\nEpoch 195/200, Batch 14/45, Loss: 0.3304484486579895\nEpoch 195/200, Batch 15/45, Loss: 0.2537195086479187\nEpoch 195/200, Batch 16/45, Loss: 0.13639242947101593\nEpoch 195/200, Batch 17/45, Loss: 0.20632097125053406\nEpoch 195/200, Batch 18/45, Loss: 0.23478209972381592\nEpoch 195/200, Batch 19/45, Loss: 0.5278624296188354\nEpoch 195/200, Batch 20/45, Loss: 0.30129745602607727\nEpoch 195/200, Batch 21/45, Loss: 0.20703336596488953\nEpoch 195/200, Batch 22/45, Loss: 0.2229369878768921\nEpoch 195/200, Batch 23/45, Loss: 0.2104983627796173\nEpoch 195/200, Batch 24/45, Loss: 0.28298062086105347\nEpoch 195/200, Batch 25/45, Loss: 0.23743024468421936\nEpoch 195/200, Batch 26/45, Loss: 0.25420981645584106\nEpoch 195/200, Batch 27/45, Loss: 0.32659590244293213\nEpoch 195/200, Batch 28/45, Loss: 0.23102667927742004\nEpoch 195/200, Batch 29/45, Loss: 0.22973448038101196\nEpoch 195/200, Batch 30/45, Loss: 0.26126712560653687\nEpoch 195/200, Batch 31/45, Loss: 0.22226619720458984\nEpoch 195/200, Batch 32/45, Loss: 0.19078832864761353\nEpoch 195/200, Batch 33/45, Loss: 0.1743507981300354\nEpoch 195/200, Batch 34/45, Loss: 0.2643049955368042\nEpoch 195/200, Batch 35/45, Loss: 0.20451252162456512\nEpoch 195/200, Batch 36/45, Loss: 0.26207083463668823\nEpoch 195/200, Batch 37/45, Loss: 0.3575788736343384\nEpoch 195/200, Batch 38/45, Loss: 0.2562040388584137\nEpoch 195/200, Batch 39/45, Loss: 0.13182136416435242\nEpoch 195/200, Batch 40/45, Loss: 0.2796441614627838\nEpoch 195/200, Batch 41/45, Loss: 0.35000109672546387\nEpoch 195/200, Batch 42/45, Loss: 0.3212411403656006\nEpoch 195/200, Batch 43/45, Loss: 0.1601344794034958\nEpoch 195/200, Batch 44/45, Loss: 0.23259001970291138\nEpoch 195/200, Batch 45/45, Loss: 0.2895241379737854\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  208.13667392730713 Best Val MSE:  3.566082552075386\nEpoch:  196 , Time Elapsed:  31.211649294694265  mins\nEpoch 196/200, Batch 1/45, Loss: 0.14783558249473572\nEpoch 196/200, Batch 2/45, Loss: 0.33630695939064026\nEpoch 196/200, Batch 3/45, Loss: 0.19585032761096954\nEpoch 196/200, Batch 4/45, Loss: 0.24958458542823792\nEpoch 196/200, Batch 5/45, Loss: 0.25850093364715576\nEpoch 196/200, Batch 6/45, Loss: 0.29190900921821594\nEpoch 196/200, Batch 7/45, Loss: 0.20409515500068665\nEpoch 196/200, Batch 8/45, Loss: 0.2606329619884491\nEpoch 196/200, Batch 9/45, Loss: 0.2686607539653778\nEpoch 196/200, Batch 10/45, Loss: 0.458185613155365\nEpoch 196/200, Batch 11/45, Loss: 0.21321609616279602\nEpoch 196/200, Batch 12/45, Loss: 0.29715609550476074\nEpoch 196/200, Batch 13/45, Loss: 0.22472722828388214\nEpoch 196/200, Batch 14/45, Loss: 0.5887393951416016\nEpoch 196/200, Batch 15/45, Loss: 0.24551156163215637\nEpoch 196/200, Batch 16/45, Loss: 0.31935223937034607\nEpoch 196/200, Batch 17/45, Loss: 0.30214619636535645\nEpoch 196/200, Batch 18/45, Loss: 0.28336644172668457\nEpoch 196/200, Batch 19/45, Loss: 0.17224514484405518\nEpoch 196/200, Batch 20/45, Loss: 0.24485862255096436\nEpoch 196/200, Batch 21/45, Loss: 0.3268479108810425\nEpoch 196/200, Batch 22/45, Loss: 0.2557058334350586\nEpoch 196/200, Batch 23/45, Loss: 0.16229015588760376\nEpoch 196/200, Batch 24/45, Loss: 0.21908894181251526\nEpoch 196/200, Batch 25/45, Loss: 0.3405478596687317\nEpoch 196/200, Batch 26/45, Loss: 0.28330928087234497\nEpoch 196/200, Batch 27/45, Loss: 0.3266097605228424\nEpoch 196/200, Batch 28/45, Loss: 0.2527254521846771\nEpoch 196/200, Batch 29/45, Loss: 0.4001702666282654\nEpoch 196/200, Batch 30/45, Loss: 0.3325059413909912\nEpoch 196/200, Batch 31/45, Loss: 0.5872867107391357\nEpoch 196/200, Batch 32/45, Loss: 0.3161083161830902\nEpoch 196/200, Batch 33/45, Loss: 0.29555565118789673\nEpoch 196/200, Batch 34/45, Loss: 0.5447450876235962\nEpoch 196/200, Batch 35/45, Loss: 0.36877715587615967\nEpoch 196/200, Batch 36/45, Loss: 0.20388203859329224\nEpoch 196/200, Batch 37/45, Loss: 0.2079697549343109\nEpoch 196/200, Batch 38/45, Loss: 0.2458164095878601\nEpoch 196/200, Batch 39/45, Loss: 0.2947380244731903\nEpoch 196/200, Batch 40/45, Loss: 0.20724700391292572\nEpoch 196/200, Batch 41/45, Loss: 0.28138455748558044\nEpoch 196/200, Batch 42/45, Loss: 0.486800879240036\nEpoch 196/200, Batch 43/45, Loss: 0.2338995635509491\nEpoch 196/200, Batch 44/45, Loss: 0.2561754584312439\nEpoch 196/200, Batch 45/45, Loss: 0.21341775357723236\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  1611.7862281799316 Best Val MSE:  3.566082552075386\nEpoch:  197 , Time Elapsed:  31.37344328959783  mins\nEpoch 197/200, Batch 1/45, Loss: 0.14241096377372742\nEpoch 197/200, Batch 2/45, Loss: 0.15596097707748413\nEpoch 197/200, Batch 3/45, Loss: 0.26324522495269775\nEpoch 197/200, Batch 4/45, Loss: 0.27022483944892883\nEpoch 197/200, Batch 5/45, Loss: 0.3761157989501953\nEpoch 197/200, Batch 6/45, Loss: 0.16025270521640778\nEpoch 197/200, Batch 7/45, Loss: 0.11752684414386749\nEpoch 197/200, Batch 8/45, Loss: 0.31574365496635437\nEpoch 197/200, Batch 9/45, Loss: 0.23493774235248566\nEpoch 197/200, Batch 10/45, Loss: 0.18212586641311646\nEpoch 197/200, Batch 11/45, Loss: 0.4047914743423462\nEpoch 197/200, Batch 12/45, Loss: 0.14371970295906067\nEpoch 197/200, Batch 13/45, Loss: 0.23342300951480865\nEpoch 197/200, Batch 14/45, Loss: 0.142540842294693\nEpoch 197/200, Batch 15/45, Loss: 0.312328040599823\nEpoch 197/200, Batch 16/45, Loss: 0.2509707808494568\nEpoch 197/200, Batch 17/45, Loss: 0.3531866669654846\nEpoch 197/200, Batch 18/45, Loss: 0.30289486050605774\nEpoch 197/200, Batch 19/45, Loss: 0.26972490549087524\nEpoch 197/200, Batch 20/45, Loss: 0.21078835427761078\nEpoch 197/200, Batch 21/45, Loss: 0.4434658885002136\nEpoch 197/200, Batch 22/45, Loss: 0.24688756465911865\nEpoch 197/200, Batch 23/45, Loss: 0.1748463660478592\nEpoch 197/200, Batch 24/45, Loss: 0.2691437602043152\nEpoch 197/200, Batch 25/45, Loss: 0.23349694907665253\nEpoch 197/200, Batch 26/45, Loss: 0.23291058838367462\nEpoch 197/200, Batch 27/45, Loss: 0.13026687502861023\nEpoch 197/200, Batch 28/45, Loss: 0.2057483047246933\nEpoch 197/200, Batch 29/45, Loss: 0.20320814847946167\nEpoch 197/200, Batch 30/45, Loss: 0.3265700340270996\nEpoch 197/200, Batch 31/45, Loss: 0.3190571069717407\nEpoch 197/200, Batch 32/45, Loss: 0.35719507932662964\nEpoch 197/200, Batch 33/45, Loss: 0.3859062194824219\nEpoch 197/200, Batch 34/45, Loss: 0.8429752588272095\nEpoch 197/200, Batch 35/45, Loss: 0.16254077851772308\nEpoch 197/200, Batch 36/45, Loss: 0.287803590297699\nEpoch 197/200, Batch 37/45, Loss: 0.14882522821426392\nEpoch 197/200, Batch 38/45, Loss: 0.3226448893547058\nEpoch 197/200, Batch 39/45, Loss: 0.2359490841627121\nEpoch 197/200, Batch 40/45, Loss: 0.3379196226596832\nEpoch 197/200, Batch 41/45, Loss: 0.3084314465522766\nEpoch 197/200, Batch 42/45, Loss: 0.22456368803977966\nEpoch 197/200, Batch 43/45, Loss: 0.20149502158164978\nEpoch 197/200, Batch 44/45, Loss: 0.2518972158432007\nEpoch 197/200, Batch 45/45, Loss: 0.2469719499349594\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  3.7971853613853455 Best Val MSE:  3.566082552075386\nEpoch:  198 , Time Elapsed:  31.533412480354308  mins\nEpoch 198/200, Batch 1/45, Loss: 0.3122360110282898\nEpoch 198/200, Batch 2/45, Loss: 0.2627304196357727\nEpoch 198/200, Batch 3/45, Loss: 0.2862384021282196\nEpoch 198/200, Batch 4/45, Loss: 0.29443487524986267\nEpoch 198/200, Batch 5/45, Loss: 0.2828563451766968\nEpoch 198/200, Batch 6/45, Loss: 0.21821409463882446\nEpoch 198/200, Batch 7/45, Loss: 0.26429203152656555\nEpoch 198/200, Batch 8/45, Loss: 0.34321752190589905\nEpoch 198/200, Batch 9/45, Loss: 0.6492111086845398\nEpoch 198/200, Batch 10/45, Loss: 0.3025394082069397\nEpoch 198/200, Batch 11/45, Loss: 0.3099219799041748\nEpoch 198/200, Batch 12/45, Loss: 0.30989542603492737\nEpoch 198/200, Batch 13/45, Loss: 0.1887647956609726\nEpoch 198/200, Batch 14/45, Loss: 0.3557819724082947\nEpoch 198/200, Batch 15/45, Loss: 0.3968076705932617\nEpoch 198/200, Batch 16/45, Loss: 0.22104886174201965\nEpoch 198/200, Batch 17/45, Loss: 0.17114344239234924\nEpoch 198/200, Batch 18/45, Loss: 0.13419921696186066\nEpoch 198/200, Batch 19/45, Loss: 0.7183722853660583\nEpoch 198/200, Batch 20/45, Loss: 0.1304120123386383\nEpoch 198/200, Batch 21/45, Loss: 0.2687441408634186\nEpoch 198/200, Batch 22/45, Loss: 0.21876366436481476\nEpoch 198/200, Batch 23/45, Loss: 0.1708756983280182\nEpoch 198/200, Batch 24/45, Loss: 0.11043569445610046\nEpoch 198/200, Batch 25/45, Loss: 0.19700653851032257\nEpoch 198/200, Batch 26/45, Loss: 0.3528638482093811\nEpoch 198/200, Batch 27/45, Loss: 0.38362953066825867\nEpoch 198/200, Batch 28/45, Loss: 0.22679409384727478\nEpoch 198/200, Batch 29/45, Loss: 0.14649005234241486\nEpoch 198/200, Batch 30/45, Loss: 0.20537954568862915\nEpoch 198/200, Batch 31/45, Loss: 0.2728310525417328\nEpoch 198/200, Batch 32/45, Loss: 0.23403418064117432\nEpoch 198/200, Batch 33/45, Loss: 0.13933265209197998\nEpoch 198/200, Batch 34/45, Loss: 0.20299020409584045\nEpoch 198/200, Batch 35/45, Loss: 0.16068828105926514\nEpoch 198/200, Batch 36/45, Loss: 0.36482083797454834\nEpoch 198/200, Batch 37/45, Loss: 0.4296436905860901\nEpoch 198/200, Batch 38/45, Loss: 0.07496073842048645\nEpoch 198/200, Batch 39/45, Loss: 0.3470131754875183\nEpoch 198/200, Batch 40/45, Loss: 0.1780160367488861\nEpoch 198/200, Batch 41/45, Loss: 0.10851734131574631\nEpoch 198/200, Batch 42/45, Loss: 0.25272607803344727\nEpoch 198/200, Batch 43/45, Loss: 0.17797613143920898\nEpoch 198/200, Batch 44/45, Loss: 0.16570258140563965\nEpoch 198/200, Batch 45/45, Loss: 0.15663808584213257\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  114.49774026870728 Best Val MSE:  3.566082552075386\nEpoch:  199 , Time Elapsed:  31.696939142545066  mins\nEpoch 199/200, Batch 1/45, Loss: 0.14475132524967194\nEpoch 199/200, Batch 2/45, Loss: 0.27031803131103516\nEpoch 199/200, Batch 3/45, Loss: 0.25060969591140747\nEpoch 199/200, Batch 4/45, Loss: 0.23068594932556152\nEpoch 199/200, Batch 5/45, Loss: 0.2284722924232483\nEpoch 199/200, Batch 6/45, Loss: 0.20984973013401031\nEpoch 199/200, Batch 7/45, Loss: 0.21476389467716217\nEpoch 199/200, Batch 8/45, Loss: 0.22821855545043945\nEpoch 199/200, Batch 9/45, Loss: 0.21192345023155212\nEpoch 199/200, Batch 10/45, Loss: 0.48961684107780457\nEpoch 199/200, Batch 11/45, Loss: 0.2878856956958771\nEpoch 199/200, Batch 12/45, Loss: 0.26699134707450867\nEpoch 199/200, Batch 13/45, Loss: 0.2562709450721741\nEpoch 199/200, Batch 14/45, Loss: 0.22901774942874908\nEpoch 199/200, Batch 15/45, Loss: 0.18183544278144836\nEpoch 199/200, Batch 16/45, Loss: 0.17590640485286713\nEpoch 199/200, Batch 17/45, Loss: 0.21002593636512756\nEpoch 199/200, Batch 18/45, Loss: 0.16729740798473358\nEpoch 199/200, Batch 19/45, Loss: 0.19618001580238342\nEpoch 199/200, Batch 20/45, Loss: 0.3468601107597351\nEpoch 199/200, Batch 21/45, Loss: 0.276642382144928\nEpoch 199/200, Batch 22/45, Loss: 0.23494330048561096\nEpoch 199/200, Batch 23/45, Loss: 0.15073074400424957\nEpoch 199/200, Batch 24/45, Loss: 0.2819308638572693\nEpoch 199/200, Batch 25/45, Loss: 0.3885268270969391\nEpoch 199/200, Batch 26/45, Loss: 0.2078256607055664\nEpoch 199/200, Batch 27/45, Loss: 0.17065861821174622\nEpoch 199/200, Batch 28/45, Loss: 0.15453405678272247\nEpoch 199/200, Batch 29/45, Loss: 0.34317469596862793\nEpoch 199/200, Batch 30/45, Loss: 0.20874664187431335\nEpoch 199/200, Batch 31/45, Loss: 0.30777764320373535\nEpoch 199/200, Batch 32/45, Loss: 0.3345298171043396\nEpoch 199/200, Batch 33/45, Loss: 0.15218347311019897\nEpoch 199/200, Batch 34/45, Loss: 0.24930520355701447\nEpoch 199/200, Batch 35/45, Loss: 0.26665574312210083\nEpoch 199/200, Batch 36/45, Loss: 0.18370339274406433\nEpoch 199/200, Batch 37/45, Loss: 0.2689560353755951\nEpoch 199/200, Batch 38/45, Loss: 0.19970092177391052\nEpoch 199/200, Batch 39/45, Loss: 0.19750066101551056\nEpoch 199/200, Batch 40/45, Loss: 0.6739451289176941\nEpoch 199/200, Batch 41/45, Loss: 0.20953938364982605\nEpoch 199/200, Batch 42/45, Loss: 0.234804168343544\nEpoch 199/200, Batch 43/45, Loss: 0.33873945474624634\nEpoch 199/200, Batch 44/45, Loss: 0.43665385246276855\nEpoch 199/200, Batch 45/45, Loss: 0.15811116993427277\nValidating and Checkpointing!\nModel is not good (might be overfitting)! Current val MSE:  37.74837899208069 Best Val MSE:  3.566082552075386\nEpoch:  200 , Time Elapsed:  31.860852058728536  mins\nEpoch 200/200, Batch 1/45, Loss: 0.10002728551626205\nEpoch 200/200, Batch 2/45, Loss: 0.16514691710472107\nEpoch 200/200, Batch 3/45, Loss: 0.19809630513191223\nEpoch 200/200, Batch 4/45, Loss: 0.2250114381313324\nEpoch 200/200, Batch 5/45, Loss: 0.2869139313697815\nEpoch 200/200, Batch 6/45, Loss: 0.15526598691940308\nEpoch 200/200, Batch 7/45, Loss: 0.3286552429199219\nEpoch 200/200, Batch 8/45, Loss: 0.19967101514339447\nEpoch 200/200, Batch 9/45, Loss: 0.16241204738616943\nEpoch 200/200, Batch 10/45, Loss: 0.1777530312538147\nEpoch 200/200, Batch 11/45, Loss: 0.19994698464870453\nEpoch 200/200, Batch 12/45, Loss: 0.21260100603103638\nEpoch 200/200, Batch 13/45, Loss: 0.21911273896694183\nEpoch 200/200, Batch 14/45, Loss: 0.21626940369606018\nEpoch 200/200, Batch 15/45, Loss: 0.24783480167388916\nEpoch 200/200, Batch 16/45, Loss: 0.33338046073913574\nEpoch 200/200, Batch 17/45, Loss: 0.18978911638259888\nEpoch 200/200, Batch 18/45, Loss: 0.17935626208782196\nEpoch 200/200, Batch 19/45, Loss: 0.17539963126182556\nEpoch 200/200, Batch 20/45, Loss: 0.23168538510799408\nEpoch 200/200, Batch 21/45, Loss: 0.3322902023792267\nEpoch 200/200, Batch 22/45, Loss: 0.2033509612083435\nEpoch 200/200, Batch 23/45, Loss: 0.1598692387342453\nEpoch 200/200, Batch 24/45, Loss: 0.15920564532279968\nEpoch 200/200, Batch 25/45, Loss: 0.262813925743103\nEpoch 200/200, Batch 26/45, Loss: 0.13730362057685852\nEpoch 200/200, Batch 27/45, Loss: 0.3267183005809784\nEpoch 200/200, Batch 28/45, Loss: 0.24483835697174072\nEpoch 200/200, Batch 29/45, Loss: 0.33970361948013306\nEpoch 200/200, Batch 30/45, Loss: 0.31636327505111694\nEpoch 200/200, Batch 31/45, Loss: 0.296169638633728\nEpoch 200/200, Batch 32/45, Loss: 0.2336941659450531\nEpoch 200/200, Batch 33/45, Loss: 0.397702157497406\nEpoch 200/200, Batch 34/45, Loss: 0.1749199628829956\nEpoch 200/200, Batch 35/45, Loss: 0.2250189334154129\nEpoch 200/200, Batch 36/45, Loss: 0.2308731973171234\nEpoch 200/200, Batch 37/45, Loss: 0.21160903573036194\nEpoch 200/200, Batch 38/45, Loss: 0.26930174231529236\nEpoch 200/200, Batch 39/45, Loss: 0.2565632164478302\nEpoch 200/200, Batch 40/45, Loss: 0.2982858419418335\nEpoch 200/200, Batch 41/45, Loss: 0.25723445415496826\nEpoch 200/200, Batch 42/45, Loss: 0.20811820030212402\nEpoch 200/200, Batch 43/45, Loss: 0.32018929719924927\nEpoch 200/200, Batch 44/45, Loss: 0.17161962389945984\nEpoch 200/200, Batch 45/45, Loss: 0.16598890721797943\nValidating and Checkpointing!\nBest model Saved! Val MSE:  3.361969009041786\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Evaluation","metadata":{}},{"cell_type":"markdown","source":"Define the test dataset","metadata":{}},{"cell_type":"code","source":"# Instantiate the PyTorch datalaoder the autonomous greenhouse dataset.\ntestset = GreenhouseDataset(rgb_dir = RGB_Data_Dir,\n                            d_dir = Depth_Data_Dir,\n                            jsonfile_dir = JSON_Files_Dir,\n                            rgb_transforms = get_transforms(train=False, means=dataset.means[:3], stds=dataset.stds[:3]),\n                            d_transforms = get_transforms(train=False, means=dataset.means[3:], stds=dataset.stds[3:]))\n\n# Grab last 50 images as test dataset\ntestset.df = testset.df[-50:]\n\n# Get testset_size\ntestset_size = testset.df.shape[0]\n\n# Create test dataloader\ntest_loader = torch.utils.data.DataLoader(testset,\n                                          batch_size = 50,\n                                          num_workers = 0,\n                                          shuffle = False)","metadata":{"id":"LNiS2urzSs2j","execution":{"iopub.status.busy":"2023-12-18T08:50:14.905955Z","iopub.execute_input":"2023-12-18T08:50:14.906787Z","iopub.status.idle":"2023-12-18T08:50:14.923799Z","shell.execute_reply.started":"2023-12-18T08:50:14.906746Z","shell.execute_reply":"2023-12-18T08:50:14.923058Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"Define loss functions for model evaluation","metadata":{}},{"cell_type":"code","source":"cri = NMSELoss()\nmse = nn.MSELoss()","metadata":{"execution":{"iopub.status.busy":"2023-12-18T08:50:18.493320Z","iopub.execute_input":"2023-12-18T08:50:18.494274Z","iopub.status.idle":"2023-12-18T08:50:18.498774Z","shell.execute_reply.started":"2023-12-18T08:50:18.494233Z","shell.execute_reply":"2023-12-18T08:50:18.497796Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"Run the evaluation Loop","metadata":{}},{"cell_type":"code","source":"# Evaluation loop\ndevice=torch.device('cuda')\n\nwith torch.no_grad():\n\n\n    device=torch.device('cuda')\n    model = FirstStageModel()\n    model.to(device)\n    model.load_state_dict(torch.load(sav_dir + 'bestmodel.pth'))\n    model.eval()\n    \n    ap=torch.zeros((0,4))\n    at=torch.zeros((0,4))\n\n    for rgb, depth, targets in test_loader:\n        rgb = rgb.to(device)\n        depth = depth.to(device)\n        targets = targets.to(device)\n        targets = targets[:, :4]\n        pred1, pred2 = model(rgb, depth)\n        pred = torch.cat([pred1[:, :2], pred2], dim=1)\n        pred = torch.cat([pred, pred1[:, 2:]], dim=1) # fresh weight, dry weight, height, diameter\n        # mse_loss=mse(preds, targets)\n        # nmse=criterion(preds, targets)\n        # nmse, pred=cri(preds, targets)\n        ap=torch.cat((ap, pred.detach().cpu()), 0)\n        at=torch.cat((at, targets.detach().cpu()), 0)\n\n\n    print('FW MSE: ', str(mse(ap[:,0],at[:,0]).tolist()))\n    print('DW MSE: ', str(mse(ap[:,1],at[:,1]).tolist()))\n    print('H MSE: ', str(mse(ap[:,2],at[:,2]).tolist()))\n    print('D MSE: ', str(mse(ap[:,3],at[:,3]).tolist()))\n#     print('LA MSE: ', str(mse(ap[:,4],at[:,4]).tolist()))\n    print('Overall NMSE: ', str(cri(ap,at).tolist()))\n","metadata":{"execution":{"iopub.status.busy":"2023-12-18T08:55:19.566507Z","iopub.execute_input":"2023-12-18T08:55:19.566878Z","iopub.status.idle":"2023-12-18T08:55:22.787607Z","shell.execute_reply.started":"2023-12-18T08:55:19.566839Z","shell.execute_reply":"2023-12-18T08:55:22.786744Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"FW MSE:  1406.3551025390625\nDW MSE:  2.855520248413086\nH MSE:  30.06348419189453\nD MSE:  13.6848783493042\nOverall NMSE:  0.2702009975910187\n","output_type":"stream"}]}]}